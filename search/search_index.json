{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"**A pipeline framework for python** [ ] 1 [ ] 13 [ ] 13 [ ] 23 Documentation | ChangeLog | Examples | API Why pipen? \u00b6 pipen is designed for data scientists, bioinformaticians, and researchers who need to create reproducible, scalable computational pipelines without the complexity of traditional workflow systems. Target Audience \u00b6 Data Scientists : Process large datasets with automatic parallelization and caching Bioinformaticians : Build reproducible analysis pipelines for genomics data Researchers : Create transparent, reproducible workflows for computational research DevOps Engineers : Orchestrate batch jobs across different schedulers (SLURM, SGE, Google Cloud) Key Benefits \u00b6 1. Zero Configuration - Get started immediately with sensible defaults - Configure only what you need, when you need it - Profile-based configuration for different environments 2. Reproducibility Built-In - Automatic job caching based on input/output signatures - Full audit trail of pipeline runs and parameters - Dependency tracking ensures processes run in correct order 3. Flexible Scheduling - Run locally for development - Scale to HPC clusters (SLURM, SGE) - Deploy to cloud (Google Cloud Batch, SSH) - Run in containers for reproducibility 4. Developer-Friendly - Define pipelines as Python classes - Use familiar Python syntax and tools - Extensible plugin system for custom functionality - Rich, informative logging and progress tracking 5. Data Flow Management - Automatic data passing between pipeline stages - Support for files, directories, and in-memory data - Built-in operations for transforming and aggregating data Comparison with Alternatives \u00b6 Feature pipen Snakemake Nextflow Airflow Target Audience Data Scientists, Bioinformaticians, Researchers, DevOps Bioinformaticians Bioinformaticians Data Engineers Learning Curve Low Medium High High Python Integration Native Limited Limited Native Scheduler Support 6+ (Local, SGE, SLURM, SSH, Container, Gbatch) Limited Limited Plugin-based Caching Built-in, automatic Manual Manual Plugin-based Cloud Native Support Yes (Google Cloud Batch) Partial Yes Yes Interactive Debugging Yes Limited No No Easy to Use Define pipelines as Python classes, familiar syntax Workflow DSL, separate config files DAG definition in Python, complex UI Zero Configuration Sensible defaults, configure only what needed Many configuration options Heavy configuration required Complex setup Nice Logging Rich, informative, color-coded, progress bars Text-based Text-based Basic logging Highly Extensible Simple plugin system, hook-based Custom rules/scripts Custom operators Custom operators/providers Data Flow Management Built-in channel operations (expand_dir, collapse_files) Manual handling Channel system XCom system Reproducibility Built-in caching, full audit trail Manual Versioned containers DAG versioning Flexible Scheduling Switch schedulers without code changes Config-based Config-based Config-based Installation \u00b6 pip install -U pipen Quickstart \u00b6 example.py from pipen import Proc , Pipen , run class P1 ( Proc ): \"\"\"Sort input file\"\"\" input = \"infile\" input_data = [ \"/tmp/data.txt\" ] output = \"outfile:file:intermediate.txt\" script = \"cat {{in.infile}} | sort > {{out.outfile}}\" class P2 ( Proc ): \"\"\"Paste line number\"\"\" requires = P1 input = \"infile:file\" output = \"outfile:file:result.txt\" script = \"paste <(seq 1 3) {{in.infile}} > {{out.outfile}}\" # class MyPipeline(Pipen): # starts = P1 if __name__ == \"__main__\" : # MyPipeline().run() run ( \"MyPipeline\" , starts = P1 ) > echo -e \"3\\n2\\n1\" > /tmp/data.txt > python example.py 04-17 16:19:35 I core _____________________________________ __ 04-17 16:19:35 I core ___ __ \\___ _/__ __ \\__ ____/__ | / / 04-17 16:19:35 I core __ /_/ /__ / __ /_/ /_ __/ __ |/ / 04-17 16:19:35 I core _ ____/__/ / _ ____/_ /___ _ /| / 04-17 16:19:35 I core /_/ /___/ /_/ /_____/ /_/ |_/ 04-17 16:19:35 I core 04-17 16:19:35 I core version: 1.1.8 04-17 16:19:35 I core 04-17 16:19:35 I core \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 MYPIPELINE \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 04-17 16:19:35 I core \u2551 My pipeline \u2551 04-17 16:19:35 I core \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d 04-17 16:19:35 I core plugins : verbose v1.1.1 04-17 16:19:35 I core # procs : 2 04-17 16:19:35 I core profile : default 04-17 16:19:35 I core outdir : /path/to/cwd/MyPipeline-output 04-17 16:19:35 I core cache : True 04-17 16:19:35 I core dirsig : 1 04-17 16:19:35 I core error_strategy : ignore 04-17 16:19:35 I core forks : 1 04-17 16:19:35 I core lang : bash 04-17 16:19:35 I core loglevel : info 04-17 16:19:35 I core num_retries : 3 04-17 16:19:35 I core scheduler : local 04-17 16:19:35 I core submission_batch: 8 04-17 16:19:35 I core template : liquid 04-17 16:19:35 I core workdir : /path/to/cwd/.pipen/MyPipeline 04-17 16:19:35 I core plugin_opts : 04-17 16:19:35 I core template_opts : filters={'realpath': <function realpath at 0x7fc3eba12... 04-17 16:19:35 I core : globals={'realpath': <function realpath at 0x7fc3eba12... 04-17 16:19:35 I core Initializing plugins ... 04-17 16:19:36 I core 04-17 16:19:36 I core \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 P1 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e 04-17 16:19:36 I core \u2502 Sort input file \u2502 04-17 16:19:36 I core \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f 04-17 16:19:36 I core P1: Workdir: '/path/to/cwd/.pipen/MyPipeline/P1' 04-17 16:19:36 I core P1: <<< [START] 04-17 16:19:36 I core P1: >>> ['P2'] 04-17 16:19:36 I verbose P1: in.infile: /tmp/data.txt 04-17 16:19:36 I verbose P1: out.outfile: /path/to/cwd/.pipen/MyPipeline/P1/0/output/intermediate.txt 04-17 16:19:38 I verbose P1: Time elapsed: 00:00:02.051s 04-17 16:19:38 I core 04-17 16:19:38 I core \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 P2 \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e 04-17 16:19:38 I core \u2551 Paste line number \u2551 04-17 16:19:38 I core \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f 04-17 16:19:38 I core P2: Workdir: '/path/to/cwd/.pipen/MyPipeline/P2' 04-17 16:19:38 I core P2: <<< ['P1'] 04-17 16:19:38 I core P2: >>> [END] 04-17 16:19:38 I verbose P2: in.infile: /path/to/cwd/.pipen/MyPipeline/P1/0/output/intermediate.txt 04-17 16:19:38 I verbose P2: out.outfile: /path/to/cwd/MyPipeline-output/P2/result.txt 04-17 16:19:41 I verbose P2: Time elapsed: 00:00:02.051s 04-17 16:19:41 I core MYPIPELINE: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:06<00:00, 0.35 procs/s] > cat ./MyPipeline-output/P2/result.txt 1 1 2 2 3 3 Examples \u00b6 See more examples at examples/ and a more realcase example at: https://github.com/pwwang/pipen-report/tree/master/example Plugin gallery \u00b6 Plugins make pipen even better. pipen-annotate : Use docstring to annotate pipen processes pipen-args : Command line argument parser for pipen pipen-board : Visualize configuration and running of pipen pipelines on the web pipen-diagram : Draw pipeline diagrams for pipen pipen-dry : Dry runner for pipen pipelines pipen-filters : Add a set of useful filters for pipen templates. pipen-lock : Process lock for pipen to prevent multiple runs at the same time. pipen-log2file : Save running logs to file for pipen pipen-poplog : Populate logs from jobs to running log of the pipeline pipen-report : Generate report for pipen pipen-runinfo : Save running information to file for pipen pipen-verbose : Add verbosal information in logs for pipen. pipen-gcs : A plugin for pipen to handle files in Google Cloud Storage. pipen-deprecated : A pipen plugin to mark processes as deprecated. pipen-cli-init : A pipen CLI plugin to create a pipen project (pipeline) pipen-cli-ref : Make reference documentation for processes pipen-cli-require : A pipen cli plugin check the requirements of a pipeline pipen-cli-run : A pipen cli plugin to run a process or a pipeline pipen-cli-gbatch : A pipen cli plugin to submit pipeline to Google Batch Jobs","title":"Overview"},{"location":"#why-pipen","text":"pipen is designed for data scientists, bioinformaticians, and researchers who need to create reproducible, scalable computational pipelines without the complexity of traditional workflow systems.","title":"Why pipen?"},{"location":"#target-audience","text":"Data Scientists : Process large datasets with automatic parallelization and caching Bioinformaticians : Build reproducible analysis pipelines for genomics data Researchers : Create transparent, reproducible workflows for computational research DevOps Engineers : Orchestrate batch jobs across different schedulers (SLURM, SGE, Google Cloud)","title":"Target Audience"},{"location":"#key-benefits","text":"1. Zero Configuration - Get started immediately with sensible defaults - Configure only what you need, when you need it - Profile-based configuration for different environments 2. Reproducibility Built-In - Automatic job caching based on input/output signatures - Full audit trail of pipeline runs and parameters - Dependency tracking ensures processes run in correct order 3. Flexible Scheduling - Run locally for development - Scale to HPC clusters (SLURM, SGE) - Deploy to cloud (Google Cloud Batch, SSH) - Run in containers for reproducibility 4. Developer-Friendly - Define pipelines as Python classes - Use familiar Python syntax and tools - Extensible plugin system for custom functionality - Rich, informative logging and progress tracking 5. Data Flow Management - Automatic data passing between pipeline stages - Support for files, directories, and in-memory data - Built-in operations for transforming and aggregating data","title":"Key Benefits"},{"location":"#comparison-with-alternatives","text":"Feature pipen Snakemake Nextflow Airflow Target Audience Data Scientists, Bioinformaticians, Researchers, DevOps Bioinformaticians Bioinformaticians Data Engineers Learning Curve Low Medium High High Python Integration Native Limited Limited Native Scheduler Support 6+ (Local, SGE, SLURM, SSH, Container, Gbatch) Limited Limited Plugin-based Caching Built-in, automatic Manual Manual Plugin-based Cloud Native Support Yes (Google Cloud Batch) Partial Yes Yes Interactive Debugging Yes Limited No No Easy to Use Define pipelines as Python classes, familiar syntax Workflow DSL, separate config files DAG definition in Python, complex UI Zero Configuration Sensible defaults, configure only what needed Many configuration options Heavy configuration required Complex setup Nice Logging Rich, informative, color-coded, progress bars Text-based Text-based Basic logging Highly Extensible Simple plugin system, hook-based Custom rules/scripts Custom operators Custom operators/providers Data Flow Management Built-in channel operations (expand_dir, collapse_files) Manual handling Channel system XCom system Reproducibility Built-in caching, full audit trail Manual Versioned containers DAG versioning Flexible Scheduling Switch schedulers without code changes Config-based Config-based Config-based","title":"Comparison with Alternatives"},{"location":"#installation","text":"pip install -U pipen","title":"Installation"},{"location":"#quickstart","text":"example.py from pipen import Proc , Pipen , run class P1 ( Proc ): \"\"\"Sort input file\"\"\" input = \"infile\" input_data = [ \"/tmp/data.txt\" ] output = \"outfile:file:intermediate.txt\" script = \"cat {{in.infile}} | sort > {{out.outfile}}\" class P2 ( Proc ): \"\"\"Paste line number\"\"\" requires = P1 input = \"infile:file\" output = \"outfile:file:result.txt\" script = \"paste <(seq 1 3) {{in.infile}} > {{out.outfile}}\" # class MyPipeline(Pipen): # starts = P1 if __name__ == \"__main__\" : # MyPipeline().run() run ( \"MyPipeline\" , starts = P1 ) > echo -e \"3\\n2\\n1\" > /tmp/data.txt > python example.py 04-17 16:19:35 I core _____________________________________ __ 04-17 16:19:35 I core ___ __ \\___ _/__ __ \\__ ____/__ | / / 04-17 16:19:35 I core __ /_/ /__ / __ /_/ /_ __/ __ |/ / 04-17 16:19:35 I core _ ____/__/ / _ ____/_ /___ _ /| / 04-17 16:19:35 I core /_/ /___/ /_/ /_____/ /_/ |_/ 04-17 16:19:35 I core 04-17 16:19:35 I core version: 1.1.8 04-17 16:19:35 I core 04-17 16:19:35 I core \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 MYPIPELINE \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 04-17 16:19:35 I core \u2551 My pipeline \u2551 04-17 16:19:35 I core \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d 04-17 16:19:35 I core plugins : verbose v1.1.1 04-17 16:19:35 I core # procs : 2 04-17 16:19:35 I core profile : default 04-17 16:19:35 I core outdir : /path/to/cwd/MyPipeline-output 04-17 16:19:35 I core cache : True 04-17 16:19:35 I core dirsig : 1 04-17 16:19:35 I core error_strategy : ignore 04-17 16:19:35 I core forks : 1 04-17 16:19:35 I core lang : bash 04-17 16:19:35 I core loglevel : info 04-17 16:19:35 I core num_retries : 3 04-17 16:19:35 I core scheduler : local 04-17 16:19:35 I core submission_batch: 8 04-17 16:19:35 I core template : liquid 04-17 16:19:35 I core workdir : /path/to/cwd/.pipen/MyPipeline 04-17 16:19:35 I core plugin_opts : 04-17 16:19:35 I core template_opts : filters={'realpath': <function realpath at 0x7fc3eba12... 04-17 16:19:35 I core : globals={'realpath': <function realpath at 0x7fc3eba12... 04-17 16:19:35 I core Initializing plugins ... 04-17 16:19:36 I core 04-17 16:19:36 I core \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 P1 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e 04-17 16:19:36 I core \u2502 Sort input file \u2502 04-17 16:19:36 I core \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f 04-17 16:19:36 I core P1: Workdir: '/path/to/cwd/.pipen/MyPipeline/P1' 04-17 16:19:36 I core P1: <<< [START] 04-17 16:19:36 I core P1: >>> ['P2'] 04-17 16:19:36 I verbose P1: in.infile: /tmp/data.txt 04-17 16:19:36 I verbose P1: out.outfile: /path/to/cwd/.pipen/MyPipeline/P1/0/output/intermediate.txt 04-17 16:19:38 I verbose P1: Time elapsed: 00:00:02.051s 04-17 16:19:38 I core 04-17 16:19:38 I core \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 P2 \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e 04-17 16:19:38 I core \u2551 Paste line number \u2551 04-17 16:19:38 I core \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f 04-17 16:19:38 I core P2: Workdir: '/path/to/cwd/.pipen/MyPipeline/P2' 04-17 16:19:38 I core P2: <<< ['P1'] 04-17 16:19:38 I core P2: >>> [END] 04-17 16:19:38 I verbose P2: in.infile: /path/to/cwd/.pipen/MyPipeline/P1/0/output/intermediate.txt 04-17 16:19:38 I verbose P2: out.outfile: /path/to/cwd/MyPipeline-output/P2/result.txt 04-17 16:19:41 I verbose P2: Time elapsed: 00:00:02.051s 04-17 16:19:41 I core MYPIPELINE: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:06<00:00, 0.35 procs/s] > cat ./MyPipeline-output/P2/result.txt 1 1 2 2 3 3","title":"Quickstart"},{"location":"#examples","text":"See more examples at examples/ and a more realcase example at: https://github.com/pwwang/pipen-report/tree/master/example","title":"Examples"},{"location":"#plugin-gallery","text":"Plugins make pipen even better. pipen-annotate : Use docstring to annotate pipen processes pipen-args : Command line argument parser for pipen pipen-board : Visualize configuration and running of pipen pipelines on the web pipen-diagram : Draw pipeline diagrams for pipen pipen-dry : Dry runner for pipen pipelines pipen-filters : Add a set of useful filters for pipen templates. pipen-lock : Process lock for pipen to prevent multiple runs at the same time. pipen-log2file : Save running logs to file for pipen pipen-poplog : Populate logs from jobs to running log of the pipeline pipen-report : Generate report for pipen pipen-runinfo : Save running information to file for pipen pipen-verbose : Add verbosal information in logs for pipen. pipen-gcs : A plugin for pipen to handle files in Google Cloud Storage. pipen-deprecated : A pipen plugin to mark processes as deprecated. pipen-cli-init : A pipen CLI plugin to create a pipen project (pipeline) pipen-cli-ref : Make reference documentation for processes pipen-cli-require : A pipen cli plugin check the requirements of a pipeline pipen-cli-run : A pipen cli plugin to run a process or a pipeline pipen-cli-gbatch : A pipen cli plugin to submit pipeline to Google Batch Jobs","title":"Plugin gallery"},{"location":"CHANGELOG/","text":"Change Log \u00b6 1.1.8 \u00b6 chore: bump up xqute to 2.0.6 feat: include script_file in Job template_data chore: update version to 1.1.8 and plugin to v1.1.1 in README docs: add development guidelines for agentic coding agents docs: add contributing guidelines and development workflow docs: enhance README with detailed features, target audience, and comparison table docs: enhance documentation and examples 1.1.7 \u00b6 fix: handle potential rate limit errors when creating output directories style: add type ignore comments for output computation and input data handling 1.1.6 \u00b6 feat: deprecate input_data taking self argument (yank v1.1.5) 1.1.5 \u00b6 feat: allow passing 'self' to input_data callback in Proc class 1.1.4 \u00b6 fix: sort jobs by index before storing output data in DataFrame 1.1.3 \u00b6 docs: update CLI plugin documentation to include AsyncCLIPlugin fix: use proc.size instead of len(jobs) to detect the size of procs to accruately detect it due to async operations 1.1.2 \u00b6 feat: implement AsyncCLIPlugin for asynchronous command execution 1.1.1 \u00b6 chore: bump python-simpleconf to 0.8 chore: update ProfileConfig loading to async method 1.1.0 \u00b6 BREAKING: change on_proc_input_computed and on_proc_script_computed to async refactor: migrate to panpath from cloudpatlib (so that path operations can be async) refactor: convert get_mtime to async and update related methods and tests refactor: streamline process initialization and script computation feat: add async versions of from_glob and from_pairs methods in Channel class feat: add async_run function and update run to support async execution feat: update path handling to use PanPath and add async support for symlink functions fix: improve job retry handling in ProcPBar and simplify success/failure updates test: add coverage pragma to retry handling methods and comment out flaky test style: fix type annotation and flake8 issues chore: bump xqute to 2.0 chore: bump argx to 0.4 chore: add pipen-verbose dependency to example group chore: update multijob example for increased parallelism and debugging 1.0.4 \u00b6 chore: adjust bar_format alignment for better readability 1.0.3 \u00b6 chore: bump xqute to 1.0.1 fix: defer setting bar_format to prevent ValueError when rendering counter feat: add queued job status to progress bar and update related hooks 1.0.2 \u00b6 fix: make sure on_job_init() hook is called when no script was specified 1.0.1 \u00b6 fix: make sure job.output is computed before on_job_init() hook feat: show direct job status for single-job process chore: add slots to Template class for improved memory efficiency feat: enhance job progress bar to indicate cached jobs for single-job processes 1.0.0 \u00b6 chore: adopt xqute 1.0.0 feat: optimize the job submission (individual job submitted to scheduler right after initiation instead of waiting for all jobs to finish initiation) feat: update documentation links and styles, add script for collapsible sections feat: enhance progress bar to display job status counts and update formatting 0.17.29 \u00b6 feat: add job submission progress update to progress bar docs: add progress bar section to running documentation 0.17.28 \u00b6 fix: fix job progress bar not showing when jobs are being prepared 0.17.27 \u00b6 fix: sort input list in brief_list function for consistent output 0.17.26 \u00b6 fix: fix progress bar for jobs with submission skipped 0.17.25 \u00b6 chore: update dependencies chore: update xqute dependency version to 0.10.19 to use per-scheduler submission_batch by default docs: add tip about cloud communication and caching in pipen fix: ensure directory creation does not fail if rmtree encounters an error fix: enhance symlink check by validating file size before download fix: update build system requirements to use poetry-core fix: update caching logic to handle missing signature files more gracefully test: add DirOutputEmptyProc to handle empty directory output and corresponding test 0.17.24 \u00b6 chore: improve error message for missing output generation 0.17.23 \u00b6 fix: update class reference for setup state to make sure on_setup hook only executes once 0.17.22 \u00b6 BREAKING: change on_setup hook parameter from config to pipen chore: upgrade dependencies and bump datar to 0.15.12 chore: update README with new plugins ci: add python 3.13 in build workflow 0.17.21 \u00b6 chore: update fake symlink prefix to 'pipen-symlink' chore: update xqute version to 0.10.16 0.17.20 \u00b6 chore: remove unnecessary self argument for the plugin for xqute and the main plugin for pipen itself 0.17.19 \u00b6 chore: update xqute version to 0.10.11 0.17.18 \u00b6 chore: update xqute version to 0.10.10 feat: add environment variables to job submission 0.17.17 \u00b6 feat: enhance argument parsing in CLI 0.17.16 \u00b6 fix: handle potential error when reading stderr file chore: update xqute version to 0.10.7 0.17.15 \u00b6 chore(deps): update xqute dependency version to 0.10.6 in pyproject.toml feat: adopt xqute 0.10.6 0.17.14 \u00b6 fix: fix scheduler_opts of processes not full inheriting from pipeline style: fix styles in example scripts 0.17.13 \u00b6 fix: update on_job_polling signature to include counter parameter 0.17.12 \u00b6 chore(deps): bump xqute to 0.10.4 0.17.11 \u00b6 fix: mapping the pipeline workdir for gbatch scheduler instead of process workdir to enable communications between processes chore: fix no argument ignore errors for path.rmtree tests: update gbatch scheduler volume paths chore(deps): bump xqute to 0.10.3 chore: update .gitignore to include git bundle files 0.17.10 \u00b6 fix: lowercase labels in GbatchScheduler 0.17.9 \u00b6 feat: add pipeline and proc names to labels for GbatchScheduler chore(deps): bump xqute to 0.10.1 chore: standardize log message for workdir 0.17.8 \u00b6 chore(deps): bump up xqute to 0.10.0 feat: add container scheduler support for Docker/Podman/Apptainer 0.17.7 \u00b6 chore(deps): bump up xqute to 0.9.4 fix: optimize path_is_symlink function to check fake symlink faster 0.17.6 \u00b6 style: update type hints 0.17.5 \u00b6 chore: bump xqute to 0.9.3 fix: update mounted directory paths for GbatchScheduler feat: add fast_container for GbatchScheduler 0.17.4 \u00b6 chore: add ipykernel dependency for example pipelines chore(deps): bump datar to 0.15.9 (numpy v2) fix: prevent adding processes to starts if already included 0.17.3 \u00b6 chore: hide job index prefix in log messages for single-job processes chore(deps): update flake8 and other dependencies in pyproject.toml docs: fix outfile in the caching example docs: add example in README as an example pipeline in the examples folder 0.17.2 \u00b6 fix: handle exceptions (KeyboardInterrupt) when closing counters in progress bar (enlighten v1.14) 0.17.1 \u00b6 docs: update templating documentation to clarify job metadata paths fix: ensure iterables other than list/tuple are treated correctly as files/dirs 0.17.0 \u00b6 fix: handle None input case in job caching logic chore(deps): update liquidpy to 0.8.4 refactor: replace DualPath with SpecPath in codebase to adopt xqute 0.9 0.16.0 \u00b6 ci: update build workflow conditions and improve dependency versions feat: support gbatch scheduler and cloud workdir/outdir refactor: replace cloudpathlib with yunpath and update related code refactor: adopt xqute 0.8.0 chore: add more processes in examples/gbatch.py fix: make sure spec is carried for cloud output files so mtime can be check for next processes ci: add Google authentication step to CI workflow fix: fix specified path not carries to the next processes fix: handle missing MOUNTED_OUTDIR gracefully and remove unused test test: add fixture for unique ID and update cloud test to support parallel execution fix: remove fake symlink output when necessary docs: add cloud support documentation feat: support mounted paths (e.g. path1:path2) as input refactor: remove redundant validation for workdir and outdir path types feat: enforce GSPath type for 'gbatch' scheduler as pipeline outdir feat: add fast_mount option to GbatchScheduler for cloud directory mounting fix: update workdir path to include pipeline name in Pipen class chore(deps): bump python-simpleconf to v0.7 feat: support DualPath output directory in Job class and ensure mounted directory creation fix: initialize cmd with a default value to prevent errors when no script is provided chore(deps): bump xqute to 0.8.1 fix: only create mounted output directory when no MOUNTED_OUTDIR with scheduler fix: update mountPath index for taskSpec volumes in GbatchScheduler feat: support fast_mount to be a list for gbatch scheduler 0.15.8 \u00b6 chore(deps): update package versions for executing, and xqute fix: change the input/output path from resolved to absolute for symlinks style(tests): update imports and add noqa comment for unused variable 0.15.7 \u00b6 chore(deps): update xqute to version 0.5.5 chore(deps): update pytest-asyncio to version 0.25.2 chore: add logging for plugin initialization 0.15.6 \u00b6 chore(deps): bump python-varname to 0.14 ci: update GitHub Actions to use ubuntu-24.04 style: fix style issues in test files 0.15.5 \u00b6 fix: fix kwargs not updated when pipeline is a Pipen object in utils.load_pipeline() fix: fix type checking in utils.load_pipeline() 0.15.4 \u00b6 fix: fix Pipen object not recognized by utils.load_pipeline() style: fix type annotations in Pipen class deps: bump argx to 0.2.11 0.15.3 \u00b6 feat: add pipen.run() as a function to run a pipeline docs: fix the decorations in the logs in README 0.15.2 \u00b6 deps: update xqute dependency to version 0.5.1 chore: update pytest options in pyproject.toml to ignore deadlock warnings feat: expose on_jobcmd_* hooks for plugins to modify the job wrapper script 0.15.1 \u00b6 deps: bump xqute to 0.5.0 xqute v0.5.0 provides 3 more hooks for the plugins to inject bash code to the job wrapper scripts. see https://github.com/pwwang/xqute?tab=readme-ov-file#plugins . 0.15.0 \u00b6 BREAKING: remove redundant argument proc for job plugin APIs deps: bump up dev deps deps: bump xqute to version 0.4.1 refactor: remove abstractproperty decorator from CLIPlugin class feat: add 5 more APIs for plugins to handle files from other platforms (e.g. the cloud) ci: add python3.12 to CI test: fork each test in test_job.py test: fork tests in test_pipen.py and test_proc.py docs: correct the documentation about dirsig enh: make better error message when set wrong type of starts for a pipeline docs: add pipen-gcs in plugin gallery 0.14.6 \u00b6 fix: fix error handling in ProcPBar class deps: bump up dev deps 0.14.5 \u00b6 fix: fix all plugins being disabled by default 0.14.4 \u00b6 deps: bump xqute to 0.4 (simplug to 0.4.1) refactor: refactor pipen.plugin_context due to simplug upgrade docs: update docs for specifiying plugins due to simplug upgrade examples: update examples for specifiying plugins due to simplug upgrade tests: add tests for plugins specification tests: use pytest v8 ci: use latest actions 0.14.3 \u00b6 choir: rename argument args to argv for utils.is_loading_pipeline() 0.14.2 \u00b6 feat: allow passing arguments to utils.is_loading_pipeline() 0.14.1 \u00b6 feat: add flags (e.g. --help ) to utils.is_loading_pipeline to check arguments in sys.argv 0.14.0 \u00b6 deps: drop support for python 3.8 deps: bump varname to 0.13 docs: update readme for more plugins 0.13.3 (yanked) \u00b6 deps: bump varname to 0.13 0.13.2 \u00b6 style: change max line length to 88 feat: add envs_depth to procs to control the depth of envs to be inherited by subclasses 0.13.1 \u00b6 test: cover on_job_polling fix: update envs recursively for subclasses test: make sure class envs kept intact when subclassed 0.13.0 \u00b6 deps: bump xqute to 0.3.1 and liquidpy to 0.8.2 breaking: change hook on_job_running to on_job_started and add on_job_polling 0.12.5 \u00b6 deps: bump xqute to 0.2.5 chore: make utils._excepthook only handle KeyboardInterrupt chore: update examples 0.12.4 \u00b6 Modify sys.argv before the module is loaded in utils.load_pipeline() 0.12.3 \u00b6 Change cli_args to argv0 and argv1p for utils.load_pipeline 0.12.2 \u00b6 Append sys.argv[1:] by default when cli_args is None in utils.load_pipeline() 0.12.1 \u00b6 Add utils.is_loading_pipeline() to check if pipeline is loading 0.12.0 \u00b6 \u2728 Add utils.load_pipeline() to load pipeline 0.11.1 \u00b6 Dismiss warning for fillna method for pandas 2.1 Fix channel.expand_dir() may add new column 0.11.0 \u00b6 Add Dockerfile for codesandbox Bump pandas to v2 Bump argx to 0.2.10 0.10.6 \u00b6 \ud83d\udc1b Fix \"DeprecationWarning: np.find_common_type is deprecated\" from pandas (due to numpy 1.25 update) 0.10.5 \u00b6 \ud83c\udfa8 Allow starts to be set as a tuple \u2b06\ufe0f Bump python-simpleconf to 0.6 and other deps to latest versions \u2795 Add rtoml to deps (as python-simpleconf 0.6 may not depend on rtoml) 0.10.4 \u00b6 \u2b06\ufe0f Bump xqute to 0.2.3 0.10.3 \u00b6 \u2b06\ufe0f Bump xqute to 0.2.2 0.10.2 \u00b6 \ud83d\udc1b Fix exception handling in ProcPBar class update method 0.10.1 \u00b6 \u2728 Add on_proc_script_computed hook 0.10.0 \u00b6 \ud83d\udca5 Change hook on_proc_init to on_proc_create \u2728 Add on_proc_init hook back but after the process initialized insteadl of before \ud83d\udc77 Add python 3.11 to CI \ud83d\udcdd Update documentation about updated hooks\u23ce 0.9.11 \u00b6 \ud83d\udc1b Make sure .envs of Proc subclasses are Diot objects 0.9.10 \u00b6 \ud83d\udc1b Fix utils.mark and get_marked when __meta__ is None 0.9.9 \u00b6 \u26a1\ufe0f utils.mark and get_marked now work with ProcGroup and other classes 0.9.8 \u00b6 \ud83d\udc1b Fix priority of core plugin 0.9.7 \u00b6 \ud83d\udd27 Allow to inherit doc from base class for Pipen/Proc 0.9.6 \u00b6 \ud83c\udfa8 Let plugins change and create workdir \ud83d\udd27 Change the default outdir suffix from _results to -output \ud83d\udcd6 Update README file and add new plugins 0.9.5 \u00b6 \ud83d\udd27 Fix workdir in log 0.9.4 \u00b6 \ud83d\udc1b Use class name as pipeline name 0.9.3 \u00b6 \ud83d\udc1b Set logging.lastResort to null handler \u2728 Allow to assign process directly to proc groups \ud83d\udd27 Change progress bar description length to 24 0.9.2 \u00b6 \ud83c\udfa8 Rename to main plugin to core \ud83c\udfa8 Reformat log of pipeline info so that paths won't be truncated 0.9.1 \u00b6 \u2b06\ufe0f Bump xqute to 0.2.1 0.9.0 \u00b6 \u2b06\ufe0f Bump xqute to 0.2 so we can have slurm and ssh schedulers available \u2728 Add ssh and slurm scheduers \ud83c\udfa8 Improve code for dropping python 3.7 \ud83d\udc77 Use 3.10 as main python version in CI \ud83d\udcdd Update docs for slurm and ssh schedulers 0.8.0 \u00b6 \u2b06\ufe0f Drop support for python3.7 \ud83c\udfa8 Don't slugify pipen or proc names anymore but require them to be valid filenames \ud83d\udc1b Fix process names being reused \ud83d\udcdd Update documentation with new job caching callback. \ud83c\udfa8 Move actions to on_job_cached hook for cached jobs 0.7.3 \u00b6 \u2728 Add --list for pipen profile to list the names of available profiles \u2728 Add exception hook to show uncaught in log \u2728 Add on_job_cached hook 0.7.2 \u00b6 \u2728 Add utils.mark and get_marked to mark a process Unlike plugin_opts, template_opts or envs, these marks are not inherited in subclasses 0.7.1 \u00b6 \u2b06\ufe0f Upgrade simplug to 0.2.3 \ud83d\udcdd Add pipen-cli-config to plugin gallery 0.7.0 \u00b6 \u2b06\ufe0f Update liquidpy to 0.8 \u2728 Add Proc.__meta__ that will not be inherited when subclassing \ud83c\udfa8 Put procgroup in Proc.__meta__ \u26a1\ufe0f Do not mutate Proc.__doc__ when subclassing \u26a1\ufe0f Use mro to detect parent class of a Proc 0.6.4 \u00b6 \ud83d\udd00 Set desc from docstring if not given for pipelines 0.6.3 \u00b6 \ud83d\udd0a Trim right spaces of logs 0.6.2 \u00b6 \u2b06\ufe0f Adopt xqute 0.1.5 0.6.1 \u00b6 \ud83d\udc1b Fix path expansion for ~/.pipen.toml in defaults. 0.6.0 \u00b6 \u2728 Allow subclassing Pipen to create a pipeline (#151) 0.5.2 \u00b6 \ud83d\udcdd Refactor codebase: unify type annotations and import future features \ud83d\udc1b Allow methods decorated by @ProcGroup.add_proc to return None 0.5.1 \u00b6 \ud83d\ude91 Remove remaining more-itertools 0.5.0 \u00b6 \u2796 Remove more-itertools \u2728 Add ProcGroup to manage groups of processes. from pipen import Proc , ProcGroup class MyGroup ( ProcGroup ): @ProcGroup . add_proc def my_proc ( self ): class MyProc ( Proc ): ... return MyProc @ProcGroup . add_proc def my_proc2 ( self ): class MyProc2 ( Proc ): requires = self . my_proc ... return MyProc2 pg = MyGroup () # Run as a pipeline pg . as_pipen () . set_data ( ... ) . run () # Integrate into a pipeline < proc_of_a_pipeline >. requires = pg . my_proc2 0.4.6 \u00b6 \ud83d\udc1b Fix plugins command not listing plugins 0.4.5 \u00b6 \ud83d\ude91 Fix banner alignment in terminal 0.4.4 \u00b6 \ud83d\udc1b Fix when cli plugin has no docstring \ud83d\ude91 Exclude help from help sub-command itself \ud83d\ude91 Add cli plugin docstring as sub-command description 0.4.3 \u00b6 \u2b06\ufe0f Bump argx to 0.2.2 \ud83c\udfa8 Expose parse_args() to cli plugins 0.4.2 \u00b6 \u2b06\ufe0f Bump argx to 0.2 0.4.1 \u00b6 \ud83d\udc1b Fix cli plugin name 0.4.0 \u00b6 \u2b06\ufe0f Upgrade python-slugify to ^0.8 \u2b06\ufe0f Upgrade xqute to 0.1.4 \u2b06\ufe0f Upgrade varname to 0.11 \ud83d\udca5 Use argx instead of pyparam 0.3.12 \u00b6 \u2b06\ufe0f Upgrade python-slugify to ^7 0.3.11 \u00b6 \ud83d\udcdd Fix github workflow badges in README \ud83e\ude79 Fix pandas warning when less-column data passed to channel 0.3.10 \u00b6 \u2b06\ufe0f Upgrade xqute to 0.1.3 \u2b06\ufe0f Upgrade datar to 0.11 and format test files \u2728 Add cli command version to show versions of deps \u2796 Remove rich as it is required by xqute already 0.3.9 \u00b6 \u2b06\ufe0f Bump pipda to 0.11 \u2b06\ufe0f Bump xqute to 0.1.2 0.3.8 \u00b6 \u2b06\ufe0f Pump xqute to 0.1.1 0.3.7 \u00b6 \u2b06\ufe0f Upgrade varname to 0.10 0.3.6 \u00b6 \u2b06\ufe0f Upgrade pipda to 0.7.2, varname to 0.9.1, datar to 0.9 0.3.5 \u00b6 \ud83d\udc1b Fix nexts being inherited for Proc subclasses 0.3.4 \u00b6 \u2728 Print pipen version in CLI: pipen plugins \ud83e\ude79 Make use of full terminal width for non-panel elements in log \ud83e\ude79 Extend width to 256 when terminal width cannot be detected while logging (most likely logging to a text file) 0.3.3 \u00b6 \u267f\ufe0f Change default log width to 100 \ud83e\ude79 Fix broken panel in log content with console width cannot be detected 0.3.2 \u00b6 \u2b06\ufe0f Upgrade rtoml to v0.8 \u2b06\ufe0f Upgrade pipda to v0.6 0.3.1 \u00b6 \ud83e\ude79 Hide config meta data in pipeline information 0.3.0 \u00b6 \u2b06\ufe0f Upgrade dependencies \ud83d\udccc Use rtoml instead of toml (see https://github.com/pwwang/toml-bench) \ud83e\ude79 Dump job signature to file directly instead of dump to a string first \ud83d\udc77 Add python 3.10 to CI \ud83d\udcdd Add dependencies badge to README.md 0.2.16 \u00b6 \ud83d\udccc Pin dep versions \ud83e\ude79 Allow to set workdir from Pipen constructor 0.2.15 \u00b6 \ud83e\ude79 Fix FutureWarning in Proc._compute_input() \ud83e\ude79 Add __doc__ for Proc.from_proc() \ud83d\udccc Pin deps for docs 0.2.14 \u00b6 \ud83e\ude79 Shorten pipeline info in log for long config options \ud83d\udc1b Fix cached jobs being put into queue \ud83e\ude79 Shorten job debug messages when hit limits \ud83d\ude91 Remove sort_dicts for pprint.pformat for py3.7 0.2.13 \u00b6 \ud83e\ude79 Don't require job.signature.toml to force cache a job 0.2.12 \u00b6 \ud83d\udc1b Hotfix for typos in Proc.__init_subclass__() 0.2.11 \u00b6 \ud83e\ude79 Update envs , plugin_opts and scheduler_opts while subclassing processes. 0.2.10 \u00b6 \u2728 Add hook on_proc_input_computed \ud83e\ude79 Default new process docstring to \"Undescribed process.\" 0.2.9 \u00b6 \u2728 Allow requires to be set by __setattr__() 0.2.8 \u00b6 \ud83e\ude79 Forward fill na for input data 0.2.7 \u00b6 \ud83e\ude79 Fix process plugin_opts not inherited from pipeline 0.2.6 \u00b6 \ud83c\udfa8 Make pipen._build_proc_relationships() public and don't rebuild the relations \u2728 Allow pipenline name to be obtained from assignment 0.2.5 \u00b6 \ud83e\ude79 Allow relative script path to be inherited \ud83d\udc1b Fix column order from depedency processes \ud83e\ude79 Fix doc not inherited for processes 0.2.4 \u00b6 \u2728 Add execution order for processes 0.2.3 \u00b6 \u26a1\ufe0fSpeed up package importing 0.2.2 \u00b6 \ud83d\udc1b Load CLI plugins at runtime 0.2.1 \u00b6 \ud83c\udfa8 Allow CLI plugin to have different name than the command 0.2.0 \u00b6 \ud83d\udca5 Restructure CLI plugins 0.1.4 \u00b6 \ud83e\ude79 Use brackets to indicate cached jobs \ud83e\ude79 Run on_complete hook only when no exception happened \ud83e\ude79 Let on_proc_init to modify process workdir \ud83d\udc1b Fix when nexts affected by parent nexts assignment when parent in __bases__ 0.1.3 \u00b6 \u2728 Add on_proc_init() hook to enables plugins to modify the default attributes of processes \ud83d\udca5 Rename Proc.args to Proc.envs 0.1.2 \u00b6 \ud83d\udca5 Use set_starts() and set_data() to set start processes of a pipeline. 0.1.1 \u00b6 \ud83d\udca5 Allow plugins to modify other configs via on_setup() hook \ud83c\udfa8 Move progress bar to the last \ud83e\ude79 Warn when no input_data specified for start process \ud83d\udcac Change end to export \ud83d\ude9a Move on_init() so it's able to redefine default configs \ud83d\udca5 Change exec_cmd hook of cli plugin to exec_command 0.1.0 \u00b6 It's now fully documented. See documentations. 0.0.4 \u00b6 Clear output if not cached. Make process running order fixed 0.0.3 \u00b6 Fix caching issue Add singleton to proc to force singleton Log an empty line after all processes finish Allow input to be None Separate channels from different required procs Move proc prepare before run Change the order proc banner printing, making sure it prints before other logs for the proc FIx job not cached if input is missing Don't redirect output only if absolute path specified Make input files resolved(absolute path) Give more detailed ProcDependencyError Force job status to be failed when Ctrl + c Fix files for input when it is a pandas dataframe Add job name prefix for scheduler Adopt datar for channels 0.0.2 \u00b6 Add on_proc_property_computed hook Add plugin options for pipen construct Keep args, envs, scheduler_opts and plugin_opts as Diot object for procs Fix shebang not working in script Make sure job rendering data are stringified. Move starts as a method so that pipelines can be initialized before processes. Use plyrda instead of siuba for channel Add template rendering error to indicate where the rendering error happens; Add succeeded to on_complete hook; Add hook on_proc_start; Add argument succedded for hook on_proc_done; Realign pipeline and proc names in progress bars Remove debug log in job preparing, which will appear on the top of the logs 0.0.1 \u00b6 Reimplement PyPPL using asyncio","title":"Change Log"},{"location":"CHANGELOG/#change-log","text":"","title":"Change Log"},{"location":"CHANGELOG/#118","text":"chore: bump up xqute to 2.0.6 feat: include script_file in Job template_data chore: update version to 1.1.8 and plugin to v1.1.1 in README docs: add development guidelines for agentic coding agents docs: add contributing guidelines and development workflow docs: enhance README with detailed features, target audience, and comparison table docs: enhance documentation and examples","title":"1.1.8"},{"location":"CHANGELOG/#117","text":"fix: handle potential rate limit errors when creating output directories style: add type ignore comments for output computation and input data handling","title":"1.1.7"},{"location":"CHANGELOG/#116","text":"feat: deprecate input_data taking self argument (yank v1.1.5)","title":"1.1.6"},{"location":"CHANGELOG/#115","text":"feat: allow passing 'self' to input_data callback in Proc class","title":"1.1.5"},{"location":"CHANGELOG/#114","text":"fix: sort jobs by index before storing output data in DataFrame","title":"1.1.4"},{"location":"CHANGELOG/#113","text":"docs: update CLI plugin documentation to include AsyncCLIPlugin fix: use proc.size instead of len(jobs) to detect the size of procs to accruately detect it due to async operations","title":"1.1.3"},{"location":"CHANGELOG/#112","text":"feat: implement AsyncCLIPlugin for asynchronous command execution","title":"1.1.2"},{"location":"CHANGELOG/#111","text":"chore: bump python-simpleconf to 0.8 chore: update ProfileConfig loading to async method","title":"1.1.1"},{"location":"CHANGELOG/#110","text":"BREAKING: change on_proc_input_computed and on_proc_script_computed to async refactor: migrate to panpath from cloudpatlib (so that path operations can be async) refactor: convert get_mtime to async and update related methods and tests refactor: streamline process initialization and script computation feat: add async versions of from_glob and from_pairs methods in Channel class feat: add async_run function and update run to support async execution feat: update path handling to use PanPath and add async support for symlink functions fix: improve job retry handling in ProcPBar and simplify success/failure updates test: add coverage pragma to retry handling methods and comment out flaky test style: fix type annotation and flake8 issues chore: bump xqute to 2.0 chore: bump argx to 0.4 chore: add pipen-verbose dependency to example group chore: update multijob example for increased parallelism and debugging","title":"1.1.0"},{"location":"CHANGELOG/#104","text":"chore: adjust bar_format alignment for better readability","title":"1.0.4"},{"location":"CHANGELOG/#103","text":"chore: bump xqute to 1.0.1 fix: defer setting bar_format to prevent ValueError when rendering counter feat: add queued job status to progress bar and update related hooks","title":"1.0.3"},{"location":"CHANGELOG/#102","text":"fix: make sure on_job_init() hook is called when no script was specified","title":"1.0.2"},{"location":"CHANGELOG/#101","text":"fix: make sure job.output is computed before on_job_init() hook feat: show direct job status for single-job process chore: add slots to Template class for improved memory efficiency feat: enhance job progress bar to indicate cached jobs for single-job processes","title":"1.0.1"},{"location":"CHANGELOG/#100","text":"chore: adopt xqute 1.0.0 feat: optimize the job submission (individual job submitted to scheduler right after initiation instead of waiting for all jobs to finish initiation) feat: update documentation links and styles, add script for collapsible sections feat: enhance progress bar to display job status counts and update formatting","title":"1.0.0"},{"location":"CHANGELOG/#01729","text":"feat: add job submission progress update to progress bar docs: add progress bar section to running documentation","title":"0.17.29"},{"location":"CHANGELOG/#01728","text":"fix: fix job progress bar not showing when jobs are being prepared","title":"0.17.28"},{"location":"CHANGELOG/#01727","text":"fix: sort input list in brief_list function for consistent output","title":"0.17.27"},{"location":"CHANGELOG/#01726","text":"fix: fix progress bar for jobs with submission skipped","title":"0.17.26"},{"location":"CHANGELOG/#01725","text":"chore: update dependencies chore: update xqute dependency version to 0.10.19 to use per-scheduler submission_batch by default docs: add tip about cloud communication and caching in pipen fix: ensure directory creation does not fail if rmtree encounters an error fix: enhance symlink check by validating file size before download fix: update build system requirements to use poetry-core fix: update caching logic to handle missing signature files more gracefully test: add DirOutputEmptyProc to handle empty directory output and corresponding test","title":"0.17.25"},{"location":"CHANGELOG/#01724","text":"chore: improve error message for missing output generation","title":"0.17.24"},{"location":"CHANGELOG/#01723","text":"fix: update class reference for setup state to make sure on_setup hook only executes once","title":"0.17.23"},{"location":"CHANGELOG/#01722","text":"BREAKING: change on_setup hook parameter from config to pipen chore: upgrade dependencies and bump datar to 0.15.12 chore: update README with new plugins ci: add python 3.13 in build workflow","title":"0.17.22"},{"location":"CHANGELOG/#01721","text":"chore: update fake symlink prefix to 'pipen-symlink' chore: update xqute version to 0.10.16","title":"0.17.21"},{"location":"CHANGELOG/#01720","text":"chore: remove unnecessary self argument for the plugin for xqute and the main plugin for pipen itself","title":"0.17.20"},{"location":"CHANGELOG/#01719","text":"chore: update xqute version to 0.10.11","title":"0.17.19"},{"location":"CHANGELOG/#01718","text":"chore: update xqute version to 0.10.10 feat: add environment variables to job submission","title":"0.17.18"},{"location":"CHANGELOG/#01717","text":"feat: enhance argument parsing in CLI","title":"0.17.17"},{"location":"CHANGELOG/#01716","text":"fix: handle potential error when reading stderr file chore: update xqute version to 0.10.7","title":"0.17.16"},{"location":"CHANGELOG/#01715","text":"chore(deps): update xqute dependency version to 0.10.6 in pyproject.toml feat: adopt xqute 0.10.6","title":"0.17.15"},{"location":"CHANGELOG/#01714","text":"fix: fix scheduler_opts of processes not full inheriting from pipeline style: fix styles in example scripts","title":"0.17.14"},{"location":"CHANGELOG/#01713","text":"fix: update on_job_polling signature to include counter parameter","title":"0.17.13"},{"location":"CHANGELOG/#01712","text":"chore(deps): bump xqute to 0.10.4","title":"0.17.12"},{"location":"CHANGELOG/#01711","text":"fix: mapping the pipeline workdir for gbatch scheduler instead of process workdir to enable communications between processes chore: fix no argument ignore errors for path.rmtree tests: update gbatch scheduler volume paths chore(deps): bump xqute to 0.10.3 chore: update .gitignore to include git bundle files","title":"0.17.11"},{"location":"CHANGELOG/#01710","text":"fix: lowercase labels in GbatchScheduler","title":"0.17.10"},{"location":"CHANGELOG/#0179","text":"feat: add pipeline and proc names to labels for GbatchScheduler chore(deps): bump xqute to 0.10.1 chore: standardize log message for workdir","title":"0.17.9"},{"location":"CHANGELOG/#0178","text":"chore(deps): bump up xqute to 0.10.0 feat: add container scheduler support for Docker/Podman/Apptainer","title":"0.17.8"},{"location":"CHANGELOG/#0177","text":"chore(deps): bump up xqute to 0.9.4 fix: optimize path_is_symlink function to check fake symlink faster","title":"0.17.7"},{"location":"CHANGELOG/#0176","text":"style: update type hints","title":"0.17.6"},{"location":"CHANGELOG/#0175","text":"chore: bump xqute to 0.9.3 fix: update mounted directory paths for GbatchScheduler feat: add fast_container for GbatchScheduler","title":"0.17.5"},{"location":"CHANGELOG/#0174","text":"chore: add ipykernel dependency for example pipelines chore(deps): bump datar to 0.15.9 (numpy v2) fix: prevent adding processes to starts if already included","title":"0.17.4"},{"location":"CHANGELOG/#0173","text":"chore: hide job index prefix in log messages for single-job processes chore(deps): update flake8 and other dependencies in pyproject.toml docs: fix outfile in the caching example docs: add example in README as an example pipeline in the examples folder","title":"0.17.3"},{"location":"CHANGELOG/#0172","text":"fix: handle exceptions (KeyboardInterrupt) when closing counters in progress bar (enlighten v1.14)","title":"0.17.2"},{"location":"CHANGELOG/#0171","text":"docs: update templating documentation to clarify job metadata paths fix: ensure iterables other than list/tuple are treated correctly as files/dirs","title":"0.17.1"},{"location":"CHANGELOG/#0170","text":"fix: handle None input case in job caching logic chore(deps): update liquidpy to 0.8.4 refactor: replace DualPath with SpecPath in codebase to adopt xqute 0.9","title":"0.17.0"},{"location":"CHANGELOG/#0160","text":"ci: update build workflow conditions and improve dependency versions feat: support gbatch scheduler and cloud workdir/outdir refactor: replace cloudpathlib with yunpath and update related code refactor: adopt xqute 0.8.0 chore: add more processes in examples/gbatch.py fix: make sure spec is carried for cloud output files so mtime can be check for next processes ci: add Google authentication step to CI workflow fix: fix specified path not carries to the next processes fix: handle missing MOUNTED_OUTDIR gracefully and remove unused test test: add fixture for unique ID and update cloud test to support parallel execution fix: remove fake symlink output when necessary docs: add cloud support documentation feat: support mounted paths (e.g. path1:path2) as input refactor: remove redundant validation for workdir and outdir path types feat: enforce GSPath type for 'gbatch' scheduler as pipeline outdir feat: add fast_mount option to GbatchScheduler for cloud directory mounting fix: update workdir path to include pipeline name in Pipen class chore(deps): bump python-simpleconf to v0.7 feat: support DualPath output directory in Job class and ensure mounted directory creation fix: initialize cmd with a default value to prevent errors when no script is provided chore(deps): bump xqute to 0.8.1 fix: only create mounted output directory when no MOUNTED_OUTDIR with scheduler fix: update mountPath index for taskSpec volumes in GbatchScheduler feat: support fast_mount to be a list for gbatch scheduler","title":"0.16.0"},{"location":"CHANGELOG/#0158","text":"chore(deps): update package versions for executing, and xqute fix: change the input/output path from resolved to absolute for symlinks style(tests): update imports and add noqa comment for unused variable","title":"0.15.8"},{"location":"CHANGELOG/#0157","text":"chore(deps): update xqute to version 0.5.5 chore(deps): update pytest-asyncio to version 0.25.2 chore: add logging for plugin initialization","title":"0.15.7"},{"location":"CHANGELOG/#0156","text":"chore(deps): bump python-varname to 0.14 ci: update GitHub Actions to use ubuntu-24.04 style: fix style issues in test files","title":"0.15.6"},{"location":"CHANGELOG/#0155","text":"fix: fix kwargs not updated when pipeline is a Pipen object in utils.load_pipeline() fix: fix type checking in utils.load_pipeline()","title":"0.15.5"},{"location":"CHANGELOG/#0154","text":"fix: fix Pipen object not recognized by utils.load_pipeline() style: fix type annotations in Pipen class deps: bump argx to 0.2.11","title":"0.15.4"},{"location":"CHANGELOG/#0153","text":"feat: add pipen.run() as a function to run a pipeline docs: fix the decorations in the logs in README","title":"0.15.3"},{"location":"CHANGELOG/#0152","text":"deps: update xqute dependency to version 0.5.1 chore: update pytest options in pyproject.toml to ignore deadlock warnings feat: expose on_jobcmd_* hooks for plugins to modify the job wrapper script","title":"0.15.2"},{"location":"CHANGELOG/#0151","text":"deps: bump xqute to 0.5.0 xqute v0.5.0 provides 3 more hooks for the plugins to inject bash code to the job wrapper scripts. see https://github.com/pwwang/xqute?tab=readme-ov-file#plugins .","title":"0.15.1"},{"location":"CHANGELOG/#0150","text":"BREAKING: remove redundant argument proc for job plugin APIs deps: bump up dev deps deps: bump xqute to version 0.4.1 refactor: remove abstractproperty decorator from CLIPlugin class feat: add 5 more APIs for plugins to handle files from other platforms (e.g. the cloud) ci: add python3.12 to CI test: fork each test in test_job.py test: fork tests in test_pipen.py and test_proc.py docs: correct the documentation about dirsig enh: make better error message when set wrong type of starts for a pipeline docs: add pipen-gcs in plugin gallery","title":"0.15.0"},{"location":"CHANGELOG/#0146","text":"fix: fix error handling in ProcPBar class deps: bump up dev deps","title":"0.14.6"},{"location":"CHANGELOG/#0145","text":"fix: fix all plugins being disabled by default","title":"0.14.5"},{"location":"CHANGELOG/#0144","text":"deps: bump xqute to 0.4 (simplug to 0.4.1) refactor: refactor pipen.plugin_context due to simplug upgrade docs: update docs for specifiying plugins due to simplug upgrade examples: update examples for specifiying plugins due to simplug upgrade tests: add tests for plugins specification tests: use pytest v8 ci: use latest actions","title":"0.14.4"},{"location":"CHANGELOG/#0143","text":"choir: rename argument args to argv for utils.is_loading_pipeline()","title":"0.14.3"},{"location":"CHANGELOG/#0142","text":"feat: allow passing arguments to utils.is_loading_pipeline()","title":"0.14.2"},{"location":"CHANGELOG/#0141","text":"feat: add flags (e.g. --help ) to utils.is_loading_pipeline to check arguments in sys.argv","title":"0.14.1"},{"location":"CHANGELOG/#0140","text":"deps: drop support for python 3.8 deps: bump varname to 0.13 docs: update readme for more plugins","title":"0.14.0"},{"location":"CHANGELOG/#0133-yanked","text":"deps: bump varname to 0.13","title":"0.13.3 (yanked)"},{"location":"CHANGELOG/#0132","text":"style: change max line length to 88 feat: add envs_depth to procs to control the depth of envs to be inherited by subclasses","title":"0.13.2"},{"location":"CHANGELOG/#0131","text":"test: cover on_job_polling fix: update envs recursively for subclasses test: make sure class envs kept intact when subclassed","title":"0.13.1"},{"location":"CHANGELOG/#0130","text":"deps: bump xqute to 0.3.1 and liquidpy to 0.8.2 breaking: change hook on_job_running to on_job_started and add on_job_polling","title":"0.13.0"},{"location":"CHANGELOG/#0125","text":"deps: bump xqute to 0.2.5 chore: make utils._excepthook only handle KeyboardInterrupt chore: update examples","title":"0.12.5"},{"location":"CHANGELOG/#0124","text":"Modify sys.argv before the module is loaded in utils.load_pipeline()","title":"0.12.4"},{"location":"CHANGELOG/#0123","text":"Change cli_args to argv0 and argv1p for utils.load_pipeline","title":"0.12.3"},{"location":"CHANGELOG/#0122","text":"Append sys.argv[1:] by default when cli_args is None in utils.load_pipeline()","title":"0.12.2"},{"location":"CHANGELOG/#0121","text":"Add utils.is_loading_pipeline() to check if pipeline is loading","title":"0.12.1"},{"location":"CHANGELOG/#0120","text":"\u2728 Add utils.load_pipeline() to load pipeline","title":"0.12.0"},{"location":"CHANGELOG/#0111","text":"Dismiss warning for fillna method for pandas 2.1 Fix channel.expand_dir() may add new column","title":"0.11.1"},{"location":"CHANGELOG/#0110","text":"Add Dockerfile for codesandbox Bump pandas to v2 Bump argx to 0.2.10","title":"0.11.0"},{"location":"CHANGELOG/#0106","text":"\ud83d\udc1b Fix \"DeprecationWarning: np.find_common_type is deprecated\" from pandas (due to numpy 1.25 update)","title":"0.10.6"},{"location":"CHANGELOG/#0105","text":"\ud83c\udfa8 Allow starts to be set as a tuple \u2b06\ufe0f Bump python-simpleconf to 0.6 and other deps to latest versions \u2795 Add rtoml to deps (as python-simpleconf 0.6 may not depend on rtoml)","title":"0.10.5"},{"location":"CHANGELOG/#0104","text":"\u2b06\ufe0f Bump xqute to 0.2.3","title":"0.10.4"},{"location":"CHANGELOG/#0103","text":"\u2b06\ufe0f Bump xqute to 0.2.2","title":"0.10.3"},{"location":"CHANGELOG/#0102","text":"\ud83d\udc1b Fix exception handling in ProcPBar class update method","title":"0.10.2"},{"location":"CHANGELOG/#0101","text":"\u2728 Add on_proc_script_computed hook","title":"0.10.1"},{"location":"CHANGELOG/#0100","text":"\ud83d\udca5 Change hook on_proc_init to on_proc_create \u2728 Add on_proc_init hook back but after the process initialized insteadl of before \ud83d\udc77 Add python 3.11 to CI \ud83d\udcdd Update documentation about updated hooks\u23ce","title":"0.10.0"},{"location":"CHANGELOG/#0911","text":"\ud83d\udc1b Make sure .envs of Proc subclasses are Diot objects","title":"0.9.11"},{"location":"CHANGELOG/#0910","text":"\ud83d\udc1b Fix utils.mark and get_marked when __meta__ is None","title":"0.9.10"},{"location":"CHANGELOG/#099","text":"\u26a1\ufe0f utils.mark and get_marked now work with ProcGroup and other classes","title":"0.9.9"},{"location":"CHANGELOG/#098","text":"\ud83d\udc1b Fix priority of core plugin","title":"0.9.8"},{"location":"CHANGELOG/#097","text":"\ud83d\udd27 Allow to inherit doc from base class for Pipen/Proc","title":"0.9.7"},{"location":"CHANGELOG/#096","text":"\ud83c\udfa8 Let plugins change and create workdir \ud83d\udd27 Change the default outdir suffix from _results to -output \ud83d\udcd6 Update README file and add new plugins","title":"0.9.6"},{"location":"CHANGELOG/#095","text":"\ud83d\udd27 Fix workdir in log","title":"0.9.5"},{"location":"CHANGELOG/#094","text":"\ud83d\udc1b Use class name as pipeline name","title":"0.9.4"},{"location":"CHANGELOG/#093","text":"\ud83d\udc1b Set logging.lastResort to null handler \u2728 Allow to assign process directly to proc groups \ud83d\udd27 Change progress bar description length to 24","title":"0.9.3"},{"location":"CHANGELOG/#092","text":"\ud83c\udfa8 Rename to main plugin to core \ud83c\udfa8 Reformat log of pipeline info so that paths won't be truncated","title":"0.9.2"},{"location":"CHANGELOG/#091","text":"\u2b06\ufe0f Bump xqute to 0.2.1","title":"0.9.1"},{"location":"CHANGELOG/#090","text":"\u2b06\ufe0f Bump xqute to 0.2 so we can have slurm and ssh schedulers available \u2728 Add ssh and slurm scheduers \ud83c\udfa8 Improve code for dropping python 3.7 \ud83d\udc77 Use 3.10 as main python version in CI \ud83d\udcdd Update docs for slurm and ssh schedulers","title":"0.9.0"},{"location":"CHANGELOG/#080","text":"\u2b06\ufe0f Drop support for python3.7 \ud83c\udfa8 Don't slugify pipen or proc names anymore but require them to be valid filenames \ud83d\udc1b Fix process names being reused \ud83d\udcdd Update documentation with new job caching callback. \ud83c\udfa8 Move actions to on_job_cached hook for cached jobs","title":"0.8.0"},{"location":"CHANGELOG/#073","text":"\u2728 Add --list for pipen profile to list the names of available profiles \u2728 Add exception hook to show uncaught in log \u2728 Add on_job_cached hook","title":"0.7.3"},{"location":"CHANGELOG/#072","text":"\u2728 Add utils.mark and get_marked to mark a process Unlike plugin_opts, template_opts or envs, these marks are not inherited in subclasses","title":"0.7.2"},{"location":"CHANGELOG/#071","text":"\u2b06\ufe0f Upgrade simplug to 0.2.3 \ud83d\udcdd Add pipen-cli-config to plugin gallery","title":"0.7.1"},{"location":"CHANGELOG/#070","text":"\u2b06\ufe0f Update liquidpy to 0.8 \u2728 Add Proc.__meta__ that will not be inherited when subclassing \ud83c\udfa8 Put procgroup in Proc.__meta__ \u26a1\ufe0f Do not mutate Proc.__doc__ when subclassing \u26a1\ufe0f Use mro to detect parent class of a Proc","title":"0.7.0"},{"location":"CHANGELOG/#064","text":"\ud83d\udd00 Set desc from docstring if not given for pipelines","title":"0.6.4"},{"location":"CHANGELOG/#063","text":"\ud83d\udd0a Trim right spaces of logs","title":"0.6.3"},{"location":"CHANGELOG/#062","text":"\u2b06\ufe0f Adopt xqute 0.1.5","title":"0.6.2"},{"location":"CHANGELOG/#061","text":"\ud83d\udc1b Fix path expansion for ~/.pipen.toml in defaults.","title":"0.6.1"},{"location":"CHANGELOG/#060","text":"\u2728 Allow subclassing Pipen to create a pipeline (#151)","title":"0.6.0"},{"location":"CHANGELOG/#052","text":"\ud83d\udcdd Refactor codebase: unify type annotations and import future features \ud83d\udc1b Allow methods decorated by @ProcGroup.add_proc to return None","title":"0.5.2"},{"location":"CHANGELOG/#051","text":"\ud83d\ude91 Remove remaining more-itertools","title":"0.5.1"},{"location":"CHANGELOG/#050","text":"\u2796 Remove more-itertools \u2728 Add ProcGroup to manage groups of processes. from pipen import Proc , ProcGroup class MyGroup ( ProcGroup ): @ProcGroup . add_proc def my_proc ( self ): class MyProc ( Proc ): ... return MyProc @ProcGroup . add_proc def my_proc2 ( self ): class MyProc2 ( Proc ): requires = self . my_proc ... return MyProc2 pg = MyGroup () # Run as a pipeline pg . as_pipen () . set_data ( ... ) . run () # Integrate into a pipeline < proc_of_a_pipeline >. requires = pg . my_proc2","title":"0.5.0"},{"location":"CHANGELOG/#046","text":"\ud83d\udc1b Fix plugins command not listing plugins","title":"0.4.6"},{"location":"CHANGELOG/#045","text":"\ud83d\ude91 Fix banner alignment in terminal","title":"0.4.5"},{"location":"CHANGELOG/#044","text":"\ud83d\udc1b Fix when cli plugin has no docstring \ud83d\ude91 Exclude help from help sub-command itself \ud83d\ude91 Add cli plugin docstring as sub-command description","title":"0.4.4"},{"location":"CHANGELOG/#043","text":"\u2b06\ufe0f Bump argx to 0.2.2 \ud83c\udfa8 Expose parse_args() to cli plugins","title":"0.4.3"},{"location":"CHANGELOG/#042","text":"\u2b06\ufe0f Bump argx to 0.2","title":"0.4.2"},{"location":"CHANGELOG/#041","text":"\ud83d\udc1b Fix cli plugin name","title":"0.4.1"},{"location":"CHANGELOG/#040","text":"\u2b06\ufe0f Upgrade python-slugify to ^0.8 \u2b06\ufe0f Upgrade xqute to 0.1.4 \u2b06\ufe0f Upgrade varname to 0.11 \ud83d\udca5 Use argx instead of pyparam","title":"0.4.0"},{"location":"CHANGELOG/#0312","text":"\u2b06\ufe0f Upgrade python-slugify to ^7","title":"0.3.12"},{"location":"CHANGELOG/#0311","text":"\ud83d\udcdd Fix github workflow badges in README \ud83e\ude79 Fix pandas warning when less-column data passed to channel","title":"0.3.11"},{"location":"CHANGELOG/#0310","text":"\u2b06\ufe0f Upgrade xqute to 0.1.3 \u2b06\ufe0f Upgrade datar to 0.11 and format test files \u2728 Add cli command version to show versions of deps \u2796 Remove rich as it is required by xqute already","title":"0.3.10"},{"location":"CHANGELOG/#039","text":"\u2b06\ufe0f Bump pipda to 0.11 \u2b06\ufe0f Bump xqute to 0.1.2","title":"0.3.9"},{"location":"CHANGELOG/#038","text":"\u2b06\ufe0f Pump xqute to 0.1.1","title":"0.3.8"},{"location":"CHANGELOG/#037","text":"\u2b06\ufe0f Upgrade varname to 0.10","title":"0.3.7"},{"location":"CHANGELOG/#036","text":"\u2b06\ufe0f Upgrade pipda to 0.7.2, varname to 0.9.1, datar to 0.9","title":"0.3.6"},{"location":"CHANGELOG/#035","text":"\ud83d\udc1b Fix nexts being inherited for Proc subclasses","title":"0.3.5"},{"location":"CHANGELOG/#034","text":"\u2728 Print pipen version in CLI: pipen plugins \ud83e\ude79 Make use of full terminal width for non-panel elements in log \ud83e\ude79 Extend width to 256 when terminal width cannot be detected while logging (most likely logging to a text file)","title":"0.3.4"},{"location":"CHANGELOG/#033","text":"\u267f\ufe0f Change default log width to 100 \ud83e\ude79 Fix broken panel in log content with console width cannot be detected","title":"0.3.3"},{"location":"CHANGELOG/#032","text":"\u2b06\ufe0f Upgrade rtoml to v0.8 \u2b06\ufe0f Upgrade pipda to v0.6","title":"0.3.2"},{"location":"CHANGELOG/#031","text":"\ud83e\ude79 Hide config meta data in pipeline information","title":"0.3.1"},{"location":"CHANGELOG/#030","text":"\u2b06\ufe0f Upgrade dependencies \ud83d\udccc Use rtoml instead of toml (see https://github.com/pwwang/toml-bench) \ud83e\ude79 Dump job signature to file directly instead of dump to a string first \ud83d\udc77 Add python 3.10 to CI \ud83d\udcdd Add dependencies badge to README.md","title":"0.3.0"},{"location":"CHANGELOG/#0216","text":"\ud83d\udccc Pin dep versions \ud83e\ude79 Allow to set workdir from Pipen constructor","title":"0.2.16"},{"location":"CHANGELOG/#0215","text":"\ud83e\ude79 Fix FutureWarning in Proc._compute_input() \ud83e\ude79 Add __doc__ for Proc.from_proc() \ud83d\udccc Pin deps for docs","title":"0.2.15"},{"location":"CHANGELOG/#0214","text":"\ud83e\ude79 Shorten pipeline info in log for long config options \ud83d\udc1b Fix cached jobs being put into queue \ud83e\ude79 Shorten job debug messages when hit limits \ud83d\ude91 Remove sort_dicts for pprint.pformat for py3.7","title":"0.2.14"},{"location":"CHANGELOG/#0213","text":"\ud83e\ude79 Don't require job.signature.toml to force cache a job","title":"0.2.13"},{"location":"CHANGELOG/#0212","text":"\ud83d\udc1b Hotfix for typos in Proc.__init_subclass__()","title":"0.2.12"},{"location":"CHANGELOG/#0211","text":"\ud83e\ude79 Update envs , plugin_opts and scheduler_opts while subclassing processes.","title":"0.2.11"},{"location":"CHANGELOG/#0210","text":"\u2728 Add hook on_proc_input_computed \ud83e\ude79 Default new process docstring to \"Undescribed process.\"","title":"0.2.10"},{"location":"CHANGELOG/#029","text":"\u2728 Allow requires to be set by __setattr__()","title":"0.2.9"},{"location":"CHANGELOG/#028","text":"\ud83e\ude79 Forward fill na for input data","title":"0.2.8"},{"location":"CHANGELOG/#027","text":"\ud83e\ude79 Fix process plugin_opts not inherited from pipeline","title":"0.2.7"},{"location":"CHANGELOG/#026","text":"\ud83c\udfa8 Make pipen._build_proc_relationships() public and don't rebuild the relations \u2728 Allow pipenline name to be obtained from assignment","title":"0.2.6"},{"location":"CHANGELOG/#025","text":"\ud83e\ude79 Allow relative script path to be inherited \ud83d\udc1b Fix column order from depedency processes \ud83e\ude79 Fix doc not inherited for processes","title":"0.2.5"},{"location":"CHANGELOG/#024","text":"\u2728 Add execution order for processes","title":"0.2.4"},{"location":"CHANGELOG/#023","text":"\u26a1\ufe0fSpeed up package importing","title":"0.2.3"},{"location":"CHANGELOG/#022","text":"\ud83d\udc1b Load CLI plugins at runtime","title":"0.2.2"},{"location":"CHANGELOG/#021","text":"\ud83c\udfa8 Allow CLI plugin to have different name than the command","title":"0.2.1"},{"location":"CHANGELOG/#020","text":"\ud83d\udca5 Restructure CLI plugins","title":"0.2.0"},{"location":"CHANGELOG/#014","text":"\ud83e\ude79 Use brackets to indicate cached jobs \ud83e\ude79 Run on_complete hook only when no exception happened \ud83e\ude79 Let on_proc_init to modify process workdir \ud83d\udc1b Fix when nexts affected by parent nexts assignment when parent in __bases__","title":"0.1.4"},{"location":"CHANGELOG/#013","text":"\u2728 Add on_proc_init() hook to enables plugins to modify the default attributes of processes \ud83d\udca5 Rename Proc.args to Proc.envs","title":"0.1.3"},{"location":"CHANGELOG/#012","text":"\ud83d\udca5 Use set_starts() and set_data() to set start processes of a pipeline.","title":"0.1.2"},{"location":"CHANGELOG/#011","text":"\ud83d\udca5 Allow plugins to modify other configs via on_setup() hook \ud83c\udfa8 Move progress bar to the last \ud83e\ude79 Warn when no input_data specified for start process \ud83d\udcac Change end to export \ud83d\ude9a Move on_init() so it's able to redefine default configs \ud83d\udca5 Change exec_cmd hook of cli plugin to exec_command","title":"0.1.1"},{"location":"CHANGELOG/#010","text":"It's now fully documented. See documentations.","title":"0.1.0"},{"location":"CHANGELOG/#004","text":"Clear output if not cached. Make process running order fixed","title":"0.0.4"},{"location":"CHANGELOG/#003","text":"Fix caching issue Add singleton to proc to force singleton Log an empty line after all processes finish Allow input to be None Separate channels from different required procs Move proc prepare before run Change the order proc banner printing, making sure it prints before other logs for the proc FIx job not cached if input is missing Don't redirect output only if absolute path specified Make input files resolved(absolute path) Give more detailed ProcDependencyError Force job status to be failed when Ctrl + c Fix files for input when it is a pandas dataframe Add job name prefix for scheduler Adopt datar for channels","title":"0.0.3"},{"location":"CHANGELOG/#002","text":"Add on_proc_property_computed hook Add plugin options for pipen construct Keep args, envs, scheduler_opts and plugin_opts as Diot object for procs Fix shebang not working in script Make sure job rendering data are stringified. Move starts as a method so that pipelines can be initialized before processes. Use plyrda instead of siuba for channel Add template rendering error to indicate where the rendering error happens; Add succeeded to on_complete hook; Add hook on_proc_start; Add argument succedded for hook on_proc_done; Realign pipeline and proc names in progress bars Remove debug log in job preparing, which will appear on the top of the logs","title":"0.0.2"},{"location":"CHANGELOG/#001","text":"Reimplement PyPPL using asyncio","title":"0.0.1"},{"location":"CONTRIBUTING/","text":"Contributing to pipen \u00b6 Thank you for your interest in contributing to pipen! This document provides guidelines and instructions for contributing to the project. Table of Contents \u00b6 Development Setup Code Style Testing Documentation Pull Request Process Reporting Issues Development Setup \u00b6 Prerequisites \u00b6 Python 3.9 or higher Poetry for dependency management Git Setting Up the Development Environment \u00b6 Fork and clone the repository # Fork the repository on GitHub first git clone https://github.com/YOUR_USERNAME/pipen.git cd pipen Install development dependencies # Install using Poetry poetry install --all-extras # Or install the development group specifically poetry install --with dev,docs,example Activate the virtual environment # Using Poetry shell poetry shell # Or use the virtual environment path source $( poetry env info --path ) /bin/activate Install pre-commit hooks pre-commit install Verify the setup # Run tests to ensure everything is working pytest tests/ # Build documentation cd docs && mkdocs build Development Workflow \u00b6 # Create a new branch for your changes git checkout -b feature/your-feature-name # Make your changes # ... # Run tests pytest tests/ # Run linting flake8 pipen mypy -p pipen # Format code black pipen # Commit changes git add . git commit -m \"Add your feature\" # Push to your fork git push origin feature/your-feature-name Code Style \u00b6 Formatting \u00b6 We use Black for code formatting: # Format code black pipen # Check formatting without making changes black --check pipen Configuration: - Line length: 88 characters - Target Python versions: 3.9, 3.10, 3.11, 3.12 Linting \u00b6 We use flake8 for code linting: flake8 pipen Type Checking \u00b6 We use mypy for static type checking: mypy -p pipen Configuration: - Ignore missing imports from external packages - Allow redefinition in some cases - Strict optional mode is disabled for flexibility Docstring Format \u00b6 We use Google-style docstrings with Args, Returns, Raises, and Attributes sections: def process_data ( input_file : str , output_dir : str , verbose : bool = False ) -> dict : \"\"\"Process a data file and save results to output directory. This function reads the input file, processes the data, and saves the results to the specified output directory. Args: input_file: Path to the input data file. output_dir: Directory where processed results will be saved. verbose: If True, print detailed progress information. Returns: Dictionary containing processing statistics and output file paths. Raises: FileNotFoundError: If input_file does not exist. ValueError: If input_file is malformed. Examples: >>> result = process_data(\"data.csv\", \"output\") >>> result['processed_count'] 100 \"\"\" pass For classes, include a description and list important attributes: class DataProcessor : \"\"\"Process and transform data files. This class provides methods for reading, transforming, and saving data in various formats. Attributes: processed_count: Number of files processed. errors: List of errors encountered during processing. config: Configuration dictionary for processing parameters. \"\"\" pass Pre-commit Hooks \u00b6 We use pre-commit hooks to automatically run checks before committing: trailing-whitespace : Remove trailing whitespace end-of-file-fixer : Ensure files end with a newline check-yaml : Validate YAML syntax check-added-large-files : Prevent large files from being committed versionchecker : Ensure version consistency between pyproject.toml and pipen/version.py mypy : Run type checking pytest : Run tests flake8 : Run linting Pre-commit hooks are configured in .pre-commit-config.yaml and automatically run on commits for files in pipen/ directory (excludes tests/ , examples/ , and docs/ ). Testing \u00b6 Running Tests \u00b6 We use pytest for testing: # Run all tests pytest tests/ # Run tests with coverage pytest --cov = pipen --cov-report = term-missing # Run specific test file pytest tests/test_pipen.py # Run with verbose output pytest -vv tests/ # Run specific test pytest tests/test_pipen.py::test_pipen_init Test Configuration \u00b6 Our test configuration (from pyproject.toml ): Parallel execution : pytest-xdist with -n auto for automatic parallelization Distribution mode : --dist loadgroup to run dependent tests together Coverage : pytest-cov for code coverage reporting Async support : pytest-asyncio for async test cases Warnings : Treat UserWarning as errors ( -W error::UserWarning ) Writing Tests \u00b6 Place tests in the tests/ directory following the structure: # tests/test_pipen.py import pytest from pipen import Pipen , Proc def test_pipen_init (): \"\"\"Test that Pipen initializes correctly.\"\"\" pipeline = Pipen () assert pipeline . name == \"Pipen\" @pytest . mark . asyncio async def test_async_pipeline (): \"\"\"Test async pipeline execution.\"\"\" pipeline = Pipen () result = await pipeline . run_async () assert result is True Test Coverage \u00b6 We aim for high test coverage. The current coverage is tracked on Codacy . To check coverage locally: pytest --cov = pipen --cov-report = html open htmlcov/index.html # macOS # or xdg-open htmlcov/index.html # Linux Documentation \u00b6 Building Documentation \u00b6 Documentation is built with MkDocs : cd docs mkdocs build # Build to site/ mkdocs serve # Serve at http://127.0.0.1:8000 mkdocs gh-deploy # Deploy to GitHub Pages Documentation Structure \u00b6 docs/ \u251c\u2500\u2500 index.md # Symlink to ../README.md \u251c\u2500\u2500 basics.md # Pipeline layers and folder structure \u251c\u2500\u2500 defining-proc.md # Process definition guide \u251c\u2500\u2500 running.md # Pipeline execution guide \u251c\u2500\u2500 configurations.md # Configuration documentation \u251c\u2500\u2500 caching.md # Job caching mechanism \u251c\u2500\u2500 channels.md # Channel system documentation \u251c\u2500\u2500 input-output.md # Input/output specification \u251c\u2500\u2500 error.md # Error handling strategies \u251c\u2500\u2500 templating.md # Template engine documentation \u251c\u2500\u2500 script.md # Script configuration \u251c\u2500\u2500 scheduler.md # Scheduler backends \u251c\u2500\u2500 cloud.md # Cloud support \u251c\u2500\u2500 proc-group.md # Process groups \u251c\u2500\u2500 plugin.md # Plugin development \u251c\u2500\u2500 cli.md # CLI tool documentation \u251c\u2500\u2500 examples.md # Example documentation \u251c\u2500\u2500 CHANGELOG.md # Version history \u251c\u2500\u2500 style.css # Custom styling \u2514\u2500\u2500 script.js # Custom JavaScript API Documentation \u00b6 API documentation is auto-generated from docstrings using the mkapi-fix plugin. To ensure your API documentation is properly generated: Write Google-style docstrings for all public classes, functions, and methods Include Args , Returns , Raises , and Attributes sections where applicable Add Examples sections for complex functions Ensure type hints are present in function signatures Adding New Documentation \u00b6 Create a new .md file in the docs/ directory Update the nav section in mkdocs.yml to include your new page Add cross-references using [](#anchor) syntax Use code blocks with language identifiers: python, bash, etc. Use admonition blocks for notes, warnings, and tips: !!! note This is a note block. !!! warning This is a warning. !!! tip This is a tip. Documentation Requirements \u00b6 All new public APIs must have docstrings Breaking changes must be documented in CHANGELOG.md New features should include examples in the documentation Visual diagrams should have descriptive alt text for accessibility Pull Request Process \u00b6 Before Submitting a PR \u00b6 Update documentation Add or update docstrings for changed code Update relevant documentation files Add examples for new features Run all tests pytest tests/ Run linting and type checking flake8 pipen mypy -p pipen black --check pipen Build documentation cd docs && mkdocs build Update CHANGELOG.md Add an entry under the appropriate version section Use the format: [<type>] <description> ([#issue]) Types: added , changed , deprecated , removed , fixed , security Submitting a PR \u00b6 Push your branch to your fork Open a pull request on GitHub Fill in the PR template with: A clear description of changes Related issues (if any) Screenshots for UI changes (if applicable) Testing performed Documentation updates PR Review Process \u00b6 Maintainers will review your PR Address review comments by pushing additional commits Keep the PR focused on a single change Squash commits if requested by maintainers Update based on review feedback Merge Criteria \u00b6 A PR can be merged when: [ ] All tests pass [ ] Code is properly formatted (Black) [ ] No linting errors (flake8) [ ] No type checking errors (mypy) [ ] Documentation is updated [ ] CHANGELOG.md is updated for breaking changes [ ] At least one maintainer approves Reporting Issues \u00b6 Bug Reports \u00b6 When reporting a bug, include: Python version : python --version pipen version : pipen --version Minimal reproducible example : Code that demonstrates the issue Expected behavior : What you expected to happen Actual behavior : What actually happened (with error messages) Environment details : OS, scheduler used, etc. Feature Requests \u00b6 When requesting a feature: Use case : Explain what problem this feature solves Proposed solution : How you envision the feature working Alternatives considered : Other approaches you've thought of Additional context : Any relevant context about the request Documentation Issues \u00b6 For documentation issues: Page location : Which documentation page has the issue Problem : What is incorrect, unclear, or missing Suggestion : How it should be improved (if you have ideas) Getting Help \u00b6 GitHub Issues : For bug reports and feature requests GitHub Discussions : For questions and general discussion Documentation : https://pwwang.github.io/pipen Examples : See the examples/ directory for usage examples License \u00b6 By contributing to pipen, you agree that your contributions will be licensed under the MIT License.","title":"Contributing"},{"location":"CONTRIBUTING/#contributing-to-pipen","text":"Thank you for your interest in contributing to pipen! This document provides guidelines and instructions for contributing to the project.","title":"Contributing to pipen"},{"location":"CONTRIBUTING/#table-of-contents","text":"Development Setup Code Style Testing Documentation Pull Request Process Reporting Issues","title":"Table of Contents"},{"location":"CONTRIBUTING/#development-setup","text":"","title":"Development Setup"},{"location":"CONTRIBUTING/#prerequisites","text":"Python 3.9 or higher Poetry for dependency management Git","title":"Prerequisites"},{"location":"CONTRIBUTING/#setting-up-the-development-environment","text":"Fork and clone the repository # Fork the repository on GitHub first git clone https://github.com/YOUR_USERNAME/pipen.git cd pipen Install development dependencies # Install using Poetry poetry install --all-extras # Or install the development group specifically poetry install --with dev,docs,example Activate the virtual environment # Using Poetry shell poetry shell # Or use the virtual environment path source $( poetry env info --path ) /bin/activate Install pre-commit hooks pre-commit install Verify the setup # Run tests to ensure everything is working pytest tests/ # Build documentation cd docs && mkdocs build","title":"Setting Up the Development Environment"},{"location":"CONTRIBUTING/#development-workflow","text":"# Create a new branch for your changes git checkout -b feature/your-feature-name # Make your changes # ... # Run tests pytest tests/ # Run linting flake8 pipen mypy -p pipen # Format code black pipen # Commit changes git add . git commit -m \"Add your feature\" # Push to your fork git push origin feature/your-feature-name","title":"Development Workflow"},{"location":"CONTRIBUTING/#code-style","text":"","title":"Code Style"},{"location":"CONTRIBUTING/#formatting","text":"We use Black for code formatting: # Format code black pipen # Check formatting without making changes black --check pipen Configuration: - Line length: 88 characters - Target Python versions: 3.9, 3.10, 3.11, 3.12","title":"Formatting"},{"location":"CONTRIBUTING/#linting","text":"We use flake8 for code linting: flake8 pipen","title":"Linting"},{"location":"CONTRIBUTING/#type-checking","text":"We use mypy for static type checking: mypy -p pipen Configuration: - Ignore missing imports from external packages - Allow redefinition in some cases - Strict optional mode is disabled for flexibility","title":"Type Checking"},{"location":"CONTRIBUTING/#docstring-format","text":"We use Google-style docstrings with Args, Returns, Raises, and Attributes sections: def process_data ( input_file : str , output_dir : str , verbose : bool = False ) -> dict : \"\"\"Process a data file and save results to output directory. This function reads the input file, processes the data, and saves the results to the specified output directory. Args: input_file: Path to the input data file. output_dir: Directory where processed results will be saved. verbose: If True, print detailed progress information. Returns: Dictionary containing processing statistics and output file paths. Raises: FileNotFoundError: If input_file does not exist. ValueError: If input_file is malformed. Examples: >>> result = process_data(\"data.csv\", \"output\") >>> result['processed_count'] 100 \"\"\" pass For classes, include a description and list important attributes: class DataProcessor : \"\"\"Process and transform data files. This class provides methods for reading, transforming, and saving data in various formats. Attributes: processed_count: Number of files processed. errors: List of errors encountered during processing. config: Configuration dictionary for processing parameters. \"\"\" pass","title":"Docstring Format"},{"location":"CONTRIBUTING/#pre-commit-hooks","text":"We use pre-commit hooks to automatically run checks before committing: trailing-whitespace : Remove trailing whitespace end-of-file-fixer : Ensure files end with a newline check-yaml : Validate YAML syntax check-added-large-files : Prevent large files from being committed versionchecker : Ensure version consistency between pyproject.toml and pipen/version.py mypy : Run type checking pytest : Run tests flake8 : Run linting Pre-commit hooks are configured in .pre-commit-config.yaml and automatically run on commits for files in pipen/ directory (excludes tests/ , examples/ , and docs/ ).","title":"Pre-commit Hooks"},{"location":"CONTRIBUTING/#testing","text":"","title":"Testing"},{"location":"CONTRIBUTING/#running-tests","text":"We use pytest for testing: # Run all tests pytest tests/ # Run tests with coverage pytest --cov = pipen --cov-report = term-missing # Run specific test file pytest tests/test_pipen.py # Run with verbose output pytest -vv tests/ # Run specific test pytest tests/test_pipen.py::test_pipen_init","title":"Running Tests"},{"location":"CONTRIBUTING/#test-configuration","text":"Our test configuration (from pyproject.toml ): Parallel execution : pytest-xdist with -n auto for automatic parallelization Distribution mode : --dist loadgroup to run dependent tests together Coverage : pytest-cov for code coverage reporting Async support : pytest-asyncio for async test cases Warnings : Treat UserWarning as errors ( -W error::UserWarning )","title":"Test Configuration"},{"location":"CONTRIBUTING/#writing-tests","text":"Place tests in the tests/ directory following the structure: # tests/test_pipen.py import pytest from pipen import Pipen , Proc def test_pipen_init (): \"\"\"Test that Pipen initializes correctly.\"\"\" pipeline = Pipen () assert pipeline . name == \"Pipen\" @pytest . mark . asyncio async def test_async_pipeline (): \"\"\"Test async pipeline execution.\"\"\" pipeline = Pipen () result = await pipeline . run_async () assert result is True","title":"Writing Tests"},{"location":"CONTRIBUTING/#test-coverage","text":"We aim for high test coverage. The current coverage is tracked on Codacy . To check coverage locally: pytest --cov = pipen --cov-report = html open htmlcov/index.html # macOS # or xdg-open htmlcov/index.html # Linux","title":"Test Coverage"},{"location":"CONTRIBUTING/#documentation","text":"","title":"Documentation"},{"location":"CONTRIBUTING/#building-documentation","text":"Documentation is built with MkDocs : cd docs mkdocs build # Build to site/ mkdocs serve # Serve at http://127.0.0.1:8000 mkdocs gh-deploy # Deploy to GitHub Pages","title":"Building Documentation"},{"location":"CONTRIBUTING/#documentation-structure","text":"docs/ \u251c\u2500\u2500 index.md # Symlink to ../README.md \u251c\u2500\u2500 basics.md # Pipeline layers and folder structure \u251c\u2500\u2500 defining-proc.md # Process definition guide \u251c\u2500\u2500 running.md # Pipeline execution guide \u251c\u2500\u2500 configurations.md # Configuration documentation \u251c\u2500\u2500 caching.md # Job caching mechanism \u251c\u2500\u2500 channels.md # Channel system documentation \u251c\u2500\u2500 input-output.md # Input/output specification \u251c\u2500\u2500 error.md # Error handling strategies \u251c\u2500\u2500 templating.md # Template engine documentation \u251c\u2500\u2500 script.md # Script configuration \u251c\u2500\u2500 scheduler.md # Scheduler backends \u251c\u2500\u2500 cloud.md # Cloud support \u251c\u2500\u2500 proc-group.md # Process groups \u251c\u2500\u2500 plugin.md # Plugin development \u251c\u2500\u2500 cli.md # CLI tool documentation \u251c\u2500\u2500 examples.md # Example documentation \u251c\u2500\u2500 CHANGELOG.md # Version history \u251c\u2500\u2500 style.css # Custom styling \u2514\u2500\u2500 script.js # Custom JavaScript","title":"Documentation Structure"},{"location":"CONTRIBUTING/#api-documentation","text":"API documentation is auto-generated from docstrings using the mkapi-fix plugin. To ensure your API documentation is properly generated: Write Google-style docstrings for all public classes, functions, and methods Include Args , Returns , Raises , and Attributes sections where applicable Add Examples sections for complex functions Ensure type hints are present in function signatures","title":"API Documentation"},{"location":"CONTRIBUTING/#adding-new-documentation","text":"Create a new .md file in the docs/ directory Update the nav section in mkdocs.yml to include your new page Add cross-references using [](#anchor) syntax Use code blocks with language identifiers: python, bash, etc. Use admonition blocks for notes, warnings, and tips: !!! note This is a note block. !!! warning This is a warning. !!! tip This is a tip.","title":"Adding New Documentation"},{"location":"CONTRIBUTING/#documentation-requirements","text":"All new public APIs must have docstrings Breaking changes must be documented in CHANGELOG.md New features should include examples in the documentation Visual diagrams should have descriptive alt text for accessibility","title":"Documentation Requirements"},{"location":"CONTRIBUTING/#pull-request-process","text":"","title":"Pull Request Process"},{"location":"CONTRIBUTING/#before-submitting-a-pr","text":"Update documentation Add or update docstrings for changed code Update relevant documentation files Add examples for new features Run all tests pytest tests/ Run linting and type checking flake8 pipen mypy -p pipen black --check pipen Build documentation cd docs && mkdocs build Update CHANGELOG.md Add an entry under the appropriate version section Use the format: [<type>] <description> ([#issue]) Types: added , changed , deprecated , removed , fixed , security","title":"Before Submitting a PR"},{"location":"CONTRIBUTING/#submitting-a-pr","text":"Push your branch to your fork Open a pull request on GitHub Fill in the PR template with: A clear description of changes Related issues (if any) Screenshots for UI changes (if applicable) Testing performed Documentation updates","title":"Submitting a PR"},{"location":"CONTRIBUTING/#pr-review-process","text":"Maintainers will review your PR Address review comments by pushing additional commits Keep the PR focused on a single change Squash commits if requested by maintainers Update based on review feedback","title":"PR Review Process"},{"location":"CONTRIBUTING/#merge-criteria","text":"A PR can be merged when: [ ] All tests pass [ ] Code is properly formatted (Black) [ ] No linting errors (flake8) [ ] No type checking errors (mypy) [ ] Documentation is updated [ ] CHANGELOG.md is updated for breaking changes [ ] At least one maintainer approves","title":"Merge Criteria"},{"location":"CONTRIBUTING/#reporting-issues","text":"","title":"Reporting Issues"},{"location":"CONTRIBUTING/#bug-reports","text":"When reporting a bug, include: Python version : python --version pipen version : pipen --version Minimal reproducible example : Code that demonstrates the issue Expected behavior : What you expected to happen Actual behavior : What actually happened (with error messages) Environment details : OS, scheduler used, etc.","title":"Bug Reports"},{"location":"CONTRIBUTING/#feature-requests","text":"When requesting a feature: Use case : Explain what problem this feature solves Proposed solution : How you envision the feature working Alternatives considered : Other approaches you've thought of Additional context : Any relevant context about the request","title":"Feature Requests"},{"location":"CONTRIBUTING/#documentation-issues","text":"For documentation issues: Page location : Which documentation page has the issue Problem : What is incorrect, unclear, or missing Suggestion : How it should be improved (if you have ideas)","title":"Documentation Issues"},{"location":"CONTRIBUTING/#getting-help","text":"GitHub Issues : For bug reports and feature requests GitHub Discussions : For questions and general discussion Documentation : https://pwwang.github.io/pipen Examples : See the examples/ directory for usage examples","title":"Getting Help"},{"location":"CONTRIBUTING/#license","text":"By contributing to pipen, you agree that your contributions will be licensed under the MIT License.","title":"License"},{"location":"architecture/","text":"Architecture \u00b6 This document explains the internal architecture of pipen, including how pipelines execute, how data flows through channels, the plugin system, caching mechanism, and template engine. Overview \u00b6 pipen is built on a layered architecture with three main components: Pipeline Layer : Orchestrates the overall workflow Process Layer : Defines individual computational steps Job Layer : Executes individual tasks graph TD A[Pipeline] --> B[Process 1] A --> C[Process 2] A --> D[Process N] B --> E[Job 1.1] B --> F[Job 1.2] B --> G[Job 1.N] C --> H[Job 2.1] C --> I[Job 2.2] D --> J[Job N.1] Pipeline Execution Flow \u00b6 1. Initialization \u00b6 When a pipeline is created, the following sequence occurs: # Pipeline creation pipeline = Pipen ( name = \"MyPipeline\" , starts = [ Proc1 , Proc2 ]) # Triggers plugin hooks: # 1. on_setup() - Called once when first pipeline is loaded # 2. on_init() - Called for each pipeline initialization 2. Dependency Resolution \u00b6 pipen automatically infers process dependencies based on the requires attribute: class P1 ( Proc ): \"\"\"First process\"\"\" pass class P2 ( Proc ): \"\"\"Second process\"\"\" requires = P1 # P2 depends on P1's output The pipeline builds a directed acyclic graph (DAG) of processes and validates that: - No circular dependencies exist - All dependencies are satisfied - Processes can be topologically sorted 3. Input Data Computation \u00b6 For each process, pipen computes the input channel: # Plugin hook called await plugin . on_proc_input_computed ( proc ) The input channel is built from: - input_data attribute (static data) - Output channel from required processes - Transformed input via custom functions 4. Job Generation \u00b6 For each row in the input channel, pipen creates a job: graph LR A[Input Channel] --> B[Process] B --> C[Job 1] B --> D[Job 2] B --> E[Job N] C --> F[Output Channel] D --> F E --> F Each job receives: - A subset of input data (one row from the channel) - A working directory - A script template to execute 5. Script Rendering \u00b6 The process script template is rendered with job-specific data: # Plugin hook called await plugin . on_proc_script_computed ( proc ) Template variables include: - in.* : Input variables and their values - out.* : Output variable placeholders - proc.* : Process metadata - envs : Environment variables (options, not shell Environment Variables) that are shared across all jobs of the process 6. Job Submission and Execution \u00b6 Jobs are submitted to the configured scheduler: # Built-in schedulers: # - Local: Execute jobs on the local machine # - SGE: Submit to Sun Grid Engine # - SLURM: Submit to SLURM workload manager # - SSH: Execute jobs on remote servers # - Container: Run jobs in containers # - Gbatch: Submit to Google Cloud Batch Job lifecycle: stateDiagram-v2 [*] --> Initiated Initiated --> Queued Queued --> Submitted Submitted --> Running Running --> Succeeded Running --> Failed Succeeded --> Cached: Job was cached Failed --> Retrying: If num_retries > 0 Retrying --> Submitted Failed --> [*] Succeeded --> [*] 7. Output Collection \u00b6 When a job completes: - Output files are collected - Output signature is computed - Results are added to the output channel 8. Pipeline Completion \u00b6 After all jobs complete: # Plugin hook called await plugin . on_complete ( pipeline , succeeded ) The pipeline: - Reports final status - Generates output summary - Cleans up temporary resources Channel Data Flow \u00b6 Channels are pandas DataFrames that structure data flow between processes. Channel Structure \u00b6 # A channel with 3 rows and 2 columns import pandas as pd from pipen import Channel df = pd . DataFrame ({ 'file' : [ 'data1.txt' , 'data2.txt' , 'data3.txt' ], 'sample' : [ 'sample1' , 'sample2' , 'sample3' ] }) channel = Channel . create ( df ) Row-Based Parallelism \u00b6 Each row in a channel becomes an independent job: # Input channel: # file sample # 0 data1.txt sample1 # 1 data2.txt sample2 # 2 data3.txt sample3 # Generates 3 jobs, each with: # Job 1: in.file = \"data1.txt\", in.sample = \"sample1\" # Job 2: in.file = \"data2.txt\", in.sample = \"sample2\" # Job 3: in.file = \"data3.txt\", in.sample = \"sample3\" Channel Operations \u00b6 Creating Channels \u00b6 # From list Channel . create ([ 1 , 2 , 3 ]) # 3 rows, 1 column # From glob pattern Channel . from_glob ( \"data/*.txt\" , sortby = \"mtime\" ) # From existing DataFrame Channel . create ( my_dataframe ) Transforming Channels \u00b6 # Expand directory to files expanded = channel . expand_dir ( 'file' ) # Collapse files to directory collapsed = channel . collapse_files ( 'file' ) # Select columns selected = channel [[ 'file' , 'sample' ]] # Filter rows filtered = channel [ channel [ 'type' ] == 'A' ] Channel Data Types \u00b6 pipen supports several input/output types: Type Syntax Description var input = \"name\" In-memory variable file input = \"name:file\" Single file path dir input = \"name:dir\" Single directory path files input = \"name:files\" Multiple file paths dirs input = \"name:dirs\" Multiple directory paths Plugin System \u00b6 pipen uses the simplug library for a flexible plugin system with hook-based extensibility. Plugin Hooks \u00b6 Pipeline-Level Hooks \u00b6 @plugin . spec def on_setup ( pipen : Pipen ) -> None : \"\"\"Called once when first pipeline is loaded. Use this to set default configurations. \"\"\" @plugin . spec async def on_init ( pipen : Pipen ) -> None : \"\"\"Called when a pipeline is initialized. Use this to access pipeline configuration. \"\"\" @plugin . spec async def on_start ( pipen : Pipen ) -> None : \"\"\"Called right before pipeline starts. Process relationships are already inferred. \"\"\" @plugin . spec async def on_complete ( pipen : Pipen , succeeded : bool ) -> None : \"\"\"Called when pipeline finishes. succeeded indicates whether pipeline succeeded. \"\"\" Process-Level Hooks \u00b6 @plugin . spec def on_proc_create ( proc : Proc ) -> None : \"\"\"Called when a process is created. Use this to modify default attributes. \"\"\" @plugin . spec async def on_proc_input_computed ( proc : Proc ) -> None : \"\"\"Called after process input data is computed. Use this to transform input data. \"\"\" @plugin . spec async def on_proc_script_computed ( proc : Proc ) -> None : \"\"\"Called after process script is computed. Use this to modify the script template. \"\"\" @plugin . spec async def on_proc_start ( proc : Proc ) -> None : \"\"\"Called when a process starts running. \"\"\" @plugin . spec async def on_proc_end ( proc : Proc , succeeded : bool ) -> None : \"\"\"Called when a process finishes. succeeded indicates whether process succeeded. \"\"\" Job-Level Hooks \u00b6 @plugin . spec async def on_job_init ( job : Job ) -> None : \"\"\"Called when a job is created. \"\"\" @plugin . spec async def on_job_submitted ( job : Job ) -> None : \"\"\"Called when a job is submitted to scheduler. \"\"\" @plugin . spec async def on_job_running ( job : Job ) -> None : \"\"\"Called when a job starts running. \"\"\" @plugin . spec async def on_job_succeeded ( job : Job ) -> None : \"\"\"Called when a job succeeds. \"\"\" @plugin . spec async def on_job_failed ( job : Job , error : Exception ) -> None : \"\"\"Called when a job fails. error contains the exception. \"\"\" Creating a Plugin \u00b6 # myplugin/__init__.py from pipen.pluginmgr import plugin @plugin . impl def on_proc_create ( proc ): \"\"\"Modify process attributes on creation.\"\"\" proc . config . forks = 4 # Set default to 4 parallel jobs # In pyproject.toml: # [tool.poetry.plugins.\"pipen\"] # myplugin = \"myplugin:main\" Caching Mechanism \u00b6 pipen implements intelligent job caching to avoid redundant computations. Cache Key Computation \u00b6 The cache key (signature) is computed from: Input data : File paths, directory paths, or in-memory values Script content : Hash of the rendered script Process metadata : Configuration values that affect output # Signature file location { workdir } / { proc . name } / { job . index } / job . signature . toml # Example signature: [ signature ] input_hash = \"a1b2c3d4...\" script_hash = \"e5f6g7h8...\" config_hash = \"i9j0k1l2...\" Cache Check Process \u00b6 When a job starts: graph TD A[Job Starts] --> B{Cached?} B -->|Yes| C[Load Cached Output] B -->|No| D[Execute Script] D --> E[Compute Output Signature] E --> F[Write Cache] C --> G[Return Results] F --> G Cache Invalidation \u00b6 A cache is invalid if: Input files are modified (based on modification time) Script content changes Process configuration changes Output files are missing or corrupted Directory Signatures \u00b6 pipen supports directory-level signatures ( dirsig ): # If dirsig is set: # - Cache considers all files in the directory # - Any file modification invalidates cache # - Useful for directory outputs with multiple files proc . dirsig = 1 # Enable directory signature Template Engine \u00b6 pipen uses template engines to render job scripts dynamically. Supported Template Engines \u00b6 Liquid (default): Safe, production-ready templating Jinja2 : Feature-rich Python templating Mako : High-performance Python templating Template Variables \u00b6 Inside a script template, you have access to: # Input variables {{ in . input_file }} # Value of 'input_file' input {{ in . config_value }} # Value of 'config_value' input # Output placeholders {{ out . output_file }} # Where to save output # Process metadata {{ proc . name }} # Process name {{ proc . workdir }} # Process workdir {{ proc . index }} # Job index # Template options (defined in proc.envs) # Note: These are NOT shell environment variables like PATH or HOME # They are custom options passed to template rendering context {{ envs .* }} Example Templates \u00b6 Bash Template \u00b6 script = \"\"\" # Process {{ in.input_file }} cat {{ in.input_file }} > {{ out.output_file }} # Report progress echo \"Processed: {{ in.input_file }}\" \"\"\" Python Template \u00b6 lang = \"python\" script = \"\"\" import pandas as pd # Read input df = pd.read_csv('{{ in.input_file }}') # Process data df['processed'] = df['value'] * 2 # Save output df.to_csv('{{ out.output_file }}', index=False) \"\"\" Using Filters \u00b6 # Built-in filters {{ in . path | basename }} # Get filename from path {{ in . path | dirname }} # Get directory from path {{ in . value | upper }} # Convert to uppercase Using Template Options (proc.envs) \u00b6 # Define template options in process class MyProc ( Proc ): envs = { \"custom_var\" : \"my_value\" , \"threshold\" : 100 } script = \"\"\" echo \"Custom value: {{ envs.custom_var }}\" echo \"Threshold: {{ envs.threshold }}\" \"\"\" # Note: envs are NOT shell environment variables (PATH, HOME, etc.) # They are custom options passed to the template rendering context Scheduler Architecture \u00b6 pipen supports multiple schedulers through a unified interface. Scheduler Interface \u00b6 All schedulers implement the xqute.Scheduler interface: class Scheduler ( ABC ): \"\"\"Base class for schedulers\"\"\" async def submit ( self , job : Job ) -> None : \"\"\"Submit a job for execution\"\"\" async def kill ( self , job : Job ) -> None : \"\"\"Kill a running job\"\"\" async def poll ( self , job : Job ) -> JobStatus : \"\"\"Check job status\"\"\" async def clean ( self , job : Job ) -> None : \"\"\"Clean up job resources\"\"\" Scheduler Selection \u00b6 # Via configuration pipeline = Pipen ( scheduler = \"local\" ) # Local execution pipeline = Pipen ( scheduler = \"sge\" ) # Sun Grid Engine pipeline = Pipen ( scheduler = \"slurm\" ) # SLURM pipeline = Pipen ( scheduler = \"gbatch\" ) # Google Cloud Batch # Via profile # .pipen.toml: # [default] # scheduler = \"slurm\" Custom Schedulers \u00b6 To create a custom scheduler: # myscheduler.py from xqute import Scheduler , JobStatus class MyScheduler ( Scheduler ): \"\"\"Custom scheduler implementation\"\"\" async def submit ( self , job ) -> None : # Submit job to your system pass async def poll ( self , job ) -> JobStatus : # Check job status return JobStatus . RUNNING Error Handling \u00b6 pipen provides multiple error handling strategies: Error Strategies \u00b6 # ignore: Continue with next job (default) pipeline = Pipen ( error_strategy = \"ignore\" ) # halt: Stop the entire pipeline pipeline = Pipen ( error_strategy = \"halt\" ) # retry: Retry failed jobs pipeline = Pipen ( error_strategy = \"retry\" ) Error Propagation \u00b6 graph TD A[Job Fails] --> B{Error Strategy} B -->|ignore| C[Mark Job Failed] C --> D[Continue Next Job] B -->|halt| E[Stop Pipeline] B -->|retry| F{Retries Left?} F -->|Yes| G[Resubmit Job] F -->|No| C G --> H{Succeeds?} H -->|No| F H -->|Yes| I[Mark Job Succeeded] Custom Error Handling \u00b6 class MyProc ( Proc ): \"\"\"Process with custom error handling\"\"\" input = \"data\" output = \"result:file\" script = \"\"\" python process.py {{ in.data }} {{ out.result }} \"\"\" # Custom error handling in script # Check exit code and handle accordingly Performance Considerations \u00b6 Parallel Execution \u00b6 # Forks: Number of concurrent jobs pipeline = Pipen ( forks = 4 ) # Run 4 jobs in parallel # Submission batch: Jobs to submit at once pipeline = Pipen ( submission_batch = 8 ) # Submit 8 jobs per batch Memory Management \u00b6 Each job runs in its own directory Output files are cleaned up after processing Cache files are stored in workdir I/O Optimization \u00b6 # Use cloud storage caching export CLOUDPATHLIB_LOCAL_CACHE_DIR =/ path / to / cache # Enable directory signatures for large outputs proc . dirsig = 1 # Use appropriate scheduler for workload # Local: Small jobs, rapid iteration # SLURM/SGE: Large compute jobs # Gbatch: Cloud-scale processing Further Reading \u00b6 Basics - Pipeline layers and folder structure Channels - Data flow between processes Configuration - Pipeline and process configuration Plugins - Plugin development guide Scheduler - Scheduler backends","title":"Architecture"},{"location":"architecture/#architecture","text":"This document explains the internal architecture of pipen, including how pipelines execute, how data flows through channels, the plugin system, caching mechanism, and template engine.","title":"Architecture"},{"location":"architecture/#overview","text":"pipen is built on a layered architecture with three main components: Pipeline Layer : Orchestrates the overall workflow Process Layer : Defines individual computational steps Job Layer : Executes individual tasks graph TD A[Pipeline] --> B[Process 1] A --> C[Process 2] A --> D[Process N] B --> E[Job 1.1] B --> F[Job 1.2] B --> G[Job 1.N] C --> H[Job 2.1] C --> I[Job 2.2] D --> J[Job N.1]","title":"Overview"},{"location":"architecture/#pipeline-execution-flow","text":"","title":"Pipeline Execution Flow"},{"location":"architecture/#1-initialization","text":"When a pipeline is created, the following sequence occurs: # Pipeline creation pipeline = Pipen ( name = \"MyPipeline\" , starts = [ Proc1 , Proc2 ]) # Triggers plugin hooks: # 1. on_setup() - Called once when first pipeline is loaded # 2. on_init() - Called for each pipeline initialization","title":"1. Initialization"},{"location":"architecture/#2-dependency-resolution","text":"pipen automatically infers process dependencies based on the requires attribute: class P1 ( Proc ): \"\"\"First process\"\"\" pass class P2 ( Proc ): \"\"\"Second process\"\"\" requires = P1 # P2 depends on P1's output The pipeline builds a directed acyclic graph (DAG) of processes and validates that: - No circular dependencies exist - All dependencies are satisfied - Processes can be topologically sorted","title":"2. Dependency Resolution"},{"location":"architecture/#3-input-data-computation","text":"For each process, pipen computes the input channel: # Plugin hook called await plugin . on_proc_input_computed ( proc ) The input channel is built from: - input_data attribute (static data) - Output channel from required processes - Transformed input via custom functions","title":"3. Input Data Computation"},{"location":"architecture/#4-job-generation","text":"For each row in the input channel, pipen creates a job: graph LR A[Input Channel] --> B[Process] B --> C[Job 1] B --> D[Job 2] B --> E[Job N] C --> F[Output Channel] D --> F E --> F Each job receives: - A subset of input data (one row from the channel) - A working directory - A script template to execute","title":"4. Job Generation"},{"location":"architecture/#5-script-rendering","text":"The process script template is rendered with job-specific data: # Plugin hook called await plugin . on_proc_script_computed ( proc ) Template variables include: - in.* : Input variables and their values - out.* : Output variable placeholders - proc.* : Process metadata - envs : Environment variables (options, not shell Environment Variables) that are shared across all jobs of the process","title":"5. Script Rendering"},{"location":"architecture/#6-job-submission-and-execution","text":"Jobs are submitted to the configured scheduler: # Built-in schedulers: # - Local: Execute jobs on the local machine # - SGE: Submit to Sun Grid Engine # - SLURM: Submit to SLURM workload manager # - SSH: Execute jobs on remote servers # - Container: Run jobs in containers # - Gbatch: Submit to Google Cloud Batch Job lifecycle: stateDiagram-v2 [*] --> Initiated Initiated --> Queued Queued --> Submitted Submitted --> Running Running --> Succeeded Running --> Failed Succeeded --> Cached: Job was cached Failed --> Retrying: If num_retries > 0 Retrying --> Submitted Failed --> [*] Succeeded --> [*]","title":"6. Job Submission and Execution"},{"location":"architecture/#7-output-collection","text":"When a job completes: - Output files are collected - Output signature is computed - Results are added to the output channel","title":"7. Output Collection"},{"location":"architecture/#8-pipeline-completion","text":"After all jobs complete: # Plugin hook called await plugin . on_complete ( pipeline , succeeded ) The pipeline: - Reports final status - Generates output summary - Cleans up temporary resources","title":"8. Pipeline Completion"},{"location":"architecture/#channel-data-flow","text":"Channels are pandas DataFrames that structure data flow between processes.","title":"Channel Data Flow"},{"location":"architecture/#channel-structure","text":"# A channel with 3 rows and 2 columns import pandas as pd from pipen import Channel df = pd . DataFrame ({ 'file' : [ 'data1.txt' , 'data2.txt' , 'data3.txt' ], 'sample' : [ 'sample1' , 'sample2' , 'sample3' ] }) channel = Channel . create ( df )","title":"Channel Structure"},{"location":"architecture/#row-based-parallelism","text":"Each row in a channel becomes an independent job: # Input channel: # file sample # 0 data1.txt sample1 # 1 data2.txt sample2 # 2 data3.txt sample3 # Generates 3 jobs, each with: # Job 1: in.file = \"data1.txt\", in.sample = \"sample1\" # Job 2: in.file = \"data2.txt\", in.sample = \"sample2\" # Job 3: in.file = \"data3.txt\", in.sample = \"sample3\"","title":"Row-Based Parallelism"},{"location":"architecture/#channel-operations","text":"","title":"Channel Operations"},{"location":"architecture/#creating-channels","text":"# From list Channel . create ([ 1 , 2 , 3 ]) # 3 rows, 1 column # From glob pattern Channel . from_glob ( \"data/*.txt\" , sortby = \"mtime\" ) # From existing DataFrame Channel . create ( my_dataframe )","title":"Creating Channels"},{"location":"architecture/#transforming-channels","text":"# Expand directory to files expanded = channel . expand_dir ( 'file' ) # Collapse files to directory collapsed = channel . collapse_files ( 'file' ) # Select columns selected = channel [[ 'file' , 'sample' ]] # Filter rows filtered = channel [ channel [ 'type' ] == 'A' ]","title":"Transforming Channels"},{"location":"architecture/#channel-data-types","text":"pipen supports several input/output types: Type Syntax Description var input = \"name\" In-memory variable file input = \"name:file\" Single file path dir input = \"name:dir\" Single directory path files input = \"name:files\" Multiple file paths dirs input = \"name:dirs\" Multiple directory paths","title":"Channel Data Types"},{"location":"architecture/#plugin-system","text":"pipen uses the simplug library for a flexible plugin system with hook-based extensibility.","title":"Plugin System"},{"location":"architecture/#plugin-hooks","text":"","title":"Plugin Hooks"},{"location":"architecture/#pipeline-level-hooks","text":"@plugin . spec def on_setup ( pipen : Pipen ) -> None : \"\"\"Called once when first pipeline is loaded. Use this to set default configurations. \"\"\" @plugin . spec async def on_init ( pipen : Pipen ) -> None : \"\"\"Called when a pipeline is initialized. Use this to access pipeline configuration. \"\"\" @plugin . spec async def on_start ( pipen : Pipen ) -> None : \"\"\"Called right before pipeline starts. Process relationships are already inferred. \"\"\" @plugin . spec async def on_complete ( pipen : Pipen , succeeded : bool ) -> None : \"\"\"Called when pipeline finishes. succeeded indicates whether pipeline succeeded. \"\"\"","title":"Pipeline-Level Hooks"},{"location":"architecture/#process-level-hooks","text":"@plugin . spec def on_proc_create ( proc : Proc ) -> None : \"\"\"Called when a process is created. Use this to modify default attributes. \"\"\" @plugin . spec async def on_proc_input_computed ( proc : Proc ) -> None : \"\"\"Called after process input data is computed. Use this to transform input data. \"\"\" @plugin . spec async def on_proc_script_computed ( proc : Proc ) -> None : \"\"\"Called after process script is computed. Use this to modify the script template. \"\"\" @plugin . spec async def on_proc_start ( proc : Proc ) -> None : \"\"\"Called when a process starts running. \"\"\" @plugin . spec async def on_proc_end ( proc : Proc , succeeded : bool ) -> None : \"\"\"Called when a process finishes. succeeded indicates whether process succeeded. \"\"\"","title":"Process-Level Hooks"},{"location":"architecture/#job-level-hooks","text":"@plugin . spec async def on_job_init ( job : Job ) -> None : \"\"\"Called when a job is created. \"\"\" @plugin . spec async def on_job_submitted ( job : Job ) -> None : \"\"\"Called when a job is submitted to scheduler. \"\"\" @plugin . spec async def on_job_running ( job : Job ) -> None : \"\"\"Called when a job starts running. \"\"\" @plugin . spec async def on_job_succeeded ( job : Job ) -> None : \"\"\"Called when a job succeeds. \"\"\" @plugin . spec async def on_job_failed ( job : Job , error : Exception ) -> None : \"\"\"Called when a job fails. error contains the exception. \"\"\"","title":"Job-Level Hooks"},{"location":"architecture/#creating-a-plugin","text":"# myplugin/__init__.py from pipen.pluginmgr import plugin @plugin . impl def on_proc_create ( proc ): \"\"\"Modify process attributes on creation.\"\"\" proc . config . forks = 4 # Set default to 4 parallel jobs # In pyproject.toml: # [tool.poetry.plugins.\"pipen\"] # myplugin = \"myplugin:main\"","title":"Creating a Plugin"},{"location":"architecture/#caching-mechanism","text":"pipen implements intelligent job caching to avoid redundant computations.","title":"Caching Mechanism"},{"location":"architecture/#cache-key-computation","text":"The cache key (signature) is computed from: Input data : File paths, directory paths, or in-memory values Script content : Hash of the rendered script Process metadata : Configuration values that affect output # Signature file location { workdir } / { proc . name } / { job . index } / job . signature . toml # Example signature: [ signature ] input_hash = \"a1b2c3d4...\" script_hash = \"e5f6g7h8...\" config_hash = \"i9j0k1l2...\"","title":"Cache Key Computation"},{"location":"architecture/#cache-check-process","text":"When a job starts: graph TD A[Job Starts] --> B{Cached?} B -->|Yes| C[Load Cached Output] B -->|No| D[Execute Script] D --> E[Compute Output Signature] E --> F[Write Cache] C --> G[Return Results] F --> G","title":"Cache Check Process"},{"location":"architecture/#cache-invalidation","text":"A cache is invalid if: Input files are modified (based on modification time) Script content changes Process configuration changes Output files are missing or corrupted","title":"Cache Invalidation"},{"location":"architecture/#directory-signatures","text":"pipen supports directory-level signatures ( dirsig ): # If dirsig is set: # - Cache considers all files in the directory # - Any file modification invalidates cache # - Useful for directory outputs with multiple files proc . dirsig = 1 # Enable directory signature","title":"Directory Signatures"},{"location":"architecture/#template-engine","text":"pipen uses template engines to render job scripts dynamically.","title":"Template Engine"},{"location":"architecture/#supported-template-engines","text":"Liquid (default): Safe, production-ready templating Jinja2 : Feature-rich Python templating Mako : High-performance Python templating","title":"Supported Template Engines"},{"location":"architecture/#template-variables","text":"Inside a script template, you have access to: # Input variables {{ in . input_file }} # Value of 'input_file' input {{ in . config_value }} # Value of 'config_value' input # Output placeholders {{ out . output_file }} # Where to save output # Process metadata {{ proc . name }} # Process name {{ proc . workdir }} # Process workdir {{ proc . index }} # Job index # Template options (defined in proc.envs) # Note: These are NOT shell environment variables like PATH or HOME # They are custom options passed to template rendering context {{ envs .* }}","title":"Template Variables"},{"location":"architecture/#example-templates","text":"","title":"Example Templates"},{"location":"architecture/#bash-template","text":"script = \"\"\" # Process {{ in.input_file }} cat {{ in.input_file }} > {{ out.output_file }} # Report progress echo \"Processed: {{ in.input_file }}\" \"\"\"","title":"Bash Template"},{"location":"architecture/#python-template","text":"lang = \"python\" script = \"\"\" import pandas as pd # Read input df = pd.read_csv('{{ in.input_file }}') # Process data df['processed'] = df['value'] * 2 # Save output df.to_csv('{{ out.output_file }}', index=False) \"\"\"","title":"Python Template"},{"location":"architecture/#using-filters","text":"# Built-in filters {{ in . path | basename }} # Get filename from path {{ in . path | dirname }} # Get directory from path {{ in . value | upper }} # Convert to uppercase","title":"Using Filters"},{"location":"architecture/#using-template-options-procenvs","text":"# Define template options in process class MyProc ( Proc ): envs = { \"custom_var\" : \"my_value\" , \"threshold\" : 100 } script = \"\"\" echo \"Custom value: {{ envs.custom_var }}\" echo \"Threshold: {{ envs.threshold }}\" \"\"\" # Note: envs are NOT shell environment variables (PATH, HOME, etc.) # They are custom options passed to the template rendering context","title":"Using Template Options (proc.envs)"},{"location":"architecture/#scheduler-architecture","text":"pipen supports multiple schedulers through a unified interface.","title":"Scheduler Architecture"},{"location":"architecture/#scheduler-interface","text":"All schedulers implement the xqute.Scheduler interface: class Scheduler ( ABC ): \"\"\"Base class for schedulers\"\"\" async def submit ( self , job : Job ) -> None : \"\"\"Submit a job for execution\"\"\" async def kill ( self , job : Job ) -> None : \"\"\"Kill a running job\"\"\" async def poll ( self , job : Job ) -> JobStatus : \"\"\"Check job status\"\"\" async def clean ( self , job : Job ) -> None : \"\"\"Clean up job resources\"\"\"","title":"Scheduler Interface"},{"location":"architecture/#scheduler-selection","text":"# Via configuration pipeline = Pipen ( scheduler = \"local\" ) # Local execution pipeline = Pipen ( scheduler = \"sge\" ) # Sun Grid Engine pipeline = Pipen ( scheduler = \"slurm\" ) # SLURM pipeline = Pipen ( scheduler = \"gbatch\" ) # Google Cloud Batch # Via profile # .pipen.toml: # [default] # scheduler = \"slurm\"","title":"Scheduler Selection"},{"location":"architecture/#custom-schedulers","text":"To create a custom scheduler: # myscheduler.py from xqute import Scheduler , JobStatus class MyScheduler ( Scheduler ): \"\"\"Custom scheduler implementation\"\"\" async def submit ( self , job ) -> None : # Submit job to your system pass async def poll ( self , job ) -> JobStatus : # Check job status return JobStatus . RUNNING","title":"Custom Schedulers"},{"location":"architecture/#error-handling","text":"pipen provides multiple error handling strategies:","title":"Error Handling"},{"location":"architecture/#error-strategies","text":"# ignore: Continue with next job (default) pipeline = Pipen ( error_strategy = \"ignore\" ) # halt: Stop the entire pipeline pipeline = Pipen ( error_strategy = \"halt\" ) # retry: Retry failed jobs pipeline = Pipen ( error_strategy = \"retry\" )","title":"Error Strategies"},{"location":"architecture/#error-propagation","text":"graph TD A[Job Fails] --> B{Error Strategy} B -->|ignore| C[Mark Job Failed] C --> D[Continue Next Job] B -->|halt| E[Stop Pipeline] B -->|retry| F{Retries Left?} F -->|Yes| G[Resubmit Job] F -->|No| C G --> H{Succeeds?} H -->|No| F H -->|Yes| I[Mark Job Succeeded]","title":"Error Propagation"},{"location":"architecture/#custom-error-handling","text":"class MyProc ( Proc ): \"\"\"Process with custom error handling\"\"\" input = \"data\" output = \"result:file\" script = \"\"\" python process.py {{ in.data }} {{ out.result }} \"\"\" # Custom error handling in script # Check exit code and handle accordingly","title":"Custom Error Handling"},{"location":"architecture/#performance-considerations","text":"","title":"Performance Considerations"},{"location":"architecture/#parallel-execution","text":"# Forks: Number of concurrent jobs pipeline = Pipen ( forks = 4 ) # Run 4 jobs in parallel # Submission batch: Jobs to submit at once pipeline = Pipen ( submission_batch = 8 ) # Submit 8 jobs per batch","title":"Parallel Execution"},{"location":"architecture/#memory-management","text":"Each job runs in its own directory Output files are cleaned up after processing Cache files are stored in workdir","title":"Memory Management"},{"location":"architecture/#io-optimization","text":"# Use cloud storage caching export CLOUDPATHLIB_LOCAL_CACHE_DIR =/ path / to / cache # Enable directory signatures for large outputs proc . dirsig = 1 # Use appropriate scheduler for workload # Local: Small jobs, rapid iteration # SLURM/SGE: Large compute jobs # Gbatch: Cloud-scale processing","title":"I/O Optimization"},{"location":"architecture/#further-reading","text":"Basics - Pipeline layers and folder structure Channels - Data flow between processes Configuration - Pipeline and process configuration Plugins - Plugin development guide Scheduler - Scheduler backends","title":"Further Reading"},{"location":"basics/","text":"Layers of a pipeline \u00b6 The pipeline consists of channels and processes. A process may have many jobs. Each job uses the corresponding elements from the input channel of the process (a row of the input channel/dataframe), and generates values for output channel. Actually, what you need to do is just specify the first input channel, and then tell pipen the dependencies of the processes. The later processes will use the output channel of the processes they depend on. Of course, you can also modify the output channel to match the input of the next processes, using functions. Folder structure \u00b6 ./ |- pipeline.py `- <pipeline-workdir>/ `- <pipeline-name>/ |- proc.name `- <job.index>/ |- input/ |- output/ |- job.signature.toml |- job.script |- job.rc |- job.stdout |- job.stderr |- job.status `- job.wrapped.<scheduler> Path Content Memo <pipeline-workdir> Where the pipeline directories of all processes of current pipeline are located. Can be set by workdir <pipeline-name> The slugified name of the pipeline. <job.index>/ The job directory Starts with 0 <job.index>/output/ Where you can find all the output files If this is an end process, it should be a link to the output directory of this process of the pipeline <job.index>/job.signature.toml The signature file of the job, used to check if job is cached <job.index>/job.script The rendered script file <job.index>/job.rc To file containing the return code <job.index>/job.stdout The STDOUT of the script <job.index>/job.stderr The STDERR of the script <job.index>/job.statis The status of the job <job.index>/job.wrapped.<scheduler> The wrapper for the scheduler to wrap the script","title":"Basics"},{"location":"basics/#layers-of-a-pipeline","text":"The pipeline consists of channels and processes. A process may have many jobs. Each job uses the corresponding elements from the input channel of the process (a row of the input channel/dataframe), and generates values for output channel. Actually, what you need to do is just specify the first input channel, and then tell pipen the dependencies of the processes. The later processes will use the output channel of the processes they depend on. Of course, you can also modify the output channel to match the input of the next processes, using functions.","title":"Layers of a pipeline"},{"location":"basics/#folder-structure","text":"./ |- pipeline.py `- <pipeline-workdir>/ `- <pipeline-name>/ |- proc.name `- <job.index>/ |- input/ |- output/ |- job.signature.toml |- job.script |- job.rc |- job.stdout |- job.stderr |- job.status `- job.wrapped.<scheduler> Path Content Memo <pipeline-workdir> Where the pipeline directories of all processes of current pipeline are located. Can be set by workdir <pipeline-name> The slugified name of the pipeline. <job.index>/ The job directory Starts with 0 <job.index>/output/ Where you can find all the output files If this is an end process, it should be a link to the output directory of this process of the pipeline <job.index>/job.signature.toml The signature file of the job, used to check if job is cached <job.index>/job.script The rendered script file <job.index>/job.rc To file containing the return code <job.index>/job.stdout The STDOUT of the script <job.index>/job.stderr The STDERR of the script <job.index>/job.statis The status of the job <job.index>/job.wrapped.<scheduler> The wrapper for the scheduler to wrap the script","title":"Folder structure"},{"location":"caching/","text":"Job caching \u00b6 If cache set to False (detected in the sequence of configuration files, Pipen constructor, and process definition), the job is running anyway regardless of previous runs. If a previous run of a job fails, the job will be running anyway. If a job is done successfully, a signature file will be generated for the job. When we try to run the job again, the signature will be used to check if we can skip running the job again but to use the results generated by previous run. We can also do a force-cache for a job by setting cache to \"force\" . This make sure of the results of previous successful run regardless of input or script changes. This is useful for the cases that, for example, you make some changes to input/script, but you don't want them to take effect immediately, especially when the job takes long time to run. Job signature \u00b6 The signature of a job consists of input types and data, output types and data, and lastest time ( lastest_time ) any files/directories from the script, input or output files are generated/modified. So these siutations will make job-cache checking fail (job will start over): Any changes in input or output types Any changes in input or output data Any changes to script Any touches to input files (since they will make the last modified time > lastest_time ) Any touches to input directories Use dirsig as the depth to check the files under the directories Otherwise if it is 0 , only the directories themselves are checked. Note that modify a file inside a directory may not change the last modified time of the directory itself. Any deletions to the output files/directories Note that only the files/directories specified by output are checked. Files or subdirectories in the output directories will NOT be checked.","title":"Caching"},{"location":"caching/#job-caching","text":"If cache set to False (detected in the sequence of configuration files, Pipen constructor, and process definition), the job is running anyway regardless of previous runs. If a previous run of a job fails, the job will be running anyway. If a job is done successfully, a signature file will be generated for the job. When we try to run the job again, the signature will be used to check if we can skip running the job again but to use the results generated by previous run. We can also do a force-cache for a job by setting cache to \"force\" . This make sure of the results of previous successful run regardless of input or script changes. This is useful for the cases that, for example, you make some changes to input/script, but you don't want them to take effect immediately, especially when the job takes long time to run.","title":"Job caching"},{"location":"caching/#job-signature","text":"The signature of a job consists of input types and data, output types and data, and lastest time ( lastest_time ) any files/directories from the script, input or output files are generated/modified. So these siutations will make job-cache checking fail (job will start over): Any changes in input or output types Any changes in input or output data Any changes to script Any touches to input files (since they will make the last modified time > lastest_time ) Any touches to input directories Use dirsig as the depth to check the files under the directories Otherwise if it is 0 , only the directories themselves are checked. Note that modify a file inside a directory may not change the last modified time of the directory itself. Any deletions to the output files/directories Note that only the files/directories specified by output are checked. Files or subdirectories in the output directories will NOT be checked.","title":"Job signature"},{"location":"channels/","text":"Channels are used to pass data from one process to another. It is actually a pandas.DataFrame object, where each column corresponds to an input key and each row corresponds to a job. The values for different variables in different jobs wil be: Job Index v1 v2 v3 0 a1 b1 c1 1 a2 b2 c2 ... ... ... ... With a process definition: class MyProcess ( Proc ): input = \"v1, v2, v3\" input_data = df # The above data frame Then: Job index Template Rendered to 0 {{in.v1}} a1 0 {{in.v2}} b1 0 {{in.v3}} c1 1 {{in.v1}} a2 1 {{in.v2}} b2 1 {{in.v3}} c2 ... ... ... The column names don't have to match the exact input keys. If pipen finds any of the input keys present in the data, just use them. However, if any input keys cannot find in the data frame, we will use the first couple of columns. For example: class MyProcess2 ( Proc ): input = \"v4, v3\" input_data = df # The above data frame The for job#0, {{in.v4}} will be rendered as a1 (using column v1 in the data), and {{in.v3}} as c1 (using column v3 ). Creating channels \u00b6 Since channels are just data frames, so whatever creates a pandas data frame, can be used to create a channel. Besides, a couple of class methods are avaible to create channels: Channel.create(...) This takes a list of values to create a channel. If a data frame is passed, will return that data frame. If each element in the list is a tuple, the list is used to create a data frame directly, just like: from pandas import DataFrame ch = Channel . create ([( 1 , 2 ), ( 3 , 4 )]) # ch = DataFrame([(1,2), (3,4)]) # 0 1 # <int64> <int64> # 0 1 2 # 1 3 4 If each element is not a tuple (even it is a list), it is converted to tuple: ch = Channel . create ([ 1 , 2 ]) # equvalent to: # ch = Channel.create([(1, ), (2, )]) The input_data is passed to this class method to create the input channel. Channel.from_glob(...) (async version: Channel.a_from_glob(...) ) This takes a glob pattern to match the files to create a single-column channel. You can also filter the types of files by ftype : - any : to match any files (default) - link : to mach any links - dir : to match any directories - file : to match any files You may also sort the files using sortby : - name : sort the files by their basename (default) - mtime : sort the files by their last modified time - size : sort by file size When reverse is True, the above sortings are reversed. Channel.from_pairs(...) (async version: Channel.a_from_pairs(...) ) Like Channel.from_glob() but create a double-column channel. Channel.from_csv(...) Uses pandas.read_csv() to create a channel Channel.from_excel(...) Uses pandas.read_excel() to create a channel Channel.from_table(...) Uses pandas.read_table() to create a channel Builtin verbs/functions to transform channels \u00b6 pipen uses pipda to create some verbs/functions to transform channels, so that you can use them with piping syntax: channel >> verb ( ... ) Expanding a channel by directory: expand_dir() \u00b6 Sometimes we prepare files in one process (for example, split a big file into small ones in a directory), then handle these files by different jobs in another process, so that they can be processed simultaneously. For example: class P1 ( Proc ): # the original file: a.txt input = \"infile:file\" input_data = [ \"a.txt\" ] output = \"outdir:dir:outdir\" script = \"# the script to split a.txt to 1.txt, 2.txt, 3.txt ... to {{out.outdir}}\" class P2 ( Proc ): requires = P1 # expand channel [(\"outdir/a/\",)] to channel: # [(\"outdir/a/1.txt\",), (\"outdir/a/2.txt\",), (\"outdir/a/3.txt\",), ...] input = \"infile:file\" input_data = lambda ch : ch >> expand_dir ( pattern = \"*.txt\" ) # outfile: 1.result, 2.result, ... output = \"outfile:file:{{in.infile.split('/')[-1].split('.')[0]}}.result\" script = \"\"\" # work on {{in.infile}} (1.txt, 2.txt, 3.txt, ...) # to result file {{out.outfile}} (1.result, 2.result, 3.result, ...) \"\"\" # Run 3 jobs in a batch simultaneously Pipen ( forks = 3 ) . run ( P1 ) If the channel is a multi-column channel, you can also specify col to expand only on that column, values of other columns will be copied to the expanded rows/jobs. You can also filter and sort the expanded files using arguments ftype , sortby and reverse , just like when we use Channel.from_glob(...) Caution expand_dir(...) only works for single-row channels, which will be expanded to N (number of files included). If original channel has more than 1 row, only first row will be used, and other rows will be ignored. Only the value of the column to be expanded will be changed, values of other columns remain the same. Collapsing a channel by files in a common ancestor directory: collapse_files(...) \u00b6 It's basically the reverse process of expand_dir() . It applies when you deal with different files and in next process you need them all involved (i.e. combine the results): For example: class P1 ( Proc ): input = \"infile:file\" input_data = [ \"/a/b/1.txt\" , \"/a/b/2.txt\" , \"/a/b/3.txt\" ] output = \"outfile:file:{{in.infile.split('/')[-1].split('.')[0] | append: '.txt2'}}\" script = \"\"\" # the script to deal with each input file: # {{in.infile}} -> {{out.outfile}} \"\"\" class P2 ( Proc ): requires = P1 # collapse channel [(\"<outdir>/1.txt2\",), (\"<outdir>/2.txt2\",), (\"<outdir>/3.txt2\",)] # to channel: [(\"<outdir>/\", )] input = \"indir:file\" input_data = lambda ch : ch >> collapse_files () output = \"outfile:file:{{in.indir.split('/')[-1]}}.result\" script = \"\"\" # combine 1.txt2, 2.txt2, 3.txt3 in {{in.indir}} to {{out.outfile}} \"\"\" Pipen () . run ( P1 ) Similarly, if we have multiple columns, you may specify the column by index or name to collapse by: ch >> collapse_files(col=...) Caution os.path.dirname(os.path.commonprefix(...)) is used to detect the common ancestor directory, so the files could be ['/a/1/1.file', '/a/2/1.file'] . In this case /a/ will be returned. values at other columns should be the same. They will NOT be checked! The values at the first row will be used.","title":"Channels"},{"location":"channels/#creating-channels","text":"Since channels are just data frames, so whatever creates a pandas data frame, can be used to create a channel. Besides, a couple of class methods are avaible to create channels: Channel.create(...) This takes a list of values to create a channel. If a data frame is passed, will return that data frame. If each element in the list is a tuple, the list is used to create a data frame directly, just like: from pandas import DataFrame ch = Channel . create ([( 1 , 2 ), ( 3 , 4 )]) # ch = DataFrame([(1,2), (3,4)]) # 0 1 # <int64> <int64> # 0 1 2 # 1 3 4 If each element is not a tuple (even it is a list), it is converted to tuple: ch = Channel . create ([ 1 , 2 ]) # equvalent to: # ch = Channel.create([(1, ), (2, )]) The input_data is passed to this class method to create the input channel. Channel.from_glob(...) (async version: Channel.a_from_glob(...) ) This takes a glob pattern to match the files to create a single-column channel. You can also filter the types of files by ftype : - any : to match any files (default) - link : to mach any links - dir : to match any directories - file : to match any files You may also sort the files using sortby : - name : sort the files by their basename (default) - mtime : sort the files by their last modified time - size : sort by file size When reverse is True, the above sortings are reversed. Channel.from_pairs(...) (async version: Channel.a_from_pairs(...) ) Like Channel.from_glob() but create a double-column channel. Channel.from_csv(...) Uses pandas.read_csv() to create a channel Channel.from_excel(...) Uses pandas.read_excel() to create a channel Channel.from_table(...) Uses pandas.read_table() to create a channel","title":"Creating channels"},{"location":"channels/#builtin-verbsfunctions-to-transform-channels","text":"pipen uses pipda to create some verbs/functions to transform channels, so that you can use them with piping syntax: channel >> verb ( ... )","title":"Builtin verbs/functions to transform channels"},{"location":"channels/#expanding-a-channel-by-directory-expand_dir","text":"Sometimes we prepare files in one process (for example, split a big file into small ones in a directory), then handle these files by different jobs in another process, so that they can be processed simultaneously. For example: class P1 ( Proc ): # the original file: a.txt input = \"infile:file\" input_data = [ \"a.txt\" ] output = \"outdir:dir:outdir\" script = \"# the script to split a.txt to 1.txt, 2.txt, 3.txt ... to {{out.outdir}}\" class P2 ( Proc ): requires = P1 # expand channel [(\"outdir/a/\",)] to channel: # [(\"outdir/a/1.txt\",), (\"outdir/a/2.txt\",), (\"outdir/a/3.txt\",), ...] input = \"infile:file\" input_data = lambda ch : ch >> expand_dir ( pattern = \"*.txt\" ) # outfile: 1.result, 2.result, ... output = \"outfile:file:{{in.infile.split('/')[-1].split('.')[0]}}.result\" script = \"\"\" # work on {{in.infile}} (1.txt, 2.txt, 3.txt, ...) # to result file {{out.outfile}} (1.result, 2.result, 3.result, ...) \"\"\" # Run 3 jobs in a batch simultaneously Pipen ( forks = 3 ) . run ( P1 ) If the channel is a multi-column channel, you can also specify col to expand only on that column, values of other columns will be copied to the expanded rows/jobs. You can also filter and sort the expanded files using arguments ftype , sortby and reverse , just like when we use Channel.from_glob(...) Caution expand_dir(...) only works for single-row channels, which will be expanded to N (number of files included). If original channel has more than 1 row, only first row will be used, and other rows will be ignored. Only the value of the column to be expanded will be changed, values of other columns remain the same.","title":"Expanding a channel by directory: expand_dir()"},{"location":"channels/#collapsing-a-channel-by-files-in-a-common-ancestor-directory-collapse_files","text":"It's basically the reverse process of expand_dir() . It applies when you deal with different files and in next process you need them all involved (i.e. combine the results): For example: class P1 ( Proc ): input = \"infile:file\" input_data = [ \"/a/b/1.txt\" , \"/a/b/2.txt\" , \"/a/b/3.txt\" ] output = \"outfile:file:{{in.infile.split('/')[-1].split('.')[0] | append: '.txt2'}}\" script = \"\"\" # the script to deal with each input file: # {{in.infile}} -> {{out.outfile}} \"\"\" class P2 ( Proc ): requires = P1 # collapse channel [(\"<outdir>/1.txt2\",), (\"<outdir>/2.txt2\",), (\"<outdir>/3.txt2\",)] # to channel: [(\"<outdir>/\", )] input = \"indir:file\" input_data = lambda ch : ch >> collapse_files () output = \"outfile:file:{{in.indir.split('/')[-1]}}.result\" script = \"\"\" # combine 1.txt2, 2.txt2, 3.txt3 in {{in.indir}} to {{out.outfile}} \"\"\" Pipen () . run ( P1 ) Similarly, if we have multiple columns, you may specify the column by index or name to collapse by: ch >> collapse_files(col=...) Caution os.path.dirname(os.path.commonprefix(...)) is used to detect the common ancestor directory, so the files could be ['/a/1/1.file', '/a/2/1.file'] . In this case /a/ will be returned. values at other columns should be the same. They will NOT be checked! The values at the first row will be used.","title":"Collapsing a channel by files in a common ancestor directory: collapse_files(...)"},{"location":"cli/","text":"pipen has a CLI tool that you can run from command line. To run it: \u276f pipen --help Usage: pipen [ -h ] { version,profile,plugins,help } ... CLI Tool for pipen v0.4.2 Optional Arguments: -h, --help show help message and exit Subcommands: version Print versions of pipen and its dependencies profile List available profiles. plugins List installed plugins help Print help for commands Writing a plugin to extend the cli \u00b6 CLI plugin super class \u00b6 A CLI plugin has to be a subclass of pipen.cli.CLIPlugin or pipen.cli.AsyncCLIPlugin for async commands (see below). A CLI plugin has to define a name property, which also is the sub-command of the plugin. There are a couple of methods of pipen.cli.CLIPlugin to extend for a plugin: __init__(self, parser, subparser) : initialize the plugin It takes the main parser and the subparser of the sub-command as arguments. You can add arguments to the parser or subparser here. Check argx for more information about how to define arguments. parse_args(self, known_parsed, unparsed_argv) : parse the arguments It takes the known parsed arguments and the unparsed argument vector as arguments, allowing you to do custom parsing. It should return the parsed arguments. By default, known_parsed is returned. exec_command(self, args) : execute the command It takes the parsed arguments as argument. It should execute the command as you wish. Async CLI plugin \u00b6 Same as pipen.cli.CLIPlugin , but the parse_args and exec_command methods are async methods. In addition, in order to run some async code post initialization, you may also implement an async method post_init(self) that will be called after the plugin is initialized. loading CLI plugins \u00b6 Like pipen plugins , templates , and schedulers , there are two ways to load the CLI plugins: Use the plugin directly: from pipen.cli import cli_plugin cli_plugin . register ( < your plugin > ) Use the entry points with group name pipen_cli The profile subcommand \u00b6 It is used to list the configurations/profiles in current directory. Run pipen profile or pipen help profile to get more information. The plugins subcommand \u00b6 This subcommand is used to list the plugins for pipen itself, templates, scheduler and cli. Run pipen plugins or pipen help plugins to get more information. The version subcommand \u00b6 This command prints the versions of pipen and its dependencies. CLI plugin gallery \u00b6 pipen-cli-init : A pipen CLI plugin to create a pipen project (pipeline) pipen-cli-ref : Make reference documentation for processes pipen-cli-require : A pipen cli plugin check the requirements of a pipeline pipen-cli-run : A pipen cli plugin to run a process or a pipeline pipen-cli-gbatch : A pipen cli plugin to run pipeline with Google Batch Jobs","title":"Command line interface"},{"location":"cli/#writing-a-plugin-to-extend-the-cli","text":"","title":"Writing a plugin to extend the cli"},{"location":"cli/#cli-plugin-super-class","text":"A CLI plugin has to be a subclass of pipen.cli.CLIPlugin or pipen.cli.AsyncCLIPlugin for async commands (see below). A CLI plugin has to define a name property, which also is the sub-command of the plugin. There are a couple of methods of pipen.cli.CLIPlugin to extend for a plugin: __init__(self, parser, subparser) : initialize the plugin It takes the main parser and the subparser of the sub-command as arguments. You can add arguments to the parser or subparser here. Check argx for more information about how to define arguments. parse_args(self, known_parsed, unparsed_argv) : parse the arguments It takes the known parsed arguments and the unparsed argument vector as arguments, allowing you to do custom parsing. It should return the parsed arguments. By default, known_parsed is returned. exec_command(self, args) : execute the command It takes the parsed arguments as argument. It should execute the command as you wish.","title":"CLI plugin super class"},{"location":"cli/#async-cli-plugin","text":"Same as pipen.cli.CLIPlugin , but the parse_args and exec_command methods are async methods. In addition, in order to run some async code post initialization, you may also implement an async method post_init(self) that will be called after the plugin is initialized.","title":"Async CLI plugin"},{"location":"cli/#loading-cli-plugins","text":"Like pipen plugins , templates , and schedulers , there are two ways to load the CLI plugins: Use the plugin directly: from pipen.cli import cli_plugin cli_plugin . register ( < your plugin > ) Use the entry points with group name pipen_cli","title":"loading CLI plugins"},{"location":"cli/#the-profile-subcommand","text":"It is used to list the configurations/profiles in current directory. Run pipen profile or pipen help profile to get more information.","title":"The profile subcommand"},{"location":"cli/#the-plugins-subcommand","text":"This subcommand is used to list the plugins for pipen itself, templates, scheduler and cli. Run pipen plugins or pipen help plugins to get more information.","title":"The plugins subcommand"},{"location":"cli/#the-version-subcommand","text":"This command prints the versions of pipen and its dependencies.","title":"The version subcommand"},{"location":"cli/#cli-plugin-gallery","text":"pipen-cli-init : A pipen CLI plugin to create a pipen project (pipeline) pipen-cli-ref : Make reference documentation for processes pipen-cli-require : A pipen cli plugin check the requirements of a pipeline pipen-cli-run : A pipen cli plugin to run a process or a pipeline pipen-cli-gbatch : A pipen cli plugin to run pipeline with Google Batch Jobs","title":"CLI plugin gallery"},{"location":"cloud/","text":"Since v0.16.0 , pipen supports the cloud naively. There are two ways by means of cloud support: Run the pipeline locally (or schedulers like sge , slurm , etc.) and save the files to the cloud. Run the pipeline on the cloud. Run the pipeline locally and save the files to the cloud \u00b6 To run the pipeline locally and save the files to the cloud, you need to install pipen with cloud support: pip install xqute [ cloudsh ] # To support a specific cloud service provider pip install cloudpathlib [ s3 ] pip install cloudpathlib [ gs ] pip install cloudpathlib [ azure ] The you can directly assign a cloud path as a pipeline working directory: from pipen import Pipen , Proc , run class P1 ( Proc ): \"\"\"Sort input file\"\"\" input = \"in:var\" input_data = [ \"Hello World\" ] output = \"outfile:file:out.txt\" # Note that out.outfile is on the cloud but the script is executed locally # we can use cloudsh to save the output to the cloud script = \"echo {{in.in}} | cloudsh sink {{out.outfile}}\" class MyPipeline ( Pipen ): starts = P1 workdir = \"gs://mybucket/mypipeline/workdir\" output = \"gs://mybucket/mypipeline/output\" if __name__ == \"__main__\" : MyPipeline () . run () Like the following figure, the pipeline is run locally but the meta information is grabbed from and saved to the cloud (workdir). No local files are generated. For the output files, if a process is a non-export process, the output files are saved to the workdir. If a process is an export process, the output files are saved to the output directory (export dir). Run the pipeline on the cloud \u00b6 Currently, pipen only supports running the pipeline on the cloud with google batch jobs. To run the pipeline on the cloud, you need to install pipen with cloud support: pip install xqute [ gs ] It is used to communicate with google cloud storage files. No cloudsh is needed, since operating the cloud files will be happening on the cloud (with the cloud paths mounted to the VM). You also need to have google cloud sdk installed and configured, which is used to communicate with google batch jobs (submit jobs, get job status, etc.). from pipen import Pipen , Proc , run class P1 ( Proc ): \"\"\"Sort input file\"\"\" input = \"in:var\" input_data = [ \"Hello World\" ] output = \"outfile:file:out.txt\" # Note that out.outfile is on the cloud but the script is executed locally # we can use cloudsh to save the output to the cloud script = \"echo {{in.in}} | cloudsh sink {{out.outfile}}\" class MyPipeline ( Pipen ): starts = P1 workdir = \"gs://mybucket/mypipeline/workdir\" output = \"gs://mybucket/mypipeline/output\" scheduler = \"gbatch\" if __name__ == \"__main__\" : MyPipeline () . run () The only difference is that we need to set scheduler to gbatch (google batch jobs). As shown in the following figure, the pipeline is run on the cloud platform, and the workdir and export dir will be mounted to the VM. So the process script can directly access the cloud files, no cloudsh or gcloud tools are needed. Tip While the workdir and output dir are on the cloud, pipen needs to communicate with the cloud storage to get and save the meta information of the pipeline (workdir) and also upload/download files if needed. We are using yunpath to manage the cloud paths, which is a wrapper of cloudpathlib . cloudpathlib uses a cache mechanism to reduce the communication with the cloud storage, which can greatly improve the performance. If you are running into issues with the local cache (e.g. no space left), you can set the environment variable CLOUDPATHLIB_LOCAL_CACHE_DIR to change the cache directory. See more details in the cloudpathlib documentation .","title":"Cloud support"},{"location":"cloud/#run-the-pipeline-locally-and-save-the-files-to-the-cloud","text":"To run the pipeline locally and save the files to the cloud, you need to install pipen with cloud support: pip install xqute [ cloudsh ] # To support a specific cloud service provider pip install cloudpathlib [ s3 ] pip install cloudpathlib [ gs ] pip install cloudpathlib [ azure ] The you can directly assign a cloud path as a pipeline working directory: from pipen import Pipen , Proc , run class P1 ( Proc ): \"\"\"Sort input file\"\"\" input = \"in:var\" input_data = [ \"Hello World\" ] output = \"outfile:file:out.txt\" # Note that out.outfile is on the cloud but the script is executed locally # we can use cloudsh to save the output to the cloud script = \"echo {{in.in}} | cloudsh sink {{out.outfile}}\" class MyPipeline ( Pipen ): starts = P1 workdir = \"gs://mybucket/mypipeline/workdir\" output = \"gs://mybucket/mypipeline/output\" if __name__ == \"__main__\" : MyPipeline () . run () Like the following figure, the pipeline is run locally but the meta information is grabbed from and saved to the cloud (workdir). No local files are generated. For the output files, if a process is a non-export process, the output files are saved to the workdir. If a process is an export process, the output files are saved to the output directory (export dir).","title":"Run the pipeline locally and save the files to the cloud"},{"location":"cloud/#run-the-pipeline-on-the-cloud","text":"Currently, pipen only supports running the pipeline on the cloud with google batch jobs. To run the pipeline on the cloud, you need to install pipen with cloud support: pip install xqute [ gs ] It is used to communicate with google cloud storage files. No cloudsh is needed, since operating the cloud files will be happening on the cloud (with the cloud paths mounted to the VM). You also need to have google cloud sdk installed and configured, which is used to communicate with google batch jobs (submit jobs, get job status, etc.). from pipen import Pipen , Proc , run class P1 ( Proc ): \"\"\"Sort input file\"\"\" input = \"in:var\" input_data = [ \"Hello World\" ] output = \"outfile:file:out.txt\" # Note that out.outfile is on the cloud but the script is executed locally # we can use cloudsh to save the output to the cloud script = \"echo {{in.in}} | cloudsh sink {{out.outfile}}\" class MyPipeline ( Pipen ): starts = P1 workdir = \"gs://mybucket/mypipeline/workdir\" output = \"gs://mybucket/mypipeline/output\" scheduler = \"gbatch\" if __name__ == \"__main__\" : MyPipeline () . run () The only difference is that we need to set scheduler to gbatch (google batch jobs). As shown in the following figure, the pipeline is run on the cloud platform, and the workdir and export dir will be mounted to the VM. So the process script can directly access the cloud files, no cloudsh or gcloud tools are needed. Tip While the workdir and output dir are on the cloud, pipen needs to communicate with the cloud storage to get and save the meta information of the pipeline (workdir) and also upload/download files if needed. We are using yunpath to manage the cloud paths, which is a wrapper of cloudpathlib . cloudpathlib uses a cache mechanism to reduce the communication with the cloud storage, which can greatly improve the performance. If you are running into issues with the local cache (e.g. no space left), you can set the environment variable CLOUDPATHLIB_LOCAL_CACHE_DIR to change the cache directory. See more details in the cloudpathlib documentation .","title":"Run the pipeline on the cloud"},{"location":"configurations/","text":"Configuration items \u00b6 There are two levels of configuration items in pipen : pipeline level and process level. There are only 3 configuration items at pipeline level: loglevel : The logging level for the logger (Default: \"info\" ) workdir : Where the metadata and intermediate files are saved for the pipeline (Default: ./.pipen ) plugins : The plugins to be enabled or disabled for the pipeline These items cannot be set or changed at process level. Following items are at process level. They can be set changed at process level so that they can be process-specific. You may also see some of the configuration items introduced here cache : Should we detect whether the jobs are cached? See also here dirsig : When checking the signature for caching, whether should we walk through the content of the directory? This is sometimes time-consuming if the directory is big. error_strategy : How to deal with the errors: retry, ignore or halt. See also here num_retries : How many times to retry to jobs once error occurs. template : efine the template engine to use. See also here template_opts : Options to initialize the template engine (will inherit from pipeline level) forks : How many jobs to run simultaneously? lang : The language for the script to run. See also here plugin_opts : Options for process-level plugins, will inherit from pipeline level scheduler : The scheduler to run the jobs scheduler_opts : The options for the scheduler, will inherit from pipeline level submission_batch : How many jobs to be submited simultaneously Configuration priorities \u00b6 There are different places to set values for the configuration items (priorities from low to high): The configuration files (priorities from low to high): ~/.pipen.toml ./.pipen.toml PIPEN.osenv See here for how the configuration files are loaded. pipen uses TOML as configuration language, see here for more information about toml format. The arguments of Pipen constructor The process definition Note The configurations from configuration files are with profiles. If the same profile name appears in multiple configuration files, the items will be inherited from the lower-priority files. Note Special note for lang . If it is not set at process level, and there are shebang in the script, whatever you specified at pipeline level (including in the configuration files), it will be ignored and the interpreter in the shebang will be used. See also script Tip If you have nothing set at Pipen constructor or process definition for a configuration item, the PIPEN.osenv is useful to use a different value than the one set in other configuration files. For example, to disable cache for all processes: PIPEN_DEFAULT_cache=0 python ./pipeline.py ... Profiles \u00b6 You can have different profiles in configuration files: ~/.pipen.toml [default] scheduler = \"local\" [sge] scheduler = \"sge\" [sge.schduler_opts] sge_q = \"1-day\" To use the sge profile: Pipen () . run ( P1 , profile = \"sge\" ) You can also have a configuration in current directory: ./.pipen.toml [sge.scheduler_opts] sge_q = \"7-days\" Then the queue to run the jobs will be 7-days . Note that we didn't specify the scheduler in ./.pipen.toml , which is inherited from ~/.pipen.toml .","title":"Configurations"},{"location":"configurations/#configuration-items","text":"There are two levels of configuration items in pipen : pipeline level and process level. There are only 3 configuration items at pipeline level: loglevel : The logging level for the logger (Default: \"info\" ) workdir : Where the metadata and intermediate files are saved for the pipeline (Default: ./.pipen ) plugins : The plugins to be enabled or disabled for the pipeline These items cannot be set or changed at process level. Following items are at process level. They can be set changed at process level so that they can be process-specific. You may also see some of the configuration items introduced here cache : Should we detect whether the jobs are cached? See also here dirsig : When checking the signature for caching, whether should we walk through the content of the directory? This is sometimes time-consuming if the directory is big. error_strategy : How to deal with the errors: retry, ignore or halt. See also here num_retries : How many times to retry to jobs once error occurs. template : efine the template engine to use. See also here template_opts : Options to initialize the template engine (will inherit from pipeline level) forks : How many jobs to run simultaneously? lang : The language for the script to run. See also here plugin_opts : Options for process-level plugins, will inherit from pipeline level scheduler : The scheduler to run the jobs scheduler_opts : The options for the scheduler, will inherit from pipeline level submission_batch : How many jobs to be submited simultaneously","title":"Configuration items"},{"location":"configurations/#configuration-priorities","text":"There are different places to set values for the configuration items (priorities from low to high): The configuration files (priorities from low to high): ~/.pipen.toml ./.pipen.toml PIPEN.osenv See here for how the configuration files are loaded. pipen uses TOML as configuration language, see here for more information about toml format. The arguments of Pipen constructor The process definition Note The configurations from configuration files are with profiles. If the same profile name appears in multiple configuration files, the items will be inherited from the lower-priority files. Note Special note for lang . If it is not set at process level, and there are shebang in the script, whatever you specified at pipeline level (including in the configuration files), it will be ignored and the interpreter in the shebang will be used. See also script Tip If you have nothing set at Pipen constructor or process definition for a configuration item, the PIPEN.osenv is useful to use a different value than the one set in other configuration files. For example, to disable cache for all processes: PIPEN_DEFAULT_cache=0 python ./pipeline.py ...","title":"Configuration priorities"},{"location":"configurations/#profiles","text":"You can have different profiles in configuration files: ~/.pipen.toml [default] scheduler = \"local\" [sge] scheduler = \"sge\" [sge.schduler_opts] sge_q = \"1-day\" To use the sge profile: Pipen () . run ( P1 , profile = \"sge\" ) You can also have a configuration in current directory: ./.pipen.toml [sge.scheduler_opts] sge_q = \"7-days\" Then the queue to run the jobs will be 7-days . Note that we didn't specify the scheduler in ./.pipen.toml , which is inherited from ~/.pipen.toml .","title":"Profiles"},{"location":"defining-proc/","text":"A pipeline consists of many processes, which could own multiple jobs that run in parallel. Defining/Creating processes \u00b6 pipen has two (preferred) ways to define processes: Subclassing pipen.Proc \u00b6 from pipen import Proc class MyProcess ( Proc ): ... # process configurations The configurations are specified as class variables of the class. Using class method Proc.from_proc() \u00b6 If you want to reuse a defined process, you can either subclass it: class MyOtherProcess ( MyProcess ): ... # configurations inherited from MyProcess Or use Proc.from_proc() : # You can also pass the configurations you want to override MyOtherProcess = Proc . from_proc ( MyProcess , ... ) Note that Proc.from_proc() cannot override all configurations/class variables, because we assume that there are some shared configurations if you want to \"copy\" from another process. These shared configurations are: Template engine and its options ( template and template_opts ) Script template ( script ) Input keys ( input ) Language/Interpreter of the script ( lang ) Output keys ( output ) All other configurations can be passed to Proc.from_proc() to override the old ones. For all configurations/class variables for a process, see next section. You don't need to specify the new name of the new process, the variable name on the left-handle side will be used if name argument is not provided to Proc.from_proc() . For example: NewProc = Proc . from_proc ( OldProc ) # NewProc.name == \"NewProc\" But you are able to assign a different name to a new process if you want. For example: NewProc = Proc . from_proc ( OldProc , name = \"NewProc2\" ) # NewProc.name = \"NewProc2\" How about instantiation of Proc directly? \u00b6 You are not allowed to do that. Proc is an abstract class, which is designed to be subclassed. How about instantiation of a Proc subclass? \u00b6 Nope, in pipen , a process is a Proc subclass itself. The instances of the subcleasses are used internally, and they are singletons. In most cases, you don't need to use the instances, unless you want to access the computed properties of the instances, including: pipeline : The pipeline, which is a Pipen object pbar : The progress bar for the process, indicating the job status of this process jobs : The jobs of this process xqute : The Xqute object to manage the job running. template : The template engine (a pipen.template.Template object) template_opts : The template options (overwritten from config by the template_opts class variable) input : The sanitized input keys and types output : The compiled output template, ready for the jobs to render with their own data scheduler : The scheduler object (inferred from the name or sheduler object from the scheduler class variable) script : The compiled script template, ready for the jobs to render with their own data How about copy/deep-copy of a Proc subclass? \u00b6 Nope. Copy or deep-copy of a Proc subclass won't trigger __init_subclass__() , where consolidate the process name from the class name if not specified and connect the required processes with the current one. Copy or deep-copy keeps all properties, but disconnect the relationships between current process and the dependency processes, even with a separate assignment, such as MyProcess.requires = ... . process configurations and Proc class variables \u00b6 The configurations of a process are specified as class variables of subclasses of Proc . Name Meaning Can be overwritten by Proc.from_proc() name The name of the process. Will use the class name by default. Yes desc The description of the process. Will use the summary from the docstring by default. Yes envs The env variables that are job-independent, useful for common options across jobs. Yes, and old ones will be inherited cache Should we detect whether the jobs are cached? Yes dirsig When checking the signature for caching, the depth we should walk through the content of the directory? This is sometimes time-consuming if the directory and the depth are big. Yes export When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes Yes error_strategy How to deal with the errors: retry, ignore, halt Yes num_retries How many times to retry to jobs once error occurs Yes template Define the template engine to use. No template_opts Options to initialize the template engine. No forks How many jobs to run simultaneously? Yes input The keys and types for the input channel No input_data The input data (will be computed for dependent processes) Yes lang The language for the script to run. No order The execution order for the same dependency-level processes Yes output The output keys for the output channel No plugin_opts Options for process-level plugins Yes requires The dependency processes Yes scheduler The scheduler to run the jobs Yes scheduler_opts The options for the scheduler Yes script The script template for the process No submission_batch How many jobs to be submited simultaneously Yes","title":"Defining a process"},{"location":"defining-proc/#definingcreating-processes","text":"pipen has two (preferred) ways to define processes:","title":"Defining/Creating processes"},{"location":"defining-proc/#subclassing-pipenproc","text":"from pipen import Proc class MyProcess ( Proc ): ... # process configurations The configurations are specified as class variables of the class.","title":"Subclassing pipen.Proc"},{"location":"defining-proc/#using-class-method-procfrom_proc","text":"If you want to reuse a defined process, you can either subclass it: class MyOtherProcess ( MyProcess ): ... # configurations inherited from MyProcess Or use Proc.from_proc() : # You can also pass the configurations you want to override MyOtherProcess = Proc . from_proc ( MyProcess , ... ) Note that Proc.from_proc() cannot override all configurations/class variables, because we assume that there are some shared configurations if you want to \"copy\" from another process. These shared configurations are: Template engine and its options ( template and template_opts ) Script template ( script ) Input keys ( input ) Language/Interpreter of the script ( lang ) Output keys ( output ) All other configurations can be passed to Proc.from_proc() to override the old ones. For all configurations/class variables for a process, see next section. You don't need to specify the new name of the new process, the variable name on the left-handle side will be used if name argument is not provided to Proc.from_proc() . For example: NewProc = Proc . from_proc ( OldProc ) # NewProc.name == \"NewProc\" But you are able to assign a different name to a new process if you want. For example: NewProc = Proc . from_proc ( OldProc , name = \"NewProc2\" ) # NewProc.name = \"NewProc2\"","title":"Using class method Proc.from_proc()"},{"location":"defining-proc/#how-about-instantiation-of-proc-directly","text":"You are not allowed to do that. Proc is an abstract class, which is designed to be subclassed.","title":"How about instantiation of Proc directly?"},{"location":"defining-proc/#how-about-instantiation-of-a-proc-subclass","text":"Nope, in pipen , a process is a Proc subclass itself. The instances of the subcleasses are used internally, and they are singletons. In most cases, you don't need to use the instances, unless you want to access the computed properties of the instances, including: pipeline : The pipeline, which is a Pipen object pbar : The progress bar for the process, indicating the job status of this process jobs : The jobs of this process xqute : The Xqute object to manage the job running. template : The template engine (a pipen.template.Template object) template_opts : The template options (overwritten from config by the template_opts class variable) input : The sanitized input keys and types output : The compiled output template, ready for the jobs to render with their own data scheduler : The scheduler object (inferred from the name or sheduler object from the scheduler class variable) script : The compiled script template, ready for the jobs to render with their own data","title":"How about instantiation of a Proc subclass?"},{"location":"defining-proc/#how-about-copydeep-copy-of-a-proc-subclass","text":"Nope. Copy or deep-copy of a Proc subclass won't trigger __init_subclass__() , where consolidate the process name from the class name if not specified and connect the required processes with the current one. Copy or deep-copy keeps all properties, but disconnect the relationships between current process and the dependency processes, even with a separate assignment, such as MyProcess.requires = ... .","title":"How about copy/deep-copy of a Proc subclass?"},{"location":"defining-proc/#process-configurations-and-proc-class-variables","text":"The configurations of a process are specified as class variables of subclasses of Proc . Name Meaning Can be overwritten by Proc.from_proc() name The name of the process. Will use the class name by default. Yes desc The description of the process. Will use the summary from the docstring by default. Yes envs The env variables that are job-independent, useful for common options across jobs. Yes, and old ones will be inherited cache Should we detect whether the jobs are cached? Yes dirsig When checking the signature for caching, the depth we should walk through the content of the directory? This is sometimes time-consuming if the directory and the depth are big. Yes export When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes Yes error_strategy How to deal with the errors: retry, ignore, halt Yes num_retries How many times to retry to jobs once error occurs Yes template Define the template engine to use. No template_opts Options to initialize the template engine. No forks How many jobs to run simultaneously? Yes input The keys and types for the input channel No input_data The input data (will be computed for dependent processes) Yes lang The language for the script to run. No order The execution order for the same dependency-level processes Yes output The output keys for the output channel No plugin_opts Options for process-level plugins Yes requires The dependency processes Yes scheduler The scheduler to run the jobs Yes scheduler_opts The options for the scheduler Yes script The script template for the process No submission_batch How many jobs to be submited simultaneously Yes","title":"process configurations and Proc class variables"},{"location":"error/","text":"You can tell pipen how to handle when a job fails to run. You can specify one of the following to error_strategy halt : Any failure will just halt the whole pipeline ignore : Ignore the error and keep running (assuming the job runs successfully anyway) retry : Retry to job running After num_retries times of retrying, if the job is still failing, then halt the pipeline. pipen uses xqute to handle the errors. See also here .","title":"Error handling"},{"location":"examples/","text":"You can find the source code of the examples under directory examples/ in the github repository. Theses examples including: Caching \u00b6 When run the script the second time, you may see from the logs that jobs are cached: \u276f python examples/caching.py [09/13/21 06:10:03] I main _____________________________________ __ [09/13/21 06:10:03] I main ___ __ \\___ _/__ __ \\__ ____/__ | / / [09/13/21 06:10:03] I main __ /_/ /__ / __ /_/ /_ __/ __ |/ / [09/13/21 06:10:03] I main _ ____/__/ / _ ____/_ /___ _ /| / [09/13/21 06:10:03] I main /_/ /___/ /_/ /_____/ /_/ |_/ [09/13/21 06:10:03] I main [09/13/21 06:10:03] I main version: 0.1.0 [09/13/21 06:10:03] D main [09/13/21 06:10:03] D main Building process relationships: [09/13/21 06:10:03] D main - Start processes: [<Proc:AProcess>] [09/13/21 06:10:03] I main [09/13/21 06:10:03] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 PIPEN-0 \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:10:03] I main \u2551 # procs = 1 \u2551 [09/13/21 06:10:03] I main \u2551 plugins = ['main', 'verbose-0.0.1'] \u2551 [09/13/21 06:10:03] I main \u2551 profile = default \u2551 [09/13/21 06:10:03] I main \u2551 outdir = Pipen-output \u2551 [09/13/21 06:10:03] I main \u2551 cache = True \u2551 [09/13/21 06:10:03] I main \u2551 dirsig = 1 \u2551 [09/13/21 06:10:03] I main \u2551 error_strategy = ignore \u2551 [09/13/21 06:10:03] I main \u2551 forks = 1 \u2551 [09/13/21 06:10:03] I main \u2551 lang = bash \u2551 [09/13/21 06:10:03] I main \u2551 loglevel = debug \u2551 [09/13/21 06:10:03] I main \u2551 num_retries = 3 \u2551 [09/13/21 06:10:03] I main \u2551 plugin_opts = {} \u2551 [09/13/21 06:10:03] I main \u2551 plugins = None \u2551 [09/13/21 06:10:03] I main \u2551 scheduler = local \u2551 [09/13/21 06:10:03] I main \u2551 scheduler_opts = {} \u2551 [09/13/21 06:10:03] I main \u2551 submission_batch = 8 \u2551 [09/13/21 06:10:03] I main \u2551 template = liquid \u2551 [09/13/21 06:10:03] I main \u2551 template_opts = {} \u2551 [09/13/21 06:10:03] I main \u2551 workdir = ./.pipen \u2551 [09/13/21 06:10:03] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:10:03] I main [09/13/21 06:10:03] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 AProcess \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:10:03] I main \u2551 A normal process \u2551 [09/13/21 06:10:03] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:10:03] I main AProcess: Workdir: '.pipen/pipen-0/aprocess' [09/13/21 06:10:03] I main AProcess: <<< [START] [09/13/21 06:10:03] I main AProcess: >>> [END] [09/13/21 06:10:03] I verbose AProcess: size: 1 [09/13/21 06:10:03] I verbose AProcess: [0/0] in.infile: /tmp/pipen_example_caching.txt [09/13/21 06:10:03] I verbose AProcess: [0/0] out.outfile: /home/pwwang/github/pipen/Pipen-output/AProcess/pipen_example_caching.txt [09/13/21 06:10:03] I main AProcess: Cached jobs: 0 [09/13/21 06:10:03] I verbose AProcess: Time elapsed: 00:00:00.040s [09/13/21 06:10:03] I main To \"de-cache\" the jobs: \u276f PIPEN_default_cache=0 python examples/caching.py [09/13/21 06:11:55] I main _____________________________________ __ [09/13/21 06:11:55] I main ___ __ \\___ _/__ __ \\__ ____/__ | / / [09/13/21 06:11:55] I main __ /_/ /__ / __ /_/ /_ __/ __ |/ / [09/13/21 06:11:55] I main _ ____/__/ / _ ____/_ /___ _ /| / [09/13/21 06:11:55] I main /_/ /___/ /_/ /_____/ /_/ |_/ [09/13/21 06:11:55] I main [09/13/21 06:11:55] I main version: 0.1.0 [09/13/21 06:11:55] D main [09/13/21 06:11:55] D main Building process relationships: [09/13/21 06:11:55] D main - Start processes: [<Proc:AProcess>] [09/13/21 06:11:55] I main [09/13/21 06:11:55] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 PIPEN-0 \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:11:55] I main \u2551 # procs = 1 \u2551 [09/13/21 06:11:55] I main \u2551 plugins = ['main', 'verbose-0.0.1'] \u2551 [09/13/21 06:11:55] I main \u2551 profile = default \u2551 [09/13/21 06:11:55] I main \u2551 outdir = Pipen-output \u2551 [09/13/21 06:11:55] I main \u2551 cache = 0 \u2551 [09/13/21 06:11:55] I main \u2551 dirsig = 1 \u2551 [09/13/21 06:11:55] I main \u2551 error_strategy = ignore \u2551 [09/13/21 06:11:55] I main \u2551 forks = 1 \u2551 [09/13/21 06:11:55] I main \u2551 lang = bash \u2551 [09/13/21 06:11:55] I main \u2551 loglevel = debug \u2551 [09/13/21 06:11:55] I main \u2551 num_retries = 3 \u2551 [09/13/21 06:11:55] I main \u2551 plugin_opts = {} \u2551 [09/13/21 06:11:55] I main \u2551 plugins = None \u2551 [09/13/21 06:11:55] I main \u2551 scheduler = local \u2551 [09/13/21 06:11:55] I main \u2551 scheduler_opts = {} \u2551 [09/13/21 06:11:55] I main \u2551 submission_batch = 8 \u2551 [09/13/21 06:11:55] I main \u2551 template = liquid \u2551 [09/13/21 06:11:55] I main \u2551 template_opts = {} \u2551 [09/13/21 06:11:55] I main \u2551 workdir = ./.pipen \u2551 [09/13/21 06:11:55] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:11:55] I main [09/13/21 06:11:55] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 AProcess \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:11:55] I main \u2551 A normal process \u2551 [09/13/21 06:11:55] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:11:55] I main AProcess: Workdir: '.pipen/pipen-0/aprocess' [09/13/21 06:11:55] I main AProcess: <<< [START] [09/13/21 06:11:55] I main AProcess: >>> [END] [09/13/21 06:11:55] I verbose AProcess: size: 1 [09/13/21 06:11:55] D main AProcess: [0/0] Not cached (proc.cache is False) [09/13/21 06:11:55] D main AProcess: [0/0] Clearing previous output files. [09/13/21 06:11:55] I verbose AProcess: [0/0] in.infile: /tmp/pipen_example_caching.txt [09/13/21 06:11:55] I verbose AProcess: [0/0] out.outfile: /home/pwwang/github/pipen/Pipen-output/AProcess/pipen_example_caching.txt [09/13/21 06:11:56] I verbose AProcess: Time elapsed: 00:00:01.060s [09/13/21 06:11:56] I main Input data callback \u00b6 \u276f python examples/input_data_callback.py [09/13/21 06:13:12] I main _____________________________________ __ [09/13/21 06:13:12] I main ___ __ \\___ _/__ __ \\__ ____/__ | / / [09/13/21 06:13:12] I main __ /_/ /__ / __ /_/ /_ __/ __ |/ / [09/13/21 06:13:12] I main _ ____/__/ / _ ____/_ /___ _ /| / [09/13/21 06:13:12] I main /_/ /___/ /_/ /_____/ /_/ |_/ [09/13/21 06:13:12] I main [09/13/21 06:13:12] I main version: 0.1.0 [09/13/21 06:13:12] I main [09/13/21 06:13:12] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 PIPEN-0 \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:13:12] I main \u2551 # procs = 2 \u2551 [09/13/21 06:13:12] I main \u2551 plugins = ['main', 'verbose-0.0.1'] \u2551 [09/13/21 06:13:12] I main \u2551 profile = default \u2551 [09/13/21 06:13:12] I main \u2551 outdir = Pipen-output \u2551 [09/13/21 06:13:12] I main \u2551 cache = True \u2551 [09/13/21 06:13:12] I main \u2551 dirsig = 1 \u2551 [09/13/21 06:13:12] I main \u2551 error_strategy = ignore \u2551 [09/13/21 06:13:13] I main \u2551 forks = 3 \u2551 [09/13/21 06:13:13] I main \u2551 lang = bash \u2551 [09/13/21 06:13:13] I main \u2551 loglevel = info \u2551 [09/13/21 06:13:13] I main \u2551 num_retries = 3 \u2551 [09/13/21 06:13:13] I main \u2551 plugin_opts = {} \u2551 [09/13/21 06:13:13] I main \u2551 plugins = None \u2551 [09/13/21 06:13:13] I main \u2551 scheduler = local \u2551 [09/13/21 06:13:13] I main \u2551 scheduler_opts = {} \u2551 [09/13/21 06:13:13] I main \u2551 submission_batch = 8 \u2551 [09/13/21 06:13:13] I main \u2551 template = liquid \u2551 [09/13/21 06:13:13] I main \u2551 template_opts = {} \u2551 [09/13/21 06:13:13] I main \u2551 workdir = ./.pipen \u2551 [09/13/21 06:13:13] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:13:13] I main [09/13/21 06:13:13] I main \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 P1 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e [09/13/21 06:13:13] I main \u2502 Sort input file \u2502 [09/13/21 06:13:13] I main \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f [09/13/21 06:13:13] I main P1: Workdir: '.pipen/pipen-0/p1' [09/13/21 06:13:13] I main P1: <<< [START] [09/13/21 06:13:13] I main P1: >>> ['P2'] [09/13/21 06:13:13] I verbose P1: size: 10 [09/13/21 06:13:13] I verbose P1: [0/9] in.infile: /tmp/pipen_example_input_data_callback/0.txt [09/13/21 06:13:13] I verbose P1: [0/9] out.outfile: /home/pwwang/github/pipen/.pipen/pipen-0/p1/0/output/intermediate.txt [09/13/21 06:13:15] I verbose P1: Time elapsed: 00:00:02.224s [09/13/21 06:13:15] I main [09/13/21 06:13:15] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 P2 \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:13:15] I main \u2551 Paste line number \u2551 [09/13/21 06:13:15] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:13:15] I main P2: Workdir: '.pipen/pipen-0/p2' [09/13/21 06:13:15] I main P2: <<< ['P1'] [09/13/21 06:13:15] I main P2: >>> [END] [09/13/21 06:13:15] I verbose P2: size: 10 [09/13/21 06:13:15] I verbose P2: [0/9] in.infile: /home/pwwang/github/pipen/.pipen/pipen-0/p1/0/output/intermediate.txt [09/13/21 06:13:15] I verbose P2: [0/9] in.nlines: 2 [09/13/21 06:13:15] I verbose P2: [0/9] out.outfile: /home/pwwang/github/pipen/Pipen-output/P2/0/result.txt [09/13/21 06:13:17] I verbose P2: Time elapsed: 00:00:02.192s [09/13/21 06:13:17] I main \u276f cat /home/pwwang/github/pipen/Pipen-output/P2/0/result.txt 1 0_0 2 0_1 mako templating \u00b6 \u276f python examples/mako-templating.py [09/13/21 06:14:57] I main _____________________________________ __ [09/13/21 06:14:57] I main ___ __ \\___ _/__ __ \\__ ____/__ | / / [09/13/21 06:14:57] I main __ /_/ /__ / __ /_/ /_ __/ __ |/ / [09/13/21 06:14:57] I main _ ____/__/ / _ ____/_ /___ _ /| / [09/13/21 06:14:57] I main /_/ /___/ /_/ /_____/ /_/ |_/ [09/13/21 06:14:57] I main [09/13/21 06:14:57] I main version: 0.1.0 [09/13/21 06:14:57] I main [09/13/21 06:14:57] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 PIPEN-0 \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:14:57] I main \u2551 # procs = 1 \u2551 [09/13/21 06:14:57] I main \u2551 plugins = ['main', 'verbose-0.0.1'] \u2551 [09/13/21 06:14:57] I main \u2551 profile = default \u2551 [09/13/21 06:14:57] I main \u2551 outdir = Pipen-output \u2551 [09/13/21 06:14:57] I main \u2551 cache = True \u2551 [09/13/21 06:14:57] I main \u2551 dirsig = 1 \u2551 [09/13/21 06:14:57] I main \u2551 error_strategy = ignore \u2551 [09/13/21 06:14:57] I main \u2551 forks = 1 \u2551 [09/13/21 06:14:57] I main \u2551 lang = bash \u2551 [09/13/21 06:14:57] I main \u2551 loglevel = info \u2551 [09/13/21 06:14:57] I main \u2551 num_retries = 3 \u2551 [09/13/21 06:14:57] I main \u2551 plugin_opts = {} \u2551 [09/13/21 06:14:57] I main \u2551 plugins = None \u2551 [09/13/21 06:14:57] I main \u2551 scheduler = local \u2551 [09/13/21 06:14:57] I main \u2551 scheduler_opts = {} \u2551 [09/13/21 06:14:57] I main \u2551 submission_batch = 8 \u2551 [09/13/21 06:14:57] I main \u2551 template = liquid \u2551 [09/13/21 06:14:57] I main \u2551 template_opts = {} \u2551 [09/13/21 06:14:57] I main \u2551 workdir = ./.pipen \u2551 [09/13/21 06:14:57] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:14:57] I main [09/13/21 06:14:57] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 MakoProcess \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:14:57] I main \u2551 A process using mako templating \u2551 [09/13/21 06:14:57] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:14:57] I main MakoProcess: Workdir: '.pipen/pipen-0/makoprocess' [09/13/21 06:14:57] I main MakoProcess: <<< [START] [09/13/21 06:14:57] I main MakoProcess: >>> [END] [09/13/21 06:14:57] I verbose MakoProcess: size : 1 [09/13/21 06:14:57] I verbose MakoProcess: template: mako [09/13/21 06:14:57] I verbose MakoProcess: [0/0] in.a: 1 [09/13/21 06:14:57] I verbose MakoProcess: [0/0] out.outfile: /home/pwwang/github/pipen/Pipen-output/MakoProcess/1.txt [09/13/21 06:14:58] I verbose MakoProcess: Time elapsed: 00:00:01.019s [09/13/21 06:14:58] I main multile jobs \u00b6 > python examples/multijobs.py [09/13/21 06:16:09] I main _____________________________________ __ [09/13/21 06:16:09] I main ___ __ \\___ _/__ __ \\__ ____/__ | / / [09/13/21 06:16:09] I main __ /_/ /__ / __ /_/ /_ __/ __ |/ / [09/13/21 06:16:09] I main _ ____/__/ / _ ____/_ /___ _ /| / [09/13/21 06:16:09] I main /_/ /___/ /_/ /_____/ /_/ |_/ [09/13/21 06:16:09] I main [09/13/21 06:16:09] I main version: 0.1.0 [09/13/21 06:16:09] I main [09/13/21 06:16:09] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 PIPEN-0 \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:16:09] I main \u2551 # procs = 1 \u2551 [09/13/21 06:16:09] I main \u2551 plugins = ['main', 'verbose-0.0.1'] \u2551 [09/13/21 06:16:09] I main \u2551 profile = default \u2551 [09/13/21 06:16:09] I main \u2551 outdir = Pipen-output \u2551 [09/13/21 06:16:09] I main \u2551 cache = True \u2551 [09/13/21 06:16:09] I main \u2551 dirsig = 1 \u2551 [09/13/21 06:16:09] I main \u2551 error_strategy = ignore \u2551 [09/13/21 06:16:09] I main \u2551 forks = 1 \u2551 [09/13/21 06:16:09] I main \u2551 lang = bash \u2551 [09/13/21 06:16:09] I main \u2551 loglevel = info \u2551 [09/13/21 06:16:09] I main \u2551 num_retries = 3 \u2551 [09/13/21 06:16:09] I main \u2551 plugin_opts = {} \u2551 [09/13/21 06:16:09] I main \u2551 plugins = None \u2551 [09/13/21 06:16:09] I main \u2551 scheduler = local \u2551 [09/13/21 06:16:09] I main \u2551 scheduler_opts = {} \u2551 [09/13/21 06:16:09] I main \u2551 submission_batch = 8 \u2551 [09/13/21 06:16:09] I main \u2551 template = liquid \u2551 [09/13/21 06:16:09] I main \u2551 template_opts = {} \u2551 [09/13/21 06:16:09] I main \u2551 workdir = ./.pipen \u2551 [09/13/21 06:16:09] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:16:09] I main [09/13/21 06:16:09] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 MultiJobProc \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:16:09] I main \u2551 A process with multiple jobs \u2551 [09/13/21 06:16:09] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:16:09] I main MultiJobProc: Workdir: '.pipen/pipen-0/multijobproc' [09/13/21 06:16:09] I main MultiJobProc: <<< [START] [09/13/21 06:16:09] I main MultiJobProc: >>> [END] [09/13/21 06:16:10] I verbose MultiJobProc: forks: 3 [09/13/21 06:16:10] I verbose MultiJobProc: cache: False [09/13/21 06:16:10] I verbose MultiJobProc: size : 10 [09/13/21 06:16:10] I verbose MultiJobProc: [0/9] in.i: 0 [09/13/21 06:16:10] I verbose MultiJobProc: [0/9] out.outfile: /home/pwwang/github/pipen/Pipen-output/MultiJobProc/0/0.txt [09/13/21 06:16:16] I verbose MultiJobProc: Time elapsed: 00:00:06.139s [09/13/21 06:16:16] I main plugin \u00b6 \u276f python examples/plugin-example.py [09/13/21 06:18:18] I notify Calling on_setup [09/13/21 06:18:18] I main _____________________________________ __ [09/13/21 06:18:18] I main ___ __ \\___ _/__ __ \\__ ____/__ | / / [09/13/21 06:18:18] I main __ /_/ /__ / __ /_/ /_ __/ __ |/ / [09/13/21 06:18:18] I main _ ____/__/ / _ ____/_ /___ _ /| / [09/13/21 06:18:18] I main /_/ /___/ /_/ /_____/ /_/ |_/ [09/13/21 06:18:18] I main [09/13/21 06:18:18] I main version: 0.1.0 [09/13/21 06:18:18] I main [09/13/21 06:18:18] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 PIPEN-0 \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:18:18] I main \u2551 # procs = 1 \u2551 [09/13/21 06:18:18] I main \u2551 plugins = ['main', 'notifyplugin-0.0.0'] \u2551 [09/13/21 06:18:18] I main \u2551 profile = default \u2551 [09/13/21 06:18:18] I main \u2551 outdir = Pipen-output \u2551 [09/13/21 06:18:18] I main \u2551 cache = True \u2551 [09/13/21 06:18:18] I main \u2551 dirsig = 1 \u2551 [09/13/21 06:18:18] I main \u2551 error_strategy = ignore \u2551 [09/13/21 06:18:18] I main \u2551 forks = 1 \u2551 [09/13/21 06:18:18] I main \u2551 lang = bash \u2551 [09/13/21 06:18:18] I main \u2551 loglevel = info \u2551 [09/13/21 06:18:18] I main \u2551 num_retries = 3 \u2551 [09/13/21 06:18:18] I main \u2551 plugin_opts = {} \u2551 [09/13/21 06:18:18] I main \u2551 plugins = [<class '__main__.NotifyPlugin'>] \u2551 [09/13/21 06:18:18] I main \u2551 scheduler = local \u2551 [09/13/21 06:18:18] I main \u2551 scheduler_opts = {} \u2551 [09/13/21 06:18:18] I main \u2551 submission_batch = 8 \u2551 [09/13/21 06:18:18] I main \u2551 template = liquid \u2551 [09/13/21 06:18:18] I main \u2551 template_opts = {} \u2551 [09/13/21 06:18:18] I main \u2551 workdir = ./.pipen \u2551 [09/13/21 06:18:18] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:18:18] I notify Calling on_start [09/13/21 06:18:18] I main [09/13/21 06:18:18] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 AProcess \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:18:18] I main \u2551 Undescribed \u2551 [09/13/21 06:18:18] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:18:18] I main AProcess: Workdir: '.pipen/pipen-0/aprocess' [09/13/21 06:18:18] I main AProcess: <<< [START] [09/13/21 06:18:18] I main AProcess: >>> [END] [09/13/21 06:18:18] W main AProcess: No script specified. [09/13/21 06:18:18] I notify Calling on_proc_start [09/13/21 06:18:18] I main AProcess: Cached jobs: 0 [09/13/21 06:18:18] I notify Calling on_proc_done, succeeded = cached [09/13/21 06:18:18] I main PIPEN-0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 2.91 procs/s] [09/13/21 06:18:18] I notify Calling on_complete, succeeded = True Using python interpreter \u00b6 \u276f python examples/python-script.py [09/13/21 06:19:45] I main _____________________________________ __ [09/13/21 06:19:45] I main ___ __ \\___ _/__ __ \\__ ____/__ | / / [09/13/21 06:19:45] I main __ /_/ /__ / __ /_/ /_ __/ __ |/ / [09/13/21 06:19:45] I main _ ____/__/ / _ ____/_ /___ _ /| / [09/13/21 06:19:45] I main /_/ /___/ /_/ /_____/ /_/ |_/ [09/13/21 06:19:45] I main [09/13/21 06:19:45] I main version: 0.1.0 [09/13/21 06:19:45] I main [09/13/21 06:19:45] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 PIPEN-0 \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:19:45] I main \u2551 # procs = 1 \u2551 [09/13/21 06:19:45] I main \u2551 plugins = ['main', 'verbose-0.0.1'] \u2551 [09/13/21 06:19:45] I main \u2551 profile = default \u2551 [09/13/21 06:19:45] I main \u2551 outdir = Pipen-output \u2551 [09/13/21 06:19:46] I main \u2551 cache = True \u2551 [09/13/21 06:19:46] I main \u2551 dirsig = 1 \u2551 [09/13/21 06:19:46] I main \u2551 error_strategy = ignore \u2551 [09/13/21 06:19:46] I main \u2551 forks = 1 \u2551 [09/13/21 06:19:46] I main \u2551 lang = bash \u2551 [09/13/21 06:19:46] I main \u2551 loglevel = info \u2551 [09/13/21 06:19:46] I main \u2551 num_retries = 3 \u2551 [09/13/21 06:19:46] I main \u2551 plugin_opts = {} \u2551 [09/13/21 06:19:46] I main \u2551 plugins = None \u2551 [09/13/21 06:19:46] I main \u2551 scheduler = local \u2551 [09/13/21 06:19:46] I main \u2551 scheduler_opts = {} \u2551 [09/13/21 06:19:46] I main \u2551 submission_batch = 8 \u2551 [09/13/21 06:19:46] I main \u2551 template = liquid \u2551 [09/13/21 06:19:46] I main \u2551 template_opts = {} \u2551 [09/13/21 06:19:46] I main \u2551 workdir = ./.pipen \u2551 [09/13/21 06:19:46] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:19:46] I main [09/13/21 06:19:46] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 PythonScriptProc \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:19:46] I main \u2551 A process using python interpreter for script \u2551 [09/13/21 06:19:46] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:19:46] I main PythonScriptProc: Workdir: '.pipen/pipen-0/pythonscriptproc' [09/13/21 06:19:46] I main PythonScriptProc: <<< [START] [09/13/21 06:19:46] I main PythonScriptProc: >>> [END] [09/13/21 06:19:46] I verbose PythonScriptProc: lang: python [09/13/21 06:19:46] I verbose PythonScriptProc: size: 1 [09/13/21 06:19:46] I verbose PythonScriptProc: [0/0] in.a: 1 [09/13/21 06:19:46] I verbose PythonScriptProc: [0/0] out.outfile: /home/pwwang/github/pipen/Pipen-output/PythonScriptProc/1.txt [09/13/21 06:19:48] I verbose PythonScriptProc: Time elapsed: 00:00:02.031s [09/13/21 06:19:48] I main Error-handling: retry \u00b6 \u276f python examples/retry.py [09/13/21 06:20:38] I main _____________________________________ __ [09/13/21 06:20:38] I main ___ __ \\___ _/__ __ \\__ ____/__ | / / [09/13/21 06:20:38] I main __ /_/ /__ / __ /_/ /_ __/ __ |/ / [09/13/21 06:20:38] I main _ ____/__/ / _ ____/_ /___ _ /| / [09/13/21 06:20:38] I main /_/ /___/ /_/ /_____/ /_/ |_/ [09/13/21 06:20:38] I main [09/13/21 06:20:38] I main version: 0.1.0 [09/13/21 06:20:38] D main [09/13/21 06:20:38] D main Building process relationships: [09/13/21 06:20:38] D main - Start processes: [<Proc:RetryProc>] [09/13/21 06:20:38] I main [09/13/21 06:20:38] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 PIPEN-0 \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:20:38] I main \u2551 # procs = 1 \u2551 [09/13/21 06:20:38] I main \u2551 plugins = ['main', 'verbose-0.0.1'] \u2551 [09/13/21 06:20:38] I main \u2551 profile = default \u2551 [09/13/21 06:20:38] I main \u2551 outdir = Pipen-output \u2551 [09/13/21 06:20:38] I main \u2551 cache = True \u2551 [09/13/21 06:20:38] I main \u2551 dirsig = 1 \u2551 [09/13/21 06:20:38] I main \u2551 error_strategy = ignore \u2551 [09/13/21 06:20:38] I main \u2551 forks = 1 \u2551 [09/13/21 06:20:38] I main \u2551 lang = bash \u2551 [09/13/21 06:20:38] I main \u2551 loglevel = debug \u2551 [09/13/21 06:20:38] I main \u2551 num_retries = 3 \u2551 [09/13/21 06:20:38] I main \u2551 plugin_opts = {} \u2551 [09/13/21 06:20:38] I main \u2551 plugins = None \u2551 [09/13/21 06:20:38] I main \u2551 scheduler = local \u2551 [09/13/21 06:20:38] I main \u2551 scheduler_opts = {} \u2551 [09/13/21 06:20:38] I main \u2551 submission_batch = 8 \u2551 [09/13/21 06:20:38] I main \u2551 template = liquid \u2551 [09/13/21 06:20:38] I main \u2551 template_opts = {} \u2551 [09/13/21 06:20:38] I main \u2551 workdir = ./.pipen \u2551 [09/13/21 06:20:38] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:20:38] I main [09/13/21 06:20:38] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 RetryProc \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:20:38] I main \u2551 Retry the jobs when fail \u2551 [09/13/21 06:20:38] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:20:38] I main RetryProc: Workdir: '.pipen/pipen-0/retryproc' [09/13/21 06:20:38] I main RetryProc: <<< [START] [09/13/21 06:20:38] I main RetryProc: >>> [END] [09/13/21 06:20:38] I verbose RetryProc: size: 1 [09/13/21 06:20:38] D main RetryProc: [0/0] Not cached (job.rc != 0) [09/13/21 06:20:38] D main RetryProc: [0/0] Clearing previous output files. [09/13/21 06:20:38] I verbose RetryProc: [0/0] in.starttime: 1631539238 [09/13/21 06:20:39] D main RetryProc: [0/0] Retrying #1 [09/13/21 06:20:40] D main RetryProc: [0/0] Retrying #2 [09/13/21 06:20:41] D main RetryProc: [0/0] Retrying #3 [09/13/21 06:20:42] D main RetryProc: [0/0] Retrying #4 [09/13/21 06:20:43] I verbose RetryProc: Time elapsed: 00:00:05.203s [09/13/21 06:20:43] I main","title":"Examples"},{"location":"examples/#caching","text":"When run the script the second time, you may see from the logs that jobs are cached: \u276f python examples/caching.py [09/13/21 06:10:03] I main _____________________________________ __ [09/13/21 06:10:03] I main ___ __ \\___ _/__ __ \\__ ____/__ | / / [09/13/21 06:10:03] I main __ /_/ /__ / __ /_/ /_ __/ __ |/ / [09/13/21 06:10:03] I main _ ____/__/ / _ ____/_ /___ _ /| / [09/13/21 06:10:03] I main /_/ /___/ /_/ /_____/ /_/ |_/ [09/13/21 06:10:03] I main [09/13/21 06:10:03] I main version: 0.1.0 [09/13/21 06:10:03] D main [09/13/21 06:10:03] D main Building process relationships: [09/13/21 06:10:03] D main - Start processes: [<Proc:AProcess>] [09/13/21 06:10:03] I main [09/13/21 06:10:03] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 PIPEN-0 \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:10:03] I main \u2551 # procs = 1 \u2551 [09/13/21 06:10:03] I main \u2551 plugins = ['main', 'verbose-0.0.1'] \u2551 [09/13/21 06:10:03] I main \u2551 profile = default \u2551 [09/13/21 06:10:03] I main \u2551 outdir = Pipen-output \u2551 [09/13/21 06:10:03] I main \u2551 cache = True \u2551 [09/13/21 06:10:03] I main \u2551 dirsig = 1 \u2551 [09/13/21 06:10:03] I main \u2551 error_strategy = ignore \u2551 [09/13/21 06:10:03] I main \u2551 forks = 1 \u2551 [09/13/21 06:10:03] I main \u2551 lang = bash \u2551 [09/13/21 06:10:03] I main \u2551 loglevel = debug \u2551 [09/13/21 06:10:03] I main \u2551 num_retries = 3 \u2551 [09/13/21 06:10:03] I main \u2551 plugin_opts = {} \u2551 [09/13/21 06:10:03] I main \u2551 plugins = None \u2551 [09/13/21 06:10:03] I main \u2551 scheduler = local \u2551 [09/13/21 06:10:03] I main \u2551 scheduler_opts = {} \u2551 [09/13/21 06:10:03] I main \u2551 submission_batch = 8 \u2551 [09/13/21 06:10:03] I main \u2551 template = liquid \u2551 [09/13/21 06:10:03] I main \u2551 template_opts = {} \u2551 [09/13/21 06:10:03] I main \u2551 workdir = ./.pipen \u2551 [09/13/21 06:10:03] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:10:03] I main [09/13/21 06:10:03] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 AProcess \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:10:03] I main \u2551 A normal process \u2551 [09/13/21 06:10:03] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:10:03] I main AProcess: Workdir: '.pipen/pipen-0/aprocess' [09/13/21 06:10:03] I main AProcess: <<< [START] [09/13/21 06:10:03] I main AProcess: >>> [END] [09/13/21 06:10:03] I verbose AProcess: size: 1 [09/13/21 06:10:03] I verbose AProcess: [0/0] in.infile: /tmp/pipen_example_caching.txt [09/13/21 06:10:03] I verbose AProcess: [0/0] out.outfile: /home/pwwang/github/pipen/Pipen-output/AProcess/pipen_example_caching.txt [09/13/21 06:10:03] I main AProcess: Cached jobs: 0 [09/13/21 06:10:03] I verbose AProcess: Time elapsed: 00:00:00.040s [09/13/21 06:10:03] I main To \"de-cache\" the jobs: \u276f PIPEN_default_cache=0 python examples/caching.py [09/13/21 06:11:55] I main _____________________________________ __ [09/13/21 06:11:55] I main ___ __ \\___ _/__ __ \\__ ____/__ | / / [09/13/21 06:11:55] I main __ /_/ /__ / __ /_/ /_ __/ __ |/ / [09/13/21 06:11:55] I main _ ____/__/ / _ ____/_ /___ _ /| / [09/13/21 06:11:55] I main /_/ /___/ /_/ /_____/ /_/ |_/ [09/13/21 06:11:55] I main [09/13/21 06:11:55] I main version: 0.1.0 [09/13/21 06:11:55] D main [09/13/21 06:11:55] D main Building process relationships: [09/13/21 06:11:55] D main - Start processes: [<Proc:AProcess>] [09/13/21 06:11:55] I main [09/13/21 06:11:55] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 PIPEN-0 \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:11:55] I main \u2551 # procs = 1 \u2551 [09/13/21 06:11:55] I main \u2551 plugins = ['main', 'verbose-0.0.1'] \u2551 [09/13/21 06:11:55] I main \u2551 profile = default \u2551 [09/13/21 06:11:55] I main \u2551 outdir = Pipen-output \u2551 [09/13/21 06:11:55] I main \u2551 cache = 0 \u2551 [09/13/21 06:11:55] I main \u2551 dirsig = 1 \u2551 [09/13/21 06:11:55] I main \u2551 error_strategy = ignore \u2551 [09/13/21 06:11:55] I main \u2551 forks = 1 \u2551 [09/13/21 06:11:55] I main \u2551 lang = bash \u2551 [09/13/21 06:11:55] I main \u2551 loglevel = debug \u2551 [09/13/21 06:11:55] I main \u2551 num_retries = 3 \u2551 [09/13/21 06:11:55] I main \u2551 plugin_opts = {} \u2551 [09/13/21 06:11:55] I main \u2551 plugins = None \u2551 [09/13/21 06:11:55] I main \u2551 scheduler = local \u2551 [09/13/21 06:11:55] I main \u2551 scheduler_opts = {} \u2551 [09/13/21 06:11:55] I main \u2551 submission_batch = 8 \u2551 [09/13/21 06:11:55] I main \u2551 template = liquid \u2551 [09/13/21 06:11:55] I main \u2551 template_opts = {} \u2551 [09/13/21 06:11:55] I main \u2551 workdir = ./.pipen \u2551 [09/13/21 06:11:55] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:11:55] I main [09/13/21 06:11:55] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 AProcess \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:11:55] I main \u2551 A normal process \u2551 [09/13/21 06:11:55] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:11:55] I main AProcess: Workdir: '.pipen/pipen-0/aprocess' [09/13/21 06:11:55] I main AProcess: <<< [START] [09/13/21 06:11:55] I main AProcess: >>> [END] [09/13/21 06:11:55] I verbose AProcess: size: 1 [09/13/21 06:11:55] D main AProcess: [0/0] Not cached (proc.cache is False) [09/13/21 06:11:55] D main AProcess: [0/0] Clearing previous output files. [09/13/21 06:11:55] I verbose AProcess: [0/0] in.infile: /tmp/pipen_example_caching.txt [09/13/21 06:11:55] I verbose AProcess: [0/0] out.outfile: /home/pwwang/github/pipen/Pipen-output/AProcess/pipen_example_caching.txt [09/13/21 06:11:56] I verbose AProcess: Time elapsed: 00:00:01.060s [09/13/21 06:11:56] I main","title":"Caching"},{"location":"examples/#input-data-callback","text":"\u276f python examples/input_data_callback.py [09/13/21 06:13:12] I main _____________________________________ __ [09/13/21 06:13:12] I main ___ __ \\___ _/__ __ \\__ ____/__ | / / [09/13/21 06:13:12] I main __ /_/ /__ / __ /_/ /_ __/ __ |/ / [09/13/21 06:13:12] I main _ ____/__/ / _ ____/_ /___ _ /| / [09/13/21 06:13:12] I main /_/ /___/ /_/ /_____/ /_/ |_/ [09/13/21 06:13:12] I main [09/13/21 06:13:12] I main version: 0.1.0 [09/13/21 06:13:12] I main [09/13/21 06:13:12] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 PIPEN-0 \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:13:12] I main \u2551 # procs = 2 \u2551 [09/13/21 06:13:12] I main \u2551 plugins = ['main', 'verbose-0.0.1'] \u2551 [09/13/21 06:13:12] I main \u2551 profile = default \u2551 [09/13/21 06:13:12] I main \u2551 outdir = Pipen-output \u2551 [09/13/21 06:13:12] I main \u2551 cache = True \u2551 [09/13/21 06:13:12] I main \u2551 dirsig = 1 \u2551 [09/13/21 06:13:12] I main \u2551 error_strategy = ignore \u2551 [09/13/21 06:13:13] I main \u2551 forks = 3 \u2551 [09/13/21 06:13:13] I main \u2551 lang = bash \u2551 [09/13/21 06:13:13] I main \u2551 loglevel = info \u2551 [09/13/21 06:13:13] I main \u2551 num_retries = 3 \u2551 [09/13/21 06:13:13] I main \u2551 plugin_opts = {} \u2551 [09/13/21 06:13:13] I main \u2551 plugins = None \u2551 [09/13/21 06:13:13] I main \u2551 scheduler = local \u2551 [09/13/21 06:13:13] I main \u2551 scheduler_opts = {} \u2551 [09/13/21 06:13:13] I main \u2551 submission_batch = 8 \u2551 [09/13/21 06:13:13] I main \u2551 template = liquid \u2551 [09/13/21 06:13:13] I main \u2551 template_opts = {} \u2551 [09/13/21 06:13:13] I main \u2551 workdir = ./.pipen \u2551 [09/13/21 06:13:13] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:13:13] I main [09/13/21 06:13:13] I main \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 P1 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e [09/13/21 06:13:13] I main \u2502 Sort input file \u2502 [09/13/21 06:13:13] I main \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f [09/13/21 06:13:13] I main P1: Workdir: '.pipen/pipen-0/p1' [09/13/21 06:13:13] I main P1: <<< [START] [09/13/21 06:13:13] I main P1: >>> ['P2'] [09/13/21 06:13:13] I verbose P1: size: 10 [09/13/21 06:13:13] I verbose P1: [0/9] in.infile: /tmp/pipen_example_input_data_callback/0.txt [09/13/21 06:13:13] I verbose P1: [0/9] out.outfile: /home/pwwang/github/pipen/.pipen/pipen-0/p1/0/output/intermediate.txt [09/13/21 06:13:15] I verbose P1: Time elapsed: 00:00:02.224s [09/13/21 06:13:15] I main [09/13/21 06:13:15] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 P2 \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:13:15] I main \u2551 Paste line number \u2551 [09/13/21 06:13:15] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:13:15] I main P2: Workdir: '.pipen/pipen-0/p2' [09/13/21 06:13:15] I main P2: <<< ['P1'] [09/13/21 06:13:15] I main P2: >>> [END] [09/13/21 06:13:15] I verbose P2: size: 10 [09/13/21 06:13:15] I verbose P2: [0/9] in.infile: /home/pwwang/github/pipen/.pipen/pipen-0/p1/0/output/intermediate.txt [09/13/21 06:13:15] I verbose P2: [0/9] in.nlines: 2 [09/13/21 06:13:15] I verbose P2: [0/9] out.outfile: /home/pwwang/github/pipen/Pipen-output/P2/0/result.txt [09/13/21 06:13:17] I verbose P2: Time elapsed: 00:00:02.192s [09/13/21 06:13:17] I main \u276f cat /home/pwwang/github/pipen/Pipen-output/P2/0/result.txt 1 0_0 2 0_1","title":"Input data callback"},{"location":"examples/#mako-templating","text":"\u276f python examples/mako-templating.py [09/13/21 06:14:57] I main _____________________________________ __ [09/13/21 06:14:57] I main ___ __ \\___ _/__ __ \\__ ____/__ | / / [09/13/21 06:14:57] I main __ /_/ /__ / __ /_/ /_ __/ __ |/ / [09/13/21 06:14:57] I main _ ____/__/ / _ ____/_ /___ _ /| / [09/13/21 06:14:57] I main /_/ /___/ /_/ /_____/ /_/ |_/ [09/13/21 06:14:57] I main [09/13/21 06:14:57] I main version: 0.1.0 [09/13/21 06:14:57] I main [09/13/21 06:14:57] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 PIPEN-0 \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:14:57] I main \u2551 # procs = 1 \u2551 [09/13/21 06:14:57] I main \u2551 plugins = ['main', 'verbose-0.0.1'] \u2551 [09/13/21 06:14:57] I main \u2551 profile = default \u2551 [09/13/21 06:14:57] I main \u2551 outdir = Pipen-output \u2551 [09/13/21 06:14:57] I main \u2551 cache = True \u2551 [09/13/21 06:14:57] I main \u2551 dirsig = 1 \u2551 [09/13/21 06:14:57] I main \u2551 error_strategy = ignore \u2551 [09/13/21 06:14:57] I main \u2551 forks = 1 \u2551 [09/13/21 06:14:57] I main \u2551 lang = bash \u2551 [09/13/21 06:14:57] I main \u2551 loglevel = info \u2551 [09/13/21 06:14:57] I main \u2551 num_retries = 3 \u2551 [09/13/21 06:14:57] I main \u2551 plugin_opts = {} \u2551 [09/13/21 06:14:57] I main \u2551 plugins = None \u2551 [09/13/21 06:14:57] I main \u2551 scheduler = local \u2551 [09/13/21 06:14:57] I main \u2551 scheduler_opts = {} \u2551 [09/13/21 06:14:57] I main \u2551 submission_batch = 8 \u2551 [09/13/21 06:14:57] I main \u2551 template = liquid \u2551 [09/13/21 06:14:57] I main \u2551 template_opts = {} \u2551 [09/13/21 06:14:57] I main \u2551 workdir = ./.pipen \u2551 [09/13/21 06:14:57] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:14:57] I main [09/13/21 06:14:57] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 MakoProcess \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:14:57] I main \u2551 A process using mako templating \u2551 [09/13/21 06:14:57] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:14:57] I main MakoProcess: Workdir: '.pipen/pipen-0/makoprocess' [09/13/21 06:14:57] I main MakoProcess: <<< [START] [09/13/21 06:14:57] I main MakoProcess: >>> [END] [09/13/21 06:14:57] I verbose MakoProcess: size : 1 [09/13/21 06:14:57] I verbose MakoProcess: template: mako [09/13/21 06:14:57] I verbose MakoProcess: [0/0] in.a: 1 [09/13/21 06:14:57] I verbose MakoProcess: [0/0] out.outfile: /home/pwwang/github/pipen/Pipen-output/MakoProcess/1.txt [09/13/21 06:14:58] I verbose MakoProcess: Time elapsed: 00:00:01.019s [09/13/21 06:14:58] I main","title":"mako templating"},{"location":"examples/#multile-jobs","text":"> python examples/multijobs.py [09/13/21 06:16:09] I main _____________________________________ __ [09/13/21 06:16:09] I main ___ __ \\___ _/__ __ \\__ ____/__ | / / [09/13/21 06:16:09] I main __ /_/ /__ / __ /_/ /_ __/ __ |/ / [09/13/21 06:16:09] I main _ ____/__/ / _ ____/_ /___ _ /| / [09/13/21 06:16:09] I main /_/ /___/ /_/ /_____/ /_/ |_/ [09/13/21 06:16:09] I main [09/13/21 06:16:09] I main version: 0.1.0 [09/13/21 06:16:09] I main [09/13/21 06:16:09] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 PIPEN-0 \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:16:09] I main \u2551 # procs = 1 \u2551 [09/13/21 06:16:09] I main \u2551 plugins = ['main', 'verbose-0.0.1'] \u2551 [09/13/21 06:16:09] I main \u2551 profile = default \u2551 [09/13/21 06:16:09] I main \u2551 outdir = Pipen-output \u2551 [09/13/21 06:16:09] I main \u2551 cache = True \u2551 [09/13/21 06:16:09] I main \u2551 dirsig = 1 \u2551 [09/13/21 06:16:09] I main \u2551 error_strategy = ignore \u2551 [09/13/21 06:16:09] I main \u2551 forks = 1 \u2551 [09/13/21 06:16:09] I main \u2551 lang = bash \u2551 [09/13/21 06:16:09] I main \u2551 loglevel = info \u2551 [09/13/21 06:16:09] I main \u2551 num_retries = 3 \u2551 [09/13/21 06:16:09] I main \u2551 plugin_opts = {} \u2551 [09/13/21 06:16:09] I main \u2551 plugins = None \u2551 [09/13/21 06:16:09] I main \u2551 scheduler = local \u2551 [09/13/21 06:16:09] I main \u2551 scheduler_opts = {} \u2551 [09/13/21 06:16:09] I main \u2551 submission_batch = 8 \u2551 [09/13/21 06:16:09] I main \u2551 template = liquid \u2551 [09/13/21 06:16:09] I main \u2551 template_opts = {} \u2551 [09/13/21 06:16:09] I main \u2551 workdir = ./.pipen \u2551 [09/13/21 06:16:09] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:16:09] I main [09/13/21 06:16:09] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 MultiJobProc \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:16:09] I main \u2551 A process with multiple jobs \u2551 [09/13/21 06:16:09] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:16:09] I main MultiJobProc: Workdir: '.pipen/pipen-0/multijobproc' [09/13/21 06:16:09] I main MultiJobProc: <<< [START] [09/13/21 06:16:09] I main MultiJobProc: >>> [END] [09/13/21 06:16:10] I verbose MultiJobProc: forks: 3 [09/13/21 06:16:10] I verbose MultiJobProc: cache: False [09/13/21 06:16:10] I verbose MultiJobProc: size : 10 [09/13/21 06:16:10] I verbose MultiJobProc: [0/9] in.i: 0 [09/13/21 06:16:10] I verbose MultiJobProc: [0/9] out.outfile: /home/pwwang/github/pipen/Pipen-output/MultiJobProc/0/0.txt [09/13/21 06:16:16] I verbose MultiJobProc: Time elapsed: 00:00:06.139s [09/13/21 06:16:16] I main","title":"multile jobs"},{"location":"examples/#plugin","text":"\u276f python examples/plugin-example.py [09/13/21 06:18:18] I notify Calling on_setup [09/13/21 06:18:18] I main _____________________________________ __ [09/13/21 06:18:18] I main ___ __ \\___ _/__ __ \\__ ____/__ | / / [09/13/21 06:18:18] I main __ /_/ /__ / __ /_/ /_ __/ __ |/ / [09/13/21 06:18:18] I main _ ____/__/ / _ ____/_ /___ _ /| / [09/13/21 06:18:18] I main /_/ /___/ /_/ /_____/ /_/ |_/ [09/13/21 06:18:18] I main [09/13/21 06:18:18] I main version: 0.1.0 [09/13/21 06:18:18] I main [09/13/21 06:18:18] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 PIPEN-0 \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:18:18] I main \u2551 # procs = 1 \u2551 [09/13/21 06:18:18] I main \u2551 plugins = ['main', 'notifyplugin-0.0.0'] \u2551 [09/13/21 06:18:18] I main \u2551 profile = default \u2551 [09/13/21 06:18:18] I main \u2551 outdir = Pipen-output \u2551 [09/13/21 06:18:18] I main \u2551 cache = True \u2551 [09/13/21 06:18:18] I main \u2551 dirsig = 1 \u2551 [09/13/21 06:18:18] I main \u2551 error_strategy = ignore \u2551 [09/13/21 06:18:18] I main \u2551 forks = 1 \u2551 [09/13/21 06:18:18] I main \u2551 lang = bash \u2551 [09/13/21 06:18:18] I main \u2551 loglevel = info \u2551 [09/13/21 06:18:18] I main \u2551 num_retries = 3 \u2551 [09/13/21 06:18:18] I main \u2551 plugin_opts = {} \u2551 [09/13/21 06:18:18] I main \u2551 plugins = [<class '__main__.NotifyPlugin'>] \u2551 [09/13/21 06:18:18] I main \u2551 scheduler = local \u2551 [09/13/21 06:18:18] I main \u2551 scheduler_opts = {} \u2551 [09/13/21 06:18:18] I main \u2551 submission_batch = 8 \u2551 [09/13/21 06:18:18] I main \u2551 template = liquid \u2551 [09/13/21 06:18:18] I main \u2551 template_opts = {} \u2551 [09/13/21 06:18:18] I main \u2551 workdir = ./.pipen \u2551 [09/13/21 06:18:18] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:18:18] I notify Calling on_start [09/13/21 06:18:18] I main [09/13/21 06:18:18] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 AProcess \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:18:18] I main \u2551 Undescribed \u2551 [09/13/21 06:18:18] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:18:18] I main AProcess: Workdir: '.pipen/pipen-0/aprocess' [09/13/21 06:18:18] I main AProcess: <<< [START] [09/13/21 06:18:18] I main AProcess: >>> [END] [09/13/21 06:18:18] W main AProcess: No script specified. [09/13/21 06:18:18] I notify Calling on_proc_start [09/13/21 06:18:18] I main AProcess: Cached jobs: 0 [09/13/21 06:18:18] I notify Calling on_proc_done, succeeded = cached [09/13/21 06:18:18] I main PIPEN-0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 2.91 procs/s] [09/13/21 06:18:18] I notify Calling on_complete, succeeded = True","title":"plugin"},{"location":"examples/#using-python-interpreter","text":"\u276f python examples/python-script.py [09/13/21 06:19:45] I main _____________________________________ __ [09/13/21 06:19:45] I main ___ __ \\___ _/__ __ \\__ ____/__ | / / [09/13/21 06:19:45] I main __ /_/ /__ / __ /_/ /_ __/ __ |/ / [09/13/21 06:19:45] I main _ ____/__/ / _ ____/_ /___ _ /| / [09/13/21 06:19:45] I main /_/ /___/ /_/ /_____/ /_/ |_/ [09/13/21 06:19:45] I main [09/13/21 06:19:45] I main version: 0.1.0 [09/13/21 06:19:45] I main [09/13/21 06:19:45] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 PIPEN-0 \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:19:45] I main \u2551 # procs = 1 \u2551 [09/13/21 06:19:45] I main \u2551 plugins = ['main', 'verbose-0.0.1'] \u2551 [09/13/21 06:19:45] I main \u2551 profile = default \u2551 [09/13/21 06:19:45] I main \u2551 outdir = Pipen-output \u2551 [09/13/21 06:19:46] I main \u2551 cache = True \u2551 [09/13/21 06:19:46] I main \u2551 dirsig = 1 \u2551 [09/13/21 06:19:46] I main \u2551 error_strategy = ignore \u2551 [09/13/21 06:19:46] I main \u2551 forks = 1 \u2551 [09/13/21 06:19:46] I main \u2551 lang = bash \u2551 [09/13/21 06:19:46] I main \u2551 loglevel = info \u2551 [09/13/21 06:19:46] I main \u2551 num_retries = 3 \u2551 [09/13/21 06:19:46] I main \u2551 plugin_opts = {} \u2551 [09/13/21 06:19:46] I main \u2551 plugins = None \u2551 [09/13/21 06:19:46] I main \u2551 scheduler = local \u2551 [09/13/21 06:19:46] I main \u2551 scheduler_opts = {} \u2551 [09/13/21 06:19:46] I main \u2551 submission_batch = 8 \u2551 [09/13/21 06:19:46] I main \u2551 template = liquid \u2551 [09/13/21 06:19:46] I main \u2551 template_opts = {} \u2551 [09/13/21 06:19:46] I main \u2551 workdir = ./.pipen \u2551 [09/13/21 06:19:46] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:19:46] I main [09/13/21 06:19:46] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 PythonScriptProc \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:19:46] I main \u2551 A process using python interpreter for script \u2551 [09/13/21 06:19:46] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:19:46] I main PythonScriptProc: Workdir: '.pipen/pipen-0/pythonscriptproc' [09/13/21 06:19:46] I main PythonScriptProc: <<< [START] [09/13/21 06:19:46] I main PythonScriptProc: >>> [END] [09/13/21 06:19:46] I verbose PythonScriptProc: lang: python [09/13/21 06:19:46] I verbose PythonScriptProc: size: 1 [09/13/21 06:19:46] I verbose PythonScriptProc: [0/0] in.a: 1 [09/13/21 06:19:46] I verbose PythonScriptProc: [0/0] out.outfile: /home/pwwang/github/pipen/Pipen-output/PythonScriptProc/1.txt [09/13/21 06:19:48] I verbose PythonScriptProc: Time elapsed: 00:00:02.031s [09/13/21 06:19:48] I main","title":"Using python interpreter"},{"location":"examples/#error-handling-retry","text":"\u276f python examples/retry.py [09/13/21 06:20:38] I main _____________________________________ __ [09/13/21 06:20:38] I main ___ __ \\___ _/__ __ \\__ ____/__ | / / [09/13/21 06:20:38] I main __ /_/ /__ / __ /_/ /_ __/ __ |/ / [09/13/21 06:20:38] I main _ ____/__/ / _ ____/_ /___ _ /| / [09/13/21 06:20:38] I main /_/ /___/ /_/ /_____/ /_/ |_/ [09/13/21 06:20:38] I main [09/13/21 06:20:38] I main version: 0.1.0 [09/13/21 06:20:38] D main [09/13/21 06:20:38] D main Building process relationships: [09/13/21 06:20:38] D main - Start processes: [<Proc:RetryProc>] [09/13/21 06:20:38] I main [09/13/21 06:20:38] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 PIPEN-0 \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:20:38] I main \u2551 # procs = 1 \u2551 [09/13/21 06:20:38] I main \u2551 plugins = ['main', 'verbose-0.0.1'] \u2551 [09/13/21 06:20:38] I main \u2551 profile = default \u2551 [09/13/21 06:20:38] I main \u2551 outdir = Pipen-output \u2551 [09/13/21 06:20:38] I main \u2551 cache = True \u2551 [09/13/21 06:20:38] I main \u2551 dirsig = 1 \u2551 [09/13/21 06:20:38] I main \u2551 error_strategy = ignore \u2551 [09/13/21 06:20:38] I main \u2551 forks = 1 \u2551 [09/13/21 06:20:38] I main \u2551 lang = bash \u2551 [09/13/21 06:20:38] I main \u2551 loglevel = debug \u2551 [09/13/21 06:20:38] I main \u2551 num_retries = 3 \u2551 [09/13/21 06:20:38] I main \u2551 plugin_opts = {} \u2551 [09/13/21 06:20:38] I main \u2551 plugins = None \u2551 [09/13/21 06:20:38] I main \u2551 scheduler = local \u2551 [09/13/21 06:20:38] I main \u2551 scheduler_opts = {} \u2551 [09/13/21 06:20:38] I main \u2551 submission_batch = 8 \u2551 [09/13/21 06:20:38] I main \u2551 template = liquid \u2551 [09/13/21 06:20:38] I main \u2551 template_opts = {} \u2551 [09/13/21 06:20:38] I main \u2551 workdir = ./.pipen \u2551 [09/13/21 06:20:38] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:20:38] I main [09/13/21 06:20:38] I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 RetryProc \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e [09/13/21 06:20:38] I main \u2551 Retry the jobs when fail \u2551 [09/13/21 06:20:38] I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f [09/13/21 06:20:38] I main RetryProc: Workdir: '.pipen/pipen-0/retryproc' [09/13/21 06:20:38] I main RetryProc: <<< [START] [09/13/21 06:20:38] I main RetryProc: >>> [END] [09/13/21 06:20:38] I verbose RetryProc: size: 1 [09/13/21 06:20:38] D main RetryProc: [0/0] Not cached (job.rc != 0) [09/13/21 06:20:38] D main RetryProc: [0/0] Clearing previous output files. [09/13/21 06:20:38] I verbose RetryProc: [0/0] in.starttime: 1631539238 [09/13/21 06:20:39] D main RetryProc: [0/0] Retrying #1 [09/13/21 06:20:40] D main RetryProc: [0/0] Retrying #2 [09/13/21 06:20:41] D main RetryProc: [0/0] Retrying #3 [09/13/21 06:20:42] D main RetryProc: [0/0] Retrying #4 [09/13/21 06:20:43] I verbose RetryProc: Time elapsed: 00:00:05.203s [09/13/21 06:20:43] I main","title":"Error-handling: retry"},{"location":"glossary/","text":"Glossary \u00b6 This glossary defines key terms used throughout the pipen documentation. A \u00b6 Args \u00b6 Short for arguments . The parameters that a function or method accepts. In docstrings, this section lists input parameters with their types and descriptions. Example: def process_data ( input_file : str , output_dir : str ) -> None : \"\"\"Process a data file. Args: input_file: Path to the input data file. output_dir: Directory where processed results will be saved. Returns: None \"\"\" B \u00b6 C \u00b6 Cache \u00b6 A mechanism that stores the results of completed jobs to avoid recomputing them when the same inputs are used again. pipen automatically computes a cache key (signature) based on input data, script content, and configuration. Related: Caching C (continued) \u00b6 Channel \u00b6 A pandas DataFrame that structures data flow between processes. Each row in a channel represents an independent job, and each column represents a piece of data that the job needs. Types of channels: - Input channel : Data fed into a process - Output channel : Data produced by a process Channel operations: create() , from_glob() , expand_dir() , collapse_files() Related: Channels D \u00b6 E \u00b6 F \u00b6 G \u00b6 H \u00b6 I \u00b6 J \u00b6 Job \u00b6 An individual execution of a process. When a process has input data with multiple rows, pipen creates multiple jobs (one for each row) that can run in parallel. Job lifecycle: 1. Initiated : Job created 2. Queued : Waiting to run 3. Submitted : Submitted to scheduler 4. Running : Currently executing 5. Succeeded : Completed successfully 6. Failed : Completed with errors 7. Cached : Used cached results instead of running Related: Job Caching K \u00b6 L \u00b6 M \u00b6 N \u00b6 O \u00b6 P \u00b6 Pipeline \u00b6 The main orchestrator that manages a workflow. A pipeline consists of multiple processes connected by dependencies. The Pipen class handles: - Process discovery and dependency resolution - Configuration management - Progress tracking - Plugin initialization Pipeline structure: Pipeline (orchestrator) \u251c\u2500\u2500 Process 1 (first step) \u2502 \u2514\u2500\u2500 Jobs (parallel execution) \u251c\u2500\u2500 Process 2 (depends on Process 1) \u2502 \u2514\u2500\u2500 Jobs \u2514\u2500\u2500 Process N (depends on previous steps) \u2514\u2500\u2500 Jobs Related: Basics , Architecture Process \u00b6 A class that inherits from Proc and defines a single step in a pipeline. A process specifies: - Input : What data it needs ( input attribute) - Output : What data it produces ( output attribute) - Script : How to transform input to output ( script attribute) - Dependencies : Which processes must complete first ( requires attribute) Example: class MyProcess ( Proc ): \"\"\"Process data files.\"\"\" input = \"infile:file\" output = \"outfile:file:result.txt\" script = \"cat {{ in.infile }} > {{ out.outfile }}\" Related: Defining a Process Proc \u00b6 Abbreviation for Process . The base class Proc is the foundation for defining pipeline steps. ProcGroup \u00b6 A collection of related processes that can be managed together. ProcGroups allow you to organize processes logically and apply configurations to multiple processes at once. Related: Process Group Q \u00b6 R \u00b6 Raises \u00b6 A section in docstrings that lists exceptions that a function or method can raise. This helps users understand what errors to expect and how to handle them. Example: def process_data ( data : str ) -> dict : \"\"\"Process data string. Args: data: The data string to process. Returns: Dictionary with processing results. Raises: ValueError: If data is empty. TypeError: If data is not a string. \"\"\" if not data : raise ValueError ( \"Data cannot be empty\" ) if not isinstance ( data , str ): raise TypeError ( \"Data must be a string\" ) return { \"processed\" : data } Returns \u00b6 A section in docstrings that describes what a function or method returns. This includes the return type and description of the returned value. Example: def calculate_sum ( numbers : list [ int ]) -> int : \"\"\"Calculate the sum of a list of numbers. Args: numbers: List of integers to sum. Returns: The sum of all numbers in the list. \"\"\" return sum ( numbers ) S \u00b6 Scheduler \u00b6 A backend that manages job execution. pipen supports multiple schedulers: - Local : Execute jobs on the local machine - SGE : Submit jobs to Sun Grid Engine - SLURM : Submit jobs to SLURM workload manager - SSH : Execute jobs on remote servers via SSH - Container : Run jobs in containers - Gbatch : Submit jobs to Google Cloud Batch Related: Scheduler T \u00b6 U \u00b6 V \u00b6 W \u00b6 X \u00b6 Y \u00b6 Z \u00b6 Related Documentation \u00b6 Architecture - Understanding how pipen components interact Basics - Pipeline layers and folder structure Defining a Process - Creating process classes Channels - Data flow between processes Caching - How job caching works Scheduler - Available scheduler backends Common Patterns \u00b6 Input/Output Types \u00b6 Type Syntax Description var \"name:var\" In-memory variable file \"name:file\" Single file path dir \"name:dir\" Single directory path files \"name:files\" Multiple file paths dirs \"name:dirs\" Multiple directory paths Error Strategies \u00b6 Strategy Behavior halt Stop entire pipeline on any failure ignore Continue pipeline, ignoring failed jobs retry Retry failed jobs up to num_retries times Template Variables \u00b6 Variable Description Example in.* Input variables {{ in.input_data }} out.* Output placeholders {{ out.result_file }} proc.* Process metadata {{ proc.name }} , {{ proc.workdir }} envs.* Template options (NOT shell env vars) {{ envs.my_option }} Defined via proc.envs attribute. These are custom options passed to template rendering context. They are NOT shell environment variables like PATH or HOME.","title":"Glossary"},{"location":"glossary/#glossary","text":"This glossary defines key terms used throughout the pipen documentation.","title":"Glossary"},{"location":"glossary/#a","text":"","title":"A"},{"location":"glossary/#args","text":"Short for arguments . The parameters that a function or method accepts. In docstrings, this section lists input parameters with their types and descriptions. Example: def process_data ( input_file : str , output_dir : str ) -> None : \"\"\"Process a data file. Args: input_file: Path to the input data file. output_dir: Directory where processed results will be saved. Returns: None \"\"\"","title":"Args"},{"location":"glossary/#b","text":"","title":"B"},{"location":"glossary/#c","text":"","title":"C"},{"location":"glossary/#cache","text":"A mechanism that stores the results of completed jobs to avoid recomputing them when the same inputs are used again. pipen automatically computes a cache key (signature) based on input data, script content, and configuration. Related: Caching","title":"Cache"},{"location":"glossary/#c-continued","text":"","title":"C (continued)"},{"location":"glossary/#channel","text":"A pandas DataFrame that structures data flow between processes. Each row in a channel represents an independent job, and each column represents a piece of data that the job needs. Types of channels: - Input channel : Data fed into a process - Output channel : Data produced by a process Channel operations: create() , from_glob() , expand_dir() , collapse_files() Related: Channels","title":"Channel"},{"location":"glossary/#d","text":"","title":"D"},{"location":"glossary/#e","text":"","title":"E"},{"location":"glossary/#f","text":"","title":"F"},{"location":"glossary/#g","text":"","title":"G"},{"location":"glossary/#h","text":"","title":"H"},{"location":"glossary/#i","text":"","title":"I"},{"location":"glossary/#j","text":"","title":"J"},{"location":"glossary/#job","text":"An individual execution of a process. When a process has input data with multiple rows, pipen creates multiple jobs (one for each row) that can run in parallel. Job lifecycle: 1. Initiated : Job created 2. Queued : Waiting to run 3. Submitted : Submitted to scheduler 4. Running : Currently executing 5. Succeeded : Completed successfully 6. Failed : Completed with errors 7. Cached : Used cached results instead of running Related: Job Caching","title":"Job"},{"location":"glossary/#k","text":"","title":"K"},{"location":"glossary/#l","text":"","title":"L"},{"location":"glossary/#m","text":"","title":"M"},{"location":"glossary/#n","text":"","title":"N"},{"location":"glossary/#o","text":"","title":"O"},{"location":"glossary/#p","text":"","title":"P"},{"location":"glossary/#pipeline","text":"The main orchestrator that manages a workflow. A pipeline consists of multiple processes connected by dependencies. The Pipen class handles: - Process discovery and dependency resolution - Configuration management - Progress tracking - Plugin initialization Pipeline structure: Pipeline (orchestrator) \u251c\u2500\u2500 Process 1 (first step) \u2502 \u2514\u2500\u2500 Jobs (parallel execution) \u251c\u2500\u2500 Process 2 (depends on Process 1) \u2502 \u2514\u2500\u2500 Jobs \u2514\u2500\u2500 Process N (depends on previous steps) \u2514\u2500\u2500 Jobs Related: Basics , Architecture","title":"Pipeline"},{"location":"glossary/#process","text":"A class that inherits from Proc and defines a single step in a pipeline. A process specifies: - Input : What data it needs ( input attribute) - Output : What data it produces ( output attribute) - Script : How to transform input to output ( script attribute) - Dependencies : Which processes must complete first ( requires attribute) Example: class MyProcess ( Proc ): \"\"\"Process data files.\"\"\" input = \"infile:file\" output = \"outfile:file:result.txt\" script = \"cat {{ in.infile }} > {{ out.outfile }}\" Related: Defining a Process","title":"Process"},{"location":"glossary/#proc","text":"Abbreviation for Process . The base class Proc is the foundation for defining pipeline steps.","title":"Proc"},{"location":"glossary/#procgroup","text":"A collection of related processes that can be managed together. ProcGroups allow you to organize processes logically and apply configurations to multiple processes at once. Related: Process Group","title":"ProcGroup"},{"location":"glossary/#q","text":"","title":"Q"},{"location":"glossary/#r","text":"","title":"R"},{"location":"glossary/#raises","text":"A section in docstrings that lists exceptions that a function or method can raise. This helps users understand what errors to expect and how to handle them. Example: def process_data ( data : str ) -> dict : \"\"\"Process data string. Args: data: The data string to process. Returns: Dictionary with processing results. Raises: ValueError: If data is empty. TypeError: If data is not a string. \"\"\" if not data : raise ValueError ( \"Data cannot be empty\" ) if not isinstance ( data , str ): raise TypeError ( \"Data must be a string\" ) return { \"processed\" : data }","title":"Raises"},{"location":"glossary/#returns","text":"A section in docstrings that describes what a function or method returns. This includes the return type and description of the returned value. Example: def calculate_sum ( numbers : list [ int ]) -> int : \"\"\"Calculate the sum of a list of numbers. Args: numbers: List of integers to sum. Returns: The sum of all numbers in the list. \"\"\" return sum ( numbers )","title":"Returns"},{"location":"glossary/#s","text":"","title":"S"},{"location":"glossary/#scheduler","text":"A backend that manages job execution. pipen supports multiple schedulers: - Local : Execute jobs on the local machine - SGE : Submit jobs to Sun Grid Engine - SLURM : Submit jobs to SLURM workload manager - SSH : Execute jobs on remote servers via SSH - Container : Run jobs in containers - Gbatch : Submit jobs to Google Cloud Batch Related: Scheduler","title":"Scheduler"},{"location":"glossary/#t","text":"","title":"T"},{"location":"glossary/#u","text":"","title":"U"},{"location":"glossary/#v","text":"","title":"V"},{"location":"glossary/#w","text":"","title":"W"},{"location":"glossary/#x","text":"","title":"X"},{"location":"glossary/#y","text":"","title":"Y"},{"location":"glossary/#z","text":"","title":"Z"},{"location":"glossary/#related-documentation","text":"Architecture - Understanding how pipen components interact Basics - Pipeline layers and folder structure Defining a Process - Creating process classes Channels - Data flow between processes Caching - How job caching works Scheduler - Available scheduler backends","title":"Related Documentation"},{"location":"glossary/#common-patterns","text":"","title":"Common Patterns"},{"location":"glossary/#inputoutput-types","text":"Type Syntax Description var \"name:var\" In-memory variable file \"name:file\" Single file path dir \"name:dir\" Single directory path files \"name:files\" Multiple file paths dirs \"name:dirs\" Multiple directory paths","title":"Input/Output Types"},{"location":"glossary/#error-strategies","text":"Strategy Behavior halt Stop entire pipeline on any failure ignore Continue pipeline, ignoring failed jobs retry Retry failed jobs up to num_retries times","title":"Error Strategies"},{"location":"glossary/#template-variables","text":"Variable Description Example in.* Input variables {{ in.input_data }} out.* Output placeholders {{ out.result_file }} proc.* Process metadata {{ proc.name }} , {{ proc.workdir }} envs.* Template options (NOT shell env vars) {{ envs.my_option }} Defined via proc.envs attribute. These are custom options passed to template rendering context. They are NOT shell environment variables like PATH or HOME.","title":"Template Variables"},{"location":"input-output/","text":"Specify input of a process \u00b6 The input of a process is specified with input , the keys of the input data, and input_data , the real input data Tip Why separate the keys and data? Because the keys and data are not always combined, for example, we need the keys to infer the output and script (using them in the template), but the data may be deferred to obtain from the output of dependency processes. The complete form of an input key ( input ) is <key>:<type> . The <type> could be var , file , dir , files and dirs . A type of var can be omitted. So ph1, ph2 is the same as ph1:var, ph2:var If a process is requiring other processes, the specified input_data will be ignored, and will use the output data of their required processes: class P1 ( Proc ): input = \"v1\" output = \"o1:{{in.v1}}\" # pass by v1 as output variable input_data = [ \"a\" ] class P2 ( Proc ): input = \"v2\" output = \"o2:{{in.v2}}\" input_data = [ \"b\" ] class P3 ( Proc ): requires = [ P1 , P2 ] input = \"i1, i2\" output = \"o3:{{in.i1}}_{{in.i2}}\" # will be \"a_b\" # input_data = [] # ignored with a warning Pipen () . run ( P1 , P2 ) Tip The direct input_data is ignore, but you can use a callback to modify the input channel. For example: class P4 ( Proc ): requires = [ P1 , P2 ] input = \"i1, i2\" input_data = lambda ch : ch . applymap ( str . upper ) output = \"o3:{{in.i1}}_{{in.i2}}\" # will be \"A_B\" Note When the input data does have enough columns, None will be used with warnings. And when the input data has more columns than the input keys, the extra columns are dropped and ignored, also with warnings Specify output of a process \u00b6 Different from input, instead of channels, you have to tell pipen how to compute the output channel. The output can be a list or str . If it's str , a comma ( , ) is used to separate different keys: To use templating in output , see templating . class P1 ( Proc ): input = \"invar, infile\" input_data = [( 1 , \"/a/b/c.txt\" )] output = ( \"outvar:{{in.invar}}2, \" \"outfile:file:{{in.infile.split('/')[-1]}}2, \" \"outdir:dir:{{in.infile.split('/')[-1].split('.')[0]}}-dir\" ) # The type 'var' is omitted in the first element. # The output channel will be: # # outvar outfile outdir # <object> <object> <object> # 0 \"12\" \"<job.outdir>/c.text2\" \"<job.outdir>/c-dir\" Types of input and output \u00b6 Input \u00b6 Type Meaning var Use the values directly file Treat the data as a file path dir Treat the data as a directory path files Treat the data as a list of file paths dirs Treat the data as a list of directory paths For file / files , when checking whether a job is cached, their last modified time will be checked. For dir / dirs , if dirsig > 0 , then the files inside the directories will be checked. Otherwise, the directories themselves are checked for last modified time. Output \u00b6 Type Meaning Memo var Use the values directly dir Use the data as a directory path The directory will be created directly file Use the data as a file path","title":"Input and output"},{"location":"input-output/#specify-input-of-a-process","text":"The input of a process is specified with input , the keys of the input data, and input_data , the real input data Tip Why separate the keys and data? Because the keys and data are not always combined, for example, we need the keys to infer the output and script (using them in the template), but the data may be deferred to obtain from the output of dependency processes. The complete form of an input key ( input ) is <key>:<type> . The <type> could be var , file , dir , files and dirs . A type of var can be omitted. So ph1, ph2 is the same as ph1:var, ph2:var If a process is requiring other processes, the specified input_data will be ignored, and will use the output data of their required processes: class P1 ( Proc ): input = \"v1\" output = \"o1:{{in.v1}}\" # pass by v1 as output variable input_data = [ \"a\" ] class P2 ( Proc ): input = \"v2\" output = \"o2:{{in.v2}}\" input_data = [ \"b\" ] class P3 ( Proc ): requires = [ P1 , P2 ] input = \"i1, i2\" output = \"o3:{{in.i1}}_{{in.i2}}\" # will be \"a_b\" # input_data = [] # ignored with a warning Pipen () . run ( P1 , P2 ) Tip The direct input_data is ignore, but you can use a callback to modify the input channel. For example: class P4 ( Proc ): requires = [ P1 , P2 ] input = \"i1, i2\" input_data = lambda ch : ch . applymap ( str . upper ) output = \"o3:{{in.i1}}_{{in.i2}}\" # will be \"A_B\" Note When the input data does have enough columns, None will be used with warnings. And when the input data has more columns than the input keys, the extra columns are dropped and ignored, also with warnings","title":"Specify input of a process"},{"location":"input-output/#specify-output-of-a-process","text":"Different from input, instead of channels, you have to tell pipen how to compute the output channel. The output can be a list or str . If it's str , a comma ( , ) is used to separate different keys: To use templating in output , see templating . class P1 ( Proc ): input = \"invar, infile\" input_data = [( 1 , \"/a/b/c.txt\" )] output = ( \"outvar:{{in.invar}}2, \" \"outfile:file:{{in.infile.split('/')[-1]}}2, \" \"outdir:dir:{{in.infile.split('/')[-1].split('.')[0]}}-dir\" ) # The type 'var' is omitted in the first element. # The output channel will be: # # outvar outfile outdir # <object> <object> <object> # 0 \"12\" \"<job.outdir>/c.text2\" \"<job.outdir>/c-dir\"","title":"Specify output of a process"},{"location":"input-output/#types-of-input-and-output","text":"","title":"Types of input and output"},{"location":"input-output/#input","text":"Type Meaning var Use the values directly file Treat the data as a file path dir Treat the data as a directory path files Treat the data as a list of file paths dirs Treat the data as a list of directory paths For file / files , when checking whether a job is cached, their last modified time will be checked. For dir / dirs , if dirsig > 0 , then the files inside the directories will be checked. Otherwise, the directories themselves are checked for last modified time.","title":"Input"},{"location":"input-output/#output","text":"Type Meaning Memo var Use the values directly dir Use the data as a directory path The directory will be created directly file Use the data as a file path","title":"Output"},{"location":"plugin/","text":"pipen uses simplug for plugin support. There are very enriched hooks available for you to write your own plugins to extend pipen . Runtime plugins \u00b6 Plugin hooks \u00b6 To implement a function in your plugin, just simply: from pipen import plugin @plugin . impl [ async ] def hook ( ... ): ... Note that you have to use keyword-arguments and they have to match the hook signature. See simplug for more details. Pipeline-level hooks \u00b6 on_setup(pipen) (sync): Setup for the plugin, mainly used for initalization and set the default values for the plugin configuration items. This is only called once even when you have multiple pipelines ( Pipen objects) in a python session. on_init(pipen) (async) Called when pipeline is initialized. Note that here only default configurations are loaded (from defaults.CONFIG and config files). The configurations from Pipen constructor and the processes are not loaded yet. It's useful for plugins to change the default configurations. on_start(pipen) (async) Right before the pipeline starts to run. The process relationships are inferred here. You can access the start processes by pipen.starts and all processes by pipen.procs in the sequence of the execution order. on_complete(pipen, succeeded) (async) After all processes finish. succeeded indicates whether all processes/jobs finish successfully. Process-level hooks \u00b6 on_proc_create(proc) (sync) Called before proc get instantiated. Enables plugins to modify the default attributes of processes on_proc_input_computed(proc) (async) Called after process input data is computed. on_proc_script_computed(proc) (async) Called after process script is computed. The script is computed as a string that is about to compiled into a template. You can modify the script here. on_proc_start(proc) (async) When process object initialization completes, including the xqute . The process is ready to run. The jobs will be then initialized and fed to the scheduler. on_proc_shutdown(proc, sig) (sync) When the process is shut down (i.e. by <ctrl-c> ). You can access the signal that shuts the process down by sig . Only first plugin (based on the priority) that implements this hook will get called. on_proc_done(proc, succeeded) (async) When a process is done. Job-level hooks \u00b6 on_job_init(job) (async) When a job is initialized on_job_queued(job) (async) When a job is queued in xqute. Note it might not be queued yet in the scheduler system. on_job_submitting(job) (async) When a job is submitting. The first plugin (based on priority) have this hook return False will cancel the submission on_job_submitted(job) (async) When a job is submitted in the scheduler system. on_job_started(job) (async) When a job starts to run in then scheduler system. on_job_polling(job) (async) When status of a job is being polled. on_job_killing(job) (async) When a job is being killed. The first plugin (based on priority) have this hook return False will cancel the killing on_job_killed(job) (async) When a job is killed on_job_succeeded(job) (async) When a job completes successfully on_job_cached(job) (async) When a job is cached on_job_failed(job) (async) When a job is done but failed (i.e. return_code == 1). on_jobcmd_init(job) -> str (sync) When the job command wrapper script is initialized before the prescript is run This should return a piece of bash code to be inserted in the wrapped job script (template), which is a python template string, with the following variables available: status and job . status is the class JobStatus from xqute.defaults.py and job is the Job instance. For multiple plugins, the code will be inserted in the order of the plugin priority. The code will replace the #![jobcmd_init] placeholder in the wrapped job script. See also https://github.com/pwwang/xqute/blob/master/xqute/defaults.py#L95 on_jobcmd_prep(job) -> str (sync) When the job command right about to be run This should return a piece of bash code to be inserted in the wrapped job script (template), which is a python template string, with the following variables available: status and job . status is the class JobStatus from xqute.defaults.py and job is the Job instance. The bash variable $cmd is accessible in the context. It is also possible to modify the cmd variable. Just remember to assign the modified value to cmd . For multiple plugins, the code will be inserted in the order of the plugin priority. Keep in mind that the $cmd may be modified by other plugins. The code will replace the #![jobcmd_prep] placeholder in the wrapped job script. See also https://github.com/pwwang/xqute/blob/master/xqute/defaults.py#L95 on_jobcmd_end(job) -> str (sync): When the job command finishes and after the postscript is run This should return a piece of bash code to be inserted in the wrapped job script (template), which is a python template string, with the following variables available: status and job . status is the class JobStatus from xqute.defaults.py and job is the Job instance. The bash variable $rc is accessible in the context, which is the return code of the job command. For multiple plugins, the code will be inserted in the order of the plugin priority. The code will replace the #![jobcmd_end] placeholder in the wrapped job script. See also https://github.com/pwwang/xqute/blob/master/xqute/defaults.py#L95 Loading plugins \u00b6 You can specify the plugins to be loaded by specifying the names or the plugin itself in plugins configuration. With names, the plugins will be loaded from entry points . You can also disable some plugins if they are set in the lower-priority configurations. For example, you want to disable pipen_verbose (enabled in a configuration file) for a pipeline: Pipen ( ... , plugins = [ \"-pipen_verbose\" ]) Note You can use + as prefix to enable a disabled plugin, or - as prefix to disable an enabled plugin. If no prefix is used, only the specified plugins will be enabled and all other plugins will be disabled. You should either use + or - for all plugins or none of them. If a plugin is not given as a string, it will be treated as +plugin . Writing a plugin \u00b6 You can write your own plugin by implementing some of the above hooks. You can import the plugin directly and add it to `Pipen(..., plugins=[...]). For example: from pipen import plugin , Pipen class PipenPlugin : @plugin . impl [ async ] def hook ( ... ): ... Pipen ( ... , plugins = [ PipenPlugin ]) You can also use the entry point to register your plugin using the group name ` pipen ` For ` setup . py ` , you will need : ``` python setup ( # ... entry_points = { \"pipen\" : [ \"pipen_verbose = pipen_verbose\" ]}, # ... ) For pyproject.toml : [tool.poetry.plugins.pipen] pipen_verbose = \"pipen_verbose\" Then the plugin pipen_verbose can be loaded by plugins=[\"+pipen_verbose\"] or disabled by plugins=[\"-pipen_verbose\"] Logging to the console from a plugin \u00b6 Of course you can do arbitrary logging from a plugin. However, to keep the consistency with main logger of pipen , The best practice is: from pipen.utils import get_logger logger = get_logger ( \"verbose\" , \"info\" ) # do some logging inside the hooks The above code will produce some logging on the console like this: 11 -04 12 :00:19 I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 Process \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e 11 -04 12 :00:19 I main \u2551 Undescribed. \u2551 11 -04 12 :00:19 I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f 11 -04 12 :00:19 I main Process: Workdir: '.pipen/process' 11 -04 12 :00:19 I verbose Process: size: 10 11 -04 12 :00:19 I verbose Process: [ 0 /9 ] in .a: 0 11 -04 12 :00:19 I verbose Process: [ 0 /9 ] out.b: pipeline-0-output/Process/0/a.txt CLI plugins \u00b6 See CLI for more details. Plugin gallery \u00b6 pipen-verbose : Add verbosal information in logs for pipen. pipen-report : Generate report for pipen pipen-filters : Add a set of useful filters for pipen templates. pipen-diagram : Draw pipeline diagrams for pipen pipen-args : Command line argument parser for pipen pipen-dry : Dry runner for pipen pipelines pipen-annotate : Use docstring to annotate pipen processes pipen-board : Visualize configuration and running of pipen pipelines on the web pipen-lock : Process lock for pipen to prevent multiple runs at the same time. pipen-log2file : Save running logs to file for pipen pipen-poplog : Populate logs from jobs to running log of the pipeline pipen-runinfo : Save running information to file for pipen pipen-gcs : A plugin for pipen to handle files in Google Cloud Storage.","title":"Plugins"},{"location":"plugin/#runtime-plugins","text":"","title":"Runtime plugins"},{"location":"plugin/#plugin-hooks","text":"To implement a function in your plugin, just simply: from pipen import plugin @plugin . impl [ async ] def hook ( ... ): ... Note that you have to use keyword-arguments and they have to match the hook signature. See simplug for more details.","title":"Plugin hooks"},{"location":"plugin/#pipeline-level-hooks","text":"on_setup(pipen) (sync): Setup for the plugin, mainly used for initalization and set the default values for the plugin configuration items. This is only called once even when you have multiple pipelines ( Pipen objects) in a python session. on_init(pipen) (async) Called when pipeline is initialized. Note that here only default configurations are loaded (from defaults.CONFIG and config files). The configurations from Pipen constructor and the processes are not loaded yet. It's useful for plugins to change the default configurations. on_start(pipen) (async) Right before the pipeline starts to run. The process relationships are inferred here. You can access the start processes by pipen.starts and all processes by pipen.procs in the sequence of the execution order. on_complete(pipen, succeeded) (async) After all processes finish. succeeded indicates whether all processes/jobs finish successfully.","title":"Pipeline-level hooks"},{"location":"plugin/#process-level-hooks","text":"on_proc_create(proc) (sync) Called before proc get instantiated. Enables plugins to modify the default attributes of processes on_proc_input_computed(proc) (async) Called after process input data is computed. on_proc_script_computed(proc) (async) Called after process script is computed. The script is computed as a string that is about to compiled into a template. You can modify the script here. on_proc_start(proc) (async) When process object initialization completes, including the xqute . The process is ready to run. The jobs will be then initialized and fed to the scheduler. on_proc_shutdown(proc, sig) (sync) When the process is shut down (i.e. by <ctrl-c> ). You can access the signal that shuts the process down by sig . Only first plugin (based on the priority) that implements this hook will get called. on_proc_done(proc, succeeded) (async) When a process is done.","title":"Process-level hooks"},{"location":"plugin/#job-level-hooks","text":"on_job_init(job) (async) When a job is initialized on_job_queued(job) (async) When a job is queued in xqute. Note it might not be queued yet in the scheduler system. on_job_submitting(job) (async) When a job is submitting. The first plugin (based on priority) have this hook return False will cancel the submission on_job_submitted(job) (async) When a job is submitted in the scheduler system. on_job_started(job) (async) When a job starts to run in then scheduler system. on_job_polling(job) (async) When status of a job is being polled. on_job_killing(job) (async) When a job is being killed. The first plugin (based on priority) have this hook return False will cancel the killing on_job_killed(job) (async) When a job is killed on_job_succeeded(job) (async) When a job completes successfully on_job_cached(job) (async) When a job is cached on_job_failed(job) (async) When a job is done but failed (i.e. return_code == 1). on_jobcmd_init(job) -> str (sync) When the job command wrapper script is initialized before the prescript is run This should return a piece of bash code to be inserted in the wrapped job script (template), which is a python template string, with the following variables available: status and job . status is the class JobStatus from xqute.defaults.py and job is the Job instance. For multiple plugins, the code will be inserted in the order of the plugin priority. The code will replace the #![jobcmd_init] placeholder in the wrapped job script. See also https://github.com/pwwang/xqute/blob/master/xqute/defaults.py#L95 on_jobcmd_prep(job) -> str (sync) When the job command right about to be run This should return a piece of bash code to be inserted in the wrapped job script (template), which is a python template string, with the following variables available: status and job . status is the class JobStatus from xqute.defaults.py and job is the Job instance. The bash variable $cmd is accessible in the context. It is also possible to modify the cmd variable. Just remember to assign the modified value to cmd . For multiple plugins, the code will be inserted in the order of the plugin priority. Keep in mind that the $cmd may be modified by other plugins. The code will replace the #![jobcmd_prep] placeholder in the wrapped job script. See also https://github.com/pwwang/xqute/blob/master/xqute/defaults.py#L95 on_jobcmd_end(job) -> str (sync): When the job command finishes and after the postscript is run This should return a piece of bash code to be inserted in the wrapped job script (template), which is a python template string, with the following variables available: status and job . status is the class JobStatus from xqute.defaults.py and job is the Job instance. The bash variable $rc is accessible in the context, which is the return code of the job command. For multiple plugins, the code will be inserted in the order of the plugin priority. The code will replace the #![jobcmd_end] placeholder in the wrapped job script. See also https://github.com/pwwang/xqute/blob/master/xqute/defaults.py#L95","title":"Job-level hooks"},{"location":"plugin/#loading-plugins","text":"You can specify the plugins to be loaded by specifying the names or the plugin itself in plugins configuration. With names, the plugins will be loaded from entry points . You can also disable some plugins if they are set in the lower-priority configurations. For example, you want to disable pipen_verbose (enabled in a configuration file) for a pipeline: Pipen ( ... , plugins = [ \"-pipen_verbose\" ]) Note You can use + as prefix to enable a disabled plugin, or - as prefix to disable an enabled plugin. If no prefix is used, only the specified plugins will be enabled and all other plugins will be disabled. You should either use + or - for all plugins or none of them. If a plugin is not given as a string, it will be treated as +plugin .","title":"Loading plugins"},{"location":"plugin/#writing-a-plugin","text":"You can write your own plugin by implementing some of the above hooks. You can import the plugin directly and add it to `Pipen(..., plugins=[...]). For example: from pipen import plugin , Pipen class PipenPlugin : @plugin . impl [ async ] def hook ( ... ): ... Pipen ( ... , plugins = [ PipenPlugin ]) You can also use the entry point to register your plugin using the group name ` pipen ` For ` setup . py ` , you will need : ``` python setup ( # ... entry_points = { \"pipen\" : [ \"pipen_verbose = pipen_verbose\" ]}, # ... ) For pyproject.toml : [tool.poetry.plugins.pipen] pipen_verbose = \"pipen_verbose\" Then the plugin pipen_verbose can be loaded by plugins=[\"+pipen_verbose\"] or disabled by plugins=[\"-pipen_verbose\"]","title":"Writing a plugin"},{"location":"plugin/#logging-to-the-console-from-a-plugin","text":"Of course you can do arbitrary logging from a plugin. However, to keep the consistency with main logger of pipen , The best practice is: from pipen.utils import get_logger logger = get_logger ( \"verbose\" , \"info\" ) # do some logging inside the hooks The above code will produce some logging on the console like this: 11 -04 12 :00:19 I main \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 Process \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e 11 -04 12 :00:19 I main \u2551 Undescribed. \u2551 11 -04 12 :00:19 I main \u2570\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256f 11 -04 12 :00:19 I main Process: Workdir: '.pipen/process' 11 -04 12 :00:19 I verbose Process: size: 10 11 -04 12 :00:19 I verbose Process: [ 0 /9 ] in .a: 0 11 -04 12 :00:19 I verbose Process: [ 0 /9 ] out.b: pipeline-0-output/Process/0/a.txt","title":"Logging to the console from a plugin"},{"location":"plugin/#cli-plugins","text":"See CLI for more details.","title":"CLI plugins"},{"location":"plugin/#plugin-gallery","text":"pipen-verbose : Add verbosal information in logs for pipen. pipen-report : Generate report for pipen pipen-filters : Add a set of useful filters for pipen templates. pipen-diagram : Draw pipeline diagrams for pipen pipen-args : Command line argument parser for pipen pipen-dry : Dry runner for pipen pipelines pipen-annotate : Use docstring to annotate pipen processes pipen-board : Visualize configuration and running of pipen pipelines on the web pipen-lock : Process lock for pipen to prevent multiple runs at the same time. pipen-log2file : Save running logs to file for pipen pipen-poplog : Populate logs from jobs to running log of the pipeline pipen-runinfo : Save running information to file for pipen pipen-gcs : A plugin for pipen to handle files in Google Cloud Storage.","title":"Plugin gallery"},{"location":"proc-group/","text":"A process group is a collection of processes that are related to each other. It is a convenient way to manage a set of processes. With pipen , not only a process can be reused, but also a group of processes can be reused. We just need to define the relationship between the processes in the group, and then we can reuse the group in other pipelines, or even run it directly as a pipeline. Define a process group \u00b6 To define a process group, we need to define a class that inherits from pipen.procgroup.ProcGroup . The class name will be the name of the group, unless we specify a name attribute. from pipen.procgroup import ProcGroup class MyGroup ( ProcGroup ): ... Note that the subclasses of ProcGroup are singleton classes. If you need to define multiple groups, you can define a base class and then inherit from it. Add processes to a group \u00b6 There are two ways to add processes to a group, using pg.add_proc or ProcGroup.add_proc , where pg is a process group instance. The first method is used after the group is instantiated and it decorates a process class directly. The second method is used before the group is instantiated and it decorates a property of ProcGroup that returns a process. Using the pg.add_proc() decorator. from pipen import Proc , ProcGroup class MyGroup ( ProcGroup ): ... pg = MyGroup () @pg . add_proc class MyProc ( Proc ): ... Using the ProcGroup.add_proc() decorator to decorate a property of the group class. from pipen import Proc , ProcGroup class MyGroup ( ProcGroup ): @ProcGroup . add_proc def my_proc ( self ): class MyProc ( Proc ): ... return MyProc This method adds a process at runtime, so it is useful when we want to add processes to a group dynamically. Access processes in a group \u00b6 We can access the processes in a group using the pg.<proc> attribute, where pg is a process group instance. Note that when we use the ProcGroup.add_proc method to add processes, the process name is the name of the property. However, you can always use pg.procs.<proc_name> to access the process, where the <proc_name> is the real name of the process. from pipen import Proc , ProcGroup class MyGroup ( ProcGroup ): @ProcGroup . add_proc def my_proc ( self ): class MyProc ( Proc ): ... return MyProc pg = MyGroup () assert pg . my_proc . name == 'MyProc' assert pg . procs . MyProc . name == 'MyProc' We can use pg.starts to get the start processes of the group, which are the processes that have no required processes. So when you add processes to a group, remember to specify .requires for each process, unless they are start processes. Run a process group as a pipeline \u00b6 To run a process group as a pipeline, we can convert it to a pipeline using the as_pipen() method. The method takes the same arguments as the Pipen constructor. from pipen import Proc , ProcGroup class MyGroup ( ProcGroup ): ... pg = MyGroup () @pg . add_proc class MyProc ( Proc ): ... pg . as_pipen () . set_data ( ... ) . run () Integrate a process group into a pipeline \u00b6 from pipen import Proc , ProcGroup class MyGroup ( ProcGroup ): @ProcGroup . add_proc def my_proc ( self ): class MyProc ( Proc ): ... return MyProc @ProcGroup . add_proc def my_proc2 ( self ): class MyProc2 ( Proc ): requires = self . my_proc ... return MyProc2 pg = MyGroup () class PrepareData ( Proc ): ... class PostGroup ( Proc ): requires = pg . my_proc2 pg . my_proc . requires = PrepareData pipen = Pipen () . set_starts ( PrepareData ) . set_data ( ... ) . run ()","title":"Process group"},{"location":"proc-group/#define-a-process-group","text":"To define a process group, we need to define a class that inherits from pipen.procgroup.ProcGroup . The class name will be the name of the group, unless we specify a name attribute. from pipen.procgroup import ProcGroup class MyGroup ( ProcGroup ): ... Note that the subclasses of ProcGroup are singleton classes. If you need to define multiple groups, you can define a base class and then inherit from it.","title":"Define a process group"},{"location":"proc-group/#add-processes-to-a-group","text":"There are two ways to add processes to a group, using pg.add_proc or ProcGroup.add_proc , where pg is a process group instance. The first method is used after the group is instantiated and it decorates a process class directly. The second method is used before the group is instantiated and it decorates a property of ProcGroup that returns a process. Using the pg.add_proc() decorator. from pipen import Proc , ProcGroup class MyGroup ( ProcGroup ): ... pg = MyGroup () @pg . add_proc class MyProc ( Proc ): ... Using the ProcGroup.add_proc() decorator to decorate a property of the group class. from pipen import Proc , ProcGroup class MyGroup ( ProcGroup ): @ProcGroup . add_proc def my_proc ( self ): class MyProc ( Proc ): ... return MyProc This method adds a process at runtime, so it is useful when we want to add processes to a group dynamically.","title":"Add processes to a group"},{"location":"proc-group/#access-processes-in-a-group","text":"We can access the processes in a group using the pg.<proc> attribute, where pg is a process group instance. Note that when we use the ProcGroup.add_proc method to add processes, the process name is the name of the property. However, you can always use pg.procs.<proc_name> to access the process, where the <proc_name> is the real name of the process. from pipen import Proc , ProcGroup class MyGroup ( ProcGroup ): @ProcGroup . add_proc def my_proc ( self ): class MyProc ( Proc ): ... return MyProc pg = MyGroup () assert pg . my_proc . name == 'MyProc' assert pg . procs . MyProc . name == 'MyProc' We can use pg.starts to get the start processes of the group, which are the processes that have no required processes. So when you add processes to a group, remember to specify .requires for each process, unless they are start processes.","title":"Access processes in a group"},{"location":"proc-group/#run-a-process-group-as-a-pipeline","text":"To run a process group as a pipeline, we can convert it to a pipeline using the as_pipen() method. The method takes the same arguments as the Pipen constructor. from pipen import Proc , ProcGroup class MyGroup ( ProcGroup ): ... pg = MyGroup () @pg . add_proc class MyProc ( Proc ): ... pg . as_pipen () . set_data ( ... ) . run ()","title":"Run a process group as a pipeline"},{"location":"proc-group/#integrate-a-process-group-into-a-pipeline","text":"from pipen import Proc , ProcGroup class MyGroup ( ProcGroup ): @ProcGroup . add_proc def my_proc ( self ): class MyProc ( Proc ): ... return MyProc @ProcGroup . add_proc def my_proc2 ( self ): class MyProc2 ( Proc ): requires = self . my_proc ... return MyProc2 pg = MyGroup () class PrepareData ( Proc ): ... class PostGroup ( Proc ): requires = pg . my_proc2 pg . my_proc . requires = PrepareData pipen = Pipen () . set_starts ( PrepareData ) . set_data ( ... ) . run ()","title":"Integrate a process group into a pipeline"},{"location":"running/","text":"Creating a Pipen object \u00b6 The arguments for the constrctor are: name : The name of the pipeline desc : The description of the pipeline outdir : The output directory of the pipeline. If not provided, defaults to <pipeline-name>_results . **kwargs : Other configurations Specification of the start processes \u00b6 Once the requirements of the processes are specified, we are able to build the entire process dependency network. To start runing a pipeline, we just need to specify the start processes to start: class P1 ( Proc ): ... class P2 ( Proc ): ... class P3 ( Proc ): requires = [ P1 , P2 ] ... Pipen () . set_starts ( P1 , P2 ) You can specify the start processes individually, like we did above, or send a list of processes: Pipen () . set_starts ([ P1 , P2 ]) Setting input data for start processes \u00b6 Other than set the input data when defining a process, you can also specify the input data for start processes: Pipen () . set_starts ( P1 , P2 ) . set_data ( < data for P1 > , < data for P2 > ) This is useful when you want to reuse the processes. The order of data in .set_data() has to be the same as the order of processes to be set in .set_starts() . When the input_data of a start process has already been set, an error will be raised. To use that input_data , use None in .set_data() . For example: class P1 ( Proc ): ... class P2 ( Proc ): input_data = [ 1 ] Pipen () . set_starts ( P1 , P2 ) . set_data ( < data for P1 > , None ) Running with a different profile \u00b6 Pipen.run() accepts an argument profile , which allows you to use different profile from configuration files to run the pipeline: Pipen () . run ( \"sge\" ) See configurations for more details. Shortcut for running a pipeline \u00b6 import pipen class P1 ( pipen . Proc ): ... class P2 ( pipen . Proc ): ... class P3 ( pipen . Proc ): requires = [ P1 , P2 ] ... pipen . run ( \"MyPipeline\" , starts = [ P1 , P2 ], data = [ < data for P1 > , < data for P2 > ]) >>> help ( pipen . run ) run ( name : 'str' , starts : 'Type[Proc] | List[Type[Proc]]' , data : 'Iterable' = None , * , desc : 'str' = None , outdir : 'str | PathLike' = None , profile : 'str' = 'default' , ** kwargs , ) -> 'bool' Shortcut to run a pipeline Args : name : The name of the pipeline starts : The start processes data : The input data for the start processes desc : The description of the pipeline outdir : The output directory of the results profile : The profile to use ** kwargs : Other options pass to Pipen to create the pipeline Returns : True if the pipeline ends successfully else False Progress bars \u00b6 When running a pipeline, a progress bar will be shown in the console to indicate the progress of the pipeline. You will see two progress bars: one for the overall progress of the pipeline (running processes vs total processes), and one for the current running process (prepared/running/failed/succeeded/cached jobs vs total jobs). The above progress bars indicate that the pipeline has 2 processes in total, and 1 process is currently running. The running process has 11 jobs in total, and all jobs are prepared, where none of them have started running, 1 has succeeded/cached, and 2 are submitted. And the above progress bars indicate that the running process has 11 jobs in total, and 1 jobs have succeeded/cached, 2 jobs started running, and the rest are still prepared. Colors are used to indicate different job statuses: \u25a0 grey: initiated \u25a0 light blue: queued (added in v1.0.3) \u25a0 cyan: submitted \u25a0 yellow: running \u25a0 green: succeeded/cached \u25a0 red: failed Since v1.0.0, the progress bars also show the number of jobs in different statuses: I: initiated Q: queued (added in v1.0.3) S: submitted R: running D: succeeded/cached | failed For single-job processes, the job progress bar will be simplified to show the status of the job directly.","title":"Defining and running a pipeline"},{"location":"running/#creating-a-pipen-object","text":"The arguments for the constrctor are: name : The name of the pipeline desc : The description of the pipeline outdir : The output directory of the pipeline. If not provided, defaults to <pipeline-name>_results . **kwargs : Other configurations","title":"Creating a Pipen object"},{"location":"running/#specification-of-the-start-processes","text":"Once the requirements of the processes are specified, we are able to build the entire process dependency network. To start runing a pipeline, we just need to specify the start processes to start: class P1 ( Proc ): ... class P2 ( Proc ): ... class P3 ( Proc ): requires = [ P1 , P2 ] ... Pipen () . set_starts ( P1 , P2 ) You can specify the start processes individually, like we did above, or send a list of processes: Pipen () . set_starts ([ P1 , P2 ])","title":"Specification of the start processes"},{"location":"running/#setting-input-data-for-start-processes","text":"Other than set the input data when defining a process, you can also specify the input data for start processes: Pipen () . set_starts ( P1 , P2 ) . set_data ( < data for P1 > , < data for P2 > ) This is useful when you want to reuse the processes. The order of data in .set_data() has to be the same as the order of processes to be set in .set_starts() . When the input_data of a start process has already been set, an error will be raised. To use that input_data , use None in .set_data() . For example: class P1 ( Proc ): ... class P2 ( Proc ): input_data = [ 1 ] Pipen () . set_starts ( P1 , P2 ) . set_data ( < data for P1 > , None )","title":"Setting input data for start processes"},{"location":"running/#running-with-a-different-profile","text":"Pipen.run() accepts an argument profile , which allows you to use different profile from configuration files to run the pipeline: Pipen () . run ( \"sge\" ) See configurations for more details.","title":"Running with a different profile"},{"location":"running/#shortcut-for-running-a-pipeline","text":"import pipen class P1 ( pipen . Proc ): ... class P2 ( pipen . Proc ): ... class P3 ( pipen . Proc ): requires = [ P1 , P2 ] ... pipen . run ( \"MyPipeline\" , starts = [ P1 , P2 ], data = [ < data for P1 > , < data for P2 > ]) >>> help ( pipen . run ) run ( name : 'str' , starts : 'Type[Proc] | List[Type[Proc]]' , data : 'Iterable' = None , * , desc : 'str' = None , outdir : 'str | PathLike' = None , profile : 'str' = 'default' , ** kwargs , ) -> 'bool' Shortcut to run a pipeline Args : name : The name of the pipeline starts : The start processes data : The input data for the start processes desc : The description of the pipeline outdir : The output directory of the results profile : The profile to use ** kwargs : Other options pass to Pipen to create the pipeline Returns : True if the pipeline ends successfully else False","title":"Shortcut for running a pipeline"},{"location":"running/#progress-bars","text":"When running a pipeline, a progress bar will be shown in the console to indicate the progress of the pipeline. You will see two progress bars: one for the overall progress of the pipeline (running processes vs total processes), and one for the current running process (prepared/running/failed/succeeded/cached jobs vs total jobs). The above progress bars indicate that the pipeline has 2 processes in total, and 1 process is currently running. The running process has 11 jobs in total, and all jobs are prepared, where none of them have started running, 1 has succeeded/cached, and 2 are submitted. And the above progress bars indicate that the running process has 11 jobs in total, and 1 jobs have succeeded/cached, 2 jobs started running, and the rest are still prepared. Colors are used to indicate different job statuses: \u25a0 grey: initiated \u25a0 light blue: queued (added in v1.0.3) \u25a0 cyan: submitted \u25a0 yellow: running \u25a0 green: succeeded/cached \u25a0 red: failed Since v1.0.0, the progress bars also show the number of jobs in different statuses: I: initiated Q: queued (added in v1.0.3) S: submitted R: running D: succeeded/cached | failed For single-job processes, the job progress bar will be simplified to show the status of the job directly.","title":"Progress bars"},{"location":"scheduler/","text":"pipen can send jobs to different scheduler system to run. To specify the scheduler, use scheduler and scheduler_opts configurations. Default supported schedulers \u00b6 pipen uses xqute for scheduler backend support. The following schedulers are supported by pipen : local \u00b6 This is the default scheduler used by pipen . The jobs will be run on the local machine. No scheduler-specific options are available. sge \u00b6 Send the jobs to run on sge scheduler. The scheduler_opts will be the ones supported by qsub . slurm \u00b6 Send the jobs to run on slurm scheduler. The scheduler_opts will be the ones supported by sbatch . ssh \u00b6 Send the jobs to run on a remote machine via ssh . The scheduler_opts will be the ones supported by ssh . See also xqute . container \u00b6 Send the jobs to run in a container (Docker/Podman/Apptainer). The scheduler_opts will be used to construct the container command. They include: - image : The container image to use. - entrypoint : The entrypoint of the container to run the wrapped job script. If not specified, the default entrypoint /bin/sh will be used. - bin : The container command to use. If not specified, it will use docker . - volumes : A list of volumes to mount to the container. The default volumes are: - workdir : The working directory of the pipeline, mounted to /mnt/disks/pipen-pipeline/workdir . - outdir : The output directory of the pipeline, mounted to /mnt/disks/pipen-pipeline/outdir . - envs : A dictionary of environment variables to set in the container. - remove : Whether to remove the container after the job is done. Default is True . Only supported by Docker and Podman. - user : The user to run the container as. Default is the current user. Only supported by Docker and Podman. - bin_args : Additional arguments to pass to the container command. For example, {\"bin_args\": [\"--privileged\"]} will run the container in privileged mode. Only supported by Docker and Podman. gbatch \u00b6 Send the jobs to run using Google Cloud Batch. The scheduler_opts will be used to construct the job configuration. This scheduler requires that the pipeline's outdir is a Google Cloud Storage path (e.g., gs://bucket/path ). The scheduler options include: - project : Google Cloud project ID - location : Google Cloud region or zone - mount : GCS path to mount (e.g. gs://my-bucket:/mnt/my-bucket ). You can pass a list of mounts. - service_account : GCP service account email (e.g. test-account@example.com ) - network : GCP network (e.g. default-network ) - subnetwork : GCP subnetwork (e.g. regions/us-central1/subnetworks/default ) - no_external_ip_address : Whether to disable external IP address - machine_type : GCP machine type (e.g. e2-standard-4 ) - provisioning_model : GCP provisioning model (e.g. SPOT ) - image_uri : Container image URI (e.g. ubuntu-2004-lts ) - entrypoint : Container entrypoint (e.g. /bin/bash ) - commands : The command list to run in the container. There are three ways to specify the commands: 1. If no entrypoint is specified, the final command will be [commands, wrapped_script], where the entrypoint is the wrapper script interpreter that is determined by JOBCMD_WRAPPER_LANG (e.g. /bin/bash), commands is the list you provided, and wrapped_script is the path to the wrapped job script. 2. You can specify something like \"-c\", then the final command will be [\"-c\", \"wrapper_script_interpreter, wrapper_script\"] 3. You can use the placeholders {lang} and {script} in the commands list, where {lang} will be replaced with the interpreter (e.g. /bin/bash) and {script} will be replaced with the path to the wrapped job script. For example, you can specify [\"{lang} {script}\"] and the final command will be [\"wrapper_interpreter, wrapper_script\"] Additional keyword arguments can be used for job configuration (e.g. taskGroups ). See more details at Google Cloud Batch documentation . By default, the pipeline's workdir is mounted to /mnt/disks/pipen-pipeline/workdir and the outdir is mounted to /mnt/disks/pipen-pipeline/outdir on the VM. Writing your own scheduler plugin \u00b6 To write a scheduler plugin, you need to subclass both xqute.schedulers.scheduler.Scheduler and pipen.scheduler.SchedulerPostInit . For examples of a scheduler plugin, see local_scheduler , sge_scheduler , slurm_scheduler , ssh_scheduler , and [gbatch_scheduler][6], and also pipen.scheduler . A scheduler class can be passed to scheduler configuration directly to be used as a scheduler. But you can also register it with entry points: For setup.py , you will need: setup ( # ... entry_points = { \"pipen_sched\" : [ \"mysched = pipen_mysched\" ]}, # ... ) For pyproject.toml : [tool.poetry.plugins.pipen_sched] mysched = \"pipen_mysched\" Then you can switch the scheduler to mysched by scheduler=\"mysched\"","title":"Scheduler"},{"location":"scheduler/#default-supported-schedulers","text":"pipen uses xqute for scheduler backend support. The following schedulers are supported by pipen :","title":"Default supported schedulers"},{"location":"scheduler/#local","text":"This is the default scheduler used by pipen . The jobs will be run on the local machine. No scheduler-specific options are available.","title":"local"},{"location":"scheduler/#sge","text":"Send the jobs to run on sge scheduler. The scheduler_opts will be the ones supported by qsub .","title":"sge"},{"location":"scheduler/#slurm","text":"Send the jobs to run on slurm scheduler. The scheduler_opts will be the ones supported by sbatch .","title":"slurm"},{"location":"scheduler/#ssh","text":"Send the jobs to run on a remote machine via ssh . The scheduler_opts will be the ones supported by ssh . See also xqute .","title":"ssh"},{"location":"scheduler/#container","text":"Send the jobs to run in a container (Docker/Podman/Apptainer). The scheduler_opts will be used to construct the container command. They include: - image : The container image to use. - entrypoint : The entrypoint of the container to run the wrapped job script. If not specified, the default entrypoint /bin/sh will be used. - bin : The container command to use. If not specified, it will use docker . - volumes : A list of volumes to mount to the container. The default volumes are: - workdir : The working directory of the pipeline, mounted to /mnt/disks/pipen-pipeline/workdir . - outdir : The output directory of the pipeline, mounted to /mnt/disks/pipen-pipeline/outdir . - envs : A dictionary of environment variables to set in the container. - remove : Whether to remove the container after the job is done. Default is True . Only supported by Docker and Podman. - user : The user to run the container as. Default is the current user. Only supported by Docker and Podman. - bin_args : Additional arguments to pass to the container command. For example, {\"bin_args\": [\"--privileged\"]} will run the container in privileged mode. Only supported by Docker and Podman.","title":"container"},{"location":"scheduler/#gbatch","text":"Send the jobs to run using Google Cloud Batch. The scheduler_opts will be used to construct the job configuration. This scheduler requires that the pipeline's outdir is a Google Cloud Storage path (e.g., gs://bucket/path ). The scheduler options include: - project : Google Cloud project ID - location : Google Cloud region or zone - mount : GCS path to mount (e.g. gs://my-bucket:/mnt/my-bucket ). You can pass a list of mounts. - service_account : GCP service account email (e.g. test-account@example.com ) - network : GCP network (e.g. default-network ) - subnetwork : GCP subnetwork (e.g. regions/us-central1/subnetworks/default ) - no_external_ip_address : Whether to disable external IP address - machine_type : GCP machine type (e.g. e2-standard-4 ) - provisioning_model : GCP provisioning model (e.g. SPOT ) - image_uri : Container image URI (e.g. ubuntu-2004-lts ) - entrypoint : Container entrypoint (e.g. /bin/bash ) - commands : The command list to run in the container. There are three ways to specify the commands: 1. If no entrypoint is specified, the final command will be [commands, wrapped_script], where the entrypoint is the wrapper script interpreter that is determined by JOBCMD_WRAPPER_LANG (e.g. /bin/bash), commands is the list you provided, and wrapped_script is the path to the wrapped job script. 2. You can specify something like \"-c\", then the final command will be [\"-c\", \"wrapper_script_interpreter, wrapper_script\"] 3. You can use the placeholders {lang} and {script} in the commands list, where {lang} will be replaced with the interpreter (e.g. /bin/bash) and {script} will be replaced with the path to the wrapped job script. For example, you can specify [\"{lang} {script}\"] and the final command will be [\"wrapper_interpreter, wrapper_script\"] Additional keyword arguments can be used for job configuration (e.g. taskGroups ). See more details at Google Cloud Batch documentation . By default, the pipeline's workdir is mounted to /mnt/disks/pipen-pipeline/workdir and the outdir is mounted to /mnt/disks/pipen-pipeline/outdir on the VM.","title":"gbatch"},{"location":"scheduler/#writing-your-own-scheduler-plugin","text":"To write a scheduler plugin, you need to subclass both xqute.schedulers.scheduler.Scheduler and pipen.scheduler.SchedulerPostInit . For examples of a scheduler plugin, see local_scheduler , sge_scheduler , slurm_scheduler , ssh_scheduler , and [gbatch_scheduler][6], and also pipen.scheduler . A scheduler class can be passed to scheduler configuration directly to be used as a scheduler. But you can also register it with entry points: For setup.py , you will need: setup ( # ... entry_points = { \"pipen_sched\" : [ \"mysched = pipen_mysched\" ]}, # ... ) For pyproject.toml : [tool.poetry.plugins.pipen_sched] mysched = \"pipen_mysched\" Then you can switch the scheduler to mysched by scheduler=\"mysched\"","title":"Writing your own scheduler plugin"},{"location":"script/","text":"For templating in script , see templating Choosing your language \u00b6 You can specify the path of interpreter to lang . If the interpreter is in $PATH , you can directly give the basename of the interpreter (i.e. python instead of /path/to/python ). For example, if you have your own perl installed at /home/user/bin/perl , then you need to tell pipen where it is: lang = \"/home/user/bin/perl\" . If /home/user/bin is in your $PATH , you can simply do: lang = \"perl\" You can also use shebang to specify the interperter: #!/home/usr/bin/perl # You perl code goes here If you have shebang in your script, the lang specified in the configuration files and Pipen constructor will be ignored (but the one specified in process definition is not). Use script from a file \u00b6 You can also put the script into a file, and use it with a file:// prefix: script = \"file:///a/b/c.pl\" Note You may also use a script file with a relative path, which is relative to where process is defined. For example: a process with script = \"file://./scripts/script.py\" is defined in /a/b/pipeline.py , then the script file refers to /a/b/scripts/script.py Hint Indents are important in python, when you write your scripts, you don't have to worry about the indents in your first empty lines. For example, you don't have to do this: class P1 ( Proc ): lang = \"python\" script = \"\"\" import os import re def somefunc (): pass \"\"\" You can do this: class P1 ( Proc ): lang = \"python\" script = \"\"\" import os import re def somefunc (): pass \"\"\" Only the first non-empty line is used to detect the indent for the whole script. Debugging your script \u00b6 If you need to debug your script, you just need to find the real running script, which is at: <pipeline-workdir>/<proc-name>/<job.index>/job.script . The template is rendered already in the file. You can debug it using the tool according to the language you used for the script. Caching your results \u00b6 Job results get automatically cached previous run is successful and input/output data are not changed, see caching . However, there are cases when you want to cache some results even when the job fails. For example, there is a very time-consuming chunk of code in your script that you don't want to run that part each time if it finishes once. In that case, you can save the intermediate results in a directory under <job.outdir> , where the directory is not specified in output . This keeps that directory untouched each time when the running data get purged if previous run fails.","title":"Script"},{"location":"script/#choosing-your-language","text":"You can specify the path of interpreter to lang . If the interpreter is in $PATH , you can directly give the basename of the interpreter (i.e. python instead of /path/to/python ). For example, if you have your own perl installed at /home/user/bin/perl , then you need to tell pipen where it is: lang = \"/home/user/bin/perl\" . If /home/user/bin is in your $PATH , you can simply do: lang = \"perl\" You can also use shebang to specify the interperter: #!/home/usr/bin/perl # You perl code goes here If you have shebang in your script, the lang specified in the configuration files and Pipen constructor will be ignored (but the one specified in process definition is not).","title":"Choosing your language"},{"location":"script/#use-script-from-a-file","text":"You can also put the script into a file, and use it with a file:// prefix: script = \"file:///a/b/c.pl\" Note You may also use a script file with a relative path, which is relative to where process is defined. For example: a process with script = \"file://./scripts/script.py\" is defined in /a/b/pipeline.py , then the script file refers to /a/b/scripts/script.py Hint Indents are important in python, when you write your scripts, you don't have to worry about the indents in your first empty lines. For example, you don't have to do this: class P1 ( Proc ): lang = \"python\" script = \"\"\" import os import re def somefunc (): pass \"\"\" You can do this: class P1 ( Proc ): lang = \"python\" script = \"\"\" import os import re def somefunc (): pass \"\"\" Only the first non-empty line is used to detect the indent for the whole script.","title":"Use script from a file"},{"location":"script/#debugging-your-script","text":"If you need to debug your script, you just need to find the real running script, which is at: <pipeline-workdir>/<proc-name>/<job.index>/job.script . The template is rendered already in the file. You can debug it using the tool according to the language you used for the script.","title":"Debugging your script"},{"location":"script/#caching-your-results","text":"Job results get automatically cached previous run is successful and input/output data are not changed, see caching . However, there are cases when you want to cache some results even when the job fails. For example, there is a very time-consuming chunk of code in your script that you don't want to run that part each time if it finishes once. In that case, you can save the intermediate results in a directory under <job.outdir> , where the directory is not specified in output . This keeps that directory untouched each time when the running data get purged if previous run fails.","title":"Caching your results"},{"location":"templating/","text":"Templates are used in output and script in process definition. Template engines \u00b6 By default, pipen uses liquid template engine to render the output and script . You can also switch the template engine to jinja2 by specifying: template = \"jinja2\" in one of the configuration files, or in the Pipen constructor: pipeline = Pipen ( ... , template = \"jinja2\" , ... ) or in the process definition class MyProcess ( Proc ): ... template = \"jinja2\" # overwrite the global template engine Besides specifying the name of a template engine, you can also specify a subclass pipen.template.Template as a template engine. This enables us to use our own template engine. You just have to wrap then use a subclass of pipen.template.Template . For example, if you want to use mako : from mako.template import Template as MakoTemplate from pipen.template import Template class TemplateMako ( Template ): def __init__ ( self , source , ** kwargs ): super () . __init__ ( source ) self . engine = MakoTemplate ( source , ** kwargs ) def _render ( self , data ): return self . engine . render ( ** data ) # Use it for a process from pipen import Proc class MyProcess ( Proc ): template = TemplateMako ... # other configurations The template_opts configuration is used to pass to TemplateMako constructor. The values is passed by to the MakoTemplate constructor. You can also register the template as a plugin of pipen: In pyproject.toml : [tool.poetry.plugins.pipen_tpl] mako = \"pipen_mako:pipen_mako\" Or in setup.py : setup ( ... , entry_points = { \"pipen_tpl\" : [ \"pipen_mako:pipen_mako\" ]}, ) Then in pipen_mako.py of your package: def pipen_mako (): # TemplateMako is defined as the above return TemplateMako Rendering data \u00b6 There are some data shared to render both output and script . However, there are some different. One of the obvious reasons is that, the script template can use the output data to render. output \u00b6 The data to render the output : Name Description job.index The index of the job, 0-based job.metadir 1 The directory where job metadata is saved, typically <pipeline-workdir>/<pipeline-name>/<proc-name>/<job.index>/ job.outdir 1 *The output directory of the job: <pipeline-workdir>/<pipeline-name>/<proc-name>/<job.index>/output job.stdout_file 1 The file that saves the stdout of the job job.stderr_file 1 The file that saves the stderr of the job in The input data of the job. You can use in.<input-key> 1 to access the data for each input key proc The process object, used to access their properties, such as proc.workdir envs The envs of the process * : If the process is an end process, it will be a symbolic link to <pipeline-outdir>/<process-name>/<job.index> . When the process has only a single job, the <job.index> is also omitted. script \u00b6 All the data used to render output can also be used to render script . Addtionally, the rendered output can also be used to render script . For example: class MyProcess ( Proc ): input = \"in\" output = \"outfile:file:{{in.in}}.txt\" script = \"echo {{in.in}} > {{out.outfile}}\" ... # other configurations With input data [\"a\"], the script is rendered as echo a > <job.outdir>/a.txt 1 The paths are MountedPath objects, which represent paths of jobs and it is useful when a job is running in a remote system (a VM, a container, etc.), where we need to mount the paths into the remote system. It has an attribute spec to get the specified path. When there is no mountings, it is the same as the path itself.","title":"Templating"},{"location":"templating/#template-engines","text":"By default, pipen uses liquid template engine to render the output and script . You can also switch the template engine to jinja2 by specifying: template = \"jinja2\" in one of the configuration files, or in the Pipen constructor: pipeline = Pipen ( ... , template = \"jinja2\" , ... ) or in the process definition class MyProcess ( Proc ): ... template = \"jinja2\" # overwrite the global template engine Besides specifying the name of a template engine, you can also specify a subclass pipen.template.Template as a template engine. This enables us to use our own template engine. You just have to wrap then use a subclass of pipen.template.Template . For example, if you want to use mako : from mako.template import Template as MakoTemplate from pipen.template import Template class TemplateMako ( Template ): def __init__ ( self , source , ** kwargs ): super () . __init__ ( source ) self . engine = MakoTemplate ( source , ** kwargs ) def _render ( self , data ): return self . engine . render ( ** data ) # Use it for a process from pipen import Proc class MyProcess ( Proc ): template = TemplateMako ... # other configurations The template_opts configuration is used to pass to TemplateMako constructor. The values is passed by to the MakoTemplate constructor. You can also register the template as a plugin of pipen: In pyproject.toml : [tool.poetry.plugins.pipen_tpl] mako = \"pipen_mako:pipen_mako\" Or in setup.py : setup ( ... , entry_points = { \"pipen_tpl\" : [ \"pipen_mako:pipen_mako\" ]}, ) Then in pipen_mako.py of your package: def pipen_mako (): # TemplateMako is defined as the above return TemplateMako","title":"Template engines"},{"location":"templating/#rendering-data","text":"There are some data shared to render both output and script . However, there are some different. One of the obvious reasons is that, the script template can use the output data to render.","title":"Rendering data"},{"location":"templating/#output","text":"The data to render the output : Name Description job.index The index of the job, 0-based job.metadir 1 The directory where job metadata is saved, typically <pipeline-workdir>/<pipeline-name>/<proc-name>/<job.index>/ job.outdir 1 *The output directory of the job: <pipeline-workdir>/<pipeline-name>/<proc-name>/<job.index>/output job.stdout_file 1 The file that saves the stdout of the job job.stderr_file 1 The file that saves the stderr of the job in The input data of the job. You can use in.<input-key> 1 to access the data for each input key proc The process object, used to access their properties, such as proc.workdir envs The envs of the process * : If the process is an end process, it will be a symbolic link to <pipeline-outdir>/<process-name>/<job.index> . When the process has only a single job, the <job.index> is also omitted.","title":"output"},{"location":"templating/#script","text":"All the data used to render output can also be used to render script . Addtionally, the rendered output can also be used to render script . For example: class MyProcess ( Proc ): input = \"in\" output = \"outfile:file:{{in.in}}.txt\" script = \"echo {{in.in}} > {{out.outfile}}\" ... # other configurations With input data [\"a\"], the script is rendered as echo a > <job.outdir>/a.txt 1 The paths are MountedPath objects, which represent paths of jobs and it is useful when a job is running in a remote system (a VM, a container, etc.), where we need to mount the paths into the remote system. It has an attribute spec to get the specified path. When there is no mountings, it is the same as the path itself.","title":"script"},{"location":"troubleshooting/","text":"Troubleshooting \u00b6 This guide helps you resolve common issues when using pipen. Table of Contents \u00b6 Common Errors Pipeline Issues Process Issues Job and Caching Issues Scheduler Issues Performance Tips Cloud Deployment Issues Template Issues Common Errors \u00b6 ProcDependencyError \u00b6 Error: pipen.exceptions.ProcDependencyError: Cycle detected in process dependencies Cause: You have created circular dependencies between processes. Solution: # WRONG: Creates a cycle class P1 ( Proc ): requires = P2 class P2 ( Proc ): requires = P1 # CORRECT: Linear dependency class P1 ( Proc ): \"\"\"First process\"\"\" pass class P2 ( Proc ): \"\"\"Second process\"\"\" requires = P1 class P3 ( Proc ): \"\"\"Third process\"\"\" requires = P2 ProcInputTypeError \u00b6 Error: pipen.exceptions.ProcInputTypeError: Unsupported input type 'invalid_type' Cause: You specified an invalid input type in the type definition. Solution: # WRONG class MyProc ( Proc ): input = \"data:invalid_type\" # CORRECT - Valid types are: var, file, dir, files, dirs class MyProc ( Proc ): input = \"data:var\" # In-memory variable # or input = \"data:file\" # Single file # or input = \"data:files\" # Multiple files ProcInputKeyError \u00b6 Error: pipen.exceptions.ProcInputKeyError: Input 'invalid_key' not found in process Cause: The input key in your script template doesn't match any defined input. Solution: # WRONG class MyProc ( Proc ): input = { \"filename\" : \"var\" } # Key is 'filename' script = \"cat {{ in.wrong_name }}\" # CORRECT class MyProc ( Proc ): input = { \"filename\" : \"var\" } script = \"cat {{ in.filename }}\" ProcScriptFileNotFound \u00b6 Error: pipen.exceptions.ProcScriptFileNotFound: Script file not found: file://missing.py Cause: Script file specified with file:// protocol doesn't exist. Solution: # Check file exists from pathlib import Path script_file = Path ( \"my_script.py\" ) assert script_file . exists (), f \"Script file { script_file } not found\" # Then use in process class MyProc ( Proc ): script = f \"file:// { script_file } \" ProcOutputNameError \u00b6 Error: pipen.exceptions.ProcOutputNameError: No output name specified Cause: Output definition is missing or malformed. Solution: # WRONG class MyProc ( Proc ): output = \"var\" # Missing output name # CORRECT class MyProc ( Proc ): output = \"result:var\" # 'result' is the name # Or for files class MyProc ( Proc ): output = \"result:file:output.txt\" NoSuchSchedulerError \u00b6 Error: pipen.exceptions.NoSuchSchedulerError: Scheduler 'invalid_scheduler' not found Cause: Specified scheduler name is incorrect or not installed. Solution: # Check available schedulers from pipen.scheduler import SCHEDULERS print ( list ( SCHEDULERS . keys ())) # Output: ['local', 'sge', 'slurm', 'ssh', 'container', 'gbatch'] # Use valid scheduler pipeline = Pipen ( scheduler = \"local\" ) # or \"sge\", \"slurm\", \"ssh\", \"container\", \"gbatch\" NoSuchTemplateEngineError \u00b6 Error: pipen.exceptions.NoSuchTemplateEngineError: Template engine 'invalid' not found Cause: Specified template engine is not available. Solution: # Available engines: liquid (default), jinja2, mako pipeline = Pipen ( template = \"liquid\" ) # Default pipeline = Pipen ( template = \"jinja2\" ) # Install: pip install jinja2 pipeline = Pipen ( template = \"mako\" ) # Install: pip install mako Pipeline Issues \u00b6 Pipeline Fails to Start \u00b6 Symptom: Pipeline doesn't start or hangs indefinitely. Possible Causes: Process definition errors # Check process classes are properly defined class MyProc ( Proc ): \"\"\"Always include docstring\"\"\" input = \"data:var\" output = \"result:var\" script = \"echo {{ in.data }}\" Dependency issues # Verify all required processes are defined class P1 ( Proc ): pass class P2 ( Proc ): requires = P1 # P1 must be defined before P2 Configuration conflicts # Check for conflicting configurations # Try with minimal config first pipeline = Pipen ( name = \"Test\" ) # Minimal config pipeline . run () Pipeline Stops Unexpectedly \u00b6 Symptom: Pipeline stops without completing all processes. Possible Causes: Error strategy is set to 'halt' # If a job fails, pipeline stops pipeline = Pipen ( error_strategy = \"halt\" ) # Change to ignore or retry for robustness pipeline = Pipen ( error_strategy = \"retry\" , num_retries = 3 ) Unhandled exception # Check the full error message # Use verbose logging pipeline = Pipen ( loglevel = \"debug\" ) pipeline . run () Import Errors \u00b6 Symptom: ImportError or ModuleNotFoundError when importing pipen. Solutions: Reinstall pipen pip uninstall pipen pip install pipen Install with all extras pip install pipen [ all ] # Installs all optional dependencies Check Python version python --version # Must be 3.9 or higher Process Issues \u00b6 Process Runs but Produces No Output \u00b6 Symptom: Jobs complete successfully but output files are missing. Possible Causes: Script doesn't write to output path # WRONG - Script writes to wrong location script = \"echo 'result' > /tmp/output.txt\" # CORRECT - Write to template output variable script = \"echo 'result' > {{ out.output_file }}\" Output path not resolved # Debug output paths class MyProc ( Proc ): input = \"data:var\" output = \"result:file:result.txt\" script = \"\"\" # Debug input/output echo \"Input: {{ in.data }}\" echo \"Output: {{ out.result }}\" echo \"Output path: {{ out.result | realpath }}\" \"\"\" Permission issues # Check write permissions in workdir ls -la ~/.pipen/ chmod +w ~/.pipen/ Process Input Not Matching Expected Data \u00b6 Symptom: Input values are None or incorrect type. Possible Causes: Channel data structure mismatch # WRONG - Column names don't match channel = Channel . create ([ ( \"value1\" , \"value2\" , \"value3\" ) # 1 row, 3 columns ]) class MyProc ( Proc ): input = { \"wrong_name\" : \"var\" } # Wrong column name # CORRECT class MyProc ( Proc ): input = { \"value1\" : \"var\" , \"value2\" : \"var\" , \"value3\" : \"var\" } Type mismatch in transform functions # Transform input data correctly def transform_input ( input_data ): # input_data is a DataFrame row return { 'processed' : input_data [ 'value' ] * 2 , 'timestamp' : str ( time . time ()) } class MyProc ( Proc ): input = \"data:var\" input_data = transform_input # Apply transform Job and Caching Issues \u00b6 Jobs Running with Old Results \u00b6 Symptom: Jobs run but use cached results from previous runs. Cause: Cache signature hasn't changed. Solutions: Force cache invalidation # Delete cache directory pipeline = Pipen () pipeline . run () # Or manually delete workdir rm - rf ~/. pipen / MyPipeline / Modify input data or script # Changing either input or script invalidates cache class MyProc ( Proc ): input = \"data:var\" script = \"echo '{{ in.data }}' > {{ out.result }}\" Disable caching temporarily # Disable caching for debugging pipeline = Pipen ( cache = False ) pipeline . run () Cache Not Working \u00b6 Symptom: Jobs rerun even when input hasn't changed. Cause: Signature computation not matching expected behavior. Solutions: Check signature file # View job signature cat ~/.pipen/MyPipeline/MyProc/0/job.signature.toml # Should contain: # [signature] # input_hash = \"...\" # script_hash = \"...\" Enable directory signatures # For directory outputs class MyProc ( Proc ): input = \"indir:dir\" output = \"outdir:dir\" dirsig = 1 # Enable directory signature Check file modification times # View file timestamps ls -la input_dir/ # Cache invalidates if input files are modified Jobs Hanging or Failing Intermittently \u00b6 Symptom: Jobs hang or fail randomly. Possible Causes: Resource limits # Reduce parallelization pipeline = Pipen ( forks = 2 ) # Lower concurrency # Reduce submission batch size pipeline = Pipen ( submission_batch = 4 ) Network issues (for remote schedulers) # Increase retry count pipeline = Pipen ( num_retries = 5 ) Memory issues # Profile memory usage class MyProc ( Proc ): script = \"\"\" # Add memory monitoring /usr/bin/time -v echo '{{ in.data }}' > {{ out.result }}' \"\"\" Scheduler Issues \u00b6 Local Scheduler Issues \u00b6 Jobs not running: # Check if any processes are running ps aux | grep python # Check workdir permissions ls -la ~/.pipen/ SLURM Issues \u00b6 Jobs not submitted: # Check SLURM configuration sinfo # View cluster status squeue -u $USER # View your jobs # Test SLURM submission manually sbatch --test job_script.sh Common solutions: # Set SLURM-specific options pipeline = Pipen ( scheduler = \"slurm\" , scheduler_opts = { \"partition\" : \"compute\" , \"time\" : \"01:00:00\" , \"mem\" : \"4G\" } ) SGE Issues \u00b6 Jobs not submitted: # Check SGE status qstat # Test SGE submission manually qsub job_script.sh Common solutions: # Set SGE-specific options pipeline = Pipen ( scheduler = \"sge\" , scheduler_opts = { \"q\" : \"all.q\" , \"pe\" : \"smp 4\" } ) Google Cloud Batch Issues \u00b6 Jobs not running: # Check Cloud Batch status gcloud batch jobs list # Check logs gcloud batch jobs describe <job-id> --format = json # Check bucket permissions gsutil ls gs://your-bucket/ Common solutions: # Set GBatch-specific options pipeline = Pipen ( scheduler = \"gbatch\" , scheduler_opts = { \"project\" : \"your-project\" , \"region\" : \"us-central1\" , \"location\" : \"us-central1\" } ) # Ensure cloud paths are correct from cloudsh import CloudPath input_path = CloudPath ( \"gs://your-bucket/input/\" ) Performance Tips \u00b6 Slow Pipeline Execution \u00b6 Solutions: Increase parallelization # Run more jobs concurrently pipeline = Pipen ( forks = 8 ) # Default is 1 # Increase submission batch size pipeline = Pipen ( submission_batch = 16 ) # Default is 8 Use appropriate scheduler # For many small jobs: Local pipeline = Pipen ( scheduler = \"local\" ) # For large compute jobs: SLURM or SGE pipeline = Pipen ( scheduler = \"slurm\" ) # For cloud-scale: Google Cloud Batch pipeline = Pipen ( scheduler = \"gbatch\" ) Optimize data flow # Use channel operations effectively # Filter data early to reduce job count channel = Channel . from_glob ( \"data/*.txt\" ) filtered = channel [ channel [ 'size' ] < 1000 ] # Use expand_dir/collapse_files for file operations channel = channel . expand_dir ( 'file' ) Enable caching # Caching is enabled by default # Ensure it's working pipeline = Pipen ( cache = True ) High Memory Usage \u00b6 Solutions: Reduce job concurrency pipeline = Pipen ( forks = 2 ) # Reduce from default Stream data instead of loading all # Process files individually class MyProc ( Proc ): input = \"infile:file\" output = \"outfile:file\" script = \"\"\" # Stream line by line while IFS= read -r line; do echo \"$line\" >> {{ out.outfile }} done < {{ in.infile }} \"\"\" Use batch processing # Group files into batches channel = Channel . from_glob ( \"data/*.txt\" , sortby = \"name\" ) # Process multiple files per job Disk Space Issues \u00b6 Symptom: Workdir fills up disk space. Solutions: Change workdir location # Use a directory with more space pipeline = Pipen ( workdir = \"/scratch/pipen_workdir\" ) Clean up old runs # Remove old pipeline runs rm -rf ~/.pipen/MyPipeline-*/ # Keep only cache rm -rf ~/.pipen/MyPipeline/*/input Use cloud storage for outputs # Set output directory to cloud pipeline = Pipen ( outdir = \"gs://your-bucket/output/\" , export = True # Mark as export process ) Cloud Deployment Issues \u00b6 Authentication Failures \u00b6 Symptom: AuthenticationError when accessing cloud resources. Solutions: # Authenticate with Google Cloud gcloud auth login gcloud auth application-default login # Check credentials gcloud auth list # Ensure cloud paths are properly configured from cloudsh import CloudPath # Check authentication works try : path = CloudPath ( \"gs://your-bucket/\" ) print ( list ( path . iterdir ())) except Exception as e : print ( f \"Authentication failed: { e } \" ) Cloud Storage Issues \u00b6 Symptom: File operations fail on cloud storage. Solutions: Use cloudpathlib cache # Set cache directory to local disk export CLOUDPATHLIB_LOCAL_CACHE_DIR = /path/to/cache Check bucket permissions # Verify bucket access gsutil ls gs://your-bucket/ gsutil acl get gs://your-bucket/ Use appropriate path types from cloudsh import CloudPath , LocalPath # Local paths local_path = LocalPath ( \"/local/data.txt\" ) # Cloud paths cloud_path = CloudPath ( \"gs://bucket/data.txt\" ) Network Timeouts \u00b6 Symptom: Jobs fail with timeout errors. Solutions: # Increase retry count for network operations pipeline = Pipen ( num_retries = 5 ) # Increase timeout (if scheduler supports it) pipeline = Pipen ( scheduler = \"gbatch\" , scheduler_opts = { \"timeout\" : \"3600\" # 1 hour timeout } ) Template Issues \u00b6 Template Rendering Errors \u00b6 Error: pipen.exceptions.TemplateRenderingError: Failed to render template Cause: Invalid template syntax or undefined variables. Solutions: Check variable names # Ensure variables are referenced correctly class MyProc ( Proc ): input = { \"filename\" : \"var\" } script = \"cat {{ in.filename }}\" # Double braces Use correct template syntax # Liquid template (default) script = \"\"\" Liquid: {{ in.data | upper }} \"\"\" # Jinja2 template pipeline = Pipen ( template = \"jinja2\" ) script = \"\"\" Jinja2: {{ in.data.upper() }} \"\"\" Debug template rendering # Add debug output script = \"\"\" echo \"Input data: {{ in.data }}\" echo \"Input type: {{ in.data | type }}\" echo \"Processing...\" \"\"\" Filter Not Working \u00b6 Symptom: Template filters don't produce expected output. Solutions: Check filter availability # Built-in filters script = \"\"\" {{ in.path | basename }} # Get filename {{ in.path | dirname }} # Get directory {{ in.value | upper }} # Uppercase \"\"\" Register custom filters def my_filter ( value ): return value . strip () . lower () pipeline = Pipen ( template_opts = { 'filters' : { 'my_filter' : my_filter } } ) class MyProc ( Proc ): script = \"{{ in.data | my_filter }}\" Debug filter application # Test filters in Python first result = my_filter ( \" Test String \" ) print ( result ) # Should be \"test string\" Getting Help \u00b6 If you encounter an issue not covered here: Enable debug logging pipeline = Pipen ( loglevel = \"debug\" ) pipeline . run () Check the logs # View pipeline logs cat ~/.pipen/MyPipeline/*.log # View job logs cat ~/.pipen/MyPipeline/*/job.log Search existing issues Check GitHub Issues Search for your error message Create a new issue Include: Full error message and traceback Python version: python --version pipen version: pipen --version Minimal reproducible example System information: OS, scheduler type Ask for help GitHub Discussions Documentation Further Reading \u00b6 Error Handling - Error strategies Configuration - Pipeline and process configuration Scheduler - Scheduler-specific options Architecture - Internal architecture details","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"This guide helps you resolve common issues when using pipen.","title":"Troubleshooting"},{"location":"troubleshooting/#table-of-contents","text":"Common Errors Pipeline Issues Process Issues Job and Caching Issues Scheduler Issues Performance Tips Cloud Deployment Issues Template Issues","title":"Table of Contents"},{"location":"troubleshooting/#common-errors","text":"","title":"Common Errors"},{"location":"troubleshooting/#procdependencyerror","text":"Error: pipen.exceptions.ProcDependencyError: Cycle detected in process dependencies Cause: You have created circular dependencies between processes. Solution: # WRONG: Creates a cycle class P1 ( Proc ): requires = P2 class P2 ( Proc ): requires = P1 # CORRECT: Linear dependency class P1 ( Proc ): \"\"\"First process\"\"\" pass class P2 ( Proc ): \"\"\"Second process\"\"\" requires = P1 class P3 ( Proc ): \"\"\"Third process\"\"\" requires = P2","title":"ProcDependencyError"},{"location":"troubleshooting/#procinputtypeerror","text":"Error: pipen.exceptions.ProcInputTypeError: Unsupported input type 'invalid_type' Cause: You specified an invalid input type in the type definition. Solution: # WRONG class MyProc ( Proc ): input = \"data:invalid_type\" # CORRECT - Valid types are: var, file, dir, files, dirs class MyProc ( Proc ): input = \"data:var\" # In-memory variable # or input = \"data:file\" # Single file # or input = \"data:files\" # Multiple files","title":"ProcInputTypeError"},{"location":"troubleshooting/#procinputkeyerror","text":"Error: pipen.exceptions.ProcInputKeyError: Input 'invalid_key' not found in process Cause: The input key in your script template doesn't match any defined input. Solution: # WRONG class MyProc ( Proc ): input = { \"filename\" : \"var\" } # Key is 'filename' script = \"cat {{ in.wrong_name }}\" # CORRECT class MyProc ( Proc ): input = { \"filename\" : \"var\" } script = \"cat {{ in.filename }}\"","title":"ProcInputKeyError"},{"location":"troubleshooting/#procscriptfilenotfound","text":"Error: pipen.exceptions.ProcScriptFileNotFound: Script file not found: file://missing.py Cause: Script file specified with file:// protocol doesn't exist. Solution: # Check file exists from pathlib import Path script_file = Path ( \"my_script.py\" ) assert script_file . exists (), f \"Script file { script_file } not found\" # Then use in process class MyProc ( Proc ): script = f \"file:// { script_file } \"","title":"ProcScriptFileNotFound"},{"location":"troubleshooting/#procoutputnameerror","text":"Error: pipen.exceptions.ProcOutputNameError: No output name specified Cause: Output definition is missing or malformed. Solution: # WRONG class MyProc ( Proc ): output = \"var\" # Missing output name # CORRECT class MyProc ( Proc ): output = \"result:var\" # 'result' is the name # Or for files class MyProc ( Proc ): output = \"result:file:output.txt\"","title":"ProcOutputNameError"},{"location":"troubleshooting/#nosuchschedulererror","text":"Error: pipen.exceptions.NoSuchSchedulerError: Scheduler 'invalid_scheduler' not found Cause: Specified scheduler name is incorrect or not installed. Solution: # Check available schedulers from pipen.scheduler import SCHEDULERS print ( list ( SCHEDULERS . keys ())) # Output: ['local', 'sge', 'slurm', 'ssh', 'container', 'gbatch'] # Use valid scheduler pipeline = Pipen ( scheduler = \"local\" ) # or \"sge\", \"slurm\", \"ssh\", \"container\", \"gbatch\"","title":"NoSuchSchedulerError"},{"location":"troubleshooting/#nosuchtemplateengineerror","text":"Error: pipen.exceptions.NoSuchTemplateEngineError: Template engine 'invalid' not found Cause: Specified template engine is not available. Solution: # Available engines: liquid (default), jinja2, mako pipeline = Pipen ( template = \"liquid\" ) # Default pipeline = Pipen ( template = \"jinja2\" ) # Install: pip install jinja2 pipeline = Pipen ( template = \"mako\" ) # Install: pip install mako","title":"NoSuchTemplateEngineError"},{"location":"troubleshooting/#pipeline-issues","text":"","title":"Pipeline Issues"},{"location":"troubleshooting/#pipeline-fails-to-start","text":"Symptom: Pipeline doesn't start or hangs indefinitely. Possible Causes: Process definition errors # Check process classes are properly defined class MyProc ( Proc ): \"\"\"Always include docstring\"\"\" input = \"data:var\" output = \"result:var\" script = \"echo {{ in.data }}\" Dependency issues # Verify all required processes are defined class P1 ( Proc ): pass class P2 ( Proc ): requires = P1 # P1 must be defined before P2 Configuration conflicts # Check for conflicting configurations # Try with minimal config first pipeline = Pipen ( name = \"Test\" ) # Minimal config pipeline . run ()","title":"Pipeline Fails to Start"},{"location":"troubleshooting/#pipeline-stops-unexpectedly","text":"Symptom: Pipeline stops without completing all processes. Possible Causes: Error strategy is set to 'halt' # If a job fails, pipeline stops pipeline = Pipen ( error_strategy = \"halt\" ) # Change to ignore or retry for robustness pipeline = Pipen ( error_strategy = \"retry\" , num_retries = 3 ) Unhandled exception # Check the full error message # Use verbose logging pipeline = Pipen ( loglevel = \"debug\" ) pipeline . run ()","title":"Pipeline Stops Unexpectedly"},{"location":"troubleshooting/#import-errors","text":"Symptom: ImportError or ModuleNotFoundError when importing pipen. Solutions: Reinstall pipen pip uninstall pipen pip install pipen Install with all extras pip install pipen [ all ] # Installs all optional dependencies Check Python version python --version # Must be 3.9 or higher","title":"Import Errors"},{"location":"troubleshooting/#process-issues","text":"","title":"Process Issues"},{"location":"troubleshooting/#process-runs-but-produces-no-output","text":"Symptom: Jobs complete successfully but output files are missing. Possible Causes: Script doesn't write to output path # WRONG - Script writes to wrong location script = \"echo 'result' > /tmp/output.txt\" # CORRECT - Write to template output variable script = \"echo 'result' > {{ out.output_file }}\" Output path not resolved # Debug output paths class MyProc ( Proc ): input = \"data:var\" output = \"result:file:result.txt\" script = \"\"\" # Debug input/output echo \"Input: {{ in.data }}\" echo \"Output: {{ out.result }}\" echo \"Output path: {{ out.result | realpath }}\" \"\"\" Permission issues # Check write permissions in workdir ls -la ~/.pipen/ chmod +w ~/.pipen/","title":"Process Runs but Produces No Output"},{"location":"troubleshooting/#process-input-not-matching-expected-data","text":"Symptom: Input values are None or incorrect type. Possible Causes: Channel data structure mismatch # WRONG - Column names don't match channel = Channel . create ([ ( \"value1\" , \"value2\" , \"value3\" ) # 1 row, 3 columns ]) class MyProc ( Proc ): input = { \"wrong_name\" : \"var\" } # Wrong column name # CORRECT class MyProc ( Proc ): input = { \"value1\" : \"var\" , \"value2\" : \"var\" , \"value3\" : \"var\" } Type mismatch in transform functions # Transform input data correctly def transform_input ( input_data ): # input_data is a DataFrame row return { 'processed' : input_data [ 'value' ] * 2 , 'timestamp' : str ( time . time ()) } class MyProc ( Proc ): input = \"data:var\" input_data = transform_input # Apply transform","title":"Process Input Not Matching Expected Data"},{"location":"troubleshooting/#job-and-caching-issues","text":"","title":"Job and Caching Issues"},{"location":"troubleshooting/#jobs-running-with-old-results","text":"Symptom: Jobs run but use cached results from previous runs. Cause: Cache signature hasn't changed. Solutions: Force cache invalidation # Delete cache directory pipeline = Pipen () pipeline . run () # Or manually delete workdir rm - rf ~/. pipen / MyPipeline / Modify input data or script # Changing either input or script invalidates cache class MyProc ( Proc ): input = \"data:var\" script = \"echo '{{ in.data }}' > {{ out.result }}\" Disable caching temporarily # Disable caching for debugging pipeline = Pipen ( cache = False ) pipeline . run ()","title":"Jobs Running with Old Results"},{"location":"troubleshooting/#cache-not-working","text":"Symptom: Jobs rerun even when input hasn't changed. Cause: Signature computation not matching expected behavior. Solutions: Check signature file # View job signature cat ~/.pipen/MyPipeline/MyProc/0/job.signature.toml # Should contain: # [signature] # input_hash = \"...\" # script_hash = \"...\" Enable directory signatures # For directory outputs class MyProc ( Proc ): input = \"indir:dir\" output = \"outdir:dir\" dirsig = 1 # Enable directory signature Check file modification times # View file timestamps ls -la input_dir/ # Cache invalidates if input files are modified","title":"Cache Not Working"},{"location":"troubleshooting/#jobs-hanging-or-failing-intermittently","text":"Symptom: Jobs hang or fail randomly. Possible Causes: Resource limits # Reduce parallelization pipeline = Pipen ( forks = 2 ) # Lower concurrency # Reduce submission batch size pipeline = Pipen ( submission_batch = 4 ) Network issues (for remote schedulers) # Increase retry count pipeline = Pipen ( num_retries = 5 ) Memory issues # Profile memory usage class MyProc ( Proc ): script = \"\"\" # Add memory monitoring /usr/bin/time -v echo '{{ in.data }}' > {{ out.result }}' \"\"\"","title":"Jobs Hanging or Failing Intermittently"},{"location":"troubleshooting/#scheduler-issues","text":"","title":"Scheduler Issues"},{"location":"troubleshooting/#local-scheduler-issues","text":"Jobs not running: # Check if any processes are running ps aux | grep python # Check workdir permissions ls -la ~/.pipen/","title":"Local Scheduler Issues"},{"location":"troubleshooting/#slurm-issues","text":"Jobs not submitted: # Check SLURM configuration sinfo # View cluster status squeue -u $USER # View your jobs # Test SLURM submission manually sbatch --test job_script.sh Common solutions: # Set SLURM-specific options pipeline = Pipen ( scheduler = \"slurm\" , scheduler_opts = { \"partition\" : \"compute\" , \"time\" : \"01:00:00\" , \"mem\" : \"4G\" } )","title":"SLURM Issues"},{"location":"troubleshooting/#sge-issues","text":"Jobs not submitted: # Check SGE status qstat # Test SGE submission manually qsub job_script.sh Common solutions: # Set SGE-specific options pipeline = Pipen ( scheduler = \"sge\" , scheduler_opts = { \"q\" : \"all.q\" , \"pe\" : \"smp 4\" } )","title":"SGE Issues"},{"location":"troubleshooting/#google-cloud-batch-issues","text":"Jobs not running: # Check Cloud Batch status gcloud batch jobs list # Check logs gcloud batch jobs describe <job-id> --format = json # Check bucket permissions gsutil ls gs://your-bucket/ Common solutions: # Set GBatch-specific options pipeline = Pipen ( scheduler = \"gbatch\" , scheduler_opts = { \"project\" : \"your-project\" , \"region\" : \"us-central1\" , \"location\" : \"us-central1\" } ) # Ensure cloud paths are correct from cloudsh import CloudPath input_path = CloudPath ( \"gs://your-bucket/input/\" )","title":"Google Cloud Batch Issues"},{"location":"troubleshooting/#performance-tips","text":"","title":"Performance Tips"},{"location":"troubleshooting/#slow-pipeline-execution","text":"Solutions: Increase parallelization # Run more jobs concurrently pipeline = Pipen ( forks = 8 ) # Default is 1 # Increase submission batch size pipeline = Pipen ( submission_batch = 16 ) # Default is 8 Use appropriate scheduler # For many small jobs: Local pipeline = Pipen ( scheduler = \"local\" ) # For large compute jobs: SLURM or SGE pipeline = Pipen ( scheduler = \"slurm\" ) # For cloud-scale: Google Cloud Batch pipeline = Pipen ( scheduler = \"gbatch\" ) Optimize data flow # Use channel operations effectively # Filter data early to reduce job count channel = Channel . from_glob ( \"data/*.txt\" ) filtered = channel [ channel [ 'size' ] < 1000 ] # Use expand_dir/collapse_files for file operations channel = channel . expand_dir ( 'file' ) Enable caching # Caching is enabled by default # Ensure it's working pipeline = Pipen ( cache = True )","title":"Slow Pipeline Execution"},{"location":"troubleshooting/#high-memory-usage","text":"Solutions: Reduce job concurrency pipeline = Pipen ( forks = 2 ) # Reduce from default Stream data instead of loading all # Process files individually class MyProc ( Proc ): input = \"infile:file\" output = \"outfile:file\" script = \"\"\" # Stream line by line while IFS= read -r line; do echo \"$line\" >> {{ out.outfile }} done < {{ in.infile }} \"\"\" Use batch processing # Group files into batches channel = Channel . from_glob ( \"data/*.txt\" , sortby = \"name\" ) # Process multiple files per job","title":"High Memory Usage"},{"location":"troubleshooting/#disk-space-issues","text":"Symptom: Workdir fills up disk space. Solutions: Change workdir location # Use a directory with more space pipeline = Pipen ( workdir = \"/scratch/pipen_workdir\" ) Clean up old runs # Remove old pipeline runs rm -rf ~/.pipen/MyPipeline-*/ # Keep only cache rm -rf ~/.pipen/MyPipeline/*/input Use cloud storage for outputs # Set output directory to cloud pipeline = Pipen ( outdir = \"gs://your-bucket/output/\" , export = True # Mark as export process )","title":"Disk Space Issues"},{"location":"troubleshooting/#cloud-deployment-issues","text":"","title":"Cloud Deployment Issues"},{"location":"troubleshooting/#authentication-failures","text":"Symptom: AuthenticationError when accessing cloud resources. Solutions: # Authenticate with Google Cloud gcloud auth login gcloud auth application-default login # Check credentials gcloud auth list # Ensure cloud paths are properly configured from cloudsh import CloudPath # Check authentication works try : path = CloudPath ( \"gs://your-bucket/\" ) print ( list ( path . iterdir ())) except Exception as e : print ( f \"Authentication failed: { e } \" )","title":"Authentication Failures"},{"location":"troubleshooting/#cloud-storage-issues","text":"Symptom: File operations fail on cloud storage. Solutions: Use cloudpathlib cache # Set cache directory to local disk export CLOUDPATHLIB_LOCAL_CACHE_DIR = /path/to/cache Check bucket permissions # Verify bucket access gsutil ls gs://your-bucket/ gsutil acl get gs://your-bucket/ Use appropriate path types from cloudsh import CloudPath , LocalPath # Local paths local_path = LocalPath ( \"/local/data.txt\" ) # Cloud paths cloud_path = CloudPath ( \"gs://bucket/data.txt\" )","title":"Cloud Storage Issues"},{"location":"troubleshooting/#network-timeouts","text":"Symptom: Jobs fail with timeout errors. Solutions: # Increase retry count for network operations pipeline = Pipen ( num_retries = 5 ) # Increase timeout (if scheduler supports it) pipeline = Pipen ( scheduler = \"gbatch\" , scheduler_opts = { \"timeout\" : \"3600\" # 1 hour timeout } )","title":"Network Timeouts"},{"location":"troubleshooting/#template-issues","text":"","title":"Template Issues"},{"location":"troubleshooting/#template-rendering-errors","text":"Error: pipen.exceptions.TemplateRenderingError: Failed to render template Cause: Invalid template syntax or undefined variables. Solutions: Check variable names # Ensure variables are referenced correctly class MyProc ( Proc ): input = { \"filename\" : \"var\" } script = \"cat {{ in.filename }}\" # Double braces Use correct template syntax # Liquid template (default) script = \"\"\" Liquid: {{ in.data | upper }} \"\"\" # Jinja2 template pipeline = Pipen ( template = \"jinja2\" ) script = \"\"\" Jinja2: {{ in.data.upper() }} \"\"\" Debug template rendering # Add debug output script = \"\"\" echo \"Input data: {{ in.data }}\" echo \"Input type: {{ in.data | type }}\" echo \"Processing...\" \"\"\"","title":"Template Rendering Errors"},{"location":"troubleshooting/#filter-not-working","text":"Symptom: Template filters don't produce expected output. Solutions: Check filter availability # Built-in filters script = \"\"\" {{ in.path | basename }} # Get filename {{ in.path | dirname }} # Get directory {{ in.value | upper }} # Uppercase \"\"\" Register custom filters def my_filter ( value ): return value . strip () . lower () pipeline = Pipen ( template_opts = { 'filters' : { 'my_filter' : my_filter } } ) class MyProc ( Proc ): script = \"{{ in.data | my_filter }}\" Debug filter application # Test filters in Python first result = my_filter ( \" Test String \" ) print ( result ) # Should be \"test string\"","title":"Filter Not Working"},{"location":"troubleshooting/#getting-help","text":"If you encounter an issue not covered here: Enable debug logging pipeline = Pipen ( loglevel = \"debug\" ) pipeline . run () Check the logs # View pipeline logs cat ~/.pipen/MyPipeline/*.log # View job logs cat ~/.pipen/MyPipeline/*/job.log Search existing issues Check GitHub Issues Search for your error message Create a new issue Include: Full error message and traceback Python version: python --version pipen version: pipen --version Minimal reproducible example System information: OS, scheduler type Ask for help GitHub Discussions Documentation","title":"Getting Help"},{"location":"troubleshooting/#further-reading","text":"Error Handling - Error strategies Configuration - Pipeline and process configuration Scheduler - Scheduler-specific options Architecture - Internal architecture details","title":"Further Reading"},{"location":"api/pipen.channel/","text":"module pipen. channel </> Provide some function for creating and modifying channels (dataframes) Classes Channel \u2014 A DataFrame wrapper with creators </> Functions collapse_files ( data , col ) (DataFrame) \u2014 Collapse a Channel according to the files in ,other cols will use the values in row 0. </> expand_dir ( data , col , pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Expand a Channel according to the files in ,other cols will keep the same. </> class pipen.channel. Channel ( data=None , index=None , columns=None , dtype=None , copy=None ) </> Bases pandas.core.frame.DataFrame pandas.core.generic.NDFrame pandas.core.base.PandasObject pandas.core.accessor.DirNamesMixin pandas.core.indexing.IndexingMixin pandas.core.arraylike.OpsMixin A DataFrame wrapper with creators Parameters data (optional) \u2014 Dict can contain Series, arrays, constants, dataclass or list-like objects. Ifdata is a dict, column order follows insertion-order. If a dict contains Series which have an index defined, it is aligned by its index. This alignment also occurs if data is a Series or a DataFrame itself. Alignment is done on Series/DataFrame inputs. If data is a list of dicts, column order follows insertion-order. index (Axes | None, optional) \u2014 Index to use for resulting frame. Will default to RangeIndex ifno indexing information part of input data and no index provided. columns (Axes | None, optional) \u2014 Column labels to use for resulting frame when data does not have them,defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels, will perform column selection instead. dtype (Dtype | None, optional) \u2014 Data type to force. Only a single dtype is allowed. If None, infer. copy (bool | none, optional) \u2014 Copy data from inputs.For dict data, the default of None behaves like copy=True . For DataFrame or 2d ndarray input, the default of None behaves like copy=False . If data is a dict containing one or more Series (possibly of different dtypes), copy=False will ensure that these inputs are not copied. .. versionchanged:: 1.3.0 Attributes T \u2014 The transpose of the DataFrame. </> at (_AtIndexer) \u2014 Access a single value for a row/column label pair. Similar to loc , in that both provide label-based lookups. Use at if you only need to get or set a single value in a DataFrame or Series. </> attrs (dict[Hashable, Any]) \u2014 Dictionary of global attributes of this dataset. .. warning:: attrs is experimental and may change without warning. </> axes (list) \u2014 Return a list representing the axes of the DataFrame. It has the row axis labels and column axis labels as the only members. They are returned in that order. </> dtypes \u2014 Return the dtypes in the DataFrame. This returns a Series with the data type of each column. The result's index is the original DataFrame's columns. Columns with mixed types are stored with the object dtype. See :ref: the User Guide <basics.dtypes> for more. </> empty \u2014 Indicator whether Series/DataFrame is empty. True if Series/DataFrame is entirely empty (no items), meaning any of the axes are of length 0. </> flags (Flags) \u2014 Get the properties associated with this pandas object. The available flags are :attr: Flags.allows_duplicate_labels </> iat (_iAtIndexer) \u2014 Access a single value for a row/column pair by integer position. Similar to iloc , in that both provide integer-based lookups. Use iat if you only need to get or set a single value in a DataFrame or Series. </> iloc (_iLocIndexer) \u2014 Purely integer-location based indexing for selection by position. .. deprecated:: 2.2.0 Returning a tuple from a callable is deprecated. .iloc[] is primarily integer position based (from 0 to length-1 of the axis), but may also be used with a boolean array. Allowed inputs are: An integer, e.g. 5 . A list or array of integers, e.g. [4, 3, 0] . A slice object with ints, e.g. 1:7 . A boolean array. A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above). This is useful in method chains, when you don't have a reference to the calling object, but would like to base your selection on some value. A tuple of row and column indexes. The tuple elements consist of one of the above inputs, e.g. (0, 1) . .iloc will raise IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing (this conforms with python/numpy slice semantics). See more at :ref: Selection by Position <indexing.integer> . </> loc (_LocIndexer) \u2014 Access a group of rows and columns by label(s) or a boolean array. .loc[] is primarily label based, but may also be used with a boolean array. Allowed inputs are: A single label, e.g. 5 or 'a' , (note that 5 is interpreted as a label of the index, and never as an integer position along the index). A list or array of labels, e.g. ['a', 'b', 'c'] . A slice object with labels, e.g. 'a':'f' . .. warning:: Note that contrary to usual python slices, both the start and the stop are included A boolean array of the same length as the axis being sliced, e.g. [True, False, True] . An alignable boolean Series. The index of the key will be aligned before masking. An alignable Index. The Index of the returned selection will be the input. A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above) See more at :ref: Selection by Label <indexing.label> . </> ndim (int) \u2014 Return an int representing the number of axes / array dimensions. Return 1 if Series. Otherwise return 2 if DataFrame. </> shape (tuple) \u2014 Return a tuple representing the dimensionality of the DataFrame. </> size (int) \u2014 Return an int representing the number of elements in this object. Return the number of rows if Series. Otherwise return the number of rows times number of columns if DataFrame. </> style (Styler) \u2014 Returns a Styler object. Contains methods for building a styled HTML representation of the DataFrame. </> values \u2014 Return a Numpy representation of the DataFrame. .. warning:: We recommend using :meth: DataFrame.to_numpy instead. Only the values in the DataFrame will be returned, the axes labels will be removed. </> Methods __add__ ( other ) (DataFrame) \u2014 Get Addition of DataFrame and other, column-wise. </> __arrow_c_stream__ ( requested_schema ) (PyCapsule) \u2014 Export the pandas DataFrame as an Arrow C stream PyCapsule. </> __contains__ ( key ) (bool) \u2014 True if the key is in the info axis </> __dataframe__ ( nan_as_null , allow_copy ) (DataFrame interchange object) \u2014 Return the dataframe interchange object implementing the interchange protocol. </> __dataframe_consortium_standard__ ( api_version ) (Any) \u2014 Provide entry point to the Consortium DataFrame Standard API. </> __delitem__ ( key ) \u2014 Delete item </> __dir__ ( ) (list) \u2014 Provide method name lookup and completion. </> __finalize__ ( other , method , **kwargs ) \u2014 Propagate metadata from other to self. </> __getattr__ ( name ) \u2014 After regular attribute access, try looking up the nameThis allows simpler access to columns for interactive use. </> __iter__ ( ) (iterator) \u2014 Iterate over info axis. </> __len__ ( ) (int) \u2014 Returns length of info axis, but here we use the index. </> __matmul__ ( other ) (pandas.core.frame.dataframe | pandas.core.series.series) \u2014 Matrix multiplication using binary @ operator. </> __repr__ ( ) (str) \u2014 Return a string representation for a particular DataFrame. </> __rmatmul__ ( other ) (DataFrame) \u2014 Matrix multiplication using binary @ operator. </> __setattr__ ( name , value ) \u2014 After regular attribute access, try setting the nameThis allows simpler access to columns for interactive use. </> __sizeof__ ( ) (int) \u2014 Generates the total memory usage for an object that returnseither a value or Series of values </> a_from_glob ( pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Create a channel with a glob pattern asynchronously </> a_from_pairs ( pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Create a width=2 channel with a glob pattern </> abs ( ) (abs) \u2014 Return a Series/DataFrame with absolute numeric value of each element. </> add ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Addition of dataframe and other, element-wise (binary operator add ). </> add_prefix ( prefix , axis ) (Series or DataFrame) \u2014 Prefix labels with string prefix . </> add_suffix ( suffix , axis ) (Series or DataFrame) \u2014 Suffix labels with string suffix . </> aggregate ( func , axis , *args , **kwargs ) (scalar, Series or DataFrame) \u2014 Aggregate using one or more operations over the specified axis. </> align ( other , join , axis , level , copy , fill_value , method , limit , fill_axis , broadcast_axis ) (tuple of (Series/DataFrame, type of other)) \u2014 Align two objects on their axes with the specified join method. </> all ( axis , bool_only , skipna , **kwargs ) (Series or DataFrame) \u2014 Return whether all elements are True, potentially over an axis. </> any ( axis , bool_only , skipna , **kwargs ) (Series or DataFrame) \u2014 Return whether any element is True, potentially over an axis. </> apply ( func , axis , raw , result_type , args , by_row , engine , engine_kwargs , **kwargs ) (Series or DataFrame) \u2014 Apply a function along an axis of the DataFrame. </> applymap ( func , na_action , **kwargs ) (DataFrame) \u2014 Apply a function to a Dataframe elementwise. </> asfreq ( freq , method , how , normalize , fill_value ) (Series/DataFrame) \u2014 Convert time series to specified frequency. </> asof ( where , subset ) (scalar, Series, or DataFrame) \u2014 Return the last row(s) without any NaNs before where . </> assign ( **kwargs ) (DataFrame) \u2014 Assign new columns to a DataFrame. </> astype ( dtype , copy , errors ) (same type as caller) \u2014 Cast a pandas object to a specified dtype dtype . </> at_time ( time , asof , axis ) (Series or DataFrame) \u2014 Select values at particular time of day (e.g., 9:30AM). </> backfill ( axis , inplace , limit , downcast ) (Series/DataFrame or None) \u2014 Fill NA/NaN values by using the next valid observation to fill the gap. </> between_time ( start_time , end_time , inclusive , axis ) (Series or DataFrame) \u2014 Select values between particular times of the day (e.g., 9:00-9:30 AM). </> bfill ( axis , inplace , limit , limit_area , downcast ) (Series/DataFrame or None) \u2014 Fill NA/NaN values by using the next valid observation to fill the gap. </> bool ( ) (bool) \u2014 Return the bool of a single element Series or DataFrame. </> clip ( lower , upper , axis , inplace , **kwargs ) (Series or DataFrame or None) \u2014 Trim values at input threshold(s). </> combine ( other , func , fill_value , overwrite ) (DataFrame) \u2014 Perform column-wise combine with another DataFrame. </> combine_first ( other ) (DataFrame) \u2014 Update null elements with value in the same location in other . </> compare ( other , align_axis , keep_shape , keep_equal , result_names ) (DataFrame) \u2014 Compare to another DataFrame and show the differences. </> convert_dtypes ( infer_objects , convert_string , convert_integer , convert_boolean , convert_floating , dtype_backend ) (Series or DataFrame) \u2014 Convert columns to the best possible dtypes using dtypes supporting pd.NA . </> copy ( deep ) (Series or DataFrame) \u2014 Make a copy of this object's indices and data. </> corr ( method , min_periods , numeric_only ) (DataFrame) \u2014 Compute pairwise correlation of columns, excluding NA/null values. </> corrwith ( other , axis , drop , method , numeric_only ) (Series) \u2014 Compute pairwise correlation. </> count ( axis , numeric_only ) (Series) \u2014 Count non-NA cells for each column or row. </> cov ( min_periods , ddof , numeric_only ) (DataFrame) \u2014 Compute pairwise covariance of columns, excluding NA/null values. </> create ( value ) (DataFrame) \u2014 Create a channel from a list. </> cummax ( axis , skipna , *args , **kwargs ) (Series or DataFrame) \u2014 Return cumulative maximum over a DataFrame or Series axis. </> cummin ( axis , skipna , *args , **kwargs ) (Series or DataFrame) \u2014 Return cumulative minimum over a DataFrame or Series axis. </> cumprod ( axis , skipna , *args , **kwargs ) (Series or DataFrame) \u2014 Return cumulative product over a DataFrame or Series axis. </> cumsum ( axis , skipna , *args , **kwargs ) (Series or DataFrame) \u2014 Return cumulative sum over a DataFrame or Series axis. </> describe ( percentiles , include , exclude ) (Series or DataFrame) \u2014 Generate descriptive statistics. </> diff ( periods , axis ) (DataFrame) \u2014 First discrete difference of element. </> dot ( other ) (Series or DataFrame) \u2014 Compute the matrix multiplication between the DataFrame and other. </> drop ( labels , axis , index , columns , level , inplace , errors ) (DataFrame or None) \u2014 Drop specified labels from rows or columns. </> drop_duplicates ( subset , keep , inplace , ignore_index ) (DataFrame or None) \u2014 Return DataFrame with duplicate rows removed. </> droplevel ( level , axis ) (Series/DataFrame) \u2014 Return Series/DataFrame with requested index / column level(s) removed. </> dropna ( axis , how , thresh , subset , inplace , ignore_index ) (DataFrame or None) \u2014 Remove missing values. </> duplicated ( subset , keep ) (Series) \u2014 Return boolean Series denoting duplicate rows. </> eq ( other , axis , level ) (DataFrame of bool) \u2014 Get Equal to of dataframe and other, element-wise (binary operator eq ). </> equals ( other ) (bool) \u2014 Test whether two objects contain the same elements. </> eval ( expr , inplace , **kwargs ) (ndarray, scalar, pandas object, or None) \u2014 Evaluate a string describing operations on DataFrame columns. </> ewm ( com , span , halflife , alpha , min_periods , adjust , ignore_na , axis , times , method ) (pandas.api.typing.ExponentialMovingWindow) \u2014 Provide exponentially weighted (EW) calculations. </> expanding ( min_periods , axis , method ) (pandas.api.typing.Expanding) \u2014 Provide expanding window calculations. </> explode ( column , ignore_index ) (DataFrame) \u2014 Transform each element of a list-like to a row, replicating index values. </> ffill ( axis , inplace , limit , limit_area , downcast ) (Series/DataFrame or None) \u2014 Fill NA/NaN values by propagating the last valid observation to next valid. </> fillna ( value , method , axis , inplace , limit , downcast ) (Series/DataFrame or None) \u2014 Fill NA/NaN values using the specified method. </> filter ( items , like , regex , axis ) (same type as input object) \u2014 Subset the dataframe rows or columns according to the specified index labels. </> first ( offset ) (Series or DataFrame) \u2014 Select initial periods of time series data based on a date offset. </> first_valid_index ( ) (type of index) \u2014 Return index for first non-NA value or None, if no non-NA value is found. </> floordiv ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Integer division of dataframe and other, element-wise (binary operator floordiv ). </> from_csv ( *args , **kwargs ) \u2014 Create a channel from a csv file </> from_dict ( data , orient , dtype , columns ) (DataFrame) \u2014 Construct DataFrame from dict of array-like or dicts. </> from_excel ( *args , **kwargs ) \u2014 Create a channel from an excel file. </> from_glob ( pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Create a channel with a glob pattern </> from_pairs ( pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Create a width=2 channel with a glob pattern </> from_records ( data , index , exclude , columns , coerce_float , nrows ) (DataFrame) \u2014 Convert structured or record ndarray to DataFrame. </> from_table ( *args , **kwargs ) \u2014 Create a channel from a table file. </> ge ( other , axis , level ) (DataFrame of bool) \u2014 Get Greater than or equal to of dataframe and other, element-wise (binary operator ge ). </> get ( key , default ) (same type as items contained in object) \u2014 Get item from object for given key (ex: DataFrame column). </> groupby ( by , axis , level , as_index , sort , group_keys , observed , dropna ) (pandas.api.typing.DataFrameGroupBy) \u2014 Group DataFrame using a mapper or by a Series of columns. </> gt ( other , axis , level ) (DataFrame of bool) \u2014 Get Greater than of dataframe and other, element-wise (binary operator gt ). </> head ( n ) (same type as caller) \u2014 Return the first n rows. </> idxmax ( axis , skipna , numeric_only ) (Series) \u2014 Return index of first occurrence of maximum over requested axis. </> idxmin ( axis , skipna , numeric_only ) (Series) \u2014 Return index of first occurrence of minimum over requested axis. </> infer_objects ( copy ) (same type as input object) \u2014 Attempt to infer better dtypes for object columns. </> info ( verbose , buf , max_cols , memory_usage , show_counts ) (None) \u2014 Print a concise summary of a DataFrame. </> insert ( loc , column , value , allow_duplicates ) \u2014 Insert column into DataFrame at specified location. </> interpolate ( method , axis , limit , inplace , limit_direction , limit_area , downcast , **kwargs ) (Series or DataFrame or None) \u2014 Fill NaN values using an interpolation method. </> isetitem ( loc , value ) \u2014 Set the given value in the column with position loc . </> isin ( values ) (DataFrame) \u2014 Whether each element in the DataFrame is contained in values. </> isna ( ) (DataFrame) \u2014 Detect missing values. </> isnull ( ) (DataFrame) \u2014 DataFrame.isnull is an alias for DataFrame.isna. </> items ( ) (label : object) \u2014 Iterate over (column name, Series) pairs. </> iterrows ( ) (index : label or tuple of label) \u2014 Iterate over DataFrame rows as (index, Series) pairs. </> itertuples ( index , name ) (iterator) \u2014 Iterate over DataFrame rows as namedtuples. </> join ( other , on , how , lsuffix , rsuffix , sort , validate ) (DataFrame) \u2014 Join columns of another DataFrame. </> keys ( ) (Index) \u2014 Get the 'info axis' (see Indexing for more). </> kurt ( axis , skipna , numeric_only , **kwargs ) (Series or scalar) \u2014 Return unbiased kurtosis over requested axis. </> last ( offset ) (Series or DataFrame) \u2014 Select final periods of time series data based on a date offset. </> last_valid_index ( ) (type of index) \u2014 Return index for last non-NA value or None, if no non-NA value is found. </> le ( other , axis , level ) (DataFrame of bool) \u2014 Get Less than or equal to of dataframe and other, element-wise (binary operator le ). </> lt ( other , axis , level ) (DataFrame of bool) \u2014 Get Less than of dataframe and other, element-wise (binary operator lt ). </> map ( func , na_action , **kwargs ) (DataFrame) \u2014 Apply a function to a Dataframe elementwise. </> mask ( cond , other , inplace , axis , level ) (Same type as caller or None if ``inplace=True``.) \u2014 Replace values where the condition is True. </> max ( axis , skipna , numeric_only , **kwargs ) (Series or scalar) \u2014 Return the maximum of the values over the requested axis. </> mean ( axis , skipna , numeric_only , **kwargs ) (Series or scalar) \u2014 Return the mean of the values over the requested axis. </> median ( axis , skipna , numeric_only , **kwargs ) (Series or scalar) \u2014 Return the median of the values over the requested axis. </> melt ( id_vars , value_vars , var_name , value_name , col_level , ignore_index ) (DataFrame) \u2014 Unpivot a DataFrame from wide to long format, optionally leaving identifiers set. </> memory_usage ( index , deep ) (Series) \u2014 Return the memory usage of each column in bytes. </> merge ( right , how , on , left_on , right_on , left_index , right_index , sort , suffixes , copy , indicator , validate ) (DataFrame) \u2014 Merge DataFrame or named Series objects with a database-style join. </> min ( axis , skipna , numeric_only , **kwargs ) (Series or scalar) \u2014 Return the minimum of the values over the requested axis. </> mod ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Modulo of dataframe and other, element-wise (binary operator mod ). </> mode ( axis , numeric_only , dropna ) (DataFrame) \u2014 Get the mode(s) of each element along the selected axis. </> mul ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Multiplication of dataframe and other, element-wise (binary operator mul ). </> ne ( other , axis , level ) (DataFrame of bool) \u2014 Get Not equal to of dataframe and other, element-wise (binary operator ne ). </> nlargest ( n , columns , keep ) (DataFrame) \u2014 Return the first n rows ordered by columns in descending order. </> notna ( ) (DataFrame) \u2014 Detect existing (non-missing) values. </> notnull ( ) (DataFrame) \u2014 DataFrame.notnull is an alias for DataFrame.notna. </> nsmallest ( n , columns , keep ) (DataFrame) \u2014 Return the first n rows ordered by columns in ascending order. </> nunique ( axis , dropna ) (Series) \u2014 Count number of distinct elements in specified axis. </> pad ( axis , inplace , limit , downcast ) (Series/DataFrame or None) \u2014 Fill NA/NaN values by propagating the last valid observation to next valid. </> pct_change ( periods , fill_method , limit , freq , **kwargs ) (Series or DataFrame) \u2014 Fractional change between the current and a prior element. </> pipe ( func , *args , **kwargs ) (the return type of ``func``.) \u2014 Apply chainable functions that expect Series or DataFrames. </> pivot ( columns , index , values ) (DataFrame) \u2014 Return reshaped DataFrame organized by given index / column values. </> pivot_table ( values , index , columns , aggfunc , fill_value , margins , dropna , margins_name , observed , sort ) (DataFrame) \u2014 Create a spreadsheet-style pivot table as a DataFrame. </> pop ( item ) (Series) \u2014 Return item and drop from frame. Raise KeyError if not found. </> pow ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Exponential power of dataframe and other, element-wise (binary operator pow ). </> prod ( axis , skipna , numeric_only , min_count , **kwargs ) (Series or scalar) \u2014 Return the product of the values over the requested axis. </> quantile ( q , axis , numeric_only , interpolation , method ) (Series or DataFrame) \u2014 Return values at the given quantile over requested axis. </> query ( expr , inplace , **kwargs ) (DataFrame or None) \u2014 Query the columns of a DataFrame with a boolean expression. </> radd ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Addition of dataframe and other, element-wise (binary operator radd ). </> rank ( axis , method , numeric_only , na_option , ascending , pct ) (same type as caller) \u2014 Compute numerical data ranks (1 through n) along axis. </> reindex ( labels , index , columns , axis , method , copy , level , fill_value , limit , tolerance ) (DataFrame with changed index.) \u2014 Conform DataFrame to new index with optional filling logic. </> reindex_like ( other , method , copy , limit , tolerance ) (Series or DataFrame) \u2014 Return an object with matching indices as other object. </> rename ( mapper , index , columns , axis , copy , inplace , level , errors ) (DataFrame or None) \u2014 Rename columns or index labels. </> rename_axis ( mapper , index , columns , axis , copy , inplace ) (Series, DataFrame, or None) \u2014 Set the name of the axis for the index or columns. </> reorder_levels ( order , axis ) (DataFrame) \u2014 Rearrange index levels using input order. May not drop or duplicate levels. </> replace ( to_replace , value , inplace , limit , regex , method ) (Series/DataFrame) \u2014 Replace values given in to_replace with value . </> resample ( rule , axis , closed , label , convention , kind , on , level , origin , offset , group_keys ) (pandas.api.typing.Resampler) \u2014 Resample time-series data. </> reset_index ( level , drop , inplace , col_level , col_fill , allow_duplicates , names ) (DataFrame or None) \u2014 Reset the index, or a level of it. </> rfloordiv ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Integer division of dataframe and other, element-wise (binary operator rfloordiv ). </> rmod ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Modulo of dataframe and other, element-wise (binary operator rmod ). </> rmul ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Multiplication of dataframe and other, element-wise (binary operator rmul ). </> rolling ( window , min_periods , center , win_type , on , axis , closed , step , method ) (pandas.api.typing.Window or pandas.api.typing.Rolling) \u2014 Provide rolling window calculations. </> round ( decimals , *args , **kwargs ) (DataFrame) \u2014 Round a DataFrame to a variable number of decimal places. </> rpow ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Exponential power of dataframe and other, element-wise (binary operator rpow ). </> rsub ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Subtraction of dataframe and other, element-wise (binary operator rsub ). </> rtruediv ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Floating division of dataframe and other, element-wise (binary operator rtruediv ). </> sample ( n , frac , replace , weights , random_state , axis , ignore_index ) (Series or DataFrame) \u2014 Return a random sample of items from an axis of object. </> select_dtypes ( include , exclude ) (DataFrame) \u2014 Return a subset of the DataFrame's columns based on the column dtypes. </> sem ( axis , skipna , ddof , numeric_only , **kwargs ) (Series or DataFrame (if level specified)) \u2014 Return unbiased standard error of the mean over requested axis. </> set_axis ( labels , axis , copy ) (DataFrame) \u2014 Assign desired index to given axis. </> set_flags ( copy , allows_duplicate_labels ) (Series or DataFrame) \u2014 Return a new object with updated flags. </> set_index ( keys , drop , append , inplace , verify_integrity ) (DataFrame or None) \u2014 Set the DataFrame index using existing columns. </> shift ( periods , freq , axis , fill_value , suffix ) (DataFrame) \u2014 Shift index by desired number of periods with an optional time freq . </> skew ( axis , skipna , numeric_only , **kwargs ) (Series or scalar) \u2014 Return unbiased skew over requested axis. </> sort_index ( axis , level , ascending , inplace , kind , na_position , sort_remaining , ignore_index , key ) (DataFrame or None) \u2014 Sort object by labels (along an axis). </> sort_values ( by , axis , ascending , inplace , kind , na_position , ignore_index , key ) (DataFrame or None) \u2014 Sort by the values along either axis. </> squeeze ( axis ) (DataFrame, Series, or scalar) \u2014 Squeeze 1 dimensional axis objects into scalars. </> stack ( level , dropna , sort , future_stack ) (DataFrame or Series) \u2014 Stack the prescribed level(s) from columns to index. </> std ( axis , skipna , ddof , numeric_only , **kwargs ) (Series or DataFrame (if level specified)) \u2014 Return sample standard deviation over requested axis. </> sub ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Subtraction of dataframe and other, element-wise (binary operator sub ). </> sum ( axis , skipna , numeric_only , min_count , **kwargs ) (Series or scalar) \u2014 Return the sum of the values over the requested axis. </> swapaxes ( axis1 , axis2 , copy ) (same as input) \u2014 Interchange axes and swap values axes appropriately. </> swaplevel ( i , j , axis ) (DataFrame) \u2014 Swap levels i and j in a :class: MultiIndex . </> tail ( n ) (type of caller) \u2014 Return the last n rows. </> take ( indices , axis , **kwargs ) (same type as caller) \u2014 Return the elements in the given positional indices along an axis. </> to_clipboard ( excel , sep , **kwargs ) \u2014 Copy object to the system clipboard. </> to_csv ( path_or_buf , sep , na_rep , float_format , columns , header , index , index_label , mode , encoding , compression , quoting , quotechar , lineterminator , chunksize , date_format , doublequote , escapechar , decimal , errors , storage_options ) (None or str) \u2014 Write object to a comma-separated values (csv) file. </> to_dict ( orient , into , index ) (dict, list or collections.abc.MutableMapping) \u2014 Convert the DataFrame to a dictionary. </> to_excel ( excel_writer , sheet_name , na_rep , float_format , columns , header , index , index_label , startrow , startcol , engine , merge_cells , inf_rep , freeze_panes , storage_options , engine_kwargs ) \u2014 Write object to an Excel sheet. </> to_feather ( path , **kwargs ) \u2014 Write a DataFrame to the binary Feather format. </> to_gbq ( destination_table , project_id , chunksize , reauth , if_exists , auth_local_webserver , table_schema , location , progress_bar , credentials ) \u2014 Write a DataFrame to a Google BigQuery table. </> to_hdf ( path_or_buf , key , mode , complevel , complib , append , format , index , min_itemsize , nan_rep , dropna , data_columns , errors , encoding ) \u2014 Write the contained data to an HDF5 file using HDFStore. </> to_html ( buf , columns , col_space , header , index , na_rep , formatters , float_format , sparsify , index_names , justify , max_rows , max_cols , show_dimensions , decimal , bold_rows , classes , escape , notebook , border , table_id , render_links , encoding ) (str or None) \u2014 Render a DataFrame as an HTML table. </> to_json ( path_or_buf , orient , date_format , double_precision , force_ascii , date_unit , default_handler , lines , compression , index , indent , storage_options , mode ) (None or str) \u2014 Convert the object to a JSON string. </> to_latex ( buf , columns , header , index , na_rep , formatters , float_format , sparsify , index_names , bold_rows , column_format , longtable , escape , encoding , decimal , multicolumn , multicolumn_format , multirow , caption , label , position ) (str or None) \u2014 Render object to a LaTeX tabular, longtable, or nested table. </> to_markdown ( buf , mode , index , storage_options , **kwargs ) (str) \u2014 Print DataFrame in Markdown-friendly format. </> to_numpy ( dtype , copy , na_value ) (numpy.ndarray) \u2014 Convert the DataFrame to a NumPy array. </> to_orc ( path , engine , index , engine_kwargs ) (bytes if no path argument is provided else None) \u2014 Write a DataFrame to the ORC format. </> to_parquet ( path , engine , compression , index , partition_cols , storage_options , **kwargs ) (bytes if no path argument is provided else None) \u2014 Write a DataFrame to the binary parquet format. </> to_period ( freq , axis , copy ) (DataFrame) \u2014 Convert DataFrame from DatetimeIndex to PeriodIndex. </> to_pickle ( path , compression , protocol , storage_options ) \u2014 Pickle (serialize) object to file. </> to_records ( index , column_dtypes , index_dtypes ) (numpy.rec.recarray) \u2014 Convert DataFrame to a NumPy record array. </> to_sql ( name , con , schema , if_exists , index , index_label , chunksize , dtype , method ) (None or int) \u2014 Write records stored in a DataFrame to a SQL database. </> to_stata ( path , convert_dates , write_index , byteorder , time_stamp , data_label , variable_labels , version , convert_strl , compression , storage_options , value_labels ) \u2014 Export DataFrame object to Stata dta format. </> to_string ( buf , columns , col_space , header , index , na_rep , formatters , float_format , sparsify , index_names , justify , max_rows , max_cols , show_dimensions , decimal , line_width , min_rows , max_colwidth , encoding ) (str or None) \u2014 Render a DataFrame to a console-friendly tabular output. </> to_timestamp ( freq , how , axis , copy ) (DataFrame) \u2014 Cast to DatetimeIndex of timestamps, at beginning of period. </> to_xarray ( ) (xarray.DataArray or xarray.Dataset) \u2014 Return an xarray object from the pandas object. </> to_xml ( path_or_buffer , index , root_name , row_name , na_rep , attr_cols , elem_cols , namespaces , prefix , encoding , xml_declaration , pretty_print , parser , stylesheet , compression , storage_options ) (None or str) \u2014 Render a DataFrame to an XML document. </> transform ( func , axis , *args , **kwargs ) (DataFrame) \u2014 Call func on self producing a DataFrame with the same axis shape as self. </> transpose ( *args , copy ) (DataFrame) \u2014 Transpose index and columns. </> truediv ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Floating division of dataframe and other, element-wise (binary operator truediv ). </> truncate ( before , after , axis , copy ) (type of caller) \u2014 Truncate a Series or DataFrame before and after some index value. </> tz_convert ( tz , axis , level , copy ) (Series/DataFrame) \u2014 Convert tz-aware axis to target time zone. </> tz_localize ( tz , axis , level , copy , ambiguous , nonexistent ) (Series/DataFrame) \u2014 Localize tz-naive index of a Series or DataFrame to target time zone. </> unstack ( level , fill_value , sort ) (Series or DataFrame) \u2014 Pivot a level of the (necessarily hierarchical) index labels. </> update ( other , join , overwrite , filter_func , errors ) (None) \u2014 Modify in place using non-NA values from another DataFrame. </> value_counts ( subset , normalize , sort , ascending , dropna ) (Series) \u2014 Return a Series containing the frequency of each distinct row in the Dataframe. </> var ( axis , skipna , ddof , numeric_only , **kwargs ) (Series or DataFrame (if level specified)) \u2014 Return unbiased variance over requested axis. </> where ( cond , other , inplace , axis , level ) (Same type as caller or None if ``inplace=True``.) \u2014 Replace values where the condition is False. </> xs ( key , axis , level , drop_level ) (Series or DataFrame) \u2014 Return cross-section from the Series/DataFrame. </> method __add__ ( other ) </> Get Addition of DataFrame and other, column-wise. Equivalent to DataFrame.add(other) . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Object to be added to the DataFrame. Returns (DataFrame) The result of adding other to DataFrame. See Also DataFrame.add : Add a DataFrame and another object, with option for index- or column-oriented addition. Examples >>> df = pd . DataFrame ({ 'height' : [ 1.5 , 2.6 ], 'weight' : [ 500 , 800 ]}, ... index = [ 'elk' , 'moose' ]) >>> df height weight elk 1.5 500 moose 2.6 800 Adding a scalar affects all rows and columns. >>> df [[ 'height' , 'weight' ]] + 1.5 height weight elk 3.0 501.5 moose 4.1 801.5 Each element of a list is added to a column of the DataFrame, in order. >>> df [[ 'height' , 'weight' ]] + [ 0.5 , 1.5 ] height weight elk 2.0 501.5 moose 3.1 801.5 Keys of a dictionary are aligned to the DataFrame, based on column names; each value in the dictionary is added to the corresponding column. >>> df [[ 'height' , 'weight' ]] + { 'height' : 0.5 , 'weight' : 1.5 } height weight elk 2.0 501.5 moose 3.1 801.5 When other is a :class: Series , the index of other is aligned with the columns of the DataFrame. >>> s1 = pd . Series ([ 0.5 , 1.5 ], index = [ 'weight' , 'height' ]) >>> df [[ 'height' , 'weight' ]] + s1 height weight elk 3.0 500.5 moose 4.1 800.5 Even when the index of other is the same as the index of the DataFrame, the :class: Series will not be reoriented. If index-wise alignment is desired, :meth: DataFrame.add should be used with axis='index' . >>> s2 = pd . Series ([ 0.5 , 1.5 ], index = [ 'elk' , 'moose' ]) >>> df [[ 'height' , 'weight' ]] + s2 elk height moose weight elk NaN NaN NaN NaN moose NaN NaN NaN NaN >>> df [[ 'height' , 'weight' ]] . add ( s2 , axis = 'index' ) height weight elk 2.0 500.5 moose 4.1 801.5 When other is a :class: DataFrame , both columns names and the index are aligned. >>> other = pd . DataFrame ({ 'height' : [ 0.2 , 0.4 , 0.6 ]}, ... index = [ 'elk' , 'moose' , 'deer' ]) >>> df [[ 'height' , 'weight' ]] + other height weight deer NaN NaN elk 1.7 NaN moose 3.0 NaN method __dir__ ( ) \u2192 list </> Provide method name lookup and completion. Notes Only provide 'public' methods. method __sizeof__ ( ) \u2192 int </> Generates the total memory usage for an object that returnseither a value or Series of values method set_flags ( copy=False , allows_duplicate_labels=None ) </> Return a new object with updated flags. Parameters copy (bool, default False) \u2014 Specify if a copy of the object should be made. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` allows_duplicate_labels (bool, optional) \u2014 Whether the returned object allows duplicate labels. Returns (Series or DataFrame) The same type as the caller. See Also DataFrame.attrs : Global metadata applying to this dataset.DataFrame.flags : Global flags applying to this object. Notes This method returns a new object that's a view on the same data as the input. Mutating the input or the output values will be reflected in the other. This method is intended to be used in method chains. \"Flags\" differ from \"metadata\". Flags reflect properties of the pandas object (the Series or DataFrame). Metadata refer to properties of the dataset, and should be stored in :attr: DataFrame.attrs . Examples >>> df = pd . DataFrame ({ \"A\" : [ 1 , 2 ]}) >>> df . flags . allows_duplicate_labels True >>> df2 = df . set_flags ( allows_duplicate_labels = False ) >>> df2 . flags . allows_duplicate_labels False method swapaxes ( axis1 , axis2 , copy=None ) </> Interchange axes and swap values axes appropriately. .. deprecated:: 2.1.0 swapaxes is deprecated and will be removed. Please use transpose instead. Examples Please see examples for :meth: DataFrame.transpose . method droplevel ( level , axis=0 ) </> Return Series/DataFrame with requested index / column level(s) removed. Parameters level (int, str, or list-like) \u2014 If a string is given, must be the name of a levelIf list-like, elements must be names or positional indexes of levels. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Axis along which the level(s) is removed: 0 or 'index': remove level(s) in column. 1 or 'columns': remove level(s) in row. For Series this parameter is unused and defaults to 0. Returns (Series/DataFrame) Series/DataFrame with requested index / column level(s) removed. Examples >>> df = pd . DataFrame ([ ... [ 1 , 2 , 3 , 4 ], ... [ 5 , 6 , 7 , 8 ], ... [ 9 , 10 , 11 , 12 ] ... ]) . set_index ([ 0 , 1 ]) . rename_axis ([ 'a' , 'b' ]) >>> df . columns = pd . MultiIndex . from_tuples ([ ... ( 'c' , 'e' ), ( 'd' , 'f' ) ... ], names = [ 'level_1' , 'level_2' ]) >>> df level_1 c d level_2 e f a b 1 2 3 4 5 6 7 8 9 10 11 12 >>> df . droplevel ( 'a' ) level_1 c d level_2 e f b 2 3 4 6 7 8 10 11 12 >>> df . droplevel ( 'level_2' , axis = 1 ) level_1 c d a b 1 2 3 4 5 6 7 8 9 10 11 12 method squeeze ( axis=None ) </> Squeeze 1 dimensional axis objects into scalars. Series or DataFrames with a single element are squeezed to a scalar. DataFrames with a single column or a single row are squeezed to a Series. Otherwise the object is unchanged. This method is most useful when you don't know if your object is a Series or DataFrame, but you do know it has just a single column. In that case you can safely call squeeze to ensure you have a Series. Parameters axis ({0 or 'index', 1 or 'columns', None}, default None) \u2014 A specific axis to squeeze. By default, all length-1 axes aresqueezed. For Series this parameter is unused and defaults to None . Returns (DataFrame, Series, or scalar) The projection after squeezing axis or all the axes. See Also Series.iloc : Integer-location based indexing for selecting scalars.DataFrame.iloc : Integer-location based indexing for selecting Series. Series.to_frame : Inverse of DataFrame.squeeze for a single-column DataFrame. Examples >>> primes = pd . Series ([ 2 , 3 , 5 , 7 ]) Slicing might produce a Series with a single value: >>> even_primes = primes [ primes % 2 == 0 ] >>> even_primes 0 2 dtype : int64 >>> even_primes . squeeze () 2 Squeezing objects with more than one value in every axis does nothing: >>> odd_primes = primes [ primes % 2 == 1 ] >>> odd_primes 1 3 2 5 3 7 dtype : int64 >>> odd_primes . squeeze () 1 3 2 5 3 7 dtype : int64 Squeezing is even more effective when used with DataFrames. >>> df = pd . DataFrame ([[ 1 , 2 ], [ 3 , 4 ]], columns = [ 'a' , 'b' ]) >>> df a b 0 1 2 1 3 4 Slicing a single column will produce a DataFrame with the columns having only one value: >>> df_a = df [[ 'a' ]] >>> df_a a 0 1 1 3 So the columns can be squeezed down, resulting in a Series: >>> df_a . squeeze ( 'columns' ) 0 1 1 3 Name : a , dtype : int64 Slicing a single row from a single column will produce a single scalar DataFrame: >>> df_0a = df . loc [ df . index < 1 , [ 'a' ]] >>> df_0a a 0 1 Squeezing the rows produces a single scalar Series: >>> df_0a . squeeze ( 'rows' ) a 1 Name : 0 , dtype : int64 Squeezing all axes will project directly into a scalar: >>> df_0a . squeeze () 1 method rename_axis ( mapper=<no_default> , index=<no_default> , columns=<no_default> , axis=0 , copy=None , inplace=False ) </> Set the name of the axis for the index or columns. Parameters mapper (scalar, list-like, optional) \u2014 Value to set the axis name attribute. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to rename. For Series this parameter is unused and defaults to 0. copy (bool, default None) \u2014 Also copy underlying data. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` inplace (bool, default False) \u2014 Modifies the object directly, instead of creating a new Seriesor DataFrame. Returns (Series, DataFrame, or None) The same type as the caller or None if inplace=True . See Also Series.rename : Alter Series index labels or name.DataFrame.rename : Alter DataFrame index labels or name. Index.rename : Set new names on index. Notes DataFrame.rename_axis supports two calling conventions (index=index_mapper, columns=columns_mapper, ...) (mapper, axis={'index', 'columns'}, ...) The first calling convention will only modify the names of the index and/or the names of the Index object that is the columns. In this case, the parameter copy is ignored. The second calling convention will modify the names of the corresponding index if mapper is a list or a scalar. However, if mapper is dict-like or a function, it will use the deprecated behavior of modifying the axis labels . We highly recommend using keyword arguments to clarify your intent. Examples Series >>> s = pd . Series ([ \"dog\" , \"cat\" , \"monkey\" ]) >>> s 0 dog 1 cat 2 monkey dtype : object >>> s . rename_axis ( \"animal\" ) animal 0 dog 1 cat 2 monkey dtype : object DataFrame >>> df = pd . DataFrame ({ \"num_legs\" : [ 4 , 4 , 2 ], ... \"num_arms\" : [ 0 , 0 , 2 ]}, ... [ \"dog\" , \"cat\" , \"monkey\" ]) >>> df num_legs num_arms dog 4 0 cat 4 0 monkey 2 2 >>> df = df . rename_axis ( \"animal\" ) >>> df num_legs num_arms animal dog 4 0 cat 4 0 monkey 2 2 >>> df = df . rename_axis ( \"limbs\" , axis = \"columns\" ) >>> df limbs num_legs num_arms animal dog 4 0 cat 4 0 monkey 2 2 MultiIndex >>> df . index = pd . MultiIndex . from_product ([[ 'mammal' ], ... [ 'dog' , 'cat' , 'monkey' ]], ... names = [ 'type' , 'name' ]) >>> df limbs num_legs num_arms type name mammal dog 4 0 cat 4 0 monkey 2 2 >>> df . rename_axis ( index = { 'type' : 'class' }) limbs num_legs num_arms class name mammal dog 4 0 cat 4 0 monkey 2 2 >>> df . rename_axis ( columns = str . upper ) LIMBS num_legs num_arms type name mammal dog 4 0 cat 4 0 monkey 2 2 method equals ( other ) </> Test whether two objects contain the same elements. This function allows two Series or DataFrames to be compared against each other to see if they have the same shape and elements. NaNs in the same location are considered equal. The row/column index do not need to have the same type, as long as the values are considered equal. Corresponding columns and index must be of the same dtype. Parameters other (Series or DataFrame) \u2014 The other Series or DataFrame to be compared with the first. Returns (bool) True if all elements are the same in both objects, Falseotherwise. See Also Series.eq : Compare two Series objects of the same length and return a Series where each element is True if the element in each Series is equal, False otherwise. DataFrame.eq : Compare two DataFrame objects of the same shape and return a DataFrame where each element is True if the respective element in each DataFrame is equal, False otherwise. testing.assert_series_equal : Raises an AssertionError if left and right are not equal. Provides an easy interface to ignore inequality in dtypes, indexes and precision among others. testing.assert_frame_equal : Like assert_series_equal, but targets DataFrames. numpy.array_equal : Return True if two arrays have the same shape and elements, False otherwise. Examples >>> df = pd . DataFrame ({ 1 : [ 10 ], 2 : [ 20 ]}) >>> df 1 2 0 10 20 DataFrames df and exactly_equal have the same types and values for their elements and column labels, which will return True. >>> exactly_equal = pd . DataFrame ({ 1 : [ 10 ], 2 : [ 20 ]}) >>> exactly_equal 1 2 0 10 20 >>> df . equals ( exactly_equal ) True DataFrames df and different_column_type have the same element types and values, but have different types for the column labels, which will still return True. >>> different_column_type = pd . DataFrame ({ 1.0 : [ 10 ], 2.0 : [ 20 ]}) >>> different_column_type 1.0 2.0 0 10 20 >>> df . equals ( different_column_type ) True DataFrames df and different_data_type have different types for the same values for their elements, and will return False even though their column labels are the same values and types. >>> different_data_type = pd . DataFrame ({ 1 : [ 10.0 ], 2 : [ 20.0 ]}) >>> different_data_type 1 2 0 10.0 20.0 >>> df . equals ( different_data_type ) False method bool ( ) </> Return the bool of a single element Series or DataFrame. .. deprecated:: 2.1.0 bool is deprecated and will be removed in future version of pandas. For Series use pandas.Series.item . This must be a boolean scalar value, either True or False. It will raise a ValueError if the Series or DataFrame does not have exactly 1 element, or that element is not boolean (integer values 0 and 1 will also raise an exception). Returns (bool) The value in the Series or DataFrame. See Also Series.astype : Change the data type of a Series, including to boolean.DataFrame.astype : Change the data type of a DataFrame, including to boolean. numpy.bool_ : NumPy boolean data type, used by pandas for boolean values. Examples The method will only work for single element objects with a boolean value: >>> pd . Series ([ True ]) . bool () # doctest: +SKIP True >>> pd . Series ([ False ]) . bool () # doctest: +SKIP False >>> pd . DataFrame ({ 'col' : [ True ]}) . bool () # doctest: +SKIP True >>> pd . DataFrame ({ 'col' : [ False ]}) . bool () # doctest: +SKIP False This is an alternative method and will only work for single element objects with a boolean value: >>> pd . Series ([ True ]) . item () # doctest: +SKIP True >>> pd . Series ([ False ]) . item () # doctest: +SKIP False method abs ( ) </> Return a Series/DataFrame with absolute numeric value of each element. This function only applies to elements that are all numeric. Returns (abs) Series/DataFrame containing the absolute value of each element. See Also numpy.absolute : Calculate the absolute value element-wise. Notes For complex inputs, 1.2 + 1j , the absolute value is :math: \\sqrt{ a^2 + b^2 } . Examples Absolute numeric values in a Series. >>> s = pd . Series ([ - 1.10 , 2 , - 3.33 , 4 ]) >>> s . abs () 0 1.10 1 2.00 2 3.33 3 4.00 dtype : float64 Absolute numeric values in a Series with complex numbers. >>> s = pd . Series ([ 1.2 + 1 j ]) >>> s . abs () 0 1.56205 dtype : float64 Absolute numeric values in a Series with a Timedelta element. >>> s = pd . Series ([ pd . Timedelta ( '1 days' )]) >>> s . abs () 0 1 days dtype : timedelta64 [ ns ] Select rows with data closest to certain value using argsort (from StackOverflow <https://stackoverflow.com/a/17758115> __). >>> df = pd . DataFrame ({ ... 'a' : [ 4 , 5 , 6 , 7 ], ... 'b' : [ 10 , 20 , 30 , 40 ], ... 'c' : [ 100 , 50 , - 30 , - 50 ] ... }) >>> df a b c 0 4 10 100 1 5 20 50 2 6 30 - 30 3 7 40 - 50 >>> df . loc [( df . c - 43 ) . abs () . argsort ()] a b c 1 5 20 50 0 4 10 100 2 6 30 - 30 3 7 40 - 50 method __iter__ ( ) </> Iterate over info axis. Returns (iterator) Info axis as iterator. Examples >>> df = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 ], 'B' : [ 4 , 5 , 6 ]}) >>> for x in df : ... print ( x ) A B method keys ( ) </> Get the 'info axis' (see Indexing for more). This is index for Series, columns for DataFrame. Returns (Index) Info axis. Examples >>> d = pd . DataFrame ( data = { 'A' : [ 1 , 2 , 3 ], 'B' : [ 0 , 4 , 8 ]}, ... index = [ 'a' , 'b' , 'c' ]) >>> d A B a 1 0 b 2 4 c 3 8 >>> d . keys () Index ([ 'A' , 'B' ], dtype = 'object' ) method __contains__ ( key ) \u2192 bool </> True if the key is in the info axis method to_excel ( excel_writer , sheet_name='Sheet1' , na_rep='' , float_format=None , columns=None , header=True , index=True , index_label=None , startrow=0 , startcol=0 , engine=None , merge_cells=True , inf_rep='inf' , freeze_panes=None , storage_options=None , engine_kwargs=None ) </> Write object to an Excel sheet. To write a single object to an Excel .xlsx file it is only necessary to specify a target file name. To write to multiple sheets it is necessary to create an ExcelWriter object with a target file name, and specify a sheet in the file to write to. Multiple sheets may be written to by specifying unique sheet_name . With all data written to the file it is necessary to save the changes. Note that creating an ExcelWriter object with a file name that already exists will result in the contents of the existing file being erased. Parameters excel_writer (path-like, file-like, or ExcelWriter object) \u2014 File path or existing ExcelWriter. sheet_name (str, default 'Sheet1') \u2014 Name of sheet which will contain DataFrame. na_rep (str, default '') \u2014 Missing data representation. float_format (str, optional) \u2014 Format string for floating point numbers. For example float_format=\"%.2f\" will format 0.1234 to 0.12. columns (sequence or list of str, optional) \u2014 Columns to write. header (bool or list of str, default True) \u2014 Write out the column names. If a list of string is given it isassumed to be aliases for the column names. index (bool, default True) \u2014 Write row names (index). index_label (str or sequence, optional) \u2014 Column label for index column(s) if desired. If not specified, and header and index are True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. startrow (int, default 0) \u2014 Upper left cell row to dump data frame. startcol (int, default 0) \u2014 Upper left cell column to dump data frame. engine (str, optional) \u2014 Write engine to use, 'openpyxl' or 'xlsxwriter'. You can also set thisvia the options io.excel.xlsx.writer or io.excel.xlsm.writer . merge_cells (bool, default True) \u2014 Write MultiIndex and Hierarchical Rows as merged cells. inf_rep (str, default 'inf') \u2014 Representation for infinity (there is no native representation forinfinity in Excel). freeze_panes (tuple of int (length 2), optional) \u2014 Specifies the one-based bottommost row and rightmost column thatis to be frozen. storage_options (dict, optional) \u2014 Extra options that make sense for a particular storage connection, e.g.host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to fsspec.open . Please see fsspec and urllib for more details, and for more examples on storage options refer here <https://pandas.pydata.org/docs/user_guide/io.html? highlight=storage_options#reading-writing-remote-files> _. .. versionadded:: 1.2.0 engine_kwargs (dict, optional) \u2014 Arbitrary keyword arguments passed to excel engine. See Also to_csv : Write DataFrame to a comma-separated values (csv) file.ExcelWriter : Class for writing DataFrame objects into excel sheets. read_excel : Read an Excel file into a pandas DataFrame. read_csv : Read a comma-separated values (csv) file into DataFrame. io.formats.style.Styler.to_excel : Add styles to Excel sheet. Notes For compatibility with :meth: ~DataFrame.to_csv , to_excel serializes lists and dicts to strings before writing. Once a workbook has been saved it is not possible to write further data without rewriting the whole workbook. Examples : , , ) P : , P s : ) P ) ) : , P ) , s : P method to_json ( path_or_buf=None , orient=None , date_format=None , double_precision=10 , force_ascii=True , date_unit='ms' , default_handler=None , lines=False , compression='infer' , index=None , indent=None , storage_options=None , mode='w' ) </> Convert the object to a JSON string. Note NaN's and None will be converted to null and datetime objects will be converted to UNIX timestamps. Parameters path_or_buf (str, path object, file-like object, or None, default None) \u2014 String, path object (implementing os.PathLike[str]), or file-likeobject implementing a write() function. If None, the result is returned as a string. orient (str) \u2014 Indication of expected JSON string format. Series: default is 'index' allowed values are: {'split', 'records', 'index', 'table'}. DataFrame: default is 'columns' allowed values are: {'split', 'records', 'index', 'columns', 'values', 'table'}. The format of the JSON string: 'split' : dict like {'index' -> [index], 'columns' -> [columns], 'data' -> [values]} 'records' : list like [{column -> value}, ... , {column -> value}] 'index' : dict like {index -> {column -> value}} 'columns' : dict like {column -> {index -> value}} 'values' : just the values array 'table' : dict like {'schema': {schema}, 'data': {data}} Describing the data, where data component is like orient='records' . date_format ({None, 'epoch', 'iso'}) \u2014 Type of date conversion. 'epoch' = epoch milliseconds,'iso' = ISO8601. The default depends on the orient . For orient='table' , the default is 'iso'. For all other orients, the default is 'epoch'. double_precision (int, default 10) \u2014 The number of decimal places to use when encodingfloating point values. The possible maximal value is 15. Passing double_precision greater than 15 will raise a ValueError. force_ascii (bool, default True) \u2014 Force encoded string to be ASCII. date_unit (str, default 'ms' (milliseconds)) \u2014 The time unit to encode to, governs timestamp and ISO8601precision. One of 's', 'ms', 'us', 'ns' for second, millisecond, microsecond, and nanosecond respectively. default_handler (callable, default None) \u2014 Handler to call if object cannot otherwise be converted to asuitable format for JSON. Should receive a single argument which is the object to convert and return a serialisable object. lines (bool, default False) \u2014 If 'orient' is 'records' write out line-delimited json format. Willthrow ValueError if incorrect 'orient' since others are not list-like. compression (str or dict, default 'infer') \u2014 For on-the-fly compression of the output data. If 'infer' and 'path_or_buf' ispath-like, then detect compression from the following extensions: '.gz', '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2' (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of { 'zip' , 'gzip' , 'bz2' , 'zstd' , 'xz' , 'tar' } and other key-value pairs are forwarded to zipfile.ZipFile , gzip.GzipFile , bz2.BZ2File , zstandard.ZstdCompressor , lzma.LZMAFile or tarfile.TarFile , respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1} . .. versionadded:: 1.5.0 Added support for .tar files. .. versionchanged:: 1.4.0 Zstandard support. index (bool or None, default None) \u2014 The index is only used when 'orient' is 'split', 'index', 'column',or 'table'. Of these, 'index' and 'column' do not support index=False . indent (int, optional) \u2014 Length of whitespace used to indent each record. storage_options (dict, optional) \u2014 Extra options that make sense for a particular storage connection, e.g.host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to fsspec.open . Please see fsspec and urllib for more details, and for more examples on storage options refer here <https://pandas.pydata.org/docs/user_guide/io.html? highlight=storage_options#reading-writing-remote-files> _. mode (str, default 'w' (writing)) \u2014 Specify the IO mode for output when supplying a path_or_buf.Accepted args are 'w' (writing) and 'a' (append) only. mode='a' is only supported when lines is True and orient is 'records'. Returns (None or str) If path_or_buf is None, returns the resulting json format as astring. Otherwise returns None. See Also read_json : Convert a JSON string to pandas object. Notes The behavior of indent=0 varies from the stdlib, which does not indent the output but does insert newlines. Currently, indent=0 and the default indent=None are equivalent in pandas, though this may change in a future release. orient='table' contains a 'pandas_version' field under 'schema'. This stores the version of pandas used in the latest revision of the schema. Examples >>> from json import loads , dumps >>> df = pd . DataFrame ( ... [[ \"a\" , \"b\" ], [ \"c\" , \"d\" ]], ... index = [ \"row 1\" , \"row 2\" ], ... columns = [ \"col 1\" , \"col 2\" ], ... ) >>> result = df . to_json ( orient = \"split\" ) >>> parsed = loads ( result ) >>> dumps ( parsed , indent = 4 ) # doctest: +SKIP { \"columns\" : [ \"col 1\" , \"col 2\" ], \"index\" : [ \"row 1\" , \"row 2\" ], \"data\" : [ [ \"a\" , \"b\" ], [ \"c\" , \"d\" ] ] } Encoding/decoding a Dataframe using 'records' formatted JSON. Note that index labels are not preserved with this encoding. >>> result = df . to_json ( orient = \"records\" ) >>> parsed = loads ( result ) >>> dumps ( parsed , indent = 4 ) # doctest: +SKIP [ { \"col 1\" : \"a\" , \"col 2\" : \"b\" }, { \"col 1\" : \"c\" , \"col 2\" : \"d\" } ] Encoding/decoding a Dataframe using 'index' formatted JSON: >>> result = df . to_json ( orient = \"index\" ) >>> parsed = loads ( result ) >>> dumps ( parsed , indent = 4 ) # doctest: +SKIP { \"row 1\" : { \"col 1\" : \"a\" , \"col 2\" : \"b\" }, \"row 2\" : { \"col 1\" : \"c\" , \"col 2\" : \"d\" } } Encoding/decoding a Dataframe using 'columns' formatted JSON: >>> result = df . to_json ( orient = \"columns\" ) >>> parsed = loads ( result ) >>> dumps ( parsed , indent = 4 ) # doctest: +SKIP { \"col 1\" : { \"row 1\" : \"a\" , \"row 2\" : \"c\" }, \"col 2\" : { \"row 1\" : \"b\" , \"row 2\" : \"d\" } } Encoding/decoding a Dataframe using 'values' formatted JSON: >>> result = df . to_json ( orient = \"values\" ) >>> parsed = loads ( result ) >>> dumps ( parsed , indent = 4 ) # doctest: +SKIP [ [ \"a\" , \"b\" ], [ \"c\" , \"d\" ] ] Encoding with Table Schema: >>> result = df . to_json ( orient = \"table\" ) >>> parsed = loads ( result ) >>> dumps ( parsed , indent = 4 ) # doctest: +SKIP { \"schema\" : { \"fields\" : [ { \"name\" : \"index\" , \"type\" : \"string\" }, { \"name\" : \"col 1\" , \"type\" : \"string\" }, { \"name\" : \"col 2\" , \"type\" : \"string\" } ], \"primaryKey\" : [ \"index\" ], \"pandas_version\" : \"1.4.0\" }, \"data\" : [ { \"index\" : \"row 1\" , \"col 1\" : \"a\" , \"col 2\" : \"b\" }, { \"index\" : \"row 2\" , \"col 1\" : \"c\" , \"col 2\" : \"d\" } ] } method to_hdf ( path_or_buf , key , mode='a' , complevel=None , complib=None , append=False , format=None , index=True , min_itemsize=None , nan_rep=None , dropna=None , data_columns=None , errors='strict' , encoding='UTF-8' ) </> Write the contained data to an HDF5 file using HDFStore. Hierarchical Data Format (HDF) is self-describing, allowing an application to interpret the structure and contents of a file with no outside information. One HDF file can hold a mix of related objects which can be accessed as a group or as individual objects. In order to add another DataFrame or Series to an existing HDF file please use append mode and a different a key. .. warning:: One can store a subclass of DataFrame or Series to HDF5, but the type of the subclass is lost upon storing. For more information see the :ref: user guide <io.hdf5> . Parameters path_or_buf (str or pandas.HDFStore) \u2014 File path or HDFStore object. key (str) \u2014 Identifier for the group in the store. mode ({'a', 'w', 'r+'}, default 'a') \u2014 Mode to open file: 'w': write, a new file is created (an existing file with the same name would be deleted). 'a': append, an existing file is opened for reading and writing, and if the file does not exist it is created. 'r+': similar to 'a', but the file must already exist. complevel ({0-9}, default None) \u2014 Specifies a compression level for data.A value of 0 or None disables compression. complib ({'zlib', 'lzo', 'bzip2', 'blosc'}, default 'zlib') \u2014 Specifies the compression library to be used.These additional compressors for Blosc are supported (default if no compressor specified: 'blosc:blosclz'): {'blosc:blosclz', 'blosc:lz4', 'blosc:lz4hc', 'blosc:snappy', 'blosc:zlib', 'blosc:zstd'}. Specifying a compression library which is not available issues a ValueError. append (bool, default False) \u2014 For Table formats, append the input data to the existing. format ({'fixed', 'table', None}, default 'fixed') \u2014 Possible values: 'fixed': Fixed format. Fast writing/reading. Not-appendable, nor searchable. 'table': Table format. Write as a PyTables Table structure which may perform worse but allow more flexible operations like searching / selecting subsets of the data. If None, pd.get_option('io.hdf.default_format') is checked, followed by fallback to \"fixed\". index (bool, default True) \u2014 Write DataFrame index as a column. min_itemsize (dict or int, optional) \u2014 Map column names to minimum string sizes for columns. nan_rep (Any, optional) \u2014 How to represent null values as str.Not allowed with append=True. dropna (bool, default False, optional) \u2014 Remove missing values. data_columns (list of columns or True, optional) \u2014 List of columns to create as indexed data columns for on-diskqueries, or True to use all columns. By default only the axes of the object are indexed. See :ref: Query via data columns<io.hdf5-query-data-columns> . for more information. Applicable only to format='table'. errors (str, default 'strict') \u2014 Specifies how encoding and decoding errors are to be handled.See the errors argument for :func: open for a full list of options. See Also read_hdf : Read from HDF file.DataFrame.to_orc : Write a DataFrame to the binary orc format. DataFrame.to_parquet : Write a DataFrame to the binary parquet format. DataFrame.to_sql : Write to a SQL table. DataFrame.to_feather : Write out feather-format for DataFrames. DataFrame.to_csv : Write out to a csv file. Examples >>> df = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 ], 'B' : [ 4 , 5 , 6 ]}, ... index = [ 'a' , 'b' , 'c' ]) # doctest: +SKIP >>> df . to_hdf ( 'data.h5' , key = 'df' , mode = 'w' ) # doctest: +SKIP We can add another object to the same file: >>> s = pd . Series ([ 1 , 2 , 3 , 4 ]) # doctest: +SKIP >>> s . to_hdf ( 'data.h5' , key = 's' ) # doctest: +SKIP Reading from HDF file: >>> pd . read_hdf ( 'data.h5' , 'df' ) # doctest: +SKIP A B a 1 4 b 2 5 c 3 6 >>> pd . read_hdf ( 'data.h5' , 's' ) # doctest: +SKIP 0 1 1 2 2 3 3 4 dtype : int64 method to_sql ( name , con , schema=None , if_exists='fail' , index=True , index_label=None , chunksize=None , dtype=None , method=None ) </> Write records stored in a DataFrame to a SQL database. Databases supported by SQLAlchemy [1]_ are supported. Tables can be newly created, appended to, or overwritten. Parameters name (str) \u2014 Name of SQL table. con (sqlalchemy.engine.(Engine or Connection) or sqlite3.Connection) \u2014 Using SQLAlchemy makes it possible to use any DB supported by thatlibrary. Legacy support is provided for sqlite3.Connection objects. The user is responsible for engine disposal and connection closure for the SQLAlchemy connectable. See here <https://docs.sqlalchemy.org/en/20/core/connections.html> _. If passing a sqlalchemy.engine.Connection which is already in a transaction, the transaction will not be committed. If passing a sqlite3.Connection, it will not be possible to roll back the record insertion. schema (str, optional) \u2014 Specify the schema (if database flavor supports this). If None, usedefault schema. if_exists ({'fail', 'replace', 'append'}, default 'fail') \u2014 How to behave if the table already exists. fail: Raise a ValueError. replace: Drop the table before inserting new values. append: Insert new values to the existing table. index (bool, default True) \u2014 Write DataFrame index as a column. Uses index_label as the columnname in the table. Creates a table index for this column. index_label (str or sequence, default None) \u2014 Column label for index column(s). If None is given (default) and index is True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. chunksize (int, optional) \u2014 Specify the number of rows in each batch to be written at a time.By default, all rows will be written at once. dtype (dict or scalar, optional) \u2014 Specifying the datatype for columns. If a dictionary is used, thekeys should be the column names and the values should be the SQLAlchemy types or strings for the sqlite3 legacy mode. If a scalar is provided, it will be applied to all columns. method ({None, 'multi', callable}, optional) \u2014 Controls the SQL insertion clause used: None : Uses standard SQL INSERT clause (one per row). 'multi': Pass multiple values in a single INSERT clause. callable with signature (pd_table, conn, keys, data_iter) . Details and a sample callable implementation can be found in the section :ref: insert method <io.sql.method> . Returns (None or int) Number of rows affected by to_sql. None is returned if the callablepassed into method does not return an integer number of rows. The number of returned rows affected is the sum of the rowcount attribute of sqlite3.Cursor or SQLAlchemy connectable which may not reflect the exact number of written rows as stipulated in the sqlite3 <https://docs.python.org/3/library/sqlite3.html#sqlite3.Cursor.rowcount> or SQLAlchemy <https://docs.sqlalchemy.org/en/20/core/connections.html#sqlalchemy.engine.CursorResult.rowcount> . .. versionadded:: 1.4.0 Raises ValueError \u2014 When the table already exists and if_exists is 'fail' (thedefault). See Also read_sql : Read a DataFrame from a table. Notes Timezone aware datetime columns will be written as Timestamp with timezone type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone. Not all datastores support method=\"multi\" . Oracle, for example, does not support multi-value insert. References .. [1] https://docs.sqlalchemy.org.. [2] https://www.python.org/dev/peps/pep-0249/ Examples Create an in-memory SQLite database. >>> from sqlalchemy import create_engine >>> engine = create_engine ( 'sqlite://' , echo = False ) Create a table from scratch with 3 rows. >>> df = pd . DataFrame ({ 'name' : [ 'User 1' , 'User 2' , 'User 3' ]}) >>> df name 0 User 1 1 User 2 2 User 3 >>> df . to_sql ( name = 'users' , con = engine ) 3 >>> from sqlalchemy import text >>> with engine . connect () as conn : ... conn . execute ( text ( \"SELECT * FROM users\" )) . fetchall () [( 0 , 'User 1' ), ( 1 , 'User 2' ), ( 2 , 'User 3' )] An sqlalchemy.engine.Connection can also be passed to con : >>> with engine . begin () as connection : ... df1 = pd . DataFrame ({ 'name' : [ 'User 4' , 'User 5' ]}) ... df1 . to_sql ( name = 'users' , con = connection , if_exists = 'append' ) 2 This is allowed to support operations that require that the same DBAPI connection is used for the entire operation. >>> df2 = pd . DataFrame ({ 'name' : [ 'User 6' , 'User 7' ]}) >>> df2 . to_sql ( name = 'users' , con = engine , if_exists = 'append' ) 2 >>> with engine . connect () as conn : ... conn . execute ( text ( \"SELECT * FROM users\" )) . fetchall () [( 0 , 'User 1' ), ( 1 , 'User 2' ), ( 2 , 'User 3' ), ( 0 , 'User 4' ), ( 1 , 'User 5' ), ( 0 , 'User 6' ), ( 1 , 'User 7' )] Overwrite the table with just df2 . >>> df2 . to_sql ( name = 'users' , con = engine , if_exists = 'replace' , ... index_label = 'id' ) 2 >>> with engine . connect () as conn : ... conn . execute ( text ( \"SELECT * FROM users\" )) . fetchall () [( 0 , 'User 6' ), ( 1 , 'User 7' )] Use method to define a callable insertion method to do nothing if there's a primary key conflict on a table in a PostgreSQL database. >>> from sqlalchemy.dialects.postgresql import insert >>> def insert_on_conflict_nothing ( table , conn , keys , data_iter ): ... # \"a\" is the primary key in \"conflict_table\" ... data = [ dict ( zip ( keys , row )) for row in data_iter ] ... stmt = insert ( table . table ) . values ( data ) . on_conflict_do_nothing ( index_elements = [ \"a\" ]) ... result = conn . execute ( stmt ) ... return result . rowcount >>> df_conflict . to_sql ( name = \"conflict_table\" , con = conn , if_exists = \"append\" , method = insert_on_conflict_nothing ) # doctest: +SKIP 0 For MySQL, a callable to update columns b and c if there's a conflict on a primary key. >>> from sqlalchemy.dialects.mysql import insert >>> def insert_on_conflict_update ( table , conn , keys , data_iter ): ... # update columns \"b\" and \"c\" on primary key conflict ... data = [ dict ( zip ( keys , row )) for row in data_iter ] ... stmt = ( ... insert ( table . table ) ... . values ( data ) ... ) ... stmt = stmt . on_duplicate_key_update ( b = stmt . inserted . b , c = stmt . inserted . c ) ... result = conn . execute ( stmt ) ... return result . rowcount >>> df_conflict . to_sql ( name = \"conflict_table\" , con = conn , if_exists = \"append\" , method = insert_on_conflict_update ) # doctest: +SKIP 2 Specify the dtype (especially useful for integers with missing values). Notice that while pandas is forced to store the data as floating point, the database supports nullable integers. When fetching the data with Python, we get back integer scalars. >>> df = pd . DataFrame ({ \"A\" : [ 1 , None , 2 ]}) >>> df A 0 1.0 1 NaN 2 2.0 >>> from sqlalchemy.types import Integer >>> df . to_sql ( name = 'integers' , con = engine , index = False , ... dtype = { \"A\" : Integer ()}) 3 >>> with engine . connect () as conn : ... conn . execute ( text ( \"SELECT * FROM integers\" )) . fetchall () [( 1 ,), ( None ,), ( 2 ,)] method to_pickle ( path , compression='infer' , protocol=5 , storage_options=None ) </> Pickle (serialize) object to file. Parameters path (str, path object, or file-like object) \u2014 String, path object (implementing os.PathLike[str] ), or file-likeobject implementing a binary write() function. File path where the pickled object will be stored. compression (str or dict, default 'infer') \u2014 For on-the-fly compression of the output data. If 'infer' and 'path' ispath-like, then detect compression from the following extensions: '.gz', '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2' (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of { 'zip' , 'gzip' , 'bz2' , 'zstd' , 'xz' , 'tar' } and other key-value pairs are forwarded to zipfile.ZipFile , gzip.GzipFile , bz2.BZ2File , zstandard.ZstdCompressor , lzma.LZMAFile or tarfile.TarFile , respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1} . .. versionadded:: 1.5.0 Added support for .tar files. protocol (int) \u2014 Int which indicates which protocol should be used by the pickler,default HIGHEST_PROTOCOL (see [1]_ paragraph 12.1.2). The possible values are 0, 1, 2, 3, 4, 5. A negative value for the protocol parameter is equivalent to setting its value to HIGHEST_PROTOCOL. .. [1] https://docs.python.org/3/library/pickle.html. storage_options (dict, optional) \u2014 Extra options that make sense for a particular storage connection, e.g.host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to fsspec.open . Please see fsspec and urllib for more details, and for more examples on storage options refer here <https://pandas.pydata.org/docs/user_guide/io.html? highlight=storage_options#reading-writing-remote-files> _. See Also read_pickle : Load pickled pandas object (or any object) from file.DataFrame.to_hdf : Write DataFrame to an HDF5 file. DataFrame.to_sql : Write DataFrame to a SQL database. DataFrame.to_parquet : Write a DataFrame to the binary parquet format. Examples >>> original_df = pd . DataFrame ({ \"foo\" : range ( 5 ), \"bar\" : range ( 5 , 10 )}) # doctest: +SKIP >>> original_df # doctest: +SKIP foo bar 0 0 5 1 1 6 2 2 7 3 3 8 4 4 9 >>> original_df . to_pickle ( \"./dummy.pkl\" ) # doctest: +SKIP >>> unpickled_df = pd . read_pickle ( \"./dummy.pkl\" ) # doctest: +SKIP >>> unpickled_df # doctest: +SKIP foo bar 0 0 5 1 1 6 2 2 7 3 3 8 4 4 9 method to_clipboard ( excel=True , sep=None , **kwargs ) </> Copy object to the system clipboard. Write a text representation of object to the system clipboard. This can be pasted into Excel, for example. Parameters excel (bool, default True) \u2014 Produce output in a csv format for easy pasting into excel. True, use the provided separator for csv pasting. False, write a string representation of the object to the clipboard. sep (str, default ``'\\t'``) \u2014 Field delimiter. **kwargs \u2014 These parameters will be passed to DataFrame.to_csv. See Also DataFrame.to_csv : Write a DataFrame to a comma-separated values (csv) file. read_clipboard : Read text from clipboard and pass to read_csv. Notes Requirements for your platform. Linux : xclip , or xsel (with PyQt4 modules) Windows : none macOS : none This method uses the processes developed for the package pyperclip . A solution to render any output string format is given in the examples. Examples Copy the contents of a DataFrame to the clipboard. >>> df = pd . DataFrame ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]], columns = [ 'A' , 'B' , 'C' ]) >>> df . to_clipboard ( sep = ',' ) # doctest: +SKIP ... # Wrote the following to the system clipboard: ... # ,A,B,C ... # 0,1,2,3 ... # 1,4,5,6 We can omit the index by passing the keyword index and setting it to false. >>> df . to_clipboard ( sep = ',' , index = False ) # doctest: +SKIP ... # Wrote the following to the system clipboard: ... # A,B,C ... # 1,2,3 ... # 4,5,6 Using the original pyperclip package for any string output format. .. code-block:: python import pyperclip html = df.style.to_html() pyperclip.copy(html) method to_xarray ( ) </> Return an xarray object from the pandas object. Returns (xarray.DataArray or xarray.Dataset) Data in the pandas structure converted to Dataset if the object isa DataFrame, or a DataArray if the object is a Series. See Also DataFrame.to_hdf : Write DataFrame to an HDF5 file.DataFrame.to_parquet : Write a DataFrame to the binary parquet format. Notes See the xarray docs <https://xarray.pydata.org/en/stable/> __ Examples >>> df = pd . DataFrame ([( 'falcon' , 'bird' , 389.0 , 2 ), ... ( 'parrot' , 'bird' , 24.0 , 2 ), ... ( 'lion' , 'mammal' , 80.5 , 4 ), ... ( 'monkey' , 'mammal' , np . nan , 4 )], ... columns = [ 'name' , 'class' , 'max_speed' , ... 'num_legs' ]) >>> df name class max_speed num_legs 0 falcon bird 389.0 2 1 parrot bird 24.0 2 2 lion mammal 80.5 4 3 monkey mammal NaN 4 >>> df . to_xarray () # doctest: +SKIP < xarray . Dataset > Dimensions : ( index : 4 ) Coordinates : * index ( index ) int64 32 B 0 1 2 3 Data variables : name ( index ) object 32 B 'falcon' 'parrot' 'lion' 'monkey' class ( index ) object 32 B 'bird' 'bird' 'mammal' 'mammal' max_speed ( index ) float64 32 B 389.0 24.0 80.5 nan num_legs ( index ) int64 32 B 2 2 4 4 >>> df [ 'max_speed' ] . to_xarray () # doctest: +SKIP < xarray . DataArray 'max_speed' ( index : 4 ) > array ([ 389. , 24. , 80.5 , nan ]) Coordinates : * index ( index ) int64 0 1 2 3 >>> dates = pd . to_datetime ([ '2018-01-01' , '2018-01-01' , ... '2018-01-02' , '2018-01-02' ]) >>> df_multiindex = pd . DataFrame ({ 'date' : dates , ... 'animal' : [ 'falcon' , 'parrot' , ... 'falcon' , 'parrot' ], ... 'speed' : [ 350 , 18 , 361 , 15 ]}) >>> df_multiindex = df_multiindex . set_index ([ 'date' , 'animal' ]) >>> df_multiindex speed date animal 2018 - 01 - 01 falcon 350 parrot 18 2018 - 01 - 02 falcon 361 parrot 15 >>> df_multiindex . to_xarray () # doctest: +SKIP < xarray . Dataset > Dimensions : ( date : 2 , animal : 2 ) Coordinates : * date ( date ) datetime64 [ ns ] 2018 - 01 - 01 2018 - 01 - 02 * animal ( animal ) object 'falcon' 'parrot' Data variables : speed ( date , animal ) int64 350 18 361 15 method to_latex ( buf=None , columns=None , header=True , index=True , na_rep='NaN' , formatters=None , float_format=None , sparsify=None , index_names=True , bold_rows=False , column_format=None , longtable=None , escape=None , encoding=None , decimal='.' , multicolumn=None , multicolumn_format=None , multirow=None , caption=None , label=None , position=None ) </> Render object to a LaTeX tabular, longtable, or nested table. Requires \\usepackage{{booktabs}} . The output can be copy/pasted into a main LaTeX document or read from an external file with \\input{{table.tex}} . .. versionchanged:: 2.0.0 Refactored to use the Styler implementation via jinja2 templating. Parameters buf (str, Path or StringIO-like, optional, default None) \u2014 Buffer to write to. If None, the output is returned as a string. columns (list of label, optional) \u2014 The subset of columns to write. Writes all columns by default. header (bool or list of str, default True) \u2014 Write out the column names. If a list of strings is given,it is assumed to be aliases for the column names. index (bool, default True) \u2014 Write row names (index). na_rep (str, default 'NaN') \u2014 Missing data representation. formatters (list of functions or dict of {{str: function}}, optional) \u2014 Formatter functions to apply to columns' elements by position orname. The result of each function must be a unicode string. List must be of length equal to the number of columns. float_format (one-parameter function or str, optional, default None) \u2014 Formatter for floating point numbers. For example float_format=\"%.2f\" and float_format=\"{{:0.2f}}\".format will both result in 0.1234 being formatted as 0.12. sparsify (bool, optional) \u2014 Set to False for a DataFrame with a hierarchical index to printevery multiindex key at each row. By default, the value will be read from the config module. index_names (bool, default True) \u2014 Prints the names of the indexes. bold_rows (bool, default False) \u2014 Make the row labels bold in the output. column_format (str, optional) \u2014 The columns format as specified in LaTeX table format<https://en.wikibooks.org/wiki/LaTeX/Tables> __ e.g. 'rcl' for 3 columns. By default, 'l' will be used for all columns except columns of numbers, which default to 'r'. longtable (bool, optional) \u2014 Use a longtable environment instead of tabular. Requiresadding a \\usepackage{{longtable}} to your LaTeX preamble. By default, the value will be read from the pandas config module, and set to True if the option styler.latex.environment is \"longtable\" . .. versionchanged:: 2.0.0 The pandas option affecting this argument has changed. escape (bool, optional) \u2014 By default, the value will be read from the pandas configmodule and set to True if the option styler.format.escape is \"latex\" . When set to False prevents from escaping latex special characters in column names. .. versionchanged:: 2.0.0 The pandas option affecting this argument has changed, as has the default value to False . encoding (str, optional) \u2014 A string representing the encoding to use in the output file,defaults to 'utf-8'. decimal (str, default '.') \u2014 Character recognized as decimal separator, e.g. ',' in Europe. multicolumn (bool, default True) \u2014 Use \\multicolumn to enhance MultiIndex columns.The default will be read from the config module, and is set as the option styler.sparse.columns . .. versionchanged:: 2.0.0 The pandas option affecting this argument has changed. multicolumn_format (str, default 'r') \u2014 The alignment for multicolumns, similar to column_format The default will be read from the config module, and is set as the option styler.latex.multicol_align . .. versionchanged:: 2.0.0 The pandas option affecting this argument has changed, as has the default value to \"r\". multirow (bool, default True) \u2014 Use \\multirow to enhance MultiIndex rows. Requires adding a\\usepackage{{multirow}} to your LaTeX preamble. Will print centered labels (instead of top-aligned) across the contained rows, separating groups via clines. The default will be read from the pandas config module, and is set as the option styler.sparse.index . .. versionchanged:: 2.0.0 The pandas option affecting this argument has changed, as has the default value to True . caption (str or tuple, optional) \u2014 Tuple (full_caption, short_caption),which results in \\caption[short_caption]{{full_caption}} ; if a single string is passed, no short caption will be set. label (str, optional) \u2014 The LaTeX label to be placed inside \\label{{}} in the output.This is used with \\ref{{}} in the main .tex file. position (str, optional) \u2014 The LaTeX positional argument for tables, to be placed after \\begin{{}} in the output. Returns (str or None) If buf is None, returns the result as a string. Otherwise returns None. See Also io.formats.style.Styler.to_latex : Render a DataFrame to LaTeX with conditional formatting. DataFrame.to_string : Render a DataFrame to a console-friendly tabular output. DataFrame.to_html : Render a DataFrame as an HTML table. Notes As of v2.0.0 this method has changed to use the Styler implementation as part of :meth: .Styler.to_latex via jinja2 templating. This means that jinja2 is a requirement, and needs to be installed, for this method to function. It is advised that users switch to using Styler, since that implementation is more frequently updated and contains much more flexibility with the output. Examples Convert a general DataFrame to LaTeX with formatting: >>> df = pd . DataFrame ( dict ( name = [ 'Raphael' , 'Donatello' ], ... age = [ 26 , 45 ], ... height = [ 181.23 , 177.65 ])) >>> print ( df . to_latex ( index = False , ... formatters = { \"name\" : str . upper }, ... float_format = \" {:.1f} \" . format , ... )) # doctest: +SKIP \\ begin { tabular }{ lrr } \\ toprule name & age & height \\\\ \\ midrule RAPHAEL & 26 & 181.2 \\\\ DONATELLO & 45 & 177.7 \\\\ \\ bottomrule \\ end { tabular } method to_csv ( path_or_buf=None , sep=',' , na_rep='' , float_format=None , columns=None , header=True , index=True , index_label=None , mode='w' , encoding=None , compression='infer' , quoting=None , quotechar='\"' , lineterminator=None , chunksize=None , date_format=None , doublequote=True , escapechar=None , decimal='.' , errors='strict' , storage_options=None ) </> Write object to a comma-separated values (csv) file. Parameters path_or_buf (str, path object, file-like object, or None, default None) \u2014 String, path object (implementing os.PathLike[str]), or file-likeobject implementing a write() function. If None, the result is returned as a string. If a non-binary file object is passed, it should be opened with newline='' , disabling universal newlines. If a binary file object is passed, mode might need to contain a 'b' . sep (str, default ',') \u2014 String of length 1. Field delimiter for the output file. na_rep (str, default '') \u2014 Missing data representation. float_format (str, Callable, default None) \u2014 Format string for floating point numbers. If a Callable is given, it takesprecedence over other numeric formatting parameters, like decimal. columns (sequence, optional) \u2014 Columns to write. header (bool or list of str, default True) \u2014 Write out the column names. If a list of strings is given it isassumed to be aliases for the column names. index (bool, default True) \u2014 Write row names (index). index_label (str or sequence, or False, default None) \u2014 Column label for index column(s) if desired. If None is given, and header and index are True, then the index names are used. A sequence should be given if the object uses MultiIndex. If False do not print fields for index names. Use index_label=False for easier importing in R. mode ({'w', 'x', 'a'}, default 'w') \u2014 Forwarded to either open(mode=) or fsspec.open(mode=) to controlthe file opening. Typical values include: 'w', truncate the file first. 'x', exclusive creation, failing if the file already exists. 'a', append to the end of file if it exists. encoding (str, optional) \u2014 A string representing the encoding to use in the output file,defaults to 'utf-8'. encoding is not supported if path_or_buf is a non-binary file object. compression (str or dict, default 'infer') \u2014 For on-the-fly compression of the output data. If 'infer' and 'path_or_buf' ispath-like, then detect compression from the following extensions: '.gz', '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2' (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of { 'zip' , 'gzip' , 'bz2' , 'zstd' , 'xz' , 'tar' } and other key-value pairs are forwarded to zipfile.ZipFile , gzip.GzipFile , bz2.BZ2File , zstandard.ZstdCompressor , lzma.LZMAFile or tarfile.TarFile , respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1} . .. versionadded:: 1.5.0 Added support for .tar files. May be a dict with key 'method' as compression mode and other entries as additional compression options if compression mode is 'zip'. Passing compression options as keys in dict is supported for compression modes 'gzip', 'bz2', 'zstd', and 'zip'. quoting (optional constant from csv module) \u2014 Defaults to csv.QUOTE_MINIMAL. If you have set a float_format then floats are converted to strings and thus csv.QUOTE_NONNUMERIC will treat them as non-numeric. quotechar (str, default '\\\"') \u2014 String of length 1. Character used to quote fields. lineterminator (str, optional) \u2014 The newline character or character sequence to use in the outputfile. Defaults to os.linesep , which depends on the OS in which this method is called ('\\n' for linux, '\\r\\n' for Windows, i.e.). .. versionchanged:: 1.5.0 Previously was line_terminator, changed for consistency with read_csv and the standard library 'csv' module. chunksize (int or None) \u2014 Rows to write at a time. date_format (str, default None) \u2014 Format string for datetime objects. doublequote (bool, default True) \u2014 Control quoting of quotechar inside a field. escapechar (str, default None) \u2014 String of length 1. Character used to escape sep and quotechar when appropriate. decimal (str, default '.') \u2014 Character recognized as decimal separator. E.g. use ',' forEuropean data. errors (str, default 'strict') \u2014 Specifies how encoding and decoding errors are to be handled.See the errors argument for :func: open for a full list of options. storage_options (dict, optional) \u2014 Extra options that make sense for a particular storage connection, e.g.host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to fsspec.open . Please see fsspec and urllib for more details, and for more examples on storage options refer here <https://pandas.pydata.org/docs/user_guide/io.html? highlight=storage_options#reading-writing-remote-files> _. Returns (None or str) If path_or_buf is None, returns the resulting csv format as astring. Otherwise returns None. See Also read_csv : Load a CSV file into a DataFrame.to_excel : Write DataFrame to an Excel file. Examples Create 'out.csv' containing 'df' without indices >>> df = pd . DataFrame ({ 'name' : [ 'Raphael' , 'Donatello' ], ... 'mask' : [ 'red' , 'purple' ], ... 'weapon' : [ 'sai' , 'bo staff' ]}) >>> df . to_csv ( 'out.csv' , index = False ) # doctest: +SKIP Create 'out.zip' containing 'out.csv' >>> df . to_csv ( index = False ) 'name,mask,weapon \\n Raphael,red,sai \\n Donatello,purple,bo staff \\n ' >>> compression_opts = dict ( method = 'zip' , ... archive_name = 'out.csv' ) # doctest: +SKIP >>> df . to_csv ( 'out.zip' , index = False , ... compression = compression_opts ) # doctest: +SKIP To write a csv file to a new folder or nested folder you will first need to create it using either Pathlib or os: >>> from pathlib import Path # doctest: +SKIP >>> filepath = Path ( 'folder/subfolder/out.csv' ) # doctest: +SKIP >>> filepath . parent . mkdir ( parents = True , exist_ok = True ) # doctest: +SKIP >>> df . to_csv ( filepath ) # doctest: +SKIP >>> import os # doctest: +SKIP >>> os . makedirs ( 'folder/subfolder' , exist_ok = True ) # doctest: +SKIP >>> df . to_csv ( 'folder/subfolder/out.csv' ) # doctest: +SKIP method take ( indices , axis=0 , **kwargs ) </> Return the elements in the given positional indices along an axis. This means that we are not indexing according to actual values in the index attribute of the object. We are indexing according to the actual position of the element in the object. Parameters indices (array-like) \u2014 An array of ints indicating which positions to take. axis ({0 or 'index', 1 or 'columns', None}, default 0) \u2014 The axis on which to select elements. 0 means that we areselecting rows, 1 means that we are selecting columns. For Series this parameter is unused and defaults to 0. **kwargs \u2014 For compatibility with :meth: numpy.take . Has no effect on theoutput. Returns (same type as caller) An array-like containing the elements taken from the object. See Also DataFrame.loc : Select a subset of a DataFrame by labels.DataFrame.iloc : Select a subset of a DataFrame by positions. numpy.take : Take elements from an array along an axis. Examples >>> df = pd . DataFrame ([( 'falcon' , 'bird' , 389.0 ), ... ( 'parrot' , 'bird' , 24.0 ), ... ( 'lion' , 'mammal' , 80.5 ), ... ( 'monkey' , 'mammal' , np . nan )], ... columns = [ 'name' , 'class' , 'max_speed' ], ... index = [ 0 , 2 , 3 , 1 ]) >>> df name class max_speed 0 falcon bird 389.0 2 parrot bird 24.0 3 lion mammal 80.5 1 monkey mammal NaN Take elements at positions 0 and 3 along the axis 0 (default). Note how the actual indices selected (0 and 1) do not correspond to our selected indices 0 and 3. That's because we are selecting the 0th and 3rd rows, not rows whose indices equal 0 and 3. >>> df . take ([ 0 , 3 ]) name class max_speed 0 falcon bird 389.0 1 monkey mammal NaN Take elements at indices 1 and 2 along the axis 1 (column selection). >>> df . take ([ 1 , 2 ], axis = 1 ) class max_speed 0 bird 389.0 2 bird 24.0 3 mammal 80.5 1 mammal NaN We may take elements using negative integers for positive indices, starting from the end of the object, just like with Python lists. >>> df . take ([ - 1 , - 2 ]) name class max_speed 1 monkey mammal NaN 3 lion mammal 80.5 method xs ( key , axis=0 , level=None , drop_level=True ) </> Return cross-section from the Series/DataFrame. This method takes a key argument to select data at a particular level of a MultiIndex. Parameters key (label or tuple of label) \u2014 Label contained in the index, or partially in a MultiIndex. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Axis to retrieve cross-section on. level (object, defaults to first n levels (n=1 or len(key))) \u2014 In case of a key partially contained in a MultiIndex, indicatewhich levels are used. Levels can be referred by label or position. drop_level (bool, default True) \u2014 If False, returns object with same levels as self. Returns (Series or DataFrame) Cross-section from the original Series or DataFramecorresponding to the selected index levels. See Also DataFrame.loc : Access a group of rows and columns by label(s) or a boolean array. DataFrame.iloc : Purely integer-location based indexing for selection by position. Notes xs can not be used to set values. MultiIndex Slicers is a generic way to get/set values on any level or levels. It is a superset of xs functionality, see :ref: MultiIndex Slicers <advanced.mi_slicers> . Examples >>> d = { 'num_legs' : [ 4 , 4 , 2 , 2 ], ... 'num_wings' : [ 0 , 0 , 2 , 2 ], ... 'class' : [ 'mammal' , 'mammal' , 'mammal' , 'bird' ], ... 'animal' : [ 'cat' , 'dog' , 'bat' , 'penguin' ], ... 'locomotion' : [ 'walks' , 'walks' , 'flies' , 'walks' ]} >>> df = pd . DataFrame ( data = d ) >>> df = df . set_index ([ 'class' , 'animal' , 'locomotion' ]) >>> df num_legs num_wings class animal locomotion mammal cat walks 4 0 dog walks 4 0 bat flies 2 2 bird penguin walks 2 2 Get values at specified index >>> df . xs ( 'mammal' ) num_legs num_wings animal locomotion cat walks 4 0 dog walks 4 0 bat flies 2 2 Get values at several indexes >>> df . xs (( 'mammal' , 'dog' , 'walks' )) num_legs 4 num_wings 0 Name : ( mammal , dog , walks ), dtype : int64 Get values at specified index and level >>> df . xs ( 'cat' , level = 1 ) num_legs num_wings class locomotion mammal walks 4 0 Get values at several indexes and levels >>> df . xs (( 'bird' , 'walks' ), ... level = [ 0 , 'locomotion' ]) num_legs num_wings animal penguin 2 2 Get values at specified column and axis >>> df . xs ( 'num_wings' , axis = 1 ) class animal locomotion mammal cat walks 0 dog walks 0 bat flies 2 bird penguin walks 2 Name : num_wings , dtype : int64 method __delitem__ ( key ) </> Delete item method get ( key , default=None ) </> Get item from object for given key (ex: DataFrame column). Returns default value if not found. Examples >>> df = pd . DataFrame ( ... [ ... [ 24.3 , 75.7 , \"high\" ], ... [ 31 , 87.8 , \"high\" ], ... [ 22 , 71.6 , \"medium\" ], ... [ 35 , 95 , \"medium\" ], ... ], ... columns = [ \"temp_celsius\" , \"temp_fahrenheit\" , \"windspeed\" ], ... index = pd . date_range ( start = \"2014-02-12\" , end = \"2014-02-15\" , freq = \"D\" ), ... ) >>> df temp_celsius temp_fahrenheit windspeed 2014 - 02 - 12 24.3 75.7 high 2014 - 02 - 13 31.0 87.8 high 2014 - 02 - 14 22.0 71.6 medium 2014 - 02 - 15 35.0 95.0 medium >>> df . get ([ \"temp_celsius\" , \"windspeed\" ]) temp_celsius windspeed 2014 - 02 - 12 24.3 high 2014 - 02 - 13 31.0 high 2014 - 02 - 14 22.0 medium 2014 - 02 - 15 35.0 medium >>> ser = df [ 'windspeed' ] >>> ser . get ( '2014-02-13' ) 'high' If the key isn't found, the default value will be used. >>> df . get ([ \"temp_celsius\" , \"temp_kelvin\" ], default = \"default_value\" ) 'default_value' >>> ser . get ( '2014-02-10' , '[unknown]' ) '[unknown]' method reindex_like ( other , method=None , copy=None , limit=None , tolerance=None ) </> Return an object with matching indices as other object. Conform the object to the same index on all axes. Optional filling logic, placing NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False. Parameters other (Object of the same data type) \u2014 Its row and column indices are used to define the new indicesof this object. method ({None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'}) \u2014 Method to use for filling holes in reindexed DataFrame.Please note: this is only applicable to DataFrames/Series with a monotonically increasing/decreasing index. None (default): don't fill gaps pad / ffill: propagate last valid observation forward to next valid backfill / bfill: use next valid observation to fill gap nearest: use nearest valid observations to fill gap. copy (bool, default True) \u2014 Return a new object, even if the passed indexes are the same. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` limit (int, default None) \u2014 Maximum number of consecutive labels to fill for inexact matches. tolerance (optional) \u2014 Maximum distance between original and new labels for inexactmatches. The values of the index at the matching locations must satisfy the equation abs(index[indexer] - target) <= tolerance . Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index's type. Returns (Series or DataFrame) Same type as caller, but with changed indices on each axis. See Also DataFrame.set_index : Set row labels.DataFrame.reset_index : Remove row labels or move them to new columns. DataFrame.reindex : Change to new indices or expand indices. Notes Same as calling .reindex(index=other.index, columns=other.columns,...) . Examples >>> df1 = pd . DataFrame ([[ 24.3 , 75.7 , 'high' ], ... [ 31 , 87.8 , 'high' ], ... [ 22 , 71.6 , 'medium' ], ... [ 35 , 95 , 'medium' ]], ... columns = [ 'temp_celsius' , 'temp_fahrenheit' , ... 'windspeed' ], ... index = pd . date_range ( start = '2014-02-12' , ... end = '2014-02-15' , freq = 'D' )) >>> df1 temp_celsius temp_fahrenheit windspeed 2014 - 02 - 12 24.3 75.7 high 2014 - 02 - 13 31.0 87.8 high 2014 - 02 - 14 22.0 71.6 medium 2014 - 02 - 15 35.0 95.0 medium >>> df2 = pd . DataFrame ([[ 28 , 'low' ], ... [ 30 , 'low' ], ... [ 35.1 , 'medium' ]], ... columns = [ 'temp_celsius' , 'windspeed' ], ... index = pd . DatetimeIndex ([ '2014-02-12' , '2014-02-13' , ... '2014-02-15' ])) >>> df2 temp_celsius windspeed 2014 - 02 - 12 28.0 low 2014 - 02 - 13 30.0 low 2014 - 02 - 15 35.1 medium >>> df2 . reindex_like ( df1 ) temp_celsius temp_fahrenheit windspeed 2014 - 02 - 12 28.0 NaN low 2014 - 02 - 13 30.0 NaN low 2014 - 02 - 14 NaN NaN NaN 2014 - 02 - 15 35.1 NaN medium method add_prefix ( prefix , axis=None ) </> Prefix labels with string prefix . For Series, the row labels are prefixed. For DataFrame, the column labels are prefixed. Parameters prefix (str) \u2014 The string to add before each label. axis ({0 or 'index', 1 or 'columns', None}, default None) \u2014 Axis to add prefix on .. versionadded:: 2.0.0 Returns (Series or DataFrame) New Series or DataFrame with updated labels. See Also Series.add_suffix: Suffix row labels with string suffix .DataFrame.add_suffix: Suffix column labels with string suffix . Examples >>> s = pd . Series ([ 1 , 2 , 3 , 4 ]) >>> s 0 1 1 2 2 3 3 4 dtype : int64 >>> s . add_prefix ( 'item_' ) item_0 1 item_1 2 item_2 3 item_3 4 dtype : int64 >>> df = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 , 4 ], 'B' : [ 3 , 4 , 5 , 6 ]}) >>> df A B 0 1 3 1 2 4 2 3 5 3 4 6 >>> df . add_prefix ( 'col_' ) col_A col_B 0 1 3 1 2 4 2 3 5 3 4 6 method add_suffix ( suffix , axis=None ) </> Suffix labels with string suffix . For Series, the row labels are suffixed. For DataFrame, the column labels are suffixed. Parameters suffix (str) \u2014 The string to add after each label. axis ({0 or 'index', 1 or 'columns', None}, default None) \u2014 Axis to add suffix on .. versionadded:: 2.0.0 Returns (Series or DataFrame) New Series or DataFrame with updated labels. See Also Series.add_prefix: Prefix row labels with string prefix .DataFrame.add_prefix: Prefix column labels with string prefix . Examples >>> s = pd . Series ([ 1 , 2 , 3 , 4 ]) >>> s 0 1 1 2 2 3 3 4 dtype : int64 >>> s . add_suffix ( '_item' ) 0 _item 1 1 _item 2 2 _item 3 3 _item 4 dtype : int64 >>> df = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 , 4 ], 'B' : [ 3 , 4 , 5 , 6 ]}) >>> df A B 0 1 3 1 2 4 2 3 5 3 4 6 >>> df . add_suffix ( '_col' ) A_col B_col 0 1 3 1 2 4 2 3 5 3 4 6 method filter ( items=None , like=None , regex=None , axis=None ) </> Subset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index. Parameters items (list-like) \u2014 Keep labels from axis which are in items. like (str) \u2014 Keep labels from axis for which \"like in label == True\". regex (str (regular expression)) \u2014 Keep labels from axis for which re.search(regex, label) == True. axis ({0 or 'index', 1 or 'columns', None}, default None) \u2014 The axis to filter on, expressed either as an index (int)or axis name (str). By default this is the info axis, 'columns' for DataFrame. For Series this parameter is unused and defaults to None . See Also DataFrame.loc : Access a group of rows and columns by label(s) or a boolean array. Notes The items , like , and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with [] . Examples >>> df = pd . DataFrame ( np . array (([ 1 , 2 , 3 ], [ 4 , 5 , 6 ])), ... index = [ 'mouse' , 'rabbit' ], ... columns = [ 'one' , 'two' , 'three' ]) >>> df one two three mouse 1 2 3 rabbit 4 5 6 >>> # select columns by name >>> df . filter ( items = [ 'one' , 'three' ]) one three mouse 1 3 rabbit 4 6 >>> # select columns by regular expression >>> df . filter ( regex = 'e$' , axis = 1 ) one three mouse 1 3 rabbit 4 6 >>> # select rows containing 'bbi' >>> df . filter ( like = 'bbi' , axis = 0 ) one two three rabbit 4 5 6 method head ( n=5 ) </> Return the first n rows. This function returns the first n rows for the object based on position. It is useful for quickly testing if your object has the right type of data in it. For negative values of n , this function returns all rows except the last |n| rows, equivalent to df[:n] . If n is larger than the number of rows, this function returns all rows. Parameters n (int, default 5) \u2014 Number of rows to select. Returns (same type as caller) The first n rows of the caller object. See Also DataFrame.tail: Returns the last n rows. Examples >>> df = pd . DataFrame ({ 'animal' : [ 'alligator' , 'bee' , 'falcon' , 'lion' , ... 'monkey' , 'parrot' , 'shark' , 'whale' , 'zebra' ]}) >>> df animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey 5 parrot 6 shark 7 whale 8 zebra Viewing the first 5 lines >>> df . head () animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey Viewing the first n lines (three in this case) >>> df . head ( 3 ) animal 0 alligator 1 bee 2 falcon For negative values of n >>> df . head ( - 3 ) animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey 5 parrot method tail ( n=5 ) </> Return the last n rows. This function returns last n rows from the object based on position. It is useful for quickly verifying data, for example, after sorting or appending rows. For negative values of n , this function returns all rows except the first |n| rows, equivalent to df[|n|:] . If n is larger than the number of rows, this function returns all rows. Parameters n (int, default 5) \u2014 Number of rows to select. Returns (type of caller) The last n rows of the caller object. See Also DataFrame.head : The first n rows of the caller object. Examples >>> df = pd . DataFrame ({ 'animal' : [ 'alligator' , 'bee' , 'falcon' , 'lion' , ... 'monkey' , 'parrot' , 'shark' , 'whale' , 'zebra' ]}) >>> df animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey 5 parrot 6 shark 7 whale 8 zebra Viewing the last 5 lines >>> df . tail () animal 4 monkey 5 parrot 6 shark 7 whale 8 zebra Viewing the last n lines (three in this case) >>> df . tail ( 3 ) animal 6 shark 7 whale 8 zebra For negative values of n >>> df . tail ( - 3 ) animal 3 lion 4 monkey 5 parrot 6 shark 7 whale 8 zebra method sample ( n=None , frac=None , replace=False , weights=None , random_state=None , axis=None , ignore_index=False ) </> Return a random sample of items from an axis of object. You can use random_state for reproducibility. Parameters n (int, optional) \u2014 Number of items from axis to return. Cannot be used with frac .Default = 1 if frac = None. frac (float, optional) \u2014 Fraction of axis items to return. Cannot be used with n . replace (bool, default False) \u2014 Allow or disallow sampling of the same row more than once. weights (str or ndarray-like, optional) \u2014 Default 'None' results in equal probability weighting.If passed a Series, will align with target object on index. Index values in weights not found in sampled object will be ignored and index values in sampled object not in weights will be assigned weights of zero. If called on a DataFrame, will accept the name of a column when axis = 0. Unless weights are a Series, weights must be same length as axis being sampled. If weights do not sum to 1, they will be normalized to sum to 1. Missing values in the weights column will be treated as zero. Infinite values not allowed. random_state (int, array-like, BitGenerator, np.random.RandomState, np.random.Generator, optional) \u2014 If int, array-like, or BitGenerator, seed for random number generator.If np.random.RandomState or np.random.Generator, use as given. .. versionchanged:: 1.4.0 np.random.Generator objects now accepted axis ({0 or 'index', 1 or 'columns', None}, default None) \u2014 Axis to sample. Accepts axis number or name. Default is stat axisfor given data type. For Series this parameter is unused and defaults to None . ignore_index (bool, default False) \u2014 If True, the resulting index will be labeled 0, 1, \u2026, n - 1. .. versionadded:: 1.3.0 Returns (Series or DataFrame) A new object of same type as caller containing n items randomlysampled from the caller object. See Also DataFrameGroupBy.sample: Generates random samples from each group of a DataFrame object. SeriesGroupBy.sample: Generates random samples from each group of a Series object. numpy.random.choice: Generates a random sample from a given 1-D numpy array. Notes If frac > 1, replacement should be set to True . Examples >>> df = pd . DataFrame ({ 'num_legs' : [ 2 , 4 , 8 , 0 ], ... 'num_wings' : [ 2 , 0 , 0 , 0 ], ... 'num_specimen_seen' : [ 10 , 2 , 1 , 8 ]}, ... index = [ 'falcon' , 'dog' , 'spider' , 'fish' ]) >>> df num_legs num_wings num_specimen_seen falcon 2 2 10 dog 4 0 2 spider 8 0 1 fish 0 0 8 Extract 3 random elements from the Series df['num_legs'] : Note that we use random_state to ensure the reproducibility of the examples. >>> df [ 'num_legs' ] . sample ( n = 3 , random_state = 1 ) fish 0 spider 8 falcon 2 Name : num_legs , dtype : int64 A random 50% sample of the DataFrame with replacement: >>> df . sample ( frac = 0.5 , replace = True , random_state = 1 ) num_legs num_wings num_specimen_seen dog 4 0 2 fish 0 0 8 An upsample sample of the DataFrame with replacement: Note that replace parameter has to be True for frac parameter > 1. >>> df . sample ( frac = 2 , replace = True , random_state = 1 ) num_legs num_wings num_specimen_seen dog 4 0 2 fish 0 0 8 falcon 2 2 10 falcon 2 2 10 fish 0 0 8 dog 4 0 2 fish 0 0 8 dog 4 0 2 Using a DataFrame column as weights. Rows with larger value in the num_specimen_seen column are more likely to be sampled. >>> df . sample ( n = 2 , weights = 'num_specimen_seen' , random_state = 1 ) num_legs num_wings num_specimen_seen falcon 2 2 10 fish 0 0 8 method pipe ( func , *args , **kwargs ) </> Apply chainable functions that expect Series or DataFrames. Parameters func (function) \u2014 Function to apply to the Series/DataFrame. args , and kwargs are passed into func . Alternatively a (callable, data_keyword) tuple where data_keyword is a string indicating the keyword of callable that expects the Series/DataFrame. *args (iterable, optional) \u2014 Positional arguments passed into func . **kwargs (mapping, optional) \u2014 A dictionary of keyword arguments passed into func . See Also DataFrame.apply : Apply a function along input axis of DataFrame.DataFrame.map : Apply a function elementwise on a whole DataFrame. Series.map : Apply a mapping correspondence on a :class: ~pandas.Series . Notes Use .pipe when chaining together functions that expect Series, DataFrames or GroupBy objects. Examples Constructing a income DataFrame from a dictionary. >>> data = [[ 8000 , 1000 ], [ 9500 , np . nan ], [ 5000 , 2000 ]] >>> df = pd . DataFrame ( data , columns = [ 'Salary' , 'Others' ]) >>> df Salary Others 0 8000 1000.0 1 9500 NaN 2 5000 2000.0 Functions that perform tax reductions on an income DataFrame. >>> def subtract_federal_tax ( df ): ... return df * 0.9 >>> def subtract_state_tax ( df , rate ): ... return df * ( 1 - rate ) >>> def subtract_national_insurance ( df , rate , rate_increase ): ... new_rate = rate + rate_increase ... return df * ( 1 - new_rate ) Instead of writing >>> subtract_national_insurance ( ... subtract_state_tax ( subtract_federal_tax ( df ), rate = 0.12 ), ... rate = 0.05 , ... rate_increase = 0.02 ) # doctest: +SKIP You can write >>> ( ... df . pipe ( subtract_federal_tax ) ... . pipe ( subtract_state_tax , rate = 0.12 ) ... . pipe ( subtract_national_insurance , rate = 0.05 , rate_increase = 0.02 ) ... ) Salary Others 0 5892.48 736.56 1 6997.32 NaN 2 3682.80 1473.12 If you have a function that takes the data as (say) the second argument, pass a tuple indicating which keyword expects the data. For example, suppose national_insurance takes its data as df in the second argument: >>> def subtract_national_insurance ( rate , df , rate_increase ): ... new_rate = rate + rate_increase ... return df * ( 1 - new_rate ) >>> ( ... df . pipe ( subtract_federal_tax ) ... . pipe ( subtract_state_tax , rate = 0.12 ) ... . pipe ( ... ( subtract_national_insurance , 'df' ), ... rate = 0.05 , ... rate_increase = 0.02 ... ) ... ) Salary Others 0 5892.48 736.56 1 6997.32 NaN 2 3682.80 1473.12 method __finalize__ ( other , method=None , **kwargs ) </> Propagate metadata from other to self. Parameters other (the object from which to get the attributes that we are going) \u2014 to propagate method (str, optional) \u2014 A passed method name providing context on where __finalize__ was called. .. warning:: The value passed as method are not currently considered stable across pandas releases. method __getattr__ ( name ) </> After regular attribute access, try looking up the nameThis allows simpler access to columns for interactive use. method __setattr__ ( name , value ) </> After regular attribute access, try setting the nameThis allows simpler access to columns for interactive use. method astype ( dtype , copy=None , errors='raise' ) </> Cast a pandas object to a specified dtype dtype . Parameters dtype (str, data type, Series or Mapping of column name -> data type) \u2014 Use a str, numpy.dtype, pandas.ExtensionDtype or Python type tocast entire pandas object to the same type. Alternatively, use a mapping, e.g. {col: dtype, ...}, where col is a column label and dtype is a numpy.dtype or Python type to cast one or more of the DataFrame's columns to column-specific types. copy (bool, default True) \u2014 Return a copy when copy=True (be very careful setting copy=False as changes to values then may propagate to other pandas objects). .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` errors ({'raise', 'ignore'}, default 'raise') \u2014 Control raising of exceptions on invalid data for provided dtype. raise : allow exceptions to be raised ignore : suppress exceptions. On error return original object. See Also to_datetime : Convert argument to datetime.to_timedelta : Convert argument to timedelta. to_numeric : Convert argument to a numeric type. numpy.ndarray.astype : Cast a numpy array to a specified type. Notes .. versionchanged:: 2.0.0 Using ``astype`` to convert from timezone-naive dtype to timezone-aware dtype will raise an exception. Use :meth:`Series.dt.tz_localize` instead. Examples Create a DataFrame: >>> d = { 'col1' : [ 1 , 2 ], 'col2' : [ 3 , 4 ]} >>> df = pd . DataFrame ( data = d ) >>> df . dtypes col1 int64 col2 int64 dtype : object Cast all columns to int32: >>> df . astype ( 'int32' ) . dtypes col1 int32 col2 int32 dtype : object Cast col1 to int32 using a dictionary: >>> df . astype ({ 'col1' : 'int32' }) . dtypes col1 int32 col2 int64 dtype : object Create a series: >>> ser = pd . Series ([ 1 , 2 ], dtype = 'int32' ) >>> ser 0 1 1 2 dtype : int32 >>> ser . astype ( 'int64' ) 0 1 1 2 dtype : int64 Convert to categorical type: >>> ser . astype ( 'category' ) 0 1 1 2 dtype : category Categories ( 2 , int32 ): [ 1 , 2 ] Convert to ordered categorical type with custom ordering: >>> from pandas.api.types import CategoricalDtype >>> cat_dtype = CategoricalDtype ( ... categories = [ 2 , 1 ], ordered = True ) >>> ser . astype ( cat_dtype ) 0 1 1 2 dtype : category Categories ( 2 , int64 ): [ 2 < 1 ] Create a series of dates: >>> ser_date = pd . Series ( pd . date_range ( '20200101' , periods = 3 )) >>> ser_date 0 2020 - 01 - 01 1 2020 - 01 - 02 2 2020 - 01 - 03 dtype : datetime64 [ ns ] method copy ( deep=True ) </> Make a copy of this object's indices and data. When deep=True (default), a new object will be created with a copy of the calling object's data and indices. Modifications to the data or indices of the copy will not be reflected in the original object (see notes below). When deep=False , a new object will be created without copying the calling object's data or index (only references to the data and index are copied). Any changes to the data of the original will be reflected in the shallow copy (and vice versa). .. note:: The deep=False behaviour as described above will change in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that the \"shallow\" copy is that is returned with deep=False will still avoid making an eager copy, but changes to the data of the original will no longer be reflected in the shallow copy (or vice versa). Instead, it makes use of a lazy (deferred) copy mechanism that will copy the data only when any changes to the original or shallow copy is made. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` Parameters deep (bool, default True) \u2014 Make a deep copy, including a copy of the data and the indices.With deep=False neither the indices nor the data are copied. Returns (Series or DataFrame) Object type matches caller. Notes When deep=True , data is copied but actual Python objects will not be copied recursively, only the reference to the object. This is in contrast to copy.deepcopy in the Standard Library, which recursively copies object data (see examples below). While Index objects are copied when deep=True , the underlying numpy array is not copied for performance reasons. Since Index is immutable, the underlying data can be safely shared and a copy is not needed. Since pandas is not thread safe, see the :ref: gotchas <gotchas.thread-safety> when copying in a threading environment. When copy_on_write in pandas config is set to True , the copy_on_write config takes effect even when deep=False . This means that any changes to the copied data would make a new copy of the data upon write (and vice versa). Changes made to either the original or copied variable would not be reflected in the counterpart. See :ref: Copy_on_Write <copy_on_write> for more information. Examples >>> s = pd . Series ([ 1 , 2 ], index = [ \"a\" , \"b\" ]) >>> s a 1 b 2 dtype : int64 >>> s_copy = s . copy () >>> s_copy a 1 b 2 dtype : int64 Shallow copy versus default (deep) copy: >>> s = pd . Series ([ 1 , 2 ], index = [ \"a\" , \"b\" ]) >>> deep = s . copy () >>> shallow = s . copy ( deep = False ) Shallow copy shares data and index with original. >>> s is shallow False >>> s . values is shallow . values and s . index is shallow . index True Deep copy has own copy of data and index. >>> s is deep False >>> s . values is deep . values or s . index is deep . index False Updates to the data shared by shallow copy and original is reflected in both (NOTE: this will no longer be true for pandas >= 3.0); deep copy remains unchanged. >>> s . iloc [ 0 ] = 3 >>> shallow . iloc [ 1 ] = 4 >>> s a 3 b 4 dtype : int64 >>> shallow a 3 b 4 dtype : int64 >>> deep a 1 b 2 dtype : int64 Note that when copying an object containing Python objects, a deep copy will copy the data, but will not do so recursively. Updating a nested data object will be reflected in the deep copy. >>> s = pd . Series ([[ 1 , 2 ], [ 3 , 4 ]]) >>> deep = s . copy () >>> s [ 0 ][ 0 ] = 10 >>> s 0 [ 10 , 2 ] 1 [ 3 , 4 ] dtype : object >>> deep 0 [ 10 , 2 ] 1 [ 3 , 4 ] dtype : object Copy-on-Write is set to true , the shallow copy is not modified when the original data is changed: >>> with pd . option_context ( \"mode.copy_on_write\" , True ): ... s = pd . Series ([ 1 , 2 ], index = [ \"a\" , \"b\" ]) ... copy = s . copy ( deep = False ) ... s . iloc [ 0 ] = 100 ... s a 100 b 2 dtype : int64 >>> copy a 1 b 2 dtype : int64 method __deepcopy__ ( memo=None ) </> method infer_objects ( copy=None ) </> Attempt to infer better dtypes for object columns. Attempts soft conversion of object-dtyped columns, leaving non-object and unconvertible columns unchanged. The inference rules are the same as during normal Series/DataFrame construction. Parameters copy (bool, default True) \u2014 Whether to make a copy for non-object or non-inferable columnsor Series. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` See Also to_datetime : Convert argument to datetime.to_timedelta : Convert argument to timedelta. to_numeric : Convert argument to numeric type. convert_dtypes : Convert argument to best possible dtype. Examples >>> df = pd . DataFrame ({ \"A\" : [ \"a\" , 1 , 2 , 3 ]}) >>> df = df . iloc [ 1 :] >>> df A 1 1 2 2 3 3 >>> df . dtypes A object dtype : object >>> df . infer_objects () . dtypes A int64 dtype : object method convert_dtypes ( infer_objects=True , convert_string=True , convert_integer=True , convert_boolean=True , convert_floating=True , dtype_backend='numpy_nullable' ) </> Convert columns to the best possible dtypes using dtypes supporting pd.NA . Parameters infer_objects (bool, default True) \u2014 Whether object dtypes should be converted to the best possible types. convert_string (bool, default True) \u2014 Whether object dtypes should be converted to StringDtype() . convert_integer (bool, default True) \u2014 Whether, if possible, conversion can be done to integer extension types. convert_boolean (bool, defaults True) \u2014 Whether object dtypes should be converted to BooleanDtypes() . convert_floating (bool, defaults True) \u2014 Whether, if possible, conversion can be done to floating extension types.If convert_integer is also True, preference will be give to integer dtypes if the floats can be faithfully casted to integers. dtype_backend ({'numpy_nullable', 'pyarrow'}, default 'numpy_nullable') \u2014 Back-end data type applied to the resultant :class: DataFrame (still experimental). Behaviour is as follows: \"numpy_nullable\" : returns nullable-dtype-backed :class: DataFrame (default). \"pyarrow\" : returns pyarrow-backed nullable :class: ArrowDtype DataFrame. .. versionadded:: 2.0 Returns (Series or DataFrame) Copy of input object with new dtype. See Also infer_objects : Infer dtypes of objects.to_datetime : Convert argument to datetime. to_timedelta : Convert argument to timedelta. to_numeric : Convert argument to a numeric type. Notes By default, convert_dtypes will attempt to convert a Series (or each Series in a DataFrame) to dtypes that support pd.NA . By using the options convert_string , convert_integer , convert_boolean and convert_floating , it is possible to turn off individual conversions to StringDtype , the integer extension types, BooleanDtype or floating extension types, respectively. For object-dtyped columns, if infer_objects is True , use the inference rules as during normal Series/DataFrame construction. Then, if possible, convert to StringDtype , BooleanDtype or an appropriate integer or floating extension type, otherwise leave as object . If the dtype is integer, convert to an appropriate integer extension type. If the dtype is numeric, and consists of all integers, convert to an appropriate integer extension type. Otherwise, convert to an appropriate floating extension type. In the future, as new dtypes are added that support pd.NA , the results of this method will change to support those new dtypes. Examples >>> df = pd . DataFrame ( ... { ... \"a\" : pd . Series ([ 1 , 2 , 3 ], dtype = np . dtype ( \"int32\" )), ... \"b\" : pd . Series ([ \"x\" , \"y\" , \"z\" ], dtype = np . dtype ( \"O\" )), ... \"c\" : pd . Series ([ True , False , np . nan ], dtype = np . dtype ( \"O\" )), ... \"d\" : pd . Series ([ \"h\" , \"i\" , np . nan ], dtype = np . dtype ( \"O\" )), ... \"e\" : pd . Series ([ 10 , np . nan , 20 ], dtype = np . dtype ( \"float\" )), ... \"f\" : pd . Series ([ np . nan , 100.5 , 200 ], dtype = np . dtype ( \"float\" )), ... } ... ) Start with a DataFrame with default dtypes. >>> df a b c d e f 0 1 x True h 10.0 NaN 1 2 y False i NaN 100.5 2 3 z NaN NaN 20.0 200.0 >>> df . dtypes a int32 b object c object d object e float64 f float64 dtype : object Convert the DataFrame to use best possible dtypes. >>> dfn = df . convert_dtypes () >>> dfn a b c d e f 0 1 x True h 10 < NA > 1 2 y False i < NA > 100.5 2 3 z < NA > < NA > 20 200.0 >>> dfn . dtypes a Int32 b string [ python ] c boolean d string [ python ] e Int64 f Float64 dtype : object Start with a Series of strings and missing data represented by np.nan . >>> s = pd . Series ([ \"a\" , \"b\" , np . nan ]) >>> s 0 a 1 b 2 NaN dtype : object Obtain a Series with dtype StringDtype . >>> s . convert_dtypes () 0 a 1 b 2 < NA > dtype : string method fillna ( value=None , method=None , axis=None , inplace=False , limit=None , downcast=<no_default> ) </> Fill NA/NaN values using the specified method. Parameters value (scalar, dict, Series, or DataFrame) \u2014 Value to use to fill holes (e.g. 0), alternately adict/Series/DataFrame of values specifying which value to use for each index (for a Series) or column (for a DataFrame). Values not in the dict/Series/DataFrame will not be filled. This value cannot be a list. method ({'backfill', 'bfill', 'ffill', None}, default None) \u2014 Method to use for filling holes in reindexed Series: ffill: propagate last valid observation forward to next valid. backfill / bfill: use next valid observation to fill gap. .. deprecated:: 2.1.0 Use ffill or bfill instead. axis ({0 or 'index'} for Series, {0 or 'index', 1 or 'columns'} for DataFrame) \u2014 Axis along which to fill missing values. For Series this parameter is unused and defaults to 0. inplace (bool, default False) \u2014 If True, fill in-place. Note: this will modify anyother views on this object (e.g., a no-copy slice for a column in a DataFrame). limit (int, default None) \u2014 If method is specified, this is the maximum number of consecutiveNaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None. downcast (dict, default is None) \u2014 A dict of item->dtype of what to downcast if possible,or the string 'infer' which will try to downcast to an appropriate equal type (e.g. float64 to int64 if possible). .. deprecated:: 2.2.0 Returns (Series/DataFrame or None) Object with missing values filled or None if inplace=True . See Also ffill : Fill values by propagating the last valid observation to next valid.bfill : Fill values by using the next valid observation to fill the gap. interpolate : Fill NaN values using interpolation. reindex : Conform object to new index. asfreq : Convert TimeSeries to specified frequency. Examples >>> df = pd . DataFrame ([[ np . nan , 2 , np . nan , 0 ], ... [ 3 , 4 , np . nan , 1 ], ... [ np . nan , np . nan , np . nan , np . nan ], ... [ np . nan , 3 , np . nan , 4 ]], ... columns = list ( \"ABCD\" )) >>> df A B C D 0 NaN 2.0 NaN 0.0 1 3.0 4.0 NaN 1.0 2 NaN NaN NaN NaN 3 NaN 3.0 NaN 4.0 Replace all NaN elements with 0s. >>> df . fillna ( 0 ) A B C D 0 0.0 2.0 0.0 0.0 1 3.0 4.0 0.0 1.0 2 0.0 0.0 0.0 0.0 3 0.0 3.0 0.0 4.0 Replace all NaN elements in column 'A', 'B', 'C', and 'D', with 0, 1, 2, and 3 respectively. >>> values = { \"A\" : 0 , \"B\" : 1 , \"C\" : 2 , \"D\" : 3 } >>> df . fillna ( value = values ) A B C D 0 0.0 2.0 2.0 0.0 1 3.0 4.0 2.0 1.0 2 0.0 1.0 2.0 3.0 3 0.0 3.0 2.0 4.0 Only replace the first NaN element. >>> df . fillna ( value = values , limit = 1 ) A B C D 0 0.0 2.0 2.0 0.0 1 3.0 4.0 NaN 1.0 2 NaN 1.0 NaN 3.0 3 NaN 3.0 NaN 4.0 When filling using a DataFrame, replacement happens along the same column names and same indices >>> df2 = pd . DataFrame ( np . zeros (( 4 , 4 )), columns = list ( \"ABCE\" )) >>> df . fillna ( df2 ) A B C D 0 0.0 2.0 0.0 0.0 1 3.0 4.0 0.0 1.0 2 0.0 0.0 0.0 NaN 3 0.0 3.0 0.0 4.0 Note that column D is not affected since it is not present in df2. method ffill ( axis=None , inplace=False , limit=None , limit_area=None , downcast=<no_default> ) </> Fill NA/NaN values by propagating the last valid observation to next valid. Parameters axis ({0 or 'index'} for Series, {0 or 'index', 1 or 'columns'} for DataFrame) \u2014 Axis along which to fill missing values. For Series this parameter is unused and defaults to 0. inplace (bool, default False) \u2014 If True, fill in-place. Note: this will modify anyother views on this object (e.g., a no-copy slice for a column in a DataFrame). limit (int, default None) \u2014 If method is specified, this is the maximum number of consecutiveNaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None. limit_area ({`None`, 'inside', 'outside'}, default None) \u2014 If limit is specified, consecutive NaNs will be filled with thisrestriction. None : No fill restriction. 'inside': Only fill NaNs surrounded by valid values (interpolate). 'outside': Only fill NaNs outside valid values (extrapolate). .. versionadded:: 2.2.0 downcast (dict, default is None) \u2014 A dict of item->dtype of what to downcast if possible,or the string 'infer' which will try to downcast to an appropriate equal type (e.g. float64 to int64 if possible). .. deprecated:: 2.2.0 Returns (Series/DataFrame or None) Object with missing values filled or None if inplace=True . Examples >>> df = pd . DataFrame ([[ np . nan , 2 , np . nan , 0 ], ... [ 3 , 4 , np . nan , 1 ], ... [ np . nan , np . nan , np . nan , np . nan ], ... [ np . nan , 3 , np . nan , 4 ]], ... columns = list ( \"ABCD\" )) >>> df A B C D 0 NaN 2.0 NaN 0.0 1 3.0 4.0 NaN 1.0 2 NaN NaN NaN NaN 3 NaN 3.0 NaN 4.0 >>> df . ffill () A B C D 0 NaN 2.0 NaN 0.0 1 3.0 4.0 NaN 1.0 2 3.0 4.0 NaN 1.0 3 3.0 3.0 NaN 4.0 >>> ser = pd . Series ([ 1 , np . nan , 2 , 3 ]) >>> ser . ffill () 0 1.0 1 1.0 2 2.0 3 3.0 dtype : float64 method pad ( axis=None , inplace=False , limit=None , downcast=<no_default> ) </> Fill NA/NaN values by propagating the last valid observation to next valid. .. deprecated:: 2.0 Series/DataFrame.pad is deprecated. Use Series/DataFrame.ffill instead. Returns (Series/DataFrame or None) Object with missing values filled or None if inplace=True . Examples Please see examples for :meth: DataFrame.ffill or :meth: Series.ffill . method bfill ( axis=None , inplace=False , limit=None , limit_area=None , downcast=<no_default> ) </> Fill NA/NaN values by using the next valid observation to fill the gap. Parameters axis ({0 or 'index'} for Series, {0 or 'index', 1 or 'columns'} for DataFrame) \u2014 Axis along which to fill missing values. For Series this parameter is unused and defaults to 0. inplace (bool, default False) \u2014 If True, fill in-place. Note: this will modify anyother views on this object (e.g., a no-copy slice for a column in a DataFrame). limit (int, default None) \u2014 If method is specified, this is the maximum number of consecutiveNaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None. limit_area ({`None`, 'inside', 'outside'}, default None) \u2014 If limit is specified, consecutive NaNs will be filled with thisrestriction. None : No fill restriction. 'inside': Only fill NaNs surrounded by valid values (interpolate). 'outside': Only fill NaNs outside valid values (extrapolate). .. versionadded:: 2.2.0 downcast (dict, default is None) \u2014 A dict of item->dtype of what to downcast if possible,or the string 'infer' which will try to downcast to an appropriate equal type (e.g. float64 to int64 if possible). .. deprecated:: 2.2.0 Returns (Series/DataFrame or None) Object with missing values filled or None if inplace=True . Examples For Series: >>> s = pd . Series ([ 1 , None , None , 2 ]) >>> s . bfill () 0 1.0 1 2.0 2 2.0 3 2.0 dtype : float64 >>> s . bfill ( limit = 1 ) 0 1.0 1 NaN 2 2.0 3 2.0 dtype : float64 With DataFrame: >>> df = pd . DataFrame ({ 'A' : [ 1 , None , None , 4 ], 'B' : [ None , 5 , None , 7 ]}) >>> df A B 0 1.0 NaN 1 NaN 5.0 2 NaN NaN 3 4.0 7.0 >>> df . bfill () A B 0 1.0 5.0 1 4.0 5.0 2 4.0 7.0 3 4.0 7.0 >>> df . bfill ( limit = 1 ) A B 0 1.0 5.0 1 NaN 5.0 2 4.0 7.0 3 4.0 7.0 method backfill ( axis=None , inplace=False , limit=None , downcast=<no_default> ) </> Fill NA/NaN values by using the next valid observation to fill the gap. .. deprecated:: 2.0 Series/DataFrame.backfill is deprecated. Use Series/DataFrame.bfill instead. Returns (Series/DataFrame or None) Object with missing values filled or None if inplace=True . Examples Please see examples for :meth: DataFrame.bfill or :meth: Series.bfill . method replace ( to_replace=None , value=<no_default> , inplace=False , limit=None , regex=False , method=<no_default> ) </> Replace values given in to_replace with value . Values of the Series/DataFrame are replaced with other values dynamically. This differs from updating with .loc or .iloc , which require you to specify a location to update with some value. Parameters to_replace (str, regex, list, dict, Series, int, float, or None) \u2014 How to find the values that will be replaced. numeric, str or regex: numeric: numeric values equal to to_replace will be replaced with value str: string exactly matching to_replace will be replaced with value regex: regexs matching to_replace will be replaced with value list of str, regex, or numeric: First, if to_replace and value are both lists, they must be the same length. Second, if regex=True then all of the strings in both lists will be interpreted as regexs otherwise they will match directly. This doesn't matter much for value since there are only a few possible substitution regexes you can use. str, regex and numeric rules apply as above. dict: Dicts can be used to specify different replacement values for different existing values. For example, {'a': 'b', 'y': 'z'} replaces the value 'a' with 'b' and 'y' with 'z'. To use a dict in this way, the optional value parameter should not be given. For a DataFrame a dict can specify that different values should be replaced in different columns. For example, {'a': 1, 'b': 'z'} looks for the value 1 in column 'a' and the value 'z' in column 'b' and replaces these values with whatever is specified in value . The value parameter should not be None in this case. You can treat this as a special case of passing two lists except that you are specifying the column to search in. For a DataFrame nested dictionaries, e.g., {'a': {'b': np.nan}} , are read as follows: look in column 'a' for the value 'b' and replace it with NaN. The optional value parameter should not be specified to use a nested dict in this way. You can nest regular expressions as well. Note that column names (the top-level dictionary keys in a nested dictionary) cannot be regular expressions. None: This means that the regex argument must be a string, compiled regular expression, or list, dict, ndarray or Series of such elements. If value is also None then this must be a nested dictionary or Series. See the examples section for examples of each of these. value (scalar, dict, list, str, regex, default None) \u2014 Value to replace any values matching to_replace with.For a DataFrame a dict of values can be used to specify which value to use for each column (columns not in the dict will not be filled). Regular expressions, strings and lists or dicts of such objects are also allowed. inplace (bool, default False) \u2014 If True, performs operation inplace and returns None. limit (int, default None) \u2014 Maximum size gap to forward or backward fill. .. deprecated:: 2.1.0 regex (bool or same types as `to_replace`, default False) \u2014 Whether to interpret to_replace and/or value as regularexpressions. Alternatively, this could be a regular expression or a list, dict, or array of regular expressions in which case to_replace must be None . method ({'pad', 'ffill', 'bfill'}) \u2014 The method to use when for replacement, when to_replace is ascalar, list or tuple and value is None . .. deprecated:: 2.1.0 Returns (Series/DataFrame) Object after replacement. Raises AssertionError \u2014 If regex is not a bool and to_replace is not None . TypeError \u2014 If to_replace is not a scalar, array-like, dict , or None If to_replace is a dict and value is not a list , dict , ndarray , or Series If to_replace is None and regex is not compilable into a regular expression or is a list, dict, ndarray, or Series. When replacing multiple bool or datetime64 objects and the arguments to to_replace does not match the type of the value being replaced ValueError \u2014 If a list or an ndarray is passed to to_replace and value but they are not the same length. See Also Series.fillna : Fill NA values.DataFrame.fillna : Fill NA values. Series.where : Replace values based on boolean condition. DataFrame.where : Replace values based on boolean condition. DataFrame.map: Apply a function to a Dataframe elementwise. Series.map: Map values of Series according to an input mapping or function. Series.str.replace : Simple string replacement. Notes Regex substitution is performed under the hood with re.sub . The rules for substitution for re.sub are the same. Regular expressions will only substitute on strings, meaning you cannot provide, for example, a regular expression matching floating point numbers and expect the columns in your frame that have a numeric dtype to be matched. However, if those floating point numbers are strings, then you can do this. This method has a lot of options. You are encouraged to experiment and play with this method to gain intuition about how it works. When dict is used as the to_replace value, it is like key(s) in the dict are the to_replace part and value(s) in the dict are the value parameter. Examples * ) ) 5 2 3 4 5 4 , , ) ) C a b c d e * ) C a b c d e ) C a b c d e ) 3 3 3 4 5 4 * ) C a b c d e ) C a b c d e ) C a b c d e * , ) ) B c w z ) B c r z ) B c w z ) B c w z ) B c w z d s : ) e . o : ) 0 e e b e t t e 0 . ) 0 0 0 b b t 0 . l : ) 0 e e b e t 0 . , . , , ) ) C e e h i j y . ) C f g e e e method interpolate ( method='linear' , axis=0 , limit=None , inplace=False , limit_direction=None , limit_area=None , downcast=<no_default> , **kwargs ) </> Fill NaN values using an interpolation method. Please note that only method='linear' is supported for DataFrame/Series with a MultiIndex. Parameters method (str, default 'linear') \u2014 Interpolation technique to use. One of: 'linear': Ignore the index and treat the values as equally spaced. This is the only method supported on MultiIndexes. 'time': Works on daily and higher resolution data to interpolate given length of interval. 'index', 'values': use the actual numerical values of the index. 'pad': Fill in NaNs using existing values. 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'barycentric', 'polynomial': Passed to scipy.interpolate.interp1d , whereas 'spline' is passed to scipy.interpolate.UnivariateSpline . These methods use the numerical values of the index. Both 'polynomial' and 'spline' require that you also specify an order (int), e.g. df.interpolate(method='polynomial', order=5) . Note that, slinear method in Pandas refers to the Scipy first order spline instead of Pandas first order spline . 'krogh', 'piecewise_polynomial', 'spline', 'pchip', 'akima', 'cubicspline': Wrappers around the SciPy interpolation methods of similar names. See Notes . 'from_derivatives': Refers to scipy.interpolate.BPoly.from_derivatives . axis ({{0 or 'index', 1 or 'columns', None}}, default None) \u2014 Axis to interpolate along. For Series this parameter is unusedand defaults to 0. limit (int, optional) \u2014 Maximum number of consecutive NaNs to fill. Must be greater than0. inplace (bool, default False) \u2014 Update the data in place if possible. limit_direction ({{'forward', 'backward', 'both'}}, Optional) \u2014 Consecutive NaNs will be filled in this direction. If limit is specified: * If 'method' is 'pad' or 'ffill', 'limit_direction' must be 'forward'. * If 'method' is 'backfill' or 'bfill', 'limit_direction' must be 'backwards'. If 'limit' is not specified: * If 'method' is 'backfill' or 'bfill', the default is 'backward' * else the default is 'forward' raises ValueError if limit_direction is 'forward' or 'both' and method is 'backfill' or 'bfill'. raises ValueError if limit_direction is 'backward' or 'both' and method is 'pad' or 'ffill'. limit_area ({{`None`, 'inside', 'outside'}}, default None) \u2014 If limit is specified, consecutive NaNs will be filled with thisrestriction. None : No fill restriction. 'inside': Only fill NaNs surrounded by valid values (interpolate). 'outside': Only fill NaNs outside valid values (extrapolate). downcast (optional, 'infer' or None, defaults to None) \u2014 Downcast dtypes if possible. .. deprecated:: 2.1.0 Returns (Series or DataFrame or None) Returns the same object type as the caller, interpolated atsome or all NaN values or None if inplace=True . See Also fillna : Fill missing values using different methods.scipy.interpolate.Akima1DInterpolator : Piecewise cubic polynomials (Akima interpolator). scipy.interpolate.BPoly.from_derivatives : Piecewise polynomial in the Bernstein basis. scipy.interpolate.interp1d : Interpolate a 1-D function. scipy.interpolate.KroghInterpolator : Interpolate polynomial (Krogh interpolator). scipy.interpolate.PchipInterpolator : PCHIP 1-d monotonic cubic interpolation. scipy.interpolate.CubicSpline : Cubic spline data interpolator. Notes The 'krogh', 'piecewise_polynomial', 'spline', 'pchip' and 'akima' methods are wrappers around the respective SciPy implementations of similar names. These use the actual numerical values of the index. For more information on their behavior, see the SciPy documentation <https://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation> __. Examples Filling in NaN in a :class: ~pandas.Series via linearinterpolation. >>> s = pd . Series ([ 0 , 1 , np . nan , 3 ]) >>> s 0 0.0 1 1.0 2 NaN 3 3.0 dtype : float64 >>> s . interpolate () 0 0.0 1 1.0 2 2.0 3 3.0 dtype : float64 Filling in NaN in a Series via polynomial interpolation or splines: Both 'polynomial' and 'spline' methods require that you also specify an order (int). >>> s = pd . Series ([ 0 , 2 , np . nan , 8 ]) >>> s . interpolate ( method = 'polynomial' , order = 2 ) 0 0.000000 1 2.000000 2 4.666667 3 8.000000 dtype : float64 Fill the DataFrame forward (that is, going down) along each column using linear interpolation. Note how the last entry in column 'a' is interpolated differently, because there is no entry after it to use for interpolation. Note how the first entry in column 'b' remains NaN , because there is no entry before it to use for interpolation. >>> df = pd . DataFrame ([( 0.0 , np . nan , - 1.0 , 1.0 ), ... ( np . nan , 2.0 , np . nan , np . nan ), ... ( 2.0 , 3.0 , np . nan , 9.0 ), ... ( np . nan , 4.0 , - 4.0 , 16.0 )], ... columns = list ( 'abcd' )) >>> df a b c d 0 0.0 NaN - 1.0 1.0 1 NaN 2.0 NaN NaN 2 2.0 3.0 NaN 9.0 3 NaN 4.0 - 4.0 16.0 >>> df . interpolate ( method = 'linear' , limit_direction = 'forward' , axis = 0 ) a b c d 0 0.0 NaN - 1.0 1.0 1 1.0 2.0 - 2.0 5.0 2 2.0 3.0 - 3.0 9.0 3 2.0 4.0 - 4.0 16.0 Using polynomial interpolation. >>> df [ 'd' ] . interpolate ( method = 'polynomial' , order = 2 ) 0 1.0 1 4.0 2 9.0 3 16.0 Name : d , dtype : float64 method asof ( where , subset=None ) </> Return the last row(s) without any NaNs before where . The last row (for each element in where , if list) without any NaN is taken. In case of a :class: ~pandas.DataFrame , the last row without NaN considering only the subset of columns (if not None ) If there is no good value, NaN is returned for a Series or a Series of NaN values for a DataFrame Parameters where (date or array-like of dates) \u2014 Date(s) before which the last row(s) are returned. subset (str or array-like of str, default `None`) \u2014 For DataFrame, if not None , only use these columns tocheck for NaNs. Returns (scalar, Series, or DataFrame) : r , r n e See Also merge_asof : Perform an asof merge. Similar to left join. Notes Dates are assumed to be sorted. Raises if this is not the case. Examples A Series and a scalar where . >>> s = pd . Series ([ 1 , 2 , np . nan , 4 ], index = [ 10 , 20 , 30 , 40 ]) >>> s 10 1.0 20 2.0 30 NaN 40 4.0 dtype : float64 >>> s . asof ( 20 ) 2.0 For a sequence where , a Series is returned. The first value is NaN, because the first element of where is before the first index value. >>> s . asof ([ 5 , 20 ]) 5 NaN 20 2.0 dtype : float64 Missing values are not considered. The following is 2.0 , not NaN, even though NaN is at the index location for 30 . >>> s . asof ( 30 ) 2.0 Take all columns into consideration >>> df = pd . DataFrame ({ 'a' : [ 10. , 20. , 30. , 40. , 50. ], ... 'b' : [ None , None , None , None , 500 ]}, ... index = pd . DatetimeIndex ([ '2018-02-27 09:01:00' , ... '2018-02-27 09:02:00' , ... '2018-02-27 09:03:00' , ... '2018-02-27 09:04:00' , ... '2018-02-27 09:05:00' ])) >>> df . asof ( pd . DatetimeIndex ([ '2018-02-27 09:03:30' , ... '2018-02-27 09:04:30' ])) a b 2018 - 02 - 27 09 : 03 : 30 NaN NaN 2018 - 02 - 27 09 : 04 : 30 NaN NaN Take a single column into consideration >>> df . asof ( pd . DatetimeIndex ([ '2018-02-27 09:03:30' , ... '2018-02-27 09:04:30' ]), ... subset = [ 'a' ]) a b 2018 - 02 - 27 09 : 03 : 30 30.0 NaN 2018 - 02 - 27 09 : 04 : 30 40.0 NaN method clip ( lower=None , upper=None , axis=None , inplace=False , **kwargs ) </> Trim values at input threshold(s). Assigns values outside boundary to boundary values. Thresholds can be singular values or array like, and in the latter case the clipping is performed element-wise in the specified axis. Parameters lower (float or array-like, default None) \u2014 Minimum threshold value. All values below thisthreshold will be set to it. A missing threshold (e.g NA ) will not clip the value. upper (float or array-like, default None) \u2014 Maximum threshold value. All values above thisthreshold will be set to it. A missing threshold (e.g NA ) will not clip the value. axis ({{0 or 'index', 1 or 'columns', None}}, default None) \u2014 Align object with lower and upper along the given axis.For Series this parameter is unused and defaults to None . inplace (bool, default False) \u2014 Whether to perform the operation in place on the data. Returns (Series or DataFrame or None) Same type as calling object with the values outside theclip boundaries replaced or None if inplace=True . See Also Series.clip : Trim values at input threshold in series.DataFrame.clip : Trim values at input threshold in dataframe. numpy.clip : Clip (limit) the values in an array. Examples >>> data = { 'col_0' : [ 9 , - 3 , 0 , - 1 , 5 ], 'col_1' : [ - 2 , - 7 , 6 , 8 , - 5 ]} >>> df = pd . DataFrame ( data ) >>> df col_0 col_1 0 9 - 2 1 - 3 - 7 2 0 6 3 - 1 8 4 5 - 5 Clips per column using lower and upper thresholds: >>> df . clip ( - 4 , 6 ) col_0 col_1 0 6 - 2 1 - 3 - 4 2 0 6 3 - 1 6 4 5 - 4 Clips using specific lower and upper thresholds per column: >>> df . clip ([ - 2 , - 1 ], [ 4 , 5 ]) col_0 col_1 0 4 - 1 1 - 2 - 1 2 0 5 3 - 1 5 4 4 - 1 Clips using specific lower and upper thresholds per column element: >>> t = pd . Series ([ 2 , - 4 , - 1 , 6 , 3 ]) >>> t 0 2 1 - 4 2 - 1 3 6 4 3 dtype : int64 >>> df . clip ( t , t + 4 , axis = 0 ) col_0 col_1 0 6 2 1 - 3 - 4 2 0 3 3 6 8 4 5 3 Clips using specific lower threshold per column element, with missing values: >>> t = pd . Series ([ 2 , - 4 , np . nan , 6 , 3 ]) >>> t 0 2.0 1 - 4.0 2 NaN 3 6.0 4 3.0 dtype : float64 >>> df . clip ( t , axis = 0 ) col_0 col_1 0 9 2 1 - 3 - 4 2 0 6 3 6 8 4 5 3 method asfreq ( freq , method=None , how=None , normalize=False , fill_value=None ) </> Convert time series to specified frequency. Returns the original data conformed to a new index with the specified frequency. If the index of this Series/DataFrame is a :class: ~pandas.PeriodIndex , the new index is the result of transforming the original index with :meth: PeriodIndex.asfreq <pandas.PeriodIndex.asfreq> (so the original index will map one-to-one to the new index). Otherwise, the new index will be equivalent to pd.date_range(start, end, freq=freq) where start and end are, respectively, the first and last entries in the original index (see :func: pandas.date_range ). The values corresponding to any timesteps in the new index which were not present in the original index will be null ( NaN ), unless a method for filling such unknowns is provided (see the method parameter below). The :meth: resample method is more appropriate if an operation on each group of timesteps (such as an aggregate) is necessary to represent the data at the new frequency. Parameters freq (DateOffset or str) \u2014 Frequency DateOffset or string. method ({'backfill'/'bfill', 'pad'/'ffill'}, default None) \u2014 Method to use for filling holes in reindexed Series (note thisdoes not fill NaNs that already were present): 'pad' / 'ffill': propagate last valid observation forward to next valid 'backfill' / 'bfill': use NEXT valid observation to fill. how ({'start', 'end'}, default end) \u2014 For PeriodIndex only (see PeriodIndex.asfreq). normalize (bool, default False) \u2014 Whether to reset output index to midnight. fill_value (scalar, optional) \u2014 Value to use for missing values, applied during upsampling (notethis does not fill NaNs that already were present). Returns (Series/DataFrame) Series/DataFrame object reindexed to the specified frequency. See Also reindex : Conform DataFrame to new index with optional filling logic. Notes To learn more about the frequency strings, please see this link <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases> __. Examples Start by creating a series with 4 one minute timestamps. >>> index = pd . date_range ( '1/1/2000' , periods = 4 , freq = 'min' ) >>> series = pd . Series ([ 0.0 , None , 2.0 , 3.0 ], index = index ) >>> df = pd . DataFrame ({ 's' : series }) >>> df s 2000 - 01 - 01 00 : 00 : 00 0.0 2000 - 01 - 01 00 : 01 : 00 NaN 2000 - 01 - 01 00 : 02 : 00 2.0 2000 - 01 - 01 00 : 03 : 00 3.0 Upsample the series into 30 second bins. >>> df . asfreq ( freq = '30s' ) s 2000 - 01 - 01 00 : 00 : 00 0.0 2000 - 01 - 01 00 : 00 : 30 NaN 2000 - 01 - 01 00 : 01 : 00 NaN 2000 - 01 - 01 00 : 01 : 30 NaN 2000 - 01 - 01 00 : 02 : 00 2.0 2000 - 01 - 01 00 : 02 : 30 NaN 2000 - 01 - 01 00 : 03 : 00 3.0 Upsample again, providing a fill value . >>> df . asfreq ( freq = '30s' , fill_value = 9.0 ) s 2000 - 01 - 01 00 : 00 : 00 0.0 2000 - 01 - 01 00 : 00 : 30 9.0 2000 - 01 - 01 00 : 01 : 00 NaN 2000 - 01 - 01 00 : 01 : 30 9.0 2000 - 01 - 01 00 : 02 : 00 2.0 2000 - 01 - 01 00 : 02 : 30 9.0 2000 - 01 - 01 00 : 03 : 00 3.0 Upsample again, providing a method . >>> df . asfreq ( freq = '30s' , method = 'bfill' ) s 2000 - 01 - 01 00 : 00 : 00 0.0 2000 - 01 - 01 00 : 00 : 30 NaN 2000 - 01 - 01 00 : 01 : 00 NaN 2000 - 01 - 01 00 : 01 : 30 2.0 2000 - 01 - 01 00 : 02 : 00 2.0 2000 - 01 - 01 00 : 02 : 30 3.0 2000 - 01 - 01 00 : 03 : 00 3.0 method at_time ( time , asof=False , axis=None ) </> Select values at particular time of day (e.g., 9:30AM). Parameters time (datetime.time or str) \u2014 The values to select. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 For Series this parameter is unused and defaults to 0. Raises TypeError \u2014 If the index is not a :class: DatetimeIndex See Also between_time : Select values between particular times of the day.first : Select initial periods of time series based on a date offset. last : Select final periods of time series based on a date offset. DatetimeIndex.indexer_at_time : Get just the index locations for values at particular time of the day. Examples >>> i = pd . date_range ( '2018-04-09' , periods = 4 , freq = '12h' ) >>> ts = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 , 4 ]}, index = i ) >>> ts A 2018 - 04 - 09 00 : 00 : 00 1 2018 - 04 - 09 12 : 00 : 00 2 2018 - 04 - 10 00 : 00 : 00 3 2018 - 04 - 10 12 : 00 : 00 4 >>> ts . at_time ( '12:00' ) A 2018 - 04 - 09 12 : 00 : 00 2 2018 - 04 - 10 12 : 00 : 00 4 method between_time ( start_time , end_time , inclusive='both' , axis=None ) </> Select values between particular times of the day (e.g., 9:00-9:30 AM). By setting start_time to be later than end_time , you can get the times that are not between the two times. Parameters start_time (datetime.time or str) \u2014 Initial time as a time filter limit. end_time (datetime.time or str) \u2014 End time as a time filter limit. inclusive ({\"both\", \"neither\", \"left\", \"right\"}, default \"both\") \u2014 Include boundaries; whether to set each bound as closed or open. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Determine range time on index or columns value.For Series this parameter is unused and defaults to 0. Returns (Series or DataFrame) Data from the original object filtered to the specified dates range. Raises TypeError \u2014 If the index is not a :class: DatetimeIndex See Also at_time : Select values at a particular time of the day.first : Select initial periods of time series based on a date offset. last : Select final periods of time series based on a date offset. DatetimeIndex.indexer_between_time : Get just the index locations for values between particular times of the day. Examples >>> i = pd . date_range ( '2018-04-09' , periods = 4 , freq = '1D20min' ) >>> ts = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 , 4 ]}, index = i ) >>> ts A 2018 - 04 - 09 00 : 00 : 00 1 2018 - 04 - 10 00 : 20 : 00 2 2018 - 04 - 11 00 : 40 : 00 3 2018 - 04 - 12 01 : 00 : 00 4 >>> ts . between_time ( '0:15' , '0:45' ) A 2018 - 04 - 10 00 : 20 : 00 2 2018 - 04 - 11 00 : 40 : 00 3 You get the times that are not between two times by setting start_time later than end_time : >>> ts . between_time ( '0:45' , '0:15' ) A 2018 - 04 - 09 00 : 00 : 00 1 2018 - 04 - 12 01 : 00 : 00 4 method resample ( rule , axis=<no_default> , closed=None , label=None , convention='start' , kind=<no_default> , on=None , level=None , origin='start_day' , offset=None , group_keys=False ) </> Resample time-series data. Convenience method for frequency conversion and resampling of time series. The object must have a datetime-like index ( DatetimeIndex , PeriodIndex , or TimedeltaIndex ), or the caller must pass the label of a datetime-like series/index to the on / level keyword parameter. Parameters rule (DateOffset, Timedelta or str) \u2014 The offset string or object representing target conversion. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Which axis to use for up- or down-sampling. For Series this parameteris unused and defaults to 0. Must be DatetimeIndex , TimedeltaIndex or PeriodIndex . .. deprecated:: 2.0.0 Use frame.T.resample(...) instead. closed ({'right', 'left'}, default None) \u2014 Which side of bin interval is closed. The default is 'left'for all frequency offsets except for 'ME', 'YE', 'QE', 'BME', 'BA', 'BQE', and 'W' which all have a default of 'right'. label ({'right', 'left'}, default None) \u2014 Which bin edge label to label bucket with. The default is 'left'for all frequency offsets except for 'ME', 'YE', 'QE', 'BME', 'BA', 'BQE', and 'W' which all have a default of 'right'. convention ({'start', 'end', 's', 'e'}, default 'start') \u2014 For PeriodIndex only, controls whether to use the start orend of rule . kind ({'timestamp', 'period'}, optional, default None) \u2014 Pass 'timestamp' to convert the resulting index to a DateTimeIndex or 'period' to convert it to a PeriodIndex . By default the input representation is retained. .. deprecated:: 2.2.0 Convert index to desired type explicitly instead. on (str, optional) \u2014 For a DataFrame, column to use instead of index for resampling.Column must be datetime-like. level (str or int, optional) \u2014 For a MultiIndex, level (name or number) to use forresampling. level must be datetime-like. origin (Timestamp or str, default 'start_day') \u2014 The timestamp on which to adjust the grouping. The timezone of originmust match the timezone of the index. If string, must be one of the following: 'epoch': origin is 1970-01-01 'start': origin is the first value of the timeseries 'start_day': origin is the first day at midnight of the timeseries 'end': origin is the last value of the timeseries 'end_day': origin is the ceiling midnight of the last day .. versionadded:: 1.3.0 .. note:: Only takes effect for Tick-frequencies (i.e. fixed frequencies like days, hours, and minutes, rather than months or quarters). offset (Timedelta or str, default is None) \u2014 An offset timedelta added to the origin. group_keys (bool, default False) \u2014 Whether to include the group keys in the result index when using .apply() on the resampled object. .. versionadded:: 1.5.0 Not specifying ``group_keys`` will retain values-dependent behavior from pandas 1.4 and earlier (see :ref:`pandas 1.5.0 Release notes <whatsnew_150.enhancements.resample_group_keys>` for examples). .. versionchanged:: 2.0.0 ``group_keys`` now defaults to ``False``. Returns (pandas.api.typing.Resampler) :class: ~pandas.core.Resampler object. See Also Series.resample : Resample a Series.DataFrame.resample : Resample a DataFrame. groupby : Group Series/DataFrame by mapping, function, label, or list of labels. asfreq : Reindex a Series/DataFrame with the given frequency without grouping. Notes See the user guide <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#resampling> __ for more. To learn more about the offset strings, please see this link <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects> __. Examples Start by creating a series with 9 one minute timestamps. >>> index = pd . date_range ( '1/1/2000' , periods = 9 , freq = 'min' ) >>> series = pd . Series ( range ( 9 ), index = index ) >>> series 2000 - 01 - 01 00 : 00 : 00 0 2000 - 01 - 01 00 : 01 : 00 1 2000 - 01 - 01 00 : 02 : 00 2 2000 - 01 - 01 00 : 03 : 00 3 2000 - 01 - 01 00 : 04 : 00 4 2000 - 01 - 01 00 : 05 : 00 5 2000 - 01 - 01 00 : 06 : 00 6 2000 - 01 - 01 00 : 07 : 00 7 2000 - 01 - 01 00 : 08 : 00 8 Freq : min , dtype : int64 Downsample the series into 3 minute bins and sum the values of the timestamps falling into a bin. >>> series . resample ( '3min' ) . sum () 2000 - 01 - 01 00 : 00 : 00 3 2000 - 01 - 01 00 : 03 : 00 12 2000 - 01 - 01 00 : 06 : 00 21 Freq : 3 min , dtype : int64 Downsample the series into 3 minute bins as above, but label each bin using the right edge instead of the left. Please note that the value in the bucket used as the label is not included in the bucket, which it labels. For example, in the original series the bucket 2000-01-01 00:03:00 contains the value 3, but the summed value in the resampled bucket with the label 2000-01-01 00:03:00 does not include 3 (if it did, the summed value would be 6, not 3). >>> series . resample ( '3min' , label = 'right' ) . sum () 2000 - 01 - 01 00 : 03 : 00 3 2000 - 01 - 01 00 : 06 : 00 12 2000 - 01 - 01 00 : 09 : 00 21 Freq : 3 min , dtype : int64 To include this value close the right side of the bin interval, as shown below. >>> series . resample ( '3min' , label = 'right' , closed = 'right' ) . sum () 2000 - 01 - 01 00 : 00 : 00 0 2000 - 01 - 01 00 : 03 : 00 6 2000 - 01 - 01 00 : 06 : 00 15 2000 - 01 - 01 00 : 09 : 00 15 Freq : 3 min , dtype : int64 Upsample the series into 30 second bins. >>> series . resample ( '30s' ) . asfreq ()[ 0 : 5 ] # Select first 5 rows 2000 - 01 - 01 00 : 00 : 00 0.0 2000 - 01 - 01 00 : 00 : 30 NaN 2000 - 01 - 01 00 : 01 : 00 1.0 2000 - 01 - 01 00 : 01 : 30 NaN 2000 - 01 - 01 00 : 02 : 00 2.0 Freq : 30 s , dtype : float64 Upsample the series into 30 second bins and fill the NaN values using the ffill method. >>> series . resample ( '30s' ) . ffill ()[ 0 : 5 ] 2000 - 01 - 01 00 : 00 : 00 0 2000 - 01 - 01 00 : 00 : 30 0 2000 - 01 - 01 00 : 01 : 00 1 2000 - 01 - 01 00 : 01 : 30 1 2000 - 01 - 01 00 : 02 : 00 2 Freq : 30 s , dtype : int64 Upsample the series into 30 second bins and fill the NaN values using the bfill method. >>> series . resample ( '30s' ) . bfill ()[ 0 : 5 ] 2000 - 01 - 01 00 : 00 : 00 0 2000 - 01 - 01 00 : 00 : 30 1 2000 - 01 - 01 00 : 01 : 00 1 2000 - 01 - 01 00 : 01 : 30 2 2000 - 01 - 01 00 : 02 : 00 2 Freq : 30 s , dtype : int64 Pass a custom function via apply >>> def custom_resampler ( arraylike ): ... return np . sum ( arraylike ) + 5 ... >>> series . resample ( '3min' ) . apply ( custom_resampler ) 2000 - 01 - 01 00 : 00 : 00 8 2000 - 01 - 01 00 : 03 : 00 17 2000 - 01 - 01 00 : 06 : 00 26 Freq : 3 min , dtype : int64 For a Series with a PeriodIndex, the keyword convention can be used to control whether to use the start or end of rule . Resample a year by quarter using 'start' convention . Values are assigned to the first quarter of the period. >>> s = pd . Series ( ... [ 1 , 2 ], index = pd . period_range ( \"2012-01-01\" , freq = \"Y\" , periods = 2 ) ... ) >>> s 2012 1 2013 2 Freq : Y - DEC , dtype : int64 >>> s . resample ( \"Q\" , convention = \"start\" ) . asfreq () 2012 Q1 1.0 2012 Q2 NaN 2012 Q3 NaN 2012 Q4 NaN 2013 Q1 2.0 2013 Q2 NaN 2013 Q3 NaN 2013 Q4 NaN Freq : Q - DEC , dtype : float64 Resample quarters by month using 'end' convention . Values are assigned to the last month of the period. >>> q = pd . Series ( ... [ 1 , 2 , 3 , 4 ], index = pd . period_range ( \"2018-01-01\" , freq = \"Q\" , periods = 4 ) ... ) >>> q 2018 Q1 1 2018 Q2 2 2018 Q3 3 2018 Q4 4 Freq : Q - DEC , dtype : int64 >>> q . resample ( \"M\" , convention = \"end\" ) . asfreq () 2018 - 03 1.0 2018 - 04 NaN 2018 - 05 NaN 2018 - 06 2.0 2018 - 07 NaN 2018 - 08 NaN 2018 - 09 3.0 2018 - 10 NaN 2018 - 11 NaN 2018 - 12 4.0 Freq : M , dtype : float64 For DataFrame objects, the keyword on can be used to specify the column instead of the index for resampling. >>> d = { 'price' : [ 10 , 11 , 9 , 13 , 14 , 18 , 17 , 19 ], ... 'volume' : [ 50 , 60 , 40 , 100 , 50 , 100 , 40 , 50 ]} >>> df = pd . DataFrame ( d ) >>> df [ 'week_starting' ] = pd . date_range ( '01/01/2018' , ... periods = 8 , ... freq = 'W' ) >>> df price volume week_starting 0 10 50 2018 - 01 - 07 1 11 60 2018 - 01 - 14 2 9 40 2018 - 01 - 21 3 13 100 2018 - 01 - 28 4 14 50 2018 - 02 - 04 5 18 100 2018 - 02 - 11 6 17 40 2018 - 02 - 18 7 19 50 2018 - 02 - 25 >>> df . resample ( 'ME' , on = 'week_starting' ) . mean () price volume week_starting 2018 - 01 - 31 10.75 62.5 2018 - 02 - 28 17.00 60.0 For a DataFrame with MultiIndex, the keyword level can be used to specify on which level the resampling needs to take place. >>> days = pd . date_range ( '1/1/2000' , periods = 4 , freq = 'D' ) >>> d2 = { 'price' : [ 10 , 11 , 9 , 13 , 14 , 18 , 17 , 19 ], ... 'volume' : [ 50 , 60 , 40 , 100 , 50 , 100 , 40 , 50 ]} >>> df2 = pd . DataFrame ( ... d2 , ... index = pd . MultiIndex . from_product ( ... [ days , [ 'morning' , 'afternoon' ]] ... ) ... ) >>> df2 price volume 2000 - 01 - 01 morning 10 50 afternoon 11 60 2000 - 01 - 02 morning 9 40 afternoon 13 100 2000 - 01 - 03 morning 14 50 afternoon 18 100 2000 - 01 - 04 morning 17 40 afternoon 19 50 >>> df2 . resample ( 'D' , level = 0 ) . sum () price volume 2000 - 01 - 01 21 110 2000 - 01 - 02 22 140 2000 - 01 - 03 32 150 2000 - 01 - 04 36 90 If you want to adjust the start of the bins based on a fixed timestamp: >>> start , end = '2000-10-01 23:30:00' , '2000-10-02 00:30:00' >>> rng = pd . date_range ( start , end , freq = '7min' ) >>> ts = pd . Series ( np . arange ( len ( rng )) * 3 , index = rng ) >>> ts 2000 - 10 - 01 23 : 30 : 00 0 2000 - 10 - 01 23 : 37 : 00 3 2000 - 10 - 01 23 : 44 : 00 6 2000 - 10 - 01 23 : 51 : 00 9 2000 - 10 - 01 23 : 58 : 00 12 2000 - 10 - 02 00 : 05 : 00 15 2000 - 10 - 02 00 : 12 : 00 18 2000 - 10 - 02 00 : 19 : 00 21 2000 - 10 - 02 00 : 26 : 00 24 Freq : 7 min , dtype : int64 >>> ts . resample ( '17min' ) . sum () 2000 - 10 - 01 23 : 14 : 00 0 2000 - 10 - 01 23 : 31 : 00 9 2000 - 10 - 01 23 : 48 : 00 21 2000 - 10 - 02 00 : 05 : 00 54 2000 - 10 - 02 00 : 22 : 00 24 Freq : 17 min , dtype : int64 >>> ts . resample ( '17min' , origin = 'epoch' ) . sum () 2000 - 10 - 01 23 : 18 : 00 0 2000 - 10 - 01 23 : 35 : 00 18 2000 - 10 - 01 23 : 52 : 00 27 2000 - 10 - 02 00 : 09 : 00 39 2000 - 10 - 02 00 : 26 : 00 24 Freq : 17 min , dtype : int64 >>> ts . resample ( '17min' , origin = '2000-01-01' ) . sum () 2000 - 10 - 01 23 : 24 : 00 3 2000 - 10 - 01 23 : 41 : 00 15 2000 - 10 - 01 23 : 58 : 00 45 2000 - 10 - 02 00 : 15 : 00 45 Freq : 17 min , dtype : int64 If you want to adjust the start of the bins with an offset Timedelta, the two following lines are equivalent: >>> ts . resample ( '17min' , origin = 'start' ) . sum () 2000 - 10 - 01 23 : 30 : 00 9 2000 - 10 - 01 23 : 47 : 00 21 2000 - 10 - 02 00 : 04 : 00 54 2000 - 10 - 02 00 : 21 : 00 24 Freq : 17 min , dtype : int64 >>> ts . resample ( '17min' , offset = '23h30min' ) . sum () 2000 - 10 - 01 23 : 30 : 00 9 2000 - 10 - 01 23 : 47 : 00 21 2000 - 10 - 02 00 : 04 : 00 54 2000 - 10 - 02 00 : 21 : 00 24 Freq : 17 min , dtype : int64 If you want to take the largest Timestamp as the end of the bins: >>> ts . resample ( '17min' , origin = 'end' ) . sum () 2000 - 10 - 01 23 : 35 : 00 0 2000 - 10 - 01 23 : 52 : 00 18 2000 - 10 - 02 00 : 09 : 00 27 2000 - 10 - 02 00 : 26 : 00 63 Freq : 17 min , dtype : int64 In contrast with the start_day , you can use end_day to take the ceiling midnight of the largest Timestamp as the end of the bins and drop the bins not containing data: >>> ts . resample ( '17min' , origin = 'end_day' ) . sum () 2000 - 10 - 01 23 : 38 : 00 3 2000 - 10 - 01 23 : 55 : 00 15 2000 - 10 - 02 00 : 12 : 00 45 2000 - 10 - 02 00 : 29 : 00 45 Freq : 17 min , dtype : int64 method first ( offset ) </> Select initial periods of time series data based on a date offset. .. deprecated:: 2.1 :meth: .first is deprecated and will be removed in a future version. Please create a mask and filter using .loc instead. For a DataFrame with a sorted DatetimeIndex, this function can select the first few rows based on a date offset. Parameters offset (str, DateOffset or dateutil.relativedelta) \u2014 The offset length of the data that will be selected. For instance,'1ME' will display all the rows having their index within the first month. Returns (Series or DataFrame) A subset of the caller. Raises TypeError \u2014 If the index is not a :class: DatetimeIndex See Also last : Select final periods of time series based on a date offset.at_time : Select values at a particular time of the day. between_time : Select values between particular times of the day. Examples >>> i = pd . date_range ( '2018-04-09' , periods = 4 , freq = '2D' ) >>> ts = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 , 4 ]}, index = i ) >>> ts A 2018 - 04 - 09 1 2018 - 04 - 11 2 2018 - 04 - 13 3 2018 - 04 - 15 4 Get the rows for the first 3 days: >>> ts . first ( '3D' ) A 2018 - 04 - 09 1 2018 - 04 - 11 2 Notice the data for 3 first calendar days were returned, not the first 3 days observed in the dataset, and therefore data for 2018-04-13 was not returned. method last ( offset ) </> Select final periods of time series data based on a date offset. .. deprecated:: 2.1 :meth: .last is deprecated and will be removed in a future version. Please create a mask and filter using .loc instead. For a DataFrame with a sorted DatetimeIndex, this function selects the last few rows based on a date offset. Parameters offset (str, DateOffset, dateutil.relativedelta) \u2014 The offset length of the data that will be selected. For instance,'3D' will display all the rows having their index within the last 3 days. Returns (Series or DataFrame) A subset of the caller. Raises TypeError \u2014 If the index is not a :class: DatetimeIndex See Also first : Select initial periods of time series based on a date offset.at_time : Select values at a particular time of the day. between_time : Select values between particular times of the day. Notes .. deprecated:: 2.1.0 Please create a mask and filter using .loc instead Examples >>> i = pd . date_range ( '2018-04-09' , periods = 4 , freq = '2D' ) >>> ts = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 , 4 ]}, index = i ) >>> ts A 2018 - 04 - 09 1 2018 - 04 - 11 2 2018 - 04 - 13 3 2018 - 04 - 15 4 Get the rows for the last 3 days: >>> ts . last ( '3D' ) # doctest: +SKIP A 2018 - 04 - 13 3 2018 - 04 - 15 4 Notice the data for 3 last calendar days were returned, not the last 3 observed days in the dataset, and therefore data for 2018-04-11 was not returned. method rank ( axis=0 , method='average' , numeric_only=False , na_option='keep' , ascending=True , pct=False ) </> Compute numerical data ranks (1 through n) along axis. By default, equal values are assigned a rank that is the average of the ranks of those values. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Index to direct ranking.For Series this parameter is unused and defaults to 0. method ({'average', 'min', 'max', 'first', 'dense'}, default 'average') \u2014 How to rank the group of records that have the same value (i.e. ties): average: average rank of the group min: lowest rank in the group max: highest rank in the group first: ranks assigned in order they appear in the array dense: like 'min', but rank always increases by 1 between groups. numeric_only (bool, default False) \u2014 For DataFrame objects, rank only numeric columns if set to True. .. versionchanged:: 2.0.0 The default value of numeric_only is now False . na_option ({'keep', 'top', 'bottom'}, default 'keep') \u2014 How to rank NaN values: keep: assign NaN rank to NaN values top: assign lowest rank to NaN values bottom: assign highest rank to NaN values ascending (bool, default True) \u2014 Whether or not the elements should be ranked in ascending order. pct (bool, default False) \u2014 Whether or not to display the returned rankings in percentileform. Returns (same type as caller) Return a Series or DataFrame with data ranks as values. See Also core.groupby.DataFrameGroupBy.rank : Rank of values within each group.core.groupby.SeriesGroupBy.rank : Rank of values within each group. Examples >>> df = pd . DataFrame ( data = { 'Animal' : [ 'cat' , 'penguin' , 'dog' , ... 'spider' , 'snake' ], ... 'Number_legs' : [ 4 , 2 , 4 , 8 , np . nan ]}) >>> df Animal Number_legs 0 cat 4.0 1 penguin 2.0 2 dog 4.0 3 spider 8.0 4 snake NaN Ties are assigned the mean of the ranks (by default) for the group. >>> s = pd . Series ( range ( 5 ), index = list ( \"abcde\" )) >>> s [ \"d\" ] = s [ \"b\" ] >>> s . rank () a 1.0 b 2.5 c 4.0 d 2.5 e 5.0 dtype : float64 The following example shows how the method behaves with the above parameters: default_rank: this is the default behaviour obtained without using any parameter. max_rank: setting method = 'max' the records that have the same values are ranked using the highest rank (e.g.: since 'cat' and 'dog' are both in the 2nd and 3rd position, rank 3 is assigned.) NA_bottom: choosing na_option = 'bottom' , if there are records with NaN values they are placed at the bottom of the ranking. pct_rank: when setting pct = True , the ranking is expressed as percentile rank. >>> df [ 'default_rank' ] = df [ 'Number_legs' ] . rank () >>> df [ 'max_rank' ] = df [ 'Number_legs' ] . rank ( method = 'max' ) >>> df [ 'NA_bottom' ] = df [ 'Number_legs' ] . rank ( na_option = 'bottom' ) >>> df [ 'pct_rank' ] = df [ 'Number_legs' ] . rank ( pct = True ) >>> df Animal Number_legs default_rank max_rank NA_bottom pct_rank 0 cat 4.0 2.5 3.0 2.5 0.625 1 penguin 2.0 1.0 1.0 1.0 0.250 2 dog 4.0 2.5 3.0 2.5 0.625 3 spider 8.0 4.0 4.0 4.0 1.000 4 snake NaN NaN NaN 5.0 NaN method align ( other , join='outer' , axis=None , level=None , copy=None , fill_value=None , method=<no_default> , limit=<no_default> , fill_axis=<no_default> , broadcast_axis=<no_default> ) </> Align two objects on their axes with the specified join method. Join method is specified for each axis Index. Parameters join ({'outer', 'inner', 'left', 'right'}, default 'outer') \u2014 Type of alignment to be performed. left: use only keys from left frame, preserve key order. right: use only keys from right frame, preserve key order. outer: use union of keys from both frames, sort keys lexicographically. inner: use intersection of keys from both frames, preserve the order of the left keys. axis (allowed axis of the other object, default None) \u2014 Align on index (0), columns (1), or both (None). level (int or level name, default None) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. copy (bool, default True) \u2014 Always returns new objects. If copy=False and no reindexing isrequired then original objects are returned. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` fill_value (scalar, default np.nan) \u2014 Value to use for missing values. Defaults to NaN, but can be any\"compatible\" value. method ({'backfill', 'bfill', 'pad', 'ffill', None}, default None) \u2014 Method to use for filling holes in reindexed Series: pad / ffill: propagate last valid observation forward to next valid. backfill / bfill: use NEXT valid observation to fill gap. .. deprecated:: 2.1 limit (int, default None) \u2014 If method is specified, this is the maximum number of consecutiveNaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None. .. deprecated:: 2.1 fill_axis ({0 or 'index'} for Series, {0 or 'index', 1 or 'columns'} for DataFrame, default 0) \u2014 Filling axis, method and limit. .. deprecated:: 2.1 broadcast_axis ({0 or 'index'} for Series, {0 or 'index', 1 or 'columns'} for DataFrame, default None) \u2014 Broadcast values along this axis, if aligning two objects ofdifferent dimensions. .. deprecated:: 2.1 Returns (tuple of (Series/DataFrame, type of other)) Aligned objects. Examples >>> df = pd . DataFrame ( ... [[ 1 , 2 , 3 , 4 ], [ 6 , 7 , 8 , 9 ]], columns = [ \"D\" , \"B\" , \"E\" , \"A\" ], index = [ 1 , 2 ] ... ) >>> other = pd . DataFrame ( ... [[ 10 , 20 , 30 , 40 ], [ 60 , 70 , 80 , 90 ], [ 600 , 700 , 800 , 900 ]], ... columns = [ \"A\" , \"B\" , \"C\" , \"D\" ], ... index = [ 2 , 3 , 4 ], ... ) >>> df D B E A 1 1 2 3 4 2 6 7 8 9 >>> other A B C D 2 10 20 30 40 3 60 70 80 90 4 600 700 800 900 Align on columns: >>> left , right = df . align ( other , join = \"outer\" , axis = 1 ) >>> left A B C D E 1 4 2 NaN 1 3 2 9 7 NaN 6 8 >>> right A B C D E 2 10 20 30 40 NaN 3 60 70 80 90 NaN 4 600 700 800 900 NaN We can also align on the index: >>> left , right = df . align ( other , join = \"outer\" , axis = 0 ) >>> left D B E A 1 1.0 2.0 3.0 4.0 2 6.0 7.0 8.0 9.0 3 NaN NaN NaN NaN 4 NaN NaN NaN NaN >>> right A B C D 1 NaN NaN NaN NaN 2 10.0 20.0 30.0 40.0 3 60.0 70.0 80.0 90.0 4 600.0 700.0 800.0 900.0 Finally, the default axis=None will align on both index and columns: >>> left , right = df . align ( other , join = \"outer\" , axis = None ) >>> left A B C D E 1 4.0 2.0 NaN 1.0 3.0 2 9.0 7.0 NaN 6.0 8.0 3 NaN NaN NaN NaN NaN 4 NaN NaN NaN NaN NaN >>> right A B C D E 1 NaN NaN NaN NaN NaN 2 10.0 20.0 30.0 40.0 NaN 3 60.0 70.0 80.0 90.0 NaN 4 600.0 700.0 800.0 900.0 NaN method where ( cond , other=nan , inplace=False , axis=None , level=None ) </> Replace values where the condition is False. Parameters cond (bool Series/DataFrame, array-like, or callable) \u2014 Where cond is True, keep the original value. WhereFalse, replace with corresponding value from other . If cond is callable, it is computed on the Series/DataFrame and should return boolean Series/DataFrame or array. The callable must not change input Series/DataFrame (though pandas doesn't check it). other (scalar, Series/DataFrame, or callable) \u2014 Entries where cond is False are replaced withcorresponding value from other . If other is callable, it is computed on the Series/DataFrame and should return scalar or Series/DataFrame. The callable must not change input Series/DataFrame (though pandas doesn't check it). If not specified, entries will be filled with the corresponding NULL value ( np.nan for numpy dtypes, pd.NA for extension dtypes). inplace (bool, default False) \u2014 Whether to perform the operation in place on the data. axis (int, default None) \u2014 Alignment axis if needed. For Series this parameter isunused and defaults to 0. level (int, default None) \u2014 Alignment level if needed. See Also :func: DataFrame.mask : Return an object of same shape as self. Notes The where method is an application of the if-then idiom. For each element in the calling DataFrame, if cond is True the element is used; otherwise the corresponding element from the DataFrame other is used. If the axis of other does not align with axis of cond Series/DataFrame, the misaligned index positions will be filled with False. The signature for :func: DataFrame.where differs from :func: numpy.where . Roughly df1.where(m, df2) is equivalent to np.where(m, df1, df2) . For further details and examples see the where documentation in :ref: indexing <indexing.where_mask> . The dtype of the object takes precedence. The fill value is casted to the object's dtype, if this can be done losslessly. Examples >>> s = pd . Series ( range ( 5 )) >>> s . where ( s > 0 ) 0 NaN 1 1.0 2 2.0 3 3.0 4 4.0 dtype : float64 >>> s . mask ( s > 0 ) 0 0.0 1 NaN 2 NaN 3 NaN 4 NaN dtype : float64 >>> s = pd . Series ( range ( 5 )) >>> t = pd . Series ([ True , False ]) >>> s . where ( t , 99 ) 0 0 1 99 2 99 3 99 4 99 dtype : int64 >>> s . mask ( t , 99 ) 0 99 1 1 2 99 3 99 4 99 dtype : int64 >>> s . where ( s > 1 , 10 ) 0 10 1 10 2 2 3 3 4 4 dtype : int64 >>> s . mask ( s > 1 , 10 ) 0 0 1 1 2 10 3 10 4 10 dtype : int64 >>> df = pd . DataFrame ( np . arange ( 10 ) . reshape ( - 1 , 2 ), columns = [ 'A' , 'B' ]) >>> df A B 0 0 1 1 2 3 2 4 5 3 6 7 4 8 9 >>> m = df % 3 == 0 >>> df . where ( m , - df ) A B 0 0 - 1 1 - 2 3 2 - 4 - 5 3 6 - 7 4 - 8 9 >>> df . where ( m , - df ) == np . where ( m , df , - df ) A B 0 True True 1 True True 2 True True 3 True True 4 True True >>> df . where ( m , - df ) == df . mask ( ~ m , - df ) A B 0 True True 1 True True 2 True True 3 True True 4 True True method mask ( cond , other=<no_default> , inplace=False , axis=None , level=None ) </> Replace values where the condition is True. Parameters cond (bool Series/DataFrame, array-like, or callable) \u2014 Where cond is False, keep the original value. WhereTrue, replace with corresponding value from other . If cond is callable, it is computed on the Series/DataFrame and should return boolean Series/DataFrame or array. The callable must not change input Series/DataFrame (though pandas doesn't check it). other (scalar, Series/DataFrame, or callable) \u2014 Entries where cond is True are replaced withcorresponding value from other . If other is callable, it is computed on the Series/DataFrame and should return scalar or Series/DataFrame. The callable must not change input Series/DataFrame (though pandas doesn't check it). If not specified, entries will be filled with the corresponding NULL value ( np.nan for numpy dtypes, pd.NA for extension dtypes). inplace (bool, default False) \u2014 Whether to perform the operation in place on the data. axis (int, default None) \u2014 Alignment axis if needed. For Series this parameter isunused and defaults to 0. level (int, default None) \u2014 Alignment level if needed. See Also :func: DataFrame.where : Return an object of same shape as self. Notes The mask method is an application of the if-then idiom. For each element in the calling DataFrame, if cond is False the element is used; otherwise the corresponding element from the DataFrame other is used. If the axis of other does not align with axis of cond Series/DataFrame, the misaligned index positions will be filled with True. The signature for :func: DataFrame.where differs from :func: numpy.where . Roughly df1.where(m, df2) is equivalent to np.where(m, df1, df2) . For further details and examples see the mask documentation in :ref: indexing <indexing.where_mask> . The dtype of the object takes precedence. The fill value is casted to the object's dtype, if this can be done losslessly. Examples >>> s = pd . Series ( range ( 5 )) >>> s . where ( s > 0 ) 0 NaN 1 1.0 2 2.0 3 3.0 4 4.0 dtype : float64 >>> s . mask ( s > 0 ) 0 0.0 1 NaN 2 NaN 3 NaN 4 NaN dtype : float64 >>> s = pd . Series ( range ( 5 )) >>> t = pd . Series ([ True , False ]) >>> s . where ( t , 99 ) 0 0 1 99 2 99 3 99 4 99 dtype : int64 >>> s . mask ( t , 99 ) 0 99 1 1 2 99 3 99 4 99 dtype : int64 >>> s . where ( s > 1 , 10 ) 0 10 1 10 2 2 3 3 4 4 dtype : int64 >>> s . mask ( s > 1 , 10 ) 0 0 1 1 2 10 3 10 4 10 dtype : int64 >>> df = pd . DataFrame ( np . arange ( 10 ) . reshape ( - 1 , 2 ), columns = [ 'A' , 'B' ]) >>> df A B 0 0 1 1 2 3 2 4 5 3 6 7 4 8 9 >>> m = df % 3 == 0 >>> df . where ( m , - df ) A B 0 0 - 1 1 - 2 3 2 - 4 - 5 3 6 - 7 4 - 8 9 >>> df . where ( m , - df ) == np . where ( m , df , - df ) A B 0 True True 1 True True 2 True True 3 True True 4 True True >>> df . where ( m , - df ) == df . mask ( ~ m , - df ) A B 0 True True 1 True True 2 True True 3 True True 4 True True method truncate ( before=None , after=None , axis=None , copy=None ) </> Truncate a Series or DataFrame before and after some index value. This is a useful shorthand for boolean indexing based on index values above or below certain thresholds. Parameters before (date, str, int) \u2014 Truncate all rows before this index value. after (date, str, int) \u2014 Truncate all rows after this index value. axis ({0 or 'index', 1 or 'columns'}, optional) \u2014 Axis to truncate. Truncates the index (rows) by default.For Series this parameter is unused and defaults to 0. copy (bool, default is True,) \u2014 Return a copy of the truncated section. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` Returns (type of caller) The truncated Series or DataFrame. See Also DataFrame.loc : Select a subset of a DataFrame by label.DataFrame.iloc : Select a subset of a DataFrame by position. Notes If the index being truncated contains only datetime values, before and after may be specified as strings instead of Timestamps. Examples >>> df = pd . DataFrame ({ 'A' : [ 'a' , 'b' , 'c' , 'd' , 'e' ], ... 'B' : [ 'f' , 'g' , 'h' , 'i' , 'j' ], ... 'C' : [ 'k' , 'l' , 'm' , 'n' , 'o' ]}, ... index = [ 1 , 2 , 3 , 4 , 5 ]) >>> df A B C 1 a f k 2 b g l 3 c h m 4 d i n 5 e j o >>> df . truncate ( before = 2 , after = 4 ) A B C 2 b g l 3 c h m 4 d i n The columns of a DataFrame can be truncated. >>> df . truncate ( before = \"A\" , after = \"B\" , axis = \"columns\" ) A B 1 a f 2 b g 3 c h 4 d i 5 e j For Series, only rows can be truncated. >>> df [ 'A' ] . truncate ( before = 2 , after = 4 ) 2 b 3 c 4 d Name : A , dtype : object The index values in truncate can be datetimes or string dates. >>> dates = pd . date_range ( '2016-01-01' , '2016-02-01' , freq = 's' ) >>> df = pd . DataFrame ( index = dates , data = { 'A' : 1 }) >>> df . tail () A 2016 - 01 - 31 23 : 59 : 56 1 2016 - 01 - 31 23 : 59 : 57 1 2016 - 01 - 31 23 : 59 : 58 1 2016 - 01 - 31 23 : 59 : 59 1 2016 - 02 - 01 00 : 00 : 00 1 >>> df . truncate ( before = pd . Timestamp ( '2016-01-05' ), ... after = pd . Timestamp ( '2016-01-10' )) . tail () A 2016 - 01 - 09 23 : 59 : 56 1 2016 - 01 - 09 23 : 59 : 57 1 2016 - 01 - 09 23 : 59 : 58 1 2016 - 01 - 09 23 : 59 : 59 1 2016 - 01 - 10 00 : 00 : 00 1 Because the index is a DatetimeIndex containing only dates, we can specify before and after as strings. They will be coerced to Timestamps before truncation. >>> df . truncate ( '2016-01-05' , '2016-01-10' ) . tail () A 2016 - 01 - 09 23 : 59 : 56 1 2016 - 01 - 09 23 : 59 : 57 1 2016 - 01 - 09 23 : 59 : 58 1 2016 - 01 - 09 23 : 59 : 59 1 2016 - 01 - 10 00 : 00 : 00 1 Note that truncate assumes a 0 value for any unspecified time component (midnight). This differs from partial string slicing, which returns any partially matching dates. >>> df . loc [ '2016-01-05' : '2016-01-10' , :] . tail () A 2016 - 01 - 10 23 : 59 : 55 1 2016 - 01 - 10 23 : 59 : 56 1 2016 - 01 - 10 23 : 59 : 57 1 2016 - 01 - 10 23 : 59 : 58 1 2016 - 01 - 10 23 : 59 : 59 1 method tz_convert ( tz , axis=0 , level=None , copy=None ) </> Convert tz-aware axis to target time zone. Parameters tz (str or tzinfo object or None) \u2014 Target time zone. Passing None will convert toUTC and remove the timezone information. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to convert level (int, str, default None) \u2014 If axis is a MultiIndex, convert a specific level. Otherwisemust be None. copy (bool, default True) \u2014 Also make a copy of the underlying data. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` Returns (Series/DataFrame) Object with time zone converted axis. Raises TypeError \u2014 If the axis is tz-naive. Examples Change to another time zone: >>> s = pd . Series ( ... [ 1 ], ... index = pd . DatetimeIndex ([ '2018-09-15 01:30:00+02:00' ]), ... ) >>> s . tz_convert ( 'Asia/Shanghai' ) 2018 - 09 - 15 07 : 30 : 00 + 08 : 00 1 dtype : int64 Pass None to convert to UTC and get a tz-naive index: >>> s = pd . Series ([ 1 ], ... index = pd . DatetimeIndex ([ '2018-09-15 01:30:00+02:00' ])) >>> s . tz_convert ( None ) 2018 - 09 - 14 23 : 30 : 00 1 dtype : int64 method tz_localize ( tz , axis=0 , level=None , copy=None , ambiguous='raise' , nonexistent='raise' ) </> Localize tz-naive index of a Series or DataFrame to target time zone. This operation localizes the Index. To localize the values in a timezone-naive Series, use :meth: Series.dt.tz_localize . Parameters tz (str or tzinfo or None) \u2014 Time zone to localize. Passing None will remove thetime zone information and preserve local time. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to localize level (int, str, default None) \u2014 If axis ia a MultiIndex, localize a specific level. Otherwisemust be None. copy (bool, default True) \u2014 Also make a copy of the underlying data. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` ambiguous ('infer', bool-ndarray, 'NaT', default 'raise') \u2014 When clocks moved backward due to DST, ambiguous times may arise.For example in Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a situation, the ambiguous parameter dictates how ambiguous times should be handled. 'infer' will attempt to infer fall dst-transition hours based on order bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times) 'NaT' will return NaT where there are ambiguous times 'raise' will raise an AmbiguousTimeError if there are ambiguous times. nonexistent (str, default 'raise') \u2014 A nonexistent time does not exist in a particular timezonewhere clocks moved forward due to DST. Valid values are: 'shift_forward' will shift the nonexistent time forward to the closest existing time 'shift_backward' will shift the nonexistent time backward to the closest existing time 'NaT' will return NaT where there are nonexistent times timedelta objects will shift nonexistent times by the timedelta 'raise' will raise an NonExistentTimeError if there are nonexistent times. Returns (Series/DataFrame) Same type as the input. Raises TypeError \u2014 If the TimeSeries is tz-aware and tz is not None. Examples Localize local times: >>> s = pd . Series ( ... [ 1 ], ... index = pd . DatetimeIndex ([ '2018-09-15 01:30:00' ]), ... ) >>> s . tz_localize ( 'CET' ) 2018 - 09 - 15 01 : 30 : 00 + 02 : 00 1 dtype : int64 Pass None to convert to tz-naive index and preserve local time: >>> s = pd . Series ([ 1 ], ... index = pd . DatetimeIndex ([ '2018-09-15 01:30:00+02:00' ])) >>> s . tz_localize ( None ) 2018 - 09 - 15 01 : 30 : 00 1 dtype : int64 Be careful with DST changes. When there is sequential data, pandas can infer the DST time: >>> s = pd . Series ( range ( 7 ), ... index = pd . DatetimeIndex ([ '2018-10-28 01:30:00' , ... '2018-10-28 02:00:00' , ... '2018-10-28 02:30:00' , ... '2018-10-28 02:00:00' , ... '2018-10-28 02:30:00' , ... '2018-10-28 03:00:00' , ... '2018-10-28 03:30:00' ])) >>> s . tz_localize ( 'CET' , ambiguous = 'infer' ) 2018 - 10 - 28 01 : 30 : 00 + 02 : 00 0 2018 - 10 - 28 02 : 00 : 00 + 02 : 00 1 2018 - 10 - 28 02 : 30 : 00 + 02 : 00 2 2018 - 10 - 28 02 : 00 : 00 + 01 : 00 3 2018 - 10 - 28 02 : 30 : 00 + 01 : 00 4 2018 - 10 - 28 03 : 00 : 00 + 01 : 00 5 2018 - 10 - 28 03 : 30 : 00 + 01 : 00 6 dtype : int64 In some cases, inferring the DST is impossible. In such cases, you can pass an ndarray to the ambiguous parameter to set the DST explicitly >>> s = pd . Series ( range ( 3 ), ... index = pd . DatetimeIndex ([ '2018-10-28 01:20:00' , ... '2018-10-28 02:36:00' , ... '2018-10-28 03:46:00' ])) >>> s . tz_localize ( 'CET' , ambiguous = np . array ([ True , True , False ])) 2018 - 10 - 28 01 : 20 : 00 + 02 : 00 0 2018 - 10 - 28 02 : 36 : 00 + 02 : 00 1 2018 - 10 - 28 03 : 46 : 00 + 01 : 00 2 dtype : int64 If the DST transition causes nonexistent times, you can shift these dates forward or backward with a timedelta object or 'shift_forward' or 'shift_backward' . >>> s = pd . Series ( range ( 2 ), ... index = pd . DatetimeIndex ([ '2015-03-29 02:30:00' , ... '2015-03-29 03:30:00' ])) >>> s . tz_localize ( 'Europe/Warsaw' , nonexistent = 'shift_forward' ) 2015 - 03 - 29 03 : 00 : 00 + 02 : 00 0 2015 - 03 - 29 03 : 30 : 00 + 02 : 00 1 dtype : int64 >>> s . tz_localize ( 'Europe/Warsaw' , nonexistent = 'shift_backward' ) 2015 - 03 - 29 01 : 59 : 59.999999999 + 01 : 00 0 2015 - 03 - 29 03 : 30 : 00 + 02 : 00 1 dtype : int64 >>> s . tz_localize ( 'Europe/Warsaw' , nonexistent = pd . Timedelta ( '1h' )) 2015 - 03 - 29 03 : 30 : 00 + 02 : 00 0 2015 - 03 - 29 03 : 30 : 00 + 02 : 00 1 dtype : int64 method describe ( percentiles=None , include=None , exclude=None ) </> Generate descriptive statistics. Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset's distribution, excluding NaN values. Analyzes both numeric and object series, as well as DataFrame column sets of mixed data types. The output will vary depending on what is provided. Refer to the notes below for more detail. Parameters percentiles (list-like of numbers, optional) \u2014 The percentiles to include in the output. All shouldfall between 0 and 1. The default is [.25, .5, .75] , which returns the 25th, 50th, and 75th percentiles. include ('all', list-like of dtypes or None (default), optional) \u2014 A white list of data types to include in the result. Ignoredfor Series . Here are the options: 'all' : All columns of the input will be included in the output. A list-like of dtypes : Limits the results to the provided data types. To limit the result to numeric types submit numpy.number . To limit it instead to object columns submit the numpy.object data type. Strings can also be used in the style of select_dtypes (e.g. df.describe(include=['O']) ). To select pandas categorical columns, use 'category' None (default) : The result will include all numeric columns. exclude (list-like of dtypes or None (default), optional,) \u2014 A black list of data types to omit from the result. Ignoredfor Series . Here are the options: A list-like of dtypes : Excludes the provided data types from the result. To exclude numeric types submit numpy.number . To exclude object columns submit the data type numpy.object . Strings can also be used in the style of select_dtypes (e.g. df.describe(exclude=['O']) ). To exclude pandas categorical columns, use 'category' None (default) : The result will exclude nothing. Returns (Series or DataFrame) Summary statistics of the Series or Dataframe provided. See Also DataFrame.count: Count number of non-NA/null observations.DataFrame.max: Maximum of the values in the object. DataFrame.min: Minimum of the values in the object. DataFrame.mean: Mean of the values. DataFrame.std: Standard deviation of the observations. DataFrame.select_dtypes: Subset of a DataFrame including/excluding columns based on their dtype. Notes For numeric data, the result's index will include count , mean , std , min , max as well as lower, 50 and upper percentiles. By default the lower percentile is 25 and the upper percentile is 75 . The 50 percentile is the same as the median. For object data (e.g. strings or timestamps), the result's index will include count , unique , top , and freq . The top is the most common value. The freq is the most common value's frequency. Timestamps also include the first and last items. If multiple object values have the highest count, then the count and top results will be arbitrarily chosen from among those with the highest count. For mixed data types provided via a DataFrame , the default is to return only an analysis of numeric columns. If the dataframe consists only of object and categorical data without any numeric columns, the default is to return an analysis of both the object and categorical columns. If include='all' is provided as an option, the result will include a union of attributes of each type. The include and exclude parameters can be used to limit which columns in a DataFrame are analyzed for the output. The parameters are ignored when analyzing a Series . Examples Describing a numeric Series . >>> s = pd . Series ([ 1 , 2 , 3 ]) >>> s . describe () count 3.0 mean 2.0 std 1.0 min 1.0 25 % 1.5 50 % 2.0 75 % 2.5 max 3.0 dtype : float64 Describing a categorical Series . >>> s = pd . Series ([ 'a' , 'a' , 'b' , 'c' ]) >>> s . describe () count 4 unique 3 top a freq 2 dtype : object Describing a timestamp Series . >>> s = pd . Series ([ ... np . datetime64 ( \"2000-01-01\" ), ... np . datetime64 ( \"2010-01-01\" ), ... np . datetime64 ( \"2010-01-01\" ) ... ]) >>> s . describe () count 3 mean 2006 - 09 - 01 08 : 00 : 00 min 2000 - 01 - 01 00 : 00 : 00 25 % 2004 - 12 - 31 12 : 00 : 00 50 % 2010 - 01 - 01 00 : 00 : 00 75 % 2010 - 01 - 01 00 : 00 : 00 max 2010 - 01 - 01 00 : 00 : 00 dtype : object Describing a DataFrame . By default only numeric fields are returned. >>> df = pd . DataFrame ({ 'categorical' : pd . Categorical ([ 'd' , 'e' , 'f' ]), ... 'numeric' : [ 1 , 2 , 3 ], ... 'object' : [ 'a' , 'b' , 'c' ] ... }) >>> df . describe () numeric count 3.0 mean 2.0 std 1.0 min 1.0 25 % 1.5 50 % 2.0 75 % 2.5 max 3.0 Describing all columns of a DataFrame regardless of data type. >>> df . describe ( include = 'all' ) # doctest: +SKIP categorical numeric object count 3 3.0 3 unique 3 NaN 3 top f NaN a freq 1 NaN 1 mean NaN 2.0 NaN std NaN 1.0 NaN min NaN 1.0 NaN 25 % NaN 1.5 NaN 50 % NaN 2.0 NaN 75 % NaN 2.5 NaN max NaN 3.0 NaN Describing a column from a DataFrame by accessing it as an attribute. >>> df . numeric . describe () count 3.0 mean 2.0 std 1.0 min 1.0 25 % 1.5 50 % 2.0 75 % 2.5 max 3.0 Name : numeric , dtype : float64 Including only numeric columns in a DataFrame description. >>> df . describe ( include = [ np . number ]) numeric count 3.0 mean 2.0 std 1.0 min 1.0 25 % 1.5 50 % 2.0 75 % 2.5 max 3.0 Including only string columns in a DataFrame description. >>> df . describe ( include = [ object ]) # doctest: +SKIP object count 3 unique 3 top a freq 1 Including only categorical columns from a DataFrame description. >>> df . describe ( include = [ 'category' ]) categorical count 3 unique 3 top d freq 1 Excluding numeric columns from a DataFrame description. >>> df . describe ( exclude = [ np . number ]) # doctest: +SKIP categorical object count 3 3 unique 3 3 top f a freq 1 1 Excluding object columns from a DataFrame description. >>> df . describe ( exclude = [ object ]) # doctest: +SKIP categorical numeric count 3 3.0 unique 3 NaN top f NaN freq 1 NaN mean NaN 2.0 std NaN 1.0 min NaN 1.0 25 % NaN 1.5 50 % NaN 2.0 75 % NaN 2.5 max NaN 3.0 method pct_change ( periods=1 , fill_method=<no_default> , limit=<no_default> , freq=None , **kwargs ) </> Fractional change between the current and a prior element. Computes the fractional change from the immediately previous row by default. This is useful in comparing the fraction of change in a time series of elements. .. note:: Despite the name of this method, it calculates fractional change (also known as per unit change or relative change) and not percentage change. If you need the percentage change, multiply these values by 100. Parameters periods (int, default 1) \u2014 Periods to shift for forming percent change. fill_method ({'backfill', 'bfill', 'pad', 'ffill', None}, default 'pad') \u2014 How to handle NAs before computing percent changes. .. deprecated:: 2.1 All options of fill_method are deprecated except fill_method=None . limit (int, default None) \u2014 The number of consecutive NAs to fill before stopping. .. deprecated:: 2.1 freq (DateOffset, timedelta, or str, optional) \u2014 Increment to use from time series API (e.g. 'ME' or BDay()). **kwargs \u2014 Additional keyword arguments are passed into DataFrame.shift or Series.shift . Returns (Series or DataFrame) The same type as the calling object. See Also Series.diff : Compute the difference of two elements in a Series.DataFrame.diff : Compute the difference of two elements in a DataFrame. Series.shift : Shift the index by some number of periods. DataFrame.shift : Shift the index by some number of periods. Examples Series >>> s = pd . Series ([ 90 , 91 , 85 ]) >>> s 0 90 1 91 2 85 dtype : int64 >>> s . pct_change () 0 NaN 1 0.011111 2 - 0.065934 dtype : float64 >>> s . pct_change ( periods = 2 ) 0 NaN 1 NaN 2 - 0.055556 dtype : float64 See the percentage change in a Series where filling NAs with last valid observation forward to next valid. >>> s = pd . Series ([ 90 , 91 , None , 85 ]) >>> s 0 90.0 1 91.0 2 NaN 3 85.0 dtype : float64 >>> s . ffill () . pct_change () 0 NaN 1 0.011111 2 0.000000 3 - 0.065934 dtype : float64 DataFrame Percentage change in French franc, Deutsche Mark, and Italian lira from 1980-01-01 to 1980-03-01. >>> df = pd . DataFrame ({ ... 'FR' : [ 4.0405 , 4.0963 , 4.3149 ], ... 'GR' : [ 1.7246 , 1.7482 , 1.8519 ], ... 'IT' : [ 804.74 , 810.01 , 860.13 ]}, ... index = [ '1980-01-01' , '1980-02-01' , '1980-03-01' ]) >>> df FR GR IT 1980 - 01 - 01 4.0405 1.7246 804.74 1980 - 02 - 01 4.0963 1.7482 810.01 1980 - 03 - 01 4.3149 1.8519 860.13 >>> df . pct_change () FR GR IT 1980 - 01 - 01 NaN NaN NaN 1980 - 02 - 01 0.013810 0.013684 0.006549 1980 - 03 - 01 0.053365 0.059318 0.061876 Percentage of change in GOOG and APPL stock volume. Shows computing the percentage change between columns. >>> df = pd . DataFrame ({ ... '2016' : [ 1769950 , 30586265 ], ... '2015' : [ 1500923 , 40912316 ], ... '2014' : [ 1371819 , 41403351 ]}, ... index = [ 'GOOG' , 'APPL' ]) >>> df 2016 2015 2014 GOOG 1769950 1500923 1371819 APPL 30586265 40912316 41403351 >>> df . pct_change ( axis = 'columns' , periods =- 1 ) 2016 2015 2014 GOOG 0.179241 0.094112 NaN APPL - 0.252395 - 0.011860 NaN method rolling ( window , min_periods=None , center=False , win_type=None , on=None , axis=<no_default> , closed=None , step=None , method='single' ) </> Provide rolling window calculations. Parameters window (int, timedelta, str, offset, or BaseIndexer subclass) \u2014 Size of the moving window. If an integer, the fixed number of observations used for each window. If a timedelta, str, or offset, the time period of each window. Each window will be a variable sized based on the observations included in the time-period. This is only valid for datetimelike indexes. To learn more about the offsets & frequency strings, please see this link <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases> __. If a BaseIndexer subclass, the window boundaries based on the defined get_window_bounds method. Additional rolling keyword arguments, namely min_periods , center , closed and step will be passed to get_window_bounds . min_periods (int, default None) \u2014 Minimum number of observations in window required to have a value;otherwise, result is np.nan . For a window that is specified by an offset, min_periods will default to 1. For a window that is specified by an integer, min_periods will default to the size of the window. center (bool, default False) \u2014 If False, set the window labels as the right edge of the window index. If True, set the window labels as the center of the window index. win_type (str, default None) \u2014 If None , all points are evenly weighted. If a string, it must be a valid scipy.signal window function <https://docs.scipy.org/doc/scipy/reference/signal.windows.html#module-scipy.signal.windows> __. Certain Scipy window types require additional parameters to be passed in the aggregation function. The additional parameters must match the keywords specified in the Scipy window type method signature. on (str, optional) \u2014 For a DataFrame, a column label or Index level on whichto calculate the rolling window, rather than the DataFrame's index. Provided integer column is ignored and excluded from result since an integer index is not used to calculate the rolling window. axis (int or str, default 0) \u2014 If 0 or 'index' , roll across the rows. If 1 or 'columns' , roll across the columns. For Series this parameter is unused and defaults to 0. .. deprecated:: 2.1.0 The axis keyword is deprecated. For ``axis=1``, transpose the DataFrame first instead. closed (str, default None) \u2014 If 'right' , the first point in the window is excluded from calculations. If 'left' , the last point in the window is excluded from calculations. If 'both' , the no points in the window are excluded from calculations. If 'neither' , the first and last points in the window are excluded from calculations. Default None ( 'right' ). step (int, default None) \u2014 0 s r . method (str {'single', 'table'}, default 'single') \u2014 0 ) . ` . Returns (pandas.api.typing.Window or pandas.api.typing.Rolling) An instance of Window is returned if win_type is passed. Otherwise,an instance of Rolling is returned. See Also expanding : Provides expanding transformations.ewm : Provides exponential weighted functions. Notes See :ref: Windowing Operations <window.generic> for further usage details and examples. Examples >>> df = pd . DataFrame ({ 'B' : [ 0 , 1 , 2 , np . nan , 4 ]}) >>> df B 0 0.0 1 1.0 2 2.0 3 NaN 4 4.0 window Rolling sum with a window length of 2 observations. >>> df . rolling ( 2 ) . sum () B 0 NaN 1 1.0 2 3.0 3 NaN 4 NaN Rolling sum with a window span of 2 seconds. >>> df_time = pd . DataFrame ({ 'B' : [ 0 , 1 , 2 , np . nan , 4 ]}, ... index = [ pd . Timestamp ( '20130101 09:00:00' ), ... pd . Timestamp ( '20130101 09:00:02' ), ... pd . Timestamp ( '20130101 09:00:03' ), ... pd . Timestamp ( '20130101 09:00:05' ), ... pd . Timestamp ( '20130101 09:00:06' )]) >>> df_time B 2013 - 01 - 01 09 : 00 : 00 0.0 2013 - 01 - 01 09 : 00 : 02 1.0 2013 - 01 - 01 09 : 00 : 03 2.0 2013 - 01 - 01 09 : 00 : 05 NaN 2013 - 01 - 01 09 : 00 : 06 4.0 >>> df_time . rolling ( '2s' ) . sum () B 2013 - 01 - 01 09 : 00 : 00 0.0 2013 - 01 - 01 09 : 00 : 02 1.0 2013 - 01 - 01 09 : 00 : 03 3.0 2013 - 01 - 01 09 : 00 : 05 NaN 2013 - 01 - 01 09 : 00 : 06 4.0 Rolling sum with forward looking windows with 2 observations. >>> indexer = pd . api . indexers . FixedForwardWindowIndexer ( window_size = 2 ) >>> df . rolling ( window = indexer , min_periods = 1 ) . sum () B 0 1.0 1 3.0 2 2.0 3 4.0 4 4.0 min_periods Rolling sum with a window length of 2 observations, but only needs a minimum of 1 observation to calculate a value. >>> df . rolling ( 2 , min_periods = 1 ) . sum () B 0 0.0 1 1.0 2 3.0 3 2.0 4 4.0 center Rolling sum with the result assigned to the center of the window index. >>> df . rolling ( 3 , min_periods = 1 , center = True ) . sum () B 0 1.0 1 3.0 2 3.0 3 6.0 4 4.0 >>> df . rolling ( 3 , min_periods = 1 , center = False ) . sum () B 0 0.0 1 1.0 2 3.0 3 3.0 4 6.0 step Rolling sum with a window length of 2 observations, minimum of 1 observation to calculate a value, and a step of 2. >>> df . rolling ( 2 , min_periods = 1 , step = 2 ) . sum () B 0 0.0 2 3.0 4 4.0 win_type Rolling sum with a window length of 2, using the Scipy 'gaussian' window type. std is required in the aggregation function. >>> df . rolling ( 2 , win_type = 'gaussian' ) . sum ( std = 3 ) B 0 NaN 1 0.986207 2 2.958621 3 NaN 4 NaN on Rolling sum with a window length of 2 days. >>> df = pd . DataFrame ({ ... 'A' : [ pd . to_datetime ( '2020-01-01' ), ... pd . to_datetime ( '2020-01-01' ), ... pd . to_datetime ( '2020-01-02' ),], ... 'B' : [ 1 , 2 , 3 ], }, ... index = pd . date_range ( '2020' , periods = 3 )) >>> df A B 2020 - 01 - 01 2020 - 01 - 01 1 2020 - 01 - 02 2020 - 01 - 01 2 2020 - 01 - 03 2020 - 01 - 02 3 >>> df . rolling ( '2D' , on = 'A' ) . sum () A B 2020 - 01 - 01 2020 - 01 - 01 1.0 2020 - 01 - 02 2020 - 01 - 01 3.0 2020 - 01 - 03 2020 - 01 - 02 6.0 method expanding ( min_periods=1 , axis=<no_default> , method='single' ) </> Provide expanding window calculations. Parameters min_periods (int, default 1) \u2014 Minimum number of observations in window required to have a value;otherwise, result is np.nan . axis (int or str, default 0) \u2014 If 0 or 'index' , roll across the rows. If 1 or 'columns' , roll across the columns. For Series this parameter is unused and defaults to 0. method (str {'single', 'table'}, default 'single') \u2014 Execute the rolling operation per single column or row ( 'single' )or over the entire object ( 'table' ). This argument is only implemented when specifying engine='numba' in the method call. .. versionadded:: 1.3.0 See Also rolling : Provides rolling window calculations.ewm : Provides exponential weighted functions. Notes See :ref: Windowing Operations <window.expanding> for further usage details and examples. Examples >>> df = pd . DataFrame ({ \"B\" : [ 0 , 1 , 2 , np . nan , 4 ]}) >>> df B 0 0.0 1 1.0 2 2.0 3 NaN 4 4.0 min_periods Expanding sum with 1 vs 3 observations needed to calculate a value. >>> df . expanding ( 1 ) . sum () B 0 0.0 1 1.0 2 3.0 3 3.0 4 7.0 >>> df . expanding ( 3 ) . sum () B 0 NaN 1 NaN 2 3.0 3 3.0 4 7.0 method ewm ( com=None , span=None , halflife=None , alpha=None , min_periods=0 , adjust=True , ignore_na=False , axis=<no_default> , times=None , method='single' ) </> Provide exponentially weighted (EW) calculations. Exactly one of com , span , halflife , or alpha must be provided if times is not provided. If times is provided, halflife and one of com , span or alpha may be provided. Parameters com (float, optional) \u2014 Specify decay in terms of center of mass :math: \\alpha = 1 / (1 + com) , for :math: com \\geq 0 . span (float, optional) \u2014 Specify decay in terms of span :math: \\alpha = 2 / (span + 1) , for :math: span \\geq 1 . halflife (float, str, timedelta, optional) \u2014 Specify decay in terms of half-life :math: \\alpha = 1 - \\exp\\left(-\\ln(2) / halflife\\right) , for :math: halflife > 0 . If times is specified, a timedelta convertible unit over which an observation decays to half its value. Only applicable to mean() , and halflife value will not apply to the other functions. alpha (float, optional) \u2014 Specify smoothing factor :math: \\alpha directly :math: 0 < \\alpha \\leq 1 . min_periods (int, default 0) \u2014 Minimum number of observations in window required to have a value;otherwise, result is np.nan . adjust (bool, default True) \u2014 Divide by decaying adjustment factor in beginning periods to accountfor imbalance in relative weightings (viewing EWMA as a moving average). When adjust=True (default), the EW function is calculated using weights :math: w_i = (1 - \\alpha)^i . For example, the EW moving average of the series [:math: x_0, x_1, ..., x_t ] would be: .. math:: y_t = \\frac{x_t + (1 - \\alpha)x_{t-1} + (1 - \\alpha)^2 x_{t-2} + ... + (1 - \\alpha)^t x_0}{1 + (1 - \\alpha) + (1 - \\alpha)^2 + ... + (1 - \\alpha)^t} When adjust=False , the exponentially weighted function is calculated recursively: .. math:: \\begin{split} y_0 &= x_0\\ y_t &= (1 - \\alpha) y_{t-1} + \\alpha x_t, \\end{split} ignore_na (bool, default False) \u2014 Ignore missing values when calculating weights. When ignore_na=False (default), weights are based on absolute positions. For example, the weights of :math: x_0 and :math: x_2 used in calculating the final weighted average of [:math: x_0 , None, :math: x_2 ] are :math: (1-\\alpha)^2 and :math: 1 if adjust=True , and :math: (1-\\alpha)^2 and :math: \\alpha if adjust=False . When ignore_na=True , weights are based on relative positions. For example, the weights of :math: x_0 and :math: x_2 used in calculating the final weighted average of [:math: x_0 , None, :math: x_2 ] are :math: 1-\\alpha and :math: 1 if adjust=True , and :math: 1-\\alpha and :math: \\alpha if adjust=False . axis ({0, 1}, default 0) \u2014 If 0 or 'index' , calculate across the rows. If 1 or 'columns' , calculate across the columns. For Series this parameter is unused and defaults to 0. times (np.ndarray, Series, default None) \u2014 . d . . method (str {'single', 'table'}, default 'single') \u2014 .. versionadded:: 1.4.0 Execute the rolling operation per single column or row ( 'single' ) or over the entire object ( 'table' ). This argument is only implemented when specifying engine='numba' in the method call. Only applicable to mean() See Also rolling : Provides rolling window calculations.expanding : Provides expanding transformations. Notes See :ref: Windowing Operations <window.exponentially_weighted> for further usage details and examples. Examples >>> df = pd . DataFrame ({ 'B' : [ 0 , 1 , 2 , np . nan , 4 ]}) >>> df B 0 0.0 1 1.0 2 2.0 3 NaN 4 4.0 >>> df . ewm ( com = 0.5 ) . mean () B 0 0.000000 1 0.750000 2 1.615385 3 1.615385 4 3.670213 >>> df . ewm ( alpha = 2 / 3 ) . mean () B 0 0.000000 1 0.750000 2 1.615385 3 1.615385 4 3.670213 adjust >>> df . ewm ( com = 0.5 , adjust = True ) . mean () B 0 0.000000 1 0.750000 2 1.615385 3 1.615385 4 3.670213 >>> df . ewm ( com = 0.5 , adjust = False ) . mean () B 0 0.000000 1 0.666667 2 1.555556 3 1.555556 4 3.650794 ignore_na >>> df . ewm ( com = 0.5 , ignore_na = True ) . mean () B 0 0.000000 1 0.750000 2 1.615385 3 1.615385 4 3.225000 >>> df . ewm ( com = 0.5 , ignore_na = False ) . mean () B 0 0.000000 1 0.750000 2 1.615385 3 1.615385 4 3.670213 times Exponentially weighted mean with weights calculated with a timedelta halflife relative to times . >>> times = [ '2020-01-01' , '2020-01-03' , '2020-01-10' , '2020-01-15' , '2020-01-17' ] >>> df . ewm ( halflife = '4 days' , times = pd . DatetimeIndex ( times )) . mean () B 0 0.000000 1 0.585786 2 1.523889 3 1.523889 4 3.233686 method first_valid_index ( ) </> Return index for first non-NA value or None, if no non-NA value is found. Examples For Series: >>> s = pd . Series ([ None , 3 , 4 ]) >>> s . first_valid_index () 1 >>> s . last_valid_index () 2 >>> s = pd . Series ([ None , None ]) >>> print ( s . first_valid_index ()) None >>> print ( s . last_valid_index ()) None If all elements in Series are NA/null, returns None. >>> s = pd . Series () >>> print ( s . first_valid_index ()) None >>> print ( s . last_valid_index ()) None If Series is empty, returns None. For DataFrame: >>> df = pd . DataFrame ({ 'A' : [ None , None , 2 ], 'B' : [ None , 3 , 4 ]}) >>> df A B 0 NaN NaN 1 NaN 3.0 2 2.0 4.0 >>> df . first_valid_index () 1 >>> df . last_valid_index () 2 >>> df = pd . DataFrame ({ 'A' : [ None , None , None ], 'B' : [ None , None , None ]}) >>> df A B 0 None None 1 None None 2 None None >>> print ( df . first_valid_index ()) None >>> print ( df . last_valid_index ()) None If all elements in DataFrame are NA/null, returns None. >>> df = pd . DataFrame () >>> df Empty DataFrame Columns : [] Index : [] >>> print ( df . first_valid_index ()) None >>> print ( df . last_valid_index ()) None If DataFrame is empty, returns None. method last_valid_index ( ) </> Return index for last non-NA value or None, if no non-NA value is found. Examples For Series: >>> s = pd . Series ([ None , 3 , 4 ]) >>> s . first_valid_index () 1 >>> s . last_valid_index () 2 >>> s = pd . Series ([ None , None ]) >>> print ( s . first_valid_index ()) None >>> print ( s . last_valid_index ()) None If all elements in Series are NA/null, returns None. >>> s = pd . Series () >>> print ( s . first_valid_index ()) None >>> print ( s . last_valid_index ()) None If Series is empty, returns None. For DataFrame: >>> df = pd . DataFrame ({ 'A' : [ None , None , 2 ], 'B' : [ None , 3 , 4 ]}) >>> df A B 0 NaN NaN 1 NaN 3.0 2 2.0 4.0 >>> df . first_valid_index () 1 >>> df . last_valid_index () 2 >>> df = pd . DataFrame ({ 'A' : [ None , None , None ], 'B' : [ None , None , None ]}) >>> df A B 0 None None 1 None None 2 None None >>> print ( df . first_valid_index ()) None >>> print ( df . last_valid_index ()) None If all elements in DataFrame are NA/null, returns None. >>> df = pd . DataFrame () >>> df Empty DataFrame Columns : [] Index : [] >>> print ( df . first_valid_index ()) None >>> print ( df . last_valid_index ()) None If DataFrame is empty, returns None. method __dataframe__ ( nan_as_null=False , allow_copy=True ) </> Return the dataframe interchange object implementing the interchange protocol. Parameters nan_as_null (bool, default False) \u2014 nan_as_null is DEPRECATED and has no effect. Please avoid usingit; it will be removed in a future release. allow_copy (bool, default True) \u2014 Whether to allow memory copying when exporting. If set to Falseit would cause non-zero-copy exports to fail. Returns (DataFrame interchange object) The object which consuming library can use to ingress the dataframe. Notes Details on the interchange protocol: https://data-apis.org/dataframe-protocol/latest/index.html Examples >>> df_not_necessarily_pandas = pd . DataFrame ({ 'A' : [ 1 , 2 ], 'B' : [ 3 , 4 ]}) >>> interchange_object = df_not_necessarily_pandas . __dataframe__ () >>> interchange_object . column_names () Index ([ 'A' , 'B' ], dtype = 'object' ) >>> df_pandas = ( pd . api . interchange . from_dataframe ... ( interchange_object . select_columns_by_name ([ 'A' ]))) >>> df_pandas A 0 1 1 2 These methods ( column_names , select_columns_by_name ) should work for any dataframe library which implements the interchange protocol. method __dataframe_consortium_standard__ ( api_version=None ) \u2192 Any </> Provide entry point to the Consortium DataFrame Standard API. This is developed and maintained outside of pandas. Please report any issues to https://github.com/data-apis/dataframe-api-compat. method __arrow_c_stream__ ( requested_schema=None ) </> Export the pandas DataFrame as an Arrow C stream PyCapsule. This relies on pyarrow to convert the pandas DataFrame to the Arrow format (and follows the default behaviour of pyarrow.Table.from_pandas in its handling of the index, i.e. store the index as a column except for RangeIndex). This conversion is not necessarily zero-copy. Parameters requested_schema (PyCapsule, default None) \u2014 The schema to which the dataframe should be casted, passed as aPyCapsule containing a C ArrowSchema representation of the requested schema. method __repr__ ( ) \u2192 str </> Return a string representation for a particular DataFrame. method to_string ( buf=None , columns=None , col_space=None , header=True , index=True , na_rep='NaN' , formatters=None , float_format=None , sparsify=None , index_names=True , justify=None , max_rows=None , max_cols=None , show_dimensions=False , decimal='.' , line_width=None , min_rows=None , max_colwidth=None , encoding=None ) </> Render a DataFrame to a console-friendly tabular output. Parameters buf (str, Path or StringIO-like, optional, default None) \u2014 Buffer to write to. If None, the output is returned as a string. columns (array-like, optional, default None) \u2014 The subset of columns to write. Writes all columns by default. col_space (int, list or dict of int, optional) \u2014 The minimum width of each column. If a list of ints is given every integers corresponds with one column. If a dict is given, the key references the column, while the value defines the space to use.. header (bool or list of str, optional) \u2014 Write out the column names. If a list of columns is given, it is assumed to be aliases for the column names. index (bool, optional, default True) \u2014 Whether to print index (row) labels. na_rep (str, optional, default 'NaN') \u2014 String representation of NaN to use. formatters (list, tuple or dict of one-param. functions, optional) \u2014 Formatter functions to apply to columns' elements by position orname. The result of each function must be a unicode string. List/tuple must be of length equal to the number of columns. float_format (one-parameter function, optional, default None) \u2014 Formatter function to apply to columns' elements if they arefloats. This function must return a unicode string and will be applied only to the non- NaN elements, with NaN being handled by na_rep . sparsify (bool, optional, default True) \u2014 Set to False for a DataFrame with a hierarchical index to printevery multiindex key at each row. index_names (bool, optional, default True) \u2014 Prints the names of the indexes. justify (str, default None) \u2014 How to justify the column labels. If None uses the option fromthe print configuration (controlled by set_option), 'right' out of the box. Valid values are left right center justify justify-all start end inherit match-parent initial unset. max_rows (int, optional) \u2014 Maximum number of rows to display in the console. max_cols (int, optional) \u2014 Maximum number of columns to display in the console. show_dimensions (bool, default False) \u2014 Display DataFrame dimensions (number of rows by number of columns). decimal (str, default '.') \u2014 Character recognized as decimal separator, e.g. ',' in Europe. line_width (int, optional) \u2014 Width to wrap a line in characters. min_rows (int, optional) \u2014 The number of rows to display in the console in a truncated repr(when number of rows is above max_rows ). max_colwidth (int, optional) \u2014 Max width to truncate each column in characters. By default, no limit. encoding (str, default \"utf-8\") \u2014 Set character encoding. Returns (str or None) If buf is None, returns the result as a string. Otherwise returnsNone. See Also to_html : Convert DataFrame to HTML. Examples >>> d = { 'col1' : [ 1 , 2 , 3 ], 'col2' : [ 4 , 5 , 6 ]} >>> df = pd . DataFrame ( d ) >>> print ( df . to_string ()) col1 col2 0 1 4 1 2 5 2 3 6 generator items ( ) </> Iterate over (column name, Series) pairs. Iterates over the DataFrame columns, returning a tuple with the column name and the content as a Series. Yields (label : object) The column names for the DataFrame being iterated over.ent : Series The column entries belonging to each label, as a Series. See Also DataFrame.iterrows : Iterate over DataFrame rows as (index, Series) pairs. DataFrame.itertuples : Iterate over DataFrame rows as namedtuples of the values. Examples >>> df = pd . DataFrame ({ 'species' : [ 'bear' , 'bear' , 'marsupial' ], ... 'population' : [ 1864 , 22000 , 80000 ]}, ... index = [ 'panda' , 'polar' , 'koala' ]) >>> df species population panda bear 1864 polar bear 22000 koala marsupial 80000 >>> for label , content in df . items (): ... print ( f 'label: { label } ' ) ... print ( f 'content: { content } ' , sep = ' \\n ' ) ... label : species content : panda bear polar bear koala marsupial Name : species , dtype : object label : population content : panda 1864 polar 22000 koala 80000 Name : population , dtype : int64 generator iterrows ( ) </> Iterate over DataFrame rows as (index, Series) pairs. Yields (index : label or tuple of label) The index of the row. A tuple for a MultiIndex . : Series The data of the row as a Series. See Also DataFrame.itertuples : Iterate over DataFrame rows as namedtuples of the values.DataFrame.items : Iterate over (column name, Series) pairs. Notes Because iterrows returns a Series for each row, it does not preserve dtypes across the rows (dtypes are preserved across columns for DataFrames). To preserve dtypes while iterating over the rows, it is better to use :meth: itertuples which returns namedtuples of the values and which is generally faster than iterrows . You should never modify something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect. Examples )] w 0 5 4 ) 4 ) 4 method itertuples ( index=True , name='Pandas' ) </> Iterate over DataFrame rows as namedtuples. Parameters index (bool, default True) \u2014 If True, return the index as the first element of the tuple. name (str or None, default \"Pandas\") \u2014 The name of the returned namedtuples or None to return regulartuples. Returns (iterator) An object to iterate over namedtuples for each row in theDataFrame with the first field possibly being the index and following fields being the column values. See Also DataFrame.iterrows : Iterate over DataFrame rows as (index, Series) pairs. DataFrame.items : Iterate over (column name, Series) pairs. Notes The column names will be renamed to positional names if they are invalid Python identifiers, repeated, or start with an underscore. Examples >>> df = pd . DataFrame ({ 'num_legs' : [ 4 , 2 ], 'num_wings' : [ 0 , 2 ]}, ... index = [ 'dog' , 'hawk' ]) >>> df num_legs num_wings dog 4 0 hawk 2 2 >>> for row in df . itertuples (): ... print ( row ) ... Pandas ( Index = 'dog' , num_legs = 4 , num_wings = 0 ) Pandas ( Index = 'hawk' , num_legs = 2 , num_wings = 2 ) By setting the index parameter to False we can remove the index as the first element of the tuple: >>> for row in df . itertuples ( index = False ): ... print ( row ) ... Pandas ( num_legs = 4 , num_wings = 0 ) Pandas ( num_legs = 2 , num_wings = 2 ) With the name parameter set we set a custom name for the yielded namedtuples: >>> for row in df . itertuples ( name = 'Animal' ): ... print ( row ) ... Animal ( Index = 'dog' , num_legs = 4 , num_wings = 0 ) Animal ( Index = 'hawk' , num_legs = 2 , num_wings = 2 ) method __len__ ( ) \u2192 int </> Returns length of info axis, but here we use the index. method dot ( other ) </> Compute the matrix multiplication between the DataFrame and other. This method computes the matrix product between the DataFrame and the values of an other Series, DataFrame or a numpy array. It can also be called using self @ other . Parameters other (Series, DataFrame or array-like) \u2014 The other object to compute the matrix product with. Returns (Series or DataFrame) If other is a Series, return the matrix product between self andother as a Series. If other is a DataFrame or a numpy.array, return the matrix product of self and other in a DataFrame of a np.array. See Also Series.dot: Similar method for Series. Notes The dimensions of DataFrame and other must be compatible in order to compute the matrix multiplication. In addition, the column names of DataFrame and the index of other must contain the same values, as they will be aligned prior to the multiplication. The dot method for Series computes the inner product, instead of the matrix product here. Examples Here we multiply a DataFrame with a Series. >>> df = pd . DataFrame ([[ 0 , 1 , - 2 , - 1 ], [ 1 , 1 , 1 , 1 ]]) >>> s = pd . Series ([ 1 , 1 , 2 , 1 ]) >>> df . dot ( s ) 0 - 4 1 5 dtype : int64 Here we multiply a DataFrame with another DataFrame. >>> other = pd . DataFrame ([[ 0 , 1 ], [ 1 , 2 ], [ - 1 , - 1 ], [ 2 , 0 ]]) >>> df . dot ( other ) 0 1 0 1 4 1 2 2 Note that the dot method give the same result as @ >>> df @ other 0 1 0 1 4 1 2 2 The dot method works also if other is an np.array. >>> arr = np . array ([[ 0 , 1 ], [ 1 , 2 ], [ - 1 , - 1 ], [ 2 , 0 ]]) >>> df . dot ( arr ) 0 1 0 1 4 1 2 2 Note how shuffling of the objects does not change the result. >>> s2 = s . reindex ([ 1 , 0 , 2 , 3 ]) >>> df . dot ( s2 ) 0 - 4 1 5 dtype : int64 method __matmul__ ( other ) \u2192 pandas.core.frame.dataframe | pandas.core.series.series </> Matrix multiplication using binary @ operator. method __rmatmul__ ( other ) \u2192 DataFrame </> Matrix multiplication using binary @ operator. classmethod from_dict ( data , orient='columns' , dtype=None , columns=None ) </> Construct DataFrame from dict of array-like or dicts. Creates DataFrame object from dictionary by columns or by index allowing dtype specification. Parameters data (dict) \u2014 Of the form {field : array-like} or {field : dict}. orient ({'columns', 'index', 'tight'}, default 'columns') \u2014 The \"orientation\" of the data. If the keys of the passed dictshould be the columns of the resulting DataFrame, pass 'columns' (default). Otherwise if the keys should be rows, pass 'index'. If 'tight', assume a dict with keys ['index', 'columns', 'data', 'index_names', 'column_names']. .. versionadded:: 1.4.0 'tight' as an allowed value for the orient argument dtype (dtype, default None) \u2014 Data type to force after DataFrame construction, otherwise infer. columns (list, default None) \u2014 Column labels to use when orient='index' . Raises a ValueErrorif used with orient='columns' or orient='tight' . See Also DataFrame.from_records : DataFrame from structured ndarray, sequence of tuples or dicts, or DataFrame. DataFrame : DataFrame object creation using constructor. DataFrame.to_dict : Convert the DataFrame to a dictionary. Examples By default the keys of the dict become the DataFrame columns: >>> data = { 'col_1' : [ 3 , 2 , 1 , 0 ], 'col_2' : [ 'a' , 'b' , 'c' , 'd' ]} >>> pd . DataFrame . from_dict ( data ) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d Specify orient='index' to create the DataFrame using dictionary keys as rows: >>> data = { 'row_1' : [ 3 , 2 , 1 , 0 ], 'row_2' : [ 'a' , 'b' , 'c' , 'd' ]} >>> pd . DataFrame . from_dict ( data , orient = 'index' ) 0 1 2 3 row_1 3 2 1 0 row_2 a b c d When using the 'index' orientation, the column names can be specified manually: >>> pd . DataFrame . from_dict ( data , orient = 'index' , ... columns = [ 'A' , 'B' , 'C' , 'D' ]) A B C D row_1 3 2 1 0 row_2 a b c d Specify orient='tight' to create the DataFrame using a 'tight' format: >>> data = { 'index' : [( 'a' , 'b' ), ( 'a' , 'c' )], ... 'columns' : [( 'x' , 1 ), ( 'y' , 2 )], ... 'data' : [[ 1 , 3 ], [ 2 , 4 ]], ... 'index_names' : [ 'n1' , 'n2' ], ... 'column_names' : [ 'z1' , 'z2' ]} >>> pd . DataFrame . from_dict ( data , orient = 'tight' ) z1 x y z2 1 2 n1 n2 a b 1 3 c 2 4 method to_numpy ( dtype=None , copy=False , na_value=<no_default> ) </> Convert the DataFrame to a NumPy array. By default, the dtype of the returned array will be the common NumPy dtype of all types in the DataFrame. For example, if the dtypes are float16 and float32 , the results dtype will be float32 . This may require copying data and coercing values, which may be expensive. Parameters dtype (str or numpy.dtype, optional) \u2014 The dtype to pass to :meth: numpy.asarray . copy (bool, default False) \u2014 Whether to ensure that the returned value is not a view onanother array. Note that copy=False does not ensure that to_numpy() is no-copy. Rather, copy=True ensure that a copy is made, even if not strictly necessary. na_value (Any, optional) \u2014 The value to use for missing values. The default value dependson dtype and the dtypes of the DataFrame columns. See Also Series.to_numpy : Similar method for Series. Examples >>> pd . DataFrame ({ \"A\" : [ 1 , 2 ], \"B\" : [ 3 , 4 ]}) . to_numpy () array ([[ 1 , 3 ], [ 2 , 4 ]]) With heterogeneous data, the lowest common type will have to be used. >>> df = pd . DataFrame ({ \"A\" : [ 1 , 2 ], \"B\" : [ 3.0 , 4.5 ]}) >>> df . to_numpy () array ([[ 1. , 3. ], [ 2. , 4.5 ]]) For a mix of numeric and non-numeric types, the output array will have object dtype. >>> df [ 'C' ] = pd . date_range ( '2000' , periods = 2 ) >>> df . to_numpy () array ([[ 1 , 3.0 , Timestamp ( '2000-01-01 00:00:00' )], [ 2 , 4.5 , Timestamp ( '2000-01-02 00:00:00' )]], dtype = object ) method to_dict ( orient='dict' , into=<class 'dict'> , index=True ) </> Convert the DataFrame to a dictionary. The type of the key-value pairs can be customized with the parameters (see below). Parameters orient (str {'dict', 'list', 'series', 'split', 'tight', 'records', 'index'}) \u2014 Determines the type of the values of the dictionary. 'dict' (default) : dict like {column -> {index -> value}} 'list' : dict like {column -> [values]} 'series' : dict like {column -> Series(values)} 'split' : dict like 'tight' : dict like {'index' -> [index], 'columns' -> [columns], 'data' -> [values], 'index_names' -> [index.names], 'column_names' -> [column.names]} 'records' : list like [{column -> value}, ... , {column -> value}] 'index' : dict like {index -> {column -> value}} .. versionadded:: 1.4.0 'tight' as an allowed value for the orient argument into (class, default dict) \u2014 The collections.abc.MutableMapping subclass used for all Mappingsin the return value. Can be the actual class or an empty instance of the mapping type you want. If you want a collections.defaultdict, you must pass it initialized. index (bool, default True) \u2014 Whether to include the index item (and index_names item if orient is 'tight') in the returned dictionary. Can only be False when orient is 'split' or 'tight'. .. versionadded:: 2.0.0 Returns (dict, list or collections.abc.MutableMapping) Return a collections.abc.MutableMapping object representing theDataFrame. The resulting transformation depends on the orient parameter. See Also DataFrame.from_dict: Create a DataFrame from a dictionary.DataFrame.to_json: Convert a DataFrame to JSON format. Examples >>> df = pd . DataFrame ({ 'col1' : [ 1 , 2 ], ... 'col2' : [ 0.5 , 0.75 ]}, ... index = [ 'row1' , 'row2' ]) >>> df col1 col2 row1 1 0.50 row2 2 0.75 >>> df . to_dict () { 'col1' : { 'row1' : 1 , 'row2' : 2 }, 'col2' : { 'row1' : 0.5 , 'row2' : 0.75 }} You can specify the return orientation. >>> df . to_dict ( 'series' ) { 'col1' : row1 1 row2 2 Name : col1 , dtype : int64 , 'col2' : row1 0.50 row2 0.75 Name : col2 , dtype : float64 } >>> df . to_dict ( 'split' ) { 'index' : [ 'row1' , 'row2' ], 'columns' : [ 'col1' , 'col2' ], 'data' : [[ 1 , 0.5 ], [ 2 , 0.75 ]]} >>> df . to_dict ( 'records' ) [{ 'col1' : 1 , 'col2' : 0.5 }, { 'col1' : 2 , 'col2' : 0.75 }] >>> df . to_dict ( 'index' ) { 'row1' : { 'col1' : 1 , 'col2' : 0.5 }, 'row2' : { 'col1' : 2 , 'col2' : 0.75 }} >>> df . to_dict ( 'tight' ) { 'index' : [ 'row1' , 'row2' ], 'columns' : [ 'col1' , 'col2' ], 'data' : [[ 1 , 0.5 ], [ 2 , 0.75 ]], 'index_names' : [ None ], 'column_names' : [ None ]} You can also specify the mapping type. >>> from collections import OrderedDict , defaultdict >>> df . to_dict ( into = OrderedDict ) OrderedDict ([( 'col1' , OrderedDict ([( 'row1' , 1 ), ( 'row2' , 2 )])), ( 'col2' , OrderedDict ([( 'row1' , 0.5 ), ( 'row2' , 0.75 )]))]) If you want a defaultdict , you need to initialize it: >>> dd = defaultdict ( list ) >>> df . to_dict ( 'records' , into = dd ) [ defaultdict ( < class ' list '>, {' col1 ': 1, ' col2 ': 0.5}), defaultdict ( < class ' list '>, {' col1 ': 2, ' col2 ': 0.75})] method to_gbq ( destination_table , project_id=None , chunksize=None , reauth=False , if_exists='fail' , auth_local_webserver=True , table_schema=None , location=None , progress_bar=True , credentials=None ) </> Write a DataFrame to a Google BigQuery table. .. deprecated:: 2.2.0 Please use pandas_gbq.to_gbq instead. This function requires the pandas-gbq package <https://pandas-gbq.readthedocs.io> __. See the How to authenticate with Google BigQuery <https://pandas-gbq.readthedocs.io/en/latest/howto/authentication.html> __ guide for authentication instructions. Parameters destination_table (str) \u2014 Name of table to be written, in the form dataset.tablename . project_id (str, optional) \u2014 Google BigQuery Account project ID. Optional when available fromthe environment. chunksize (int, optional) \u2014 Number of rows to be inserted in each chunk from the dataframe.Set to None to load the whole dataframe at once. reauth (bool, default False) \u2014 Force Google BigQuery to re-authenticate the user. This is usefulif multiple accounts are used. if_exists (str, default 'fail') \u2014 Behavior when the destination table exists. Value can be one of: 'fail' If table exists raise pandasgbq.gbq.TableCreationError. 'replace' If table exists, drop it, recreate it, and insert data. 'append' If table exists, insert data. Create if does not exist. auth_local_webserver (bool, default True) \u2014 Use the local webserver flow instead of the console flow when getting user credentials. .. _local webserver flow: https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_local_server .. _console flow: https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_console New in version 0.2.0 of pandas-gbq . .. versionchanged:: 1.5.0 Default value is changed to True . Google has deprecated the auth_local_webserver = False \"out of band\" (copy-paste) flow <https://developers.googleblog.com/2022/02/making-oauth-flows-safer.html?m=1#disallowed-oob> _. table_schema (list of dicts, optional) \u2014 List of BigQuery table fields to which according DataFramecolumns conform to, e.g. [{'name': 'col1', 'type': 'STRING'},...] . If schema is not provided, it will be generated according to dtypes of DataFrame columns. See BigQuery API documentation on available names of a field. New in version 0.3.1 of pandas-gbq . location (str, optional) \u2014 Location where the load job should run. See the BigQuery locationsdocumentation <https://cloud.google.com/bigquery/docs/dataset-locations> __ for a list of available locations. The location must match that of the target dataset. New in version 0.5.0 of pandas-gbq . progress_bar (bool, default True) \u2014 Use the library tqdm to show the progress bar for the upload,chunk by chunk. New in version 0.5.0 of pandas-gbq . credentials (google.auth.credentials.Credentials, optional) \u2014 Credentials for accessing Google APIs. Use this parameter tooverride default credentials, such as to use Compute Engine :class: google.auth.compute_engine.Credentials or Service Account :class: google.oauth2.service_account.Credentials directly. New in version 0.8.0 of pandas-gbq . See Also pandas_gbq.to_gbq : This function in the pandas-gbq library.read_gbq : Read a DataFrame from Google BigQuery. Examples Example taken from Google BigQuery documentation<https://cloud.google.com/bigquery/docs/samples/bigquery-pandas-gbq-to-gbq-simple> _ >>> project_id = \"my-project\" >>> table_id = 'my_dataset.my_table' >>> df = pd . DataFrame ({ ... \"my_string\" : [ \"a\" , \"b\" , \"c\" ], ... \"my_int64\" : [ 1 , 2 , 3 ], ... \"my_float64\" : [ 4.0 , 5.0 , 6.0 ], ... \"my_bool1\" : [ True , False , True ], ... \"my_bool2\" : [ False , True , False ], ... \"my_dates\" : pd . date_range ( \"now\" , periods = 3 ), ... } ... ) >>> df . to_gbq ( table_id , project_id = project_id ) # doctest: +SKIP classmethod from_records ( data , index=None , exclude=None , columns=None , coerce_float=False , nrows=None ) </> Convert structured or record ndarray to DataFrame. Creates a DataFrame object from a structured ndarray, sequence of tuples or dicts, or DataFrame. Parameters data (structured ndarray, sequence of tuples or dicts, or DataFrame) \u2014 Structured input data. .. deprecated:: 2.1.0 Passing a DataFrame is deprecated. index (str, list of fields, array-like) \u2014 Field of array to use as the index, alternately a specific set ofinput labels to use. exclude (sequence, default None) \u2014 Columns or fields to exclude. columns (sequence, default None) \u2014 Column names to use. If the passed data do not have namesassociated with them, this argument provides names for the columns. Otherwise this argument indicates the order of the columns in the result (any names not found in the data will become all-NA columns). coerce_float (bool, default False) \u2014 Attempt to convert values of non-string, non-numeric objects (likedecimal.Decimal) to floating point, useful for SQL result sets. nrows (int, default None) \u2014 Number of rows to read if data is an iterator. See Also DataFrame.from_dict : DataFrame from dict of array-like or dicts.DataFrame : DataFrame object creation using constructor. Examples Data can be provided as a structured ndarray: >>> data = np . array ([( 3 , 'a' ), ( 2 , 'b' ), ( 1 , 'c' ), ( 0 , 'd' )], ... dtype = [( 'col_1' , 'i4' ), ( 'col_2' , 'U1' )]) >>> pd . DataFrame . from_records ( data ) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d Data can be provided as a list of dicts: >>> data = [{ 'col_1' : 3 , 'col_2' : 'a' }, ... { 'col_1' : 2 , 'col_2' : 'b' }, ... { 'col_1' : 1 , 'col_2' : 'c' }, ... { 'col_1' : 0 , 'col_2' : 'd' }] >>> pd . DataFrame . from_records ( data ) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d Data can be provided as a list of tuples with corresponding columns: >>> data = [( 3 , 'a' ), ( 2 , 'b' ), ( 1 , 'c' ), ( 0 , 'd' )] >>> pd . DataFrame . from_records ( data , columns = [ 'col_1' , 'col_2' ]) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d method to_records ( index=True , column_dtypes=None , index_dtypes=None ) </> Convert DataFrame to a NumPy record array. Index will be included as the first field of the record array if requested. Parameters index (bool, default True) \u2014 Include index in resulting record array, stored in 'index'field or using the index label, if set. column_dtypes (str, type, dict, default None) \u2014 If a string or type, the data type to store all columns. Ifa dictionary, a mapping of column names and indices (zero-indexed) to specific data types. index_dtypes (str, type, dict, default None) \u2014 If a string or type, the data type to store all index levels. Ifa dictionary, a mapping of index level names and indices (zero-indexed) to specific data types. This mapping is applied only if index=True . Returns (numpy.rec.recarray) NumPy ndarray with the DataFrame labels as fields and each rowof the DataFrame as entries. See Also DataFrame.from_records: Convert structured or record ndarray to DataFrame. numpy.rec.recarray: An ndarray that allows field access using attributes, analogous to typed columns in a spreadsheet. Examples >>> df = pd . DataFrame ({ 'A' : [ 1 , 2 ], 'B' : [ 0.5 , 0.75 ]}, ... index = [ 'a' , 'b' ]) >>> df A B a 1 0.50 b 2 0.75 >>> df . to_records () rec . array ([( 'a' , 1 , 0.5 ), ( 'b' , 2 , 0.75 )], dtype = [( 'index' , 'O' ), ( 'A' , '<i8' ), ( 'B' , '<f8' )]) If the DataFrame index has no label then the recarray field name is set to 'index'. If the index has a label then this is used as the field name: >>> df . index = df . index . rename ( \"I\" ) >>> df . to_records () rec . array ([( 'a' , 1 , 0.5 ), ( 'b' , 2 , 0.75 )], dtype = [( 'I' , 'O' ), ( 'A' , '<i8' ), ( 'B' , '<f8' )]) The index can be excluded from the record array: >>> df . to_records ( index = False ) rec . array ([( 1 , 0.5 ), ( 2 , 0.75 )], dtype = [( 'A' , '<i8' ), ( 'B' , '<f8' )]) Data types can be specified for the columns: >>> df . to_records ( column_dtypes = { \"A\" : \"int32\" }) rec . array ([( 'a' , 1 , 0.5 ), ( 'b' , 2 , 0.75 )], dtype = [( 'I' , 'O' ), ( 'A' , '<i4' ), ( 'B' , '<f8' )]) As well as for the index: >>> df . to_records ( index_dtypes = \"<S2\" ) rec . array ([( b 'a' , 1 , 0.5 ), ( b 'b' , 2 , 0.75 )], dtype = [( 'I' , 'S2' ), ( 'A' , '<i8' ), ( 'B' , '<f8' )]) >>> index_dtypes = f \"<S { df . index . str . len () . max () } \" >>> df . to_records ( index_dtypes = index_dtypes ) rec . array ([( b 'a' , 1 , 0.5 ), ( b 'b' , 2 , 0.75 )], dtype = [( 'I' , 'S1' ), ( 'A' , '<i8' ), ( 'B' , '<f8' )]) method to_stata ( path , convert_dates=None , write_index=True , byteorder=None , time_stamp=None , data_label=None , variable_labels=None , version=114 , convert_strl=None , compression='infer' , storage_options=None , value_labels=None ) </> Export DataFrame object to Stata dta format. Writes the DataFrame to a Stata dataset file. \"dta\" files contain a Stata dataset. Parameters path (str, path object, or buffer) \u2014 String, path object (implementing os.PathLike[str] ), or file-likeobject implementing a binary write() function. convert_dates (dict) \u2014 Dictionary mapping columns containing datetime types to statainternal format to use when writing the dates. Options are 'tc', 'td', 'tm', 'tw', 'th', 'tq', 'ty'. Column can be either an integer or a name. Datetime columns that do not have a conversion type specified will be converted to 'tc'. Raises NotImplementedError if a datetime column has timezone information. write_index (bool) \u2014 Write the index to Stata dataset. byteorder (str) \u2014 Can be \">\", \"<\", \"little\", or \"big\". default is sys.byteorder . time_stamp (datetime) \u2014 A datetime to use as file creation date. Default is the currenttime. data_label (str, optional) \u2014 A label for the data set. Must be 80 characters or smaller. variable_labels (dict) \u2014 Dictionary containing columns as keys and variable labels asvalues. Each label must be 80 characters or smaller. version ({114, 117, 118, 119, None}, default 114) \u2014 Version to use in the output dta file. Set to None to let pandasdecide between 118 or 119 formats depending on the number of columns in the frame. Version 114 can be read by Stata 10 and later. Version 117 can be read by Stata 13 or later. Version 118 is supported in Stata 14 and later. Version 119 is supported in Stata 15 and later. Version 114 limits string variables to 244 characters or fewer while versions 117 and later allow strings with lengths up to 2,000,000 characters. Versions 118 and 119 support Unicode characters, and version 119 supports more than 32,767 variables. Version 119 should usually only be used when the number of variables exceeds the capacity of dta format 118. Exporting smaller datasets in format 119 may have unintended consequences, and, as of November 2020, Stata SE cannot read version 119 files. convert_strl (list, optional) \u2014 List of column names to convert to string columns to Stata StrLformat. Only available if version is 117. Storing strings in the StrL format can produce smaller dta files if strings have more than 8 characters and values are repeated. compression (str or dict, default 'infer') \u2014 For on-the-fly compression of the output data. If 'infer' and 'path' ispath-like, then detect compression from the following extensions: '.gz', '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2' (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of { 'zip' , 'gzip' , 'bz2' , 'zstd' , 'xz' , 'tar' } and other key-value pairs are forwarded to zipfile.ZipFile , gzip.GzipFile , bz2.BZ2File , zstandard.ZstdCompressor , lzma.LZMAFile or tarfile.TarFile , respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1} . .. versionadded:: 1.5.0 Added support for .tar files. .. versionchanged:: 1.4.0 Zstandard support. storage_options (dict, optional) \u2014 Extra options that make sense for a particular storage connection, e.g.host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to fsspec.open . Please see fsspec and urllib for more details, and for more examples on storage options refer here <https://pandas.pydata.org/docs/user_guide/io.html? highlight=storage_options#reading-writing-remote-files> _. value_labels (dict of dicts) \u2014 Dictionary containing columns as keys and dictionaries of column valueto labels as values. Labels for a single variable must be 32,000 characters or smaller. .. versionadded:: 1.4.0 Raises NotImplementedError \u2014 If datetimes contain timezone information Column dtype is not representable in Stata ValueError \u2014 Columns listed in convert_dates are neither datetime64[ns] or datetime.datetime Column listed in convert_dates is not in DataFrame Categorical label contains more than 32,000 characters See Also read_stata : Import Stata data files.io.stata.StataWriter : Low-level writer for Stata data files. io.stata.StataWriter117 : Low-level writer for version 117 files. Examples >>> df = pd . DataFrame ({ 'animal' : [ 'falcon' , 'parrot' , 'falcon' , ... 'parrot' ], ... 'speed' : [ 350 , 18 , 361 , 15 ]}) >>> df . to_stata ( 'animals.dta' ) # doctest: +SKIP method to_feather ( path , **kwargs ) </> Write a DataFrame to the binary Feather format. Parameters path (str, path object, file-like object) \u2014 String, path object (implementing os.PathLike[str] ), or file-likeobject implementing a binary write() function. If a string or a path, it will be used as Root Directory path when writing a partitioned dataset. **kwargs \u2014 Additional keywords passed to :func: pyarrow.feather.write_feather .This includes the compression , compression_level , chunksize and version keywords. Notes This function writes the dataframe as a feather file <https://arrow.apache.org/docs/python/feather.html> _. Requires a default index. For saving the DataFrame with your custom index use a method that supports custom indices e.g. to_parquet . Examples >>> df = pd . DataFrame ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> df . to_feather ( \"file.feather\" ) # doctest: +SKIP method to_markdown ( buf=None , mode='wt' , index=True , storage_options=None , **kwargs ) </> Print DataFrame in Markdown-friendly format. Parameters buf (str, Path or StringIO-like, optional, default None) \u2014 Buffer to write to. If None, the output is returned as a string. mode (str, optional) \u2014 Mode in which file is opened, \"wt\" by default. index (bool, optional, default True) \u2014 Add index (row) labels. storage_options (dict, optional) \u2014 Extra options that make sense for a particular storage connection, e.g.host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to fsspec.open . Please see fsspec and urllib for more details, and for more examples on storage options refer here <https://pandas.pydata.org/docs/user_guide/io.html? highlight=storage_options#reading-writing-remote-files> _. **kwargs \u2014 These parameters will be passed to tabulate <https://pypi.org/project/tabulate> _. Returns (str) DataFrame in Markdown-friendly format. Notes Requires the tabulate <https://pypi.org/project/tabulate> _ package. Examples >>> df = pd . DataFrame ( ... data = { \"animal_1\" : [ \"elk\" , \"pig\" ], \"animal_2\" : [ \"dog\" , \"quetzal\" ]} ... ) >>> print ( df . to_markdown ()) | | animal_1 | animal_2 | |--- : | : -----------| : -----------| | 0 | elk | dog | | 1 | pig | quetzal | Output markdown with a tabulate option. >>> print ( df . to_markdown ( tablefmt = \"grid\" )) +----+------------+------------+ | | animal_1 | animal_2 | +====+============+============+ | 0 | elk | dog | +----+------------+------------+ | 1 | pig | quetzal | +----+------------+------------+ method to_parquet ( path=None , engine='auto' , compression='snappy' , index=None , partition_cols=None , storage_options=None , **kwargs ) </> Write a DataFrame to the binary parquet format. This function writes the dataframe as a parquet file <https://parquet.apache.org/> _. You can choose different parquet backends, and have the option of compression. See :ref: the user guide <io.parquet> for more details. Parameters path (str, path object, file-like object, or None, default None) \u2014 String, path object (implementing os.PathLike[str] ), or file-likeobject implementing a binary write() function. If None, the result is returned as bytes. If a string or path, it will be used as Root Directory path when writing a partitioned dataset. engine ({'auto', 'pyarrow', 'fastparquet'}, default 'auto') \u2014 Parquet library to use. If 'auto', then the option io.parquet.engine is used. The default io.parquet.engine behavior is to try 'pyarrow', falling back to 'fastparquet' if 'pyarrow' is unavailable. compression (str or None, default 'snappy') \u2014 Name of the compression to use. Use None for no compression.Supported options: 'snappy', 'gzip', 'brotli', 'lz4', 'zstd'. index (bool, default None) \u2014 If True , include the dataframe's index(es) in the file output.If False , they will not be written to the file. If None , similar to True the dataframe's index(es) will be saved. However, instead of being saved as values, the RangeIndex will be stored as a range in the metadata so it doesn't require much space and is faster. Other indexes will be included as columns in the file output. partition_cols (list, optional, default None) \u2014 Column names by which to partition the dataset.Columns are partitioned in the order they are given. Must be None if path is not a string. storage_options (dict, optional) \u2014 Extra options that make sense for a particular storage connection, e.g.host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to fsspec.open . Please see fsspec and urllib for more details, and for more examples on storage options refer here <https://pandas.pydata.org/docs/user_guide/io.html? highlight=storage_options#reading-writing-remote-files> _. **kwargs \u2014 Additional arguments passed to the parquet library. See:ref: pandas io <io.parquet> for more details. See Also read_parquet : Read a parquet file.DataFrame.to_orc : Write an orc file. DataFrame.to_csv : Write a csv file. DataFrame.to_sql : Write to a sql table. DataFrame.to_hdf : Write to hdf. Notes This function requires either the fastparquet <https://pypi.org/project/fastparquet> or pyarrow <https://arrow.apache.org/docs/python/> library. Examples >>> df = pd . DataFrame ( data = { 'col1' : [ 1 , 2 ], 'col2' : [ 3 , 4 ]}) >>> df . to_parquet ( 'df.parquet.gzip' , ... compression = 'gzip' ) # doctest: +SKIP >>> pd . read_parquet ( 'df.parquet.gzip' ) # doctest: +SKIP col1 col2 0 1 3 1 2 4 If you want to get a buffer to the parquet content you can use a io.BytesIO object, as long as you don't use partition_cols, which creates multiple files. >>> import io >>> f = io . BytesIO () >>> df . to_parquet ( f ) >>> f . seek ( 0 ) 0 >>> content = f . read () method to_orc ( path=None , engine='pyarrow' , index=None , engine_kwargs=None ) </> Write a DataFrame to the ORC format. .. versionadded:: 1.5.0 Parameters path (str, file-like object or None, default None) \u2014 If a string, it will be used as Root Directory pathwhen writing a partitioned dataset. By file-like object, we refer to objects with a write() method, such as a file handle (e.g. via builtin open function). If path is None, a bytes object is returned. engine ({'pyarrow'}, default 'pyarrow') \u2014 ORC library to use. index (bool, optional) \u2014 If True , include the dataframe's index(es) in the file output.If False , they will not be written to the file. If None , similar to infer the dataframe's index(es) will be saved. However, instead of being saved as values, the RangeIndex will be stored as a range in the metadata so it doesn't require much space and is faster. Other indexes will be included as columns in the file output. engine_kwargs (dict[str, Any] or None, default None) \u2014 Additional keyword arguments passed to :func: pyarrow.orc.write_table . Raises NotImplementedError \u2014 Dtype of one or more columns is category, unsigned integers, interval,period or sparse. ValueError \u2014 engine is not pyarrow. See Also read_orc : Read a ORC file.DataFrame.to_parquet : Write a parquet file. DataFrame.to_csv : Write a csv file. DataFrame.to_sql : Write to a sql table. DataFrame.to_hdf : Write to hdf. Notes Before using this function you should read the :ref: user guide about ORC <io.orc> and :ref: install optional dependencies <install.warn_orc> . This function requires pyarrow <https://arrow.apache.org/docs/python/> _ library. For supported dtypes please refer to supported ORC features in Arrow <https://arrow.apache.org/docs/cpp/orc.html#data-types> __. Currently timezones in datetime columns are not preserved when a dataframe is converted into ORC files. Examples >>> df = pd . DataFrame ( data = { 'col1' : [ 1 , 2 ], 'col2' : [ 4 , 3 ]}) >>> df . to_orc ( 'df.orc' ) # doctest: +SKIP >>> pd . read_orc ( 'df.orc' ) # doctest: +SKIP col1 col2 0 1 4 1 2 3 If you want to get a buffer to the orc content you can write it to io.BytesIO >>> import io >>> b = io . BytesIO ( df . to_orc ()) # doctest: +SKIP >>> b . seek ( 0 ) # doctest: +SKIP 0 >>> content = b . read () # doctest: +SKIP method to_html ( buf=None , columns=None , col_space=None , header=True , index=True , na_rep='NaN' , formatters=None , float_format=None , sparsify=None , index_names=True , justify=None , max_rows=None , max_cols=None , show_dimensions=False , decimal='.' , bold_rows=True , classes=None , escape=True , notebook=False , border=None , table_id=None , render_links=False , encoding=None ) </> Render a DataFrame as an HTML table. Parameters buf (str, Path or StringIO-like, optional, default None) \u2014 Buffer to write to. If None, the output is returned as a string. columns (array-like, optional, default None) \u2014 The subset of columns to write. Writes all columns by default. col_space (str or int, list or dict of int or str, optional) \u2014 The minimum width of each column in CSS length units. An int is assumed to be px units.. header (bool, optional) \u2014 Whether to print column labels, default True. index (bool, optional, default True) \u2014 Whether to print index (row) labels. na_rep (str, optional, default 'NaN') \u2014 String representation of NaN to use. formatters (list, tuple or dict of one-param. functions, optional) \u2014 Formatter functions to apply to columns' elements by position orname. The result of each function must be a unicode string. List/tuple must be of length equal to the number of columns. float_format (one-parameter function, optional, default None) \u2014 Formatter function to apply to columns' elements if they arefloats. This function must return a unicode string and will be applied only to the non- NaN elements, with NaN being handled by na_rep . sparsify (bool, optional, default True) \u2014 Set to False for a DataFrame with a hierarchical index to printevery multiindex key at each row. index_names (bool, optional, default True) \u2014 Prints the names of the indexes. justify (str, default None) \u2014 How to justify the column labels. If None uses the option fromthe print configuration (controlled by set_option), 'right' out of the box. Valid values are left right center justify justify-all start end inherit match-parent initial unset. max_rows (int, optional) \u2014 Maximum number of rows to display in the console. max_cols (int, optional) \u2014 Maximum number of columns to display in the console. show_dimensions (bool, default False) \u2014 Display DataFrame dimensions (number of rows by number of columns). decimal (str, default '.') \u2014 Character recognized as decimal separator, e.g. ',' in Europe. bold_rows (bool, default True) \u2014 Make the row labels bold in the output. classes (str or list or tuple, default None) \u2014 CSS class(es) to apply to the resulting html table. escape (bool, default True) \u2014 Convert the characters <, >, and & to HTML-safe sequences. notebook ({True, False}, default False) \u2014 Whether the generated HTML is for IPython Notebook. border (int) \u2014 A border=border attribute is included in the opening <table> tag. Default pd.options.display.html.border . table_id (str, optional) \u2014 A css id is included in the opening <table> tag if specified. render_links (bool, default False) \u2014 Convert URLs to HTML links. encoding (str, default \"utf-8\") \u2014 Set character encoding. Returns (str or None) If buf is None, returns the result as a string. Otherwise returnsNone. See Also to_string : Convert DataFrame to a string. Examples >>> df = pd . DataFrame ( data = { 'col1' : [ 1 , 2 ], 'col2' : [ 4 , 3 ]}) >>> html_string = '''<table border=\"1\" class=\"dataframe\"> ... <thead> ... <tr style=\"text-align: right;\"> ... <th></th> ... <th>col1</th> ... <th>col2</th> ... </tr> ... </thead> ... <tbody> ... <tr> ... <th>0</th> ... <td>1</td> ... <td>4</td> ... </tr> ... <tr> ... <th>1</th> ... <td>2</td> ... <td>3</td> ... </tr> ... </tbody> ... </table>''' >>> assert html_string == df . to_html () method to_xml ( path_or_buffer=None , index=True , root_name='data' , row_name='row' , na_rep=None , attr_cols=None , elem_cols=None , namespaces=None , prefix=None , encoding='utf-8' , xml_declaration=True , pretty_print=True , parser='lxml' , stylesheet=None , compression='infer' , storage_options=None ) </> Render a DataFrame to an XML document. .. versionadded:: 1.3.0 Parameters path_or_buffer (str, path object, file-like object, or None, default None) \u2014 String, path object (implementing os.PathLike[str] ), or file-likeobject implementing a write() function. If None, the result is returned as a string. index (bool, default True) \u2014 Whether to include index in XML document. root_name (str, default 'data') \u2014 The name of root element in XML document. row_name (str, default 'row') \u2014 The name of row element in XML document. na_rep (str, optional) \u2014 Missing data representation. attr_cols (list-like, optional) \u2014 List of columns to write as attributes in row element.Hierarchical columns will be flattened with underscore delimiting the different levels. elem_cols (list-like, optional) \u2014 List of columns to write as children in row element. By default,all columns output as children of row element. Hierarchical columns will be flattened with underscore delimiting the different levels. namespaces (dict, optional) \u2014 All namespaces to be defined in root element. Keys of dictshould be prefix names and values of dict corresponding URIs. Default namespaces should be given empty string key. For example, :: namespaces = {\"\": \"https://example.com\"} prefix (str, optional) \u2014 Namespace prefix to be used for every element and/or attributein document. This should be one of the keys in namespaces dict. encoding (str, default 'utf-8') \u2014 Encoding of the resulting document. xml_declaration (bool, default True) \u2014 Whether to include the XML declaration at start of document. pretty_print (bool, default True) \u2014 Whether output should be pretty printed with indentation andline breaks. parser ({'lxml','etree'}, default 'lxml') \u2014 Parser module to use for building of tree. Only 'lxml' and'etree' are supported. With 'lxml', the ability to use XSLT stylesheet is supported. stylesheet (str, path object or file-like object, optional) \u2014 A URL, file-like object, or a raw string containing an XSLTscript used to transform the raw XML output. Script should use layout of elements and attributes from original output. This argument requires lxml to be installed. Only XSLT 1.0 scripts and not later versions is currently supported. compression (str or dict, default 'infer') \u2014 For on-the-fly compression of the output data. If 'infer' and 'path_or_buffer' ispath-like, then detect compression from the following extensions: '.gz', '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2' (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of { 'zip' , 'gzip' , 'bz2' , 'zstd' , 'xz' , 'tar' } and other key-value pairs are forwarded to zipfile.ZipFile , gzip.GzipFile , bz2.BZ2File , zstandard.ZstdCompressor , lzma.LZMAFile or tarfile.TarFile , respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1} . .. versionadded:: 1.5.0 Added support for .tar files. .. versionchanged:: 1.4.0 Zstandard support. storage_options (dict, optional) \u2014 Extra options that make sense for a particular storage connection, e.g.host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to fsspec.open . Please see fsspec and urllib for more details, and for more examples on storage options refer here <https://pandas.pydata.org/docs/user_guide/io.html? highlight=storage_options#reading-writing-remote-files> _. Returns (None or str) If io is None, returns the resulting XML format as astring. Otherwise returns None. See Also to_json : Convert the pandas object to a JSON string.to_html : Convert DataFrame to a html. Examples >>> df = pd . DataFrame ({ 'shape' : [ 'square' , 'circle' , 'triangle' ], ... 'degrees' : [ 360 , 360 , 180 ], ... 'sides' : [ 4 , np . nan , 3 ]}) >>> df . to_xml () # doctest: +SKIP < ? xml version = '1.0' encoding = 'utf-8' ? > < data > < row > < index > 0 </ index > < shape > square </ shape > < degrees > 360 </ degrees > < sides > 4.0 </ sides > </ row > < row > < index > 1 </ index > < shape > circle </ shape > < degrees > 360 </ degrees > < sides /> </ row > < row > < index > 2 </ index > < shape > triangle </ shape > < degrees > 180 </ degrees > < sides > 3.0 </ sides > </ row > </ data > >>> df . to_xml ( attr_cols = [ ... 'index' , 'shape' , 'degrees' , 'sides' ... ]) # doctest: +SKIP < ? xml version = '1.0' encoding = 'utf-8' ? > < data > < row index = \"0\" shape = \"square\" degrees = \"360\" sides = \"4.0\" /> < row index = \"1\" shape = \"circle\" degrees = \"360\" /> < row index = \"2\" shape = \"triangle\" degrees = \"180\" sides = \"3.0\" /> </ data > >>> df . to_xml ( namespaces = { \"doc\" : \"https://example.com\" }, ... prefix = \"doc\" ) # doctest: +SKIP < ? xml version = '1.0' encoding = 'utf-8' ? > < doc : data xmlns : doc = \"https://example.com\" > < doc : row > < doc : index > 0 </ doc : index > < doc : shape > square </ doc : shape > < doc : degrees > 360 </ doc : degrees > < doc : sides > 4.0 </ doc : sides > </ doc : row > < doc : row > < doc : index > 1 </ doc : index > < doc : shape > circle </ doc : shape > < doc : degrees > 360 </ doc : degrees > < doc : sides /> </ doc : row > < doc : row > < doc : index > 2 </ doc : index > < doc : shape > triangle </ doc : shape > < doc : degrees > 180 </ doc : degrees > < doc : sides > 3.0 </ doc : sides > </ doc : row > </ doc : data > method info ( verbose=None , buf=None , max_cols=None , memory_usage=None , show_counts=None ) </> Print a concise summary of a DataFrame. This method prints information about a DataFrame including the index dtype and columns, non-null values and memory usage. Parameters verbose (bool, optional) \u2014 Whether to print the full summary. By default, the setting in pandas.options.display.max_info_columns is followed. buf (writable buffer, defaults to sys.stdout) \u2014 Where to send the output. By default, the output is printed tosys.stdout. Pass a writable buffer if you need to further process the output. max_cols (int, optional) \u2014 When to switch from the verbose to the truncated output. If theDataFrame has more than max_cols columns, the truncated output is used. By default, the setting in pandas.options.display.max_info_columns is used. memory_usage (bool, str, optional) \u2014 Specifies whether total memory usage of the DataFrameelements (including the index) should be displayed. By default, this follows the pandas.options.display.memory_usage setting. True always show memory usage. False never shows memory usage. A value of 'deep' is equivalent to \"True with deep introspection\". Memory usage is shown in human-readable units (base-2 representation). Without deep introspection a memory estimation is made based in column dtype and number of rows assuming values consume the same memory amount for corresponding dtypes. With deep memory introspection, a real memory usage calculation is performed at the cost of computational resources. See the :ref: Frequently Asked Questions <df-memory-usage> for more details. show_counts (bool, optional) \u2014 Whether to show the non-null counts. By default, this is shownonly if the DataFrame is smaller than pandas.options.display.max_info_rows and pandas.options.display.max_info_columns . A value of True always shows the counts, and False never shows the counts. Returns (None) This method prints a summary of a DataFrame and returns None. See Also DataFrame.describe: Generate descriptive statistics of DataFrame columns. DataFrame.memory_usage: Memory usage of DataFrame columns. Examples >>> int_values = [ 1 , 2 , 3 , 4 , 5 ] >>> text_values = [ 'alpha' , 'beta' , 'gamma' , 'delta' , 'epsilon' ] >>> float_values = [ 0.0 , 0.25 , 0.5 , 0.75 , 1.0 ] >>> df = pd . DataFrame ({ \"int_col\" : int_values , \"text_col\" : text_values , ... \"float_col\" : float_values }) >>> df int_col text_col float_col 0 1 alpha 0.00 1 2 beta 0.25 2 3 gamma 0.50 3 4 delta 0.75 4 5 epsilon 1.00 Prints information of all columns: >>> df . info ( verbose = True ) < class ' pandas . core . frame . DataFrame '> RangeIndex : 5 entries , 0 to 4 Data columns ( total 3 columns ): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 int_col 5 non - null int64 1 text_col 5 non - null object 2 float_col 5 non - null float64 dtypes : float64 ( 1 ), int64 ( 1 ), object ( 1 ) memory usage : 248.0 + bytes Prints a summary of columns count and its dtypes but not per column information: >>> df . info ( verbose = False ) < class ' pandas . core . frame . DataFrame '> RangeIndex : 5 entries , 0 to 4 Columns : 3 entries , int_col to float_col dtypes : float64 ( 1 ), int64 ( 1 ), object ( 1 ) memory usage : 248.0 + bytes Pipe output of DataFrame.info to buffer instead of sys.stdout, get buffer content and writes to a text file: >>> import io >>> buffer = io . StringIO () >>> df . info ( buf = buffer ) >>> s = buffer . getvalue () >>> with open ( \"df_info.txt\" , \"w\" , ... encoding = \"utf-8\" ) as f : # doctest: +SKIP ... f . write ( s ) 260 The memory_usage parameter allows deep introspection mode, specially useful for big DataFrames and fine-tune memory optimization: >>> random_strings_array = np . random . choice ([ 'a' , 'b' , 'c' ], 10 ** 6 ) >>> df = pd . DataFrame ({ ... 'column_1' : np . random . choice ([ 'a' , 'b' , 'c' ], 10 ** 6 ), ... 'column_2' : np . random . choice ([ 'a' , 'b' , 'c' ], 10 ** 6 ), ... 'column_3' : np . random . choice ([ 'a' , 'b' , 'c' ], 10 ** 6 ) ... }) >>> df . info () < class ' pandas . core . frame . DataFrame '> RangeIndex : 1000000 entries , 0 to 999999 Data columns ( total 3 columns ): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 column_1 1000000 non - null object 1 column_2 1000000 non - null object 2 column_3 1000000 non - null object dtypes : object ( 3 ) memory usage : 22.9 + MB >>> df . info ( memory_usage = 'deep' ) < class ' pandas . core . frame . DataFrame '> RangeIndex : 1000000 entries , 0 to 999999 Data columns ( total 3 columns ): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 column_1 1000000 non - null object 1 column_2 1000000 non - null object 2 column_3 1000000 non - null object dtypes : object ( 3 ) memory usage : 165.9 MB method memory_usage ( index=True , deep=False ) </> Return the memory usage of each column in bytes. The memory usage can optionally include the contribution of the index and elements of object dtype. This value is displayed in DataFrame.info by default. This can be suppressed by setting pandas.options.display.memory_usage to False. Parameters index (bool, default True) \u2014 Specifies whether to include the memory usage of the DataFrame'sindex in returned Series. If index=True , the memory usage of the index is the first item in the output. deep (bool, default False) \u2014 If True, introspect the data deeply by interrogating object dtypes for system-level memory consumption, and include it in the returned values. Returns (Series) A Series whose index is the original column names and whose valuesis the memory usage of each column in bytes. See Also numpy.ndarray.nbytes : Total bytes consumed by the elements of an ndarray. Series.memory_usage : Bytes consumed by a Series. Categorical : Memory-efficient array for string values with many repeated values. DataFrame.info : Concise summary of a DataFrame. Notes See the :ref: Frequently Asked Questions <df-memory-usage> for more details. Examples >>> dtypes = [ 'int64' , 'float64' , 'complex128' , 'object' , 'bool' ] >>> data = dict ([( t , np . ones ( shape = 5000 , dtype = int ) . astype ( t )) ... for t in dtypes ]) >>> df = pd . DataFrame ( data ) >>> df . head () int64 float64 complex128 object bool 0 1 1.0 1.0 + 0.0 j 1 True 1 1 1.0 1.0 + 0.0 j 1 True 2 1 1.0 1.0 + 0.0 j 1 True 3 1 1.0 1.0 + 0.0 j 1 True 4 1 1.0 1.0 + 0.0 j 1 True >>> df . memory_usage () Index 128 int64 40000 float64 40000 complex128 80000 object 40000 bool 5000 dtype : int64 >>> df . memory_usage ( index = False ) int64 40000 float64 40000 complex128 80000 object 40000 bool 5000 dtype : int64 The memory footprint of object dtype columns is ignored by default: >>> df . memory_usage ( deep = True ) Index 128 int64 40000 float64 40000 complex128 80000 object 180000 bool 5000 dtype : int64 Use a Categorical for efficient storage of an object-dtype column with many repeated values. >>> df [ 'object' ] . astype ( 'category' ) . memory_usage ( deep = True ) 5244 method transpose ( *args , copy=False ) </> Transpose index and columns. Reflect the DataFrame over its main diagonal by writing rows as columns and vice-versa. The property :attr: .T is an accessor to the method :meth: transpose . Parameters *args (tuple, optional) \u2014 Accepted for compatibility with NumPy. copy (bool, default False) \u2014 Whether to copy the data after transposing, even for DataFrameswith a single dtype. Note that a copy is always required for mixed dtype DataFrames, or for DataFrames with any extension types. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` Returns (DataFrame) The transposed DataFrame. See Also numpy.transpose : Permute the dimensions of a given array. Notes Transposing a DataFrame with mixed dtypes will result in a homogeneous DataFrame with the object dtype. In such a case, a copy of the data is always made. Examples Square DataFrame with homogeneous dtype >>> d1 = { 'col1' : [ 1 , 2 ], 'col2' : [ 3 , 4 ]} >>> df1 = pd . DataFrame ( data = d1 ) >>> df1 col1 col2 0 1 3 1 2 4 >>> df1_transposed = df1 . T # or df1.transpose() >>> df1_transposed 0 1 col1 1 2 col2 3 4 When the dtype is homogeneous in the original DataFrame, we get a transposed DataFrame with the same dtype: >>> df1 . dtypes col1 int64 col2 int64 dtype : object >>> df1_transposed . dtypes 0 int64 1 int64 dtype : object Non-square DataFrame with mixed dtypes >>> d2 = { 'name' : [ 'Alice' , 'Bob' ], ... 'score' : [ 9.5 , 8 ], ... 'employed' : [ False , True ], ... 'kids' : [ 0 , 0 ]} >>> df2 = pd . DataFrame ( data = d2 ) >>> df2 name score employed kids 0 Alice 9.5 False 0 1 Bob 8.0 True 0 >>> df2_transposed = df2 . T # or df2.transpose() >>> df2_transposed 0 1 name Alice Bob score 9.5 8.0 employed False True kids 0 0 When the DataFrame has mixed dtypes, we get a transposed DataFrame with the object dtype: >>> df2 . dtypes name object score float64 employed bool kids int64 dtype : object >>> df2_transposed . dtypes 0 object 1 object dtype : object method isetitem ( loc , value ) </> Set the given value in the column with position loc . This is a positional analogue to __setitem__ . Parameters loc (int or sequence of ints) \u2014 Index position for the column. value (scalar or arraylike) \u2014 Value(s) for the column. Notes frame.isetitem(loc, value) is an in-place method as it will modify the DataFrame in place (not returning a new object). In contrast to frame.iloc[:, i] = value which will try to update the existing values in place, frame.isetitem(loc, value) will not update the values of the column itself in place, it will instead insert a new array. In cases where frame.columns is unique, this is equivalent to frame[frame.columns[i]] = value . method query ( expr , inplace=False , **kwargs ) </> Query the columns of a DataFrame with a boolean expression. Parameters expr (str) \u2014 The query string to evaluate. You can refer to variables in the environment by prefixing them with an '@' character like @a + b . You can refer to column names that are not valid Python variable names by surrounding them in backticks. Thus, column names containing spaces or punctuations (besides underscores) or starting with digits must be surrounded by backticks. (For example, a column named \"Area (cm^2)\" would be referenced as Area (cm^2) ). Column names which are Python keywords (like \"list\", \"for\", \"import\", etc) cannot be used. For example, if one of your columns is called a a and you want to sum it with b , your query should be `a a` + b . inplace (bool) \u2014 Whether to modify the DataFrame rather than creating a new one. **kwargs \u2014 See the documentation for :func: eval for complete detailson the keyword arguments accepted by :meth: DataFrame.query . Returns (DataFrame or None) DataFrame resulting from the provided query expression orNone if inplace=True . See Also eval : Evaluate a string describing operations on DataFrame columns. DataFrame.eval : Evaluate a string describing operations on DataFrame columns. Notes The result of the evaluation of this expression is first passed to :attr: DataFrame.loc and if that fails because of a multidimensional key (e.g., a DataFrame) then the result will be passed to :meth: DataFrame.__getitem__ . This method uses the top-level :func: eval function to evaluate the passed query. The :meth: ~pandas.DataFrame.query method uses a slightly modified Python syntax by default. For example, the & and | (bitwise) operators have the precedence of their boolean cousins, :keyword: and and :keyword: or . This is syntactically valid Python, however the semantics are different. You can change the semantics of the expression by passing the keyword argument parser='python' . This enforces the same semantics as evaluation in Python space. Likewise, you can pass engine='python' to evaluate an expression using Python itself as a backend. This is not recommended as it is inefficient compared to using numexpr as the engine. The :attr: DataFrame.index and :attr: DataFrame.columns attributes of the :class: ~pandas.DataFrame instance are placed in the query namespace by default, which allows you to treat both the index and columns of the frame as a column in the frame. The identifier index is used for the frame index; you can also use the name of the index to identify it in a query. Please note that Python keywords may not be used as identifiers. For further details and examples see the query documentation in :ref: indexing <indexing.query> . Backtick quoted variables Backtick quoted variables are parsed as literal Python code and are converted internally to a Python valid identifier. This can lead to the following problems. During parsing a number of disallowed characters inside the backtick quoted string are replaced by strings that are allowed as a Python identifier. These characters include all operators in Python, the space character, the question mark, the exclamation mark, the dollar sign, and the euro sign. For other characters that fall outside the ASCII range (U+0001..U+007F) and those that are not further specified in PEP 3131, the query parser will raise an error. This excludes whitespace different than the space character, but also the hashtag (as it is used for comments) and the backtick itself (backtick can also not be escaped). In a special case, quotes that make a pair around a backtick can confuse the parser. For example, it's` > `that's will raise an error, as it forms a quoted string ( 's > `that' ) with a backtick inside. See also the Python documentation about lexical analysis (https://docs.python.org/3/reference/lexical_analysis.html) in combination with the source code in :mod: pandas.core.computation.parsing . Examples >>> df = pd . DataFrame ({ 'A' : range ( 1 , 6 ), ... 'B' : range ( 10 , 0 , - 2 ), ... 'C C' : range ( 10 , 5 , - 1 )}) >>> df A B C C 0 1 10 10 1 2 8 9 2 3 6 8 3 4 4 7 4 5 2 6 >>> df . query ( 'A > B' ) A B C C 4 5 2 6 The previous expression is equivalent to >>> df [ df . A > df . B ] A B C C 4 5 2 6 For columns with spaces in their name, you can use backtick quoting. >>> df . query ( 'B == `C C`' ) A B C C 0 1 10 10 The previous expression is equivalent to >>> df [ df . B == df [ 'C C' ]] A B C C 0 1 10 10 method eval ( expr , inplace=False , **kwargs ) </> Evaluate a string describing operations on DataFrame columns. Operates on columns only, not specific rows or elements. This allows eval to run arbitrary code, which can make you vulnerable to code injection if you pass user input to this function. Parameters expr (str) \u2014 The expression string to evaluate. inplace (bool, default False) \u2014 If the expression contains an assignment, whether to perform theoperation inplace and mutate the existing DataFrame. Otherwise, a new DataFrame is returned. **kwargs \u2014 See the documentation for :func: eval for complete detailson the keyword arguments accepted by :meth: ~pandas.DataFrame.query . Returns (ndarray, scalar, pandas object, or None) The result of the evaluation or None if inplace=True . See Also DataFrame.query : Evaluates a boolean expression to query the columns of a frame. DataFrame.assign : Can evaluate an expression or function to create new values for a column. eval : Evaluate a Python expression as a string using various backends. Notes For more details see the API documentation for :func: ~eval . For detailed examples see :ref: enhancing performance with eval <enhancingperf.eval> . Examples >>> df = pd . DataFrame ({ 'A' : range ( 1 , 6 ), 'B' : range ( 10 , 0 , - 2 )}) >>> df A B 0 1 10 1 2 8 2 3 6 3 4 4 4 5 2 >>> df . eval ( 'A + B' ) 0 11 1 10 2 9 3 8 4 7 dtype : int64 Assignment is allowed though by default the original DataFrame is not modified. >>> df . eval ( 'C = A + B' ) A B C 0 1 10 11 1 2 8 10 2 3 6 9 3 4 4 8 4 5 2 7 >>> df A B 0 1 10 1 2 8 2 3 6 3 4 4 4 5 2 Multiple columns can be assigned to using multi-line expressions: >>> df . eval ( ... ''' ... C = A + B ... D = A - B ... ''' ... ) A B C D 0 1 10 11 - 9 1 2 8 10 - 6 2 3 6 9 - 3 3 4 4 8 0 4 5 2 7 3 method select_dtypes ( include=None , exclude=None ) </> Return a subset of the DataFrame's columns based on the column dtypes. Returns (DataFrame) The subset of the frame including the dtypes in include andexcluding the dtypes in exclude . Raises ValueError \u2014 If both of include and exclude are empty If include and exclude have overlapping elements If any kind of string dtype is passed in. See Also DataFrame.dtypes: Return Series with the data type of each column. Notes To select all numeric types, use np.number or 'number' To select strings you must use the object dtype, but note that this will return all object dtype columns. With pd.options.future.infer_string enabled, using \"str\" will work to select all string columns. See the numpy dtype hierarchy <https://numpy.org/doc/stable/reference/arrays.scalars.html> __ To select datetimes, use np.datetime64 , 'datetime' or 'datetime64' To select timedeltas, use np.timedelta64 , 'timedelta' or 'timedelta64' To select Pandas categorical dtypes, use 'category' To select Pandas datetimetz dtypes, use 'datetimetz' or 'datetime64[ns, tz]' Examples >>> df = pd . DataFrame ({ 'a' : [ 1 , 2 ] * 3 , ... 'b' : [ True , False ] * 3 , ... 'c' : [ 1.0 , 2.0 ] * 3 }) >>> df a b c 0 1 True 1.0 1 2 False 2.0 2 1 True 1.0 3 2 False 2.0 4 1 True 1.0 5 2 False 2.0 >>> df . select_dtypes ( include = 'bool' ) b 0 True 1 False 2 True 3 False 4 True 5 False >>> df . select_dtypes ( include = [ 'float64' ]) c 0 1.0 1 2.0 2 1.0 3 2.0 4 1.0 5 2.0 >>> df . select_dtypes ( exclude = [ 'int64' ]) b c 0 True 1.0 1 False 2.0 2 True 1.0 3 False 2.0 4 True 1.0 5 False 2.0 method insert ( loc , column , value , allow_duplicates=<no_default> ) </> Insert column into DataFrame at specified location. Raises a ValueError if column is already contained in the DataFrame, unless allow_duplicates is set to True. Parameters loc (int) \u2014 Insertion index. Must verify 0 <= loc <= len(columns). column (str, number, or hashable object) \u2014 Label of the inserted column. value (Scalar, Series, or array-like) \u2014 Content of the inserted column. allow_duplicates (bool, optional, default lib.no_default) \u2014 Allow duplicate column labels to be created. See Also Index.insert : Insert new item by index. Examples >>> df = pd . DataFrame ({ 'col1' : [ 1 , 2 ], 'col2' : [ 3 , 4 ]}) >>> df col1 col2 0 1 3 1 2 4 >>> df . insert ( 1 , \"newcol\" , [ 99 , 99 ]) >>> df col1 newcol col2 0 1 99 3 1 2 99 4 >>> df . insert ( 0 , \"col1\" , [ 100 , 100 ], allow_duplicates = True ) >>> df col1 col1 newcol col2 0 100 1 99 3 1 100 2 99 4 Notice that pandas uses index alignment in case of value from type Series : >>> df . insert ( 0 , \"col0\" , pd . Series ([ 5 , 6 ], index = [ 1 , 2 ])) >>> df col0 col1 col1 newcol col2 0 NaN 100 1 99 3 1 5.0 100 2 99 4 method assign ( **kwargs ) </> Assign new columns to a DataFrame. Returns a new object with all original columns in addition to new ones. Existing columns that are re-assigned will be overwritten. Parameters **kwargs (dict of {str: callable or Series}) \u2014 The column names are keywords. If the values arecallable, they are computed on the DataFrame and assigned to the new columns. The callable must not change input DataFrame (though pandas doesn't check it). If the values are not callable, (e.g. a Series, scalar, or array), they are simply assigned. Returns (DataFrame) A new DataFrame with the new columns in addition toall the existing columns. Notes Assigning multiple columns within the same assign is possible. Later items in '**kwargs' may refer to newly created or modified columns in 'df'; items are computed and assigned into 'df' in order. Examples >>> df = pd . DataFrame ({ 'temp_c' : [ 17.0 , 25.0 ]}, ... index = [ 'Portland' , 'Berkeley' ]) >>> df temp_c Portland 17.0 Berkeley 25.0 Where the value is a callable, evaluated on df : >>> df . assign ( temp_f = lambda x : x . temp_c * 9 / 5 + 32 ) temp_c temp_f Portland 17.0 62.6 Berkeley 25.0 77.0 Alternatively, the same behavior can be achieved by directly referencing an existing Series or sequence: >>> df . assign ( temp_f = df [ 'temp_c' ] * 9 / 5 + 32 ) temp_c temp_f Portland 17.0 62.6 Berkeley 25.0 77.0 You can create multiple columns within the same assign where one of the columns depends on another one defined within the same assign: >>> df . assign ( temp_f = lambda x : x [ 'temp_c' ] * 9 / 5 + 32 , ... temp_k = lambda x : ( x [ 'temp_f' ] + 459.67 ) * 5 / 9 ) temp_c temp_f temp_k Portland 17.0 62.6 290.15 Berkeley 25.0 77.0 298.15 method set_axis ( labels , axis=0 , copy=None ) </> Assign desired index to given axis. Indexes for column or row labels can be changed by assigning a list-like or Index. Parameters labels (list-like, Index) \u2014 The values for the new index. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to update. The value 0 identifies the rows. For Series this parameter is unused and defaults to 0. copy (bool, default True) \u2014 Whether to make a copy of the underlying data. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` Returns (DataFrame) An object of type DataFrame. See Also DataFrame.renameaxis : Alter the name of the index or columns. Examples -------- ~~~python df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]}) ~~~ Change the row labels. >>> df . set_axis ([ 'a' , 'b' , 'c' ], axis = 'index' ) A B a 1 4 b 2 5 c 3 6 Change the column labels. >>> df . set_axis ([ 'I' , 'II' ], axis = 'columns' ) I II 0 1 4 1 2 5 2 3 6 method reindex ( labels=None , index=None , columns=None , axis=None , method=None , copy=None , level=None , fill_value=nan , limit=None , tolerance=None ) </> Conform DataFrame to new index with optional filling logic. Places NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False . See Also DataFrame.set_index : Set row labels.DataFrame.reset_index : Remove row labels or move them to new columns. DataFrame.reindexlike : Change to same indices as other DataFrame. Examples DataFrame.reindex supports two calling conventions (index=index_labels, columns=column_labels, ...) (labels, axis={'index', 'columns'}, ...) We highly recommend using keyword arguments to clarify your intent. Create a dataframe with some fictional data. >>> index = [ 'Firefox' , 'Chrome' , 'Safari' , 'IE10' , 'Konqueror' ] >>> df = pd . DataFrame ({ 'http_status' : [ 200 , 200 , 404 , 404 , 301 ], ... 'response_time' : [ 0.04 , 0.02 , 0.07 , 0.08 , 1.0 ]}, ... index = index ) >>> df http_status response_time Firefox 200 0.04 Chrome 200 0.02 Safari 404 0.07 IE10 404 0.08 Konqueror 301 1.00 Create a new index and reindex the dataframe. By default values in the new index that do not have corresponding records in the dataframe are assigned NaN . >>> new_index = [ 'Safari' , 'Iceweasel' , 'Comodo Dragon' , 'IE10' , ... 'Chrome' ] >>> df . reindex ( new_index ) http_status response_time Safari 404.0 0.07 Iceweasel NaN NaN Comodo Dragon NaN NaN IE10 404.0 0.08 Chrome 200.0 0.02 We can fill in the missing values by passing a value to the keyword fill_value . Because the index is not monotonically increasing or decreasing, we cannot use arguments to the keyword method to fill the NaN values. >>> df . reindex ( new_index , fill_value = 0 ) http_status response_time Safari 404 0.07 Iceweasel 0 0.00 Comodo Dragon 0 0.00 IE10 404 0.08 Chrome 200 0.02 >>> df . reindex ( new_index , fill_value = 'missing' ) http_status response_time Safari 404 0.07 Iceweasel missing missing Comodo Dragon missing missing IE10 404 0.08 Chrome 200 0.02 We can also reindex the columns. >>> df . reindex ( columns = [ 'http_status' , 'user_agent' ]) http_status user_agent Firefox 200 NaN Chrome 200 NaN Safari 404 NaN IE10 404 NaN Konqueror 301 NaN Or we can use \"axis-style\" keyword arguments >>> df . reindex ([ 'http_status' , 'user_agent' ], axis = \"columns\" ) http_status user_agent Firefox 200 NaN Chrome 200 NaN Safari 404 NaN IE10 404 NaN Konqueror 301 NaN To further illustrate the filling functionality in reindex , we will create a dataframe with a monotonically increasing index (for example, a sequence of dates). >>> date_index = pd . date_range ( '1/1/2010' , periods = 6 , freq = 'D' ) >>> df2 = pd . DataFrame ({ \"prices\" : [ 100 , 101 , np . nan , 100 , 89 , 88 ]}, ... index = date_index ) >>> df2 prices 2010 - 01 - 01 100.0 2010 - 01 - 02 101.0 2010 - 01 - 03 NaN 2010 - 01 - 04 100.0 2010 - 01 - 05 89.0 2010 - 01 - 06 88.0 Suppose we decide to expand the dataframe to cover a wider date range. >>> date_index2 = pd . date_range ( '12/29/2009' , periods = 10 , freq = 'D' ) >>> df2 . reindex ( date_index2 ) prices 2009 - 12 - 29 NaN 2009 - 12 - 30 NaN 2009 - 12 - 31 NaN 2010 - 01 - 01 100.0 2010 - 01 - 02 101.0 2010 - 01 - 03 NaN 2010 - 01 - 04 100.0 2010 - 01 - 05 89.0 2010 - 01 - 06 88.0 2010 - 01 - 07 NaN The index entries that did not have a value in the original data frame (for example, '2009-12-29') are by default filled with NaN . If desired, we can fill in the missing values using one of several options. For example, to back-propagate the last valid value to fill the NaN values, pass bfill as an argument to the method keyword. >>> df2 . reindex ( date_index2 , method = 'bfill' ) prices 2009 - 12 - 29 100.0 2009 - 12 - 30 100.0 2009 - 12 - 31 100.0 2010 - 01 - 01 100.0 2010 - 01 - 02 101.0 2010 - 01 - 03 NaN 2010 - 01 - 04 100.0 2010 - 01 - 05 89.0 2010 - 01 - 06 88.0 2010 - 01 - 07 NaN Please note that the NaN value present in the original dataframe (at index value 2010-01-03) will not be filled by any of the value propagation schemes. This is because filling while reindexing does not look at dataframe values, but only compares the original and desired indexes. If you do want to fill in the NaN values present in the original dataframe, use the fillna() method. See the :ref: user guide <basics.reindexing> for more. method drop ( labels=None , axis=0 , index=None , columns=None , level=None , inplace=False , errors='raise' ) </> Drop specified labels from rows or columns. Remove rows or columns by specifying label names and corresponding axis, or by directly specifying index or column names. When using a multi-index, labels on different levels can be removed by specifying the level. See the :ref: user guide <advanced.shown_levels> for more information about the now unused levels. Parameters labels (single label or list-like) \u2014 Index or column labels to drop. A tuple will be used as a singlelabel and not treated as a list-like. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Whether to drop labels from the index (0 or 'index') orcolumns (1 or 'columns'). index (single label or list-like) \u2014 Alternative to specifying axis ( labels, axis=0 is equivalent to index=labels ). columns (single label or list-like) \u2014 Alternative to specifying axis ( labels, axis=1 is equivalent to columns=labels ). level (int or level name, optional) \u2014 For MultiIndex, level from which the labels will be removed. inplace (bool, default False) \u2014 If False, return a copy. Otherwise, do operationin place and return None. errors ({'ignore', 'raise'}, default 'raise') \u2014 If 'ignore', suppress error and only existing labels aredropped. Returns (DataFrame or None) Returns DataFrame or None DataFrame with the specifiedindex or column labels removed or None if inplace=True. Raises KeyError \u2014 If any of the labels is not found in the selected axis. See Also DataFrame.loc : Label-location based indexer for selection by label.DataFrame.dropna : Return DataFrame with labels on given axis omitted where (all or any) data are missing. DataFrame.dropduplicates : Return DataFrame with duplicate rows removed, optionally only considering certain columns. Series.drop : Return Series with specified index labels removed. Examples >>> df = pd . DataFrame ( np . arange ( 12 ) . reshape ( 3 , 4 ), ... columns = [ 'A' , 'B' , 'C' , 'D' ]) >>> df A B C D 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 Drop columns >>> df . drop ([ 'B' , 'C' ], axis = 1 ) A D 0 0 3 1 4 7 2 8 11 >>> df . drop ( columns = [ 'B' , 'C' ]) A D 0 0 3 1 4 7 2 8 11 Drop a row by index >>> df . drop ([ 0 , 1 ]) A B C D 2 8 9 10 11 Drop columns and/or rows of MultiIndex DataFrame >>> midx = pd . MultiIndex ( levels = [[ 'llama' , 'cow' , 'falcon' ], ... [ 'speed' , 'weight' , 'length' ]], ... codes = [[ 0 , 0 , 0 , 1 , 1 , 1 , 2 , 2 , 2 ], ... [ 0 , 1 , 2 , 0 , 1 , 2 , 0 , 1 , 2 ]]) >>> df = pd . DataFrame ( index = midx , columns = [ 'big' , 'small' ], ... data = [[ 45 , 30 ], [ 200 , 100 ], [ 1.5 , 1 ], [ 30 , 20 ], ... [ 250 , 150 ], [ 1.5 , 0.8 ], [ 320 , 250 ], ... [ 1 , 0.8 ], [ 0.3 , 0.2 ]]) >>> df big small llama speed 45.0 30.0 weight 200.0 100.0 length 1.5 1.0 cow speed 30.0 20.0 weight 250.0 150.0 length 1.5 0.8 falcon speed 320.0 250.0 weight 1.0 0.8 length 0.3 0.2 Drop a specific index combination from the MultiIndex DataFrame, i.e., drop the combination 'falcon' and 'weight' , which deletes only the corresponding row >>> df . drop ( index = ( 'falcon' , 'weight' )) big small llama speed 45.0 30.0 weight 200.0 100.0 length 1.5 1.0 cow speed 30.0 20.0 weight 250.0 150.0 length 1.5 0.8 falcon speed 320.0 250.0 length 0.3 0.2 >>> df . drop ( index = 'cow' , columns = 'small' ) big llama speed 45.0 weight 200.0 length 1.5 falcon speed 320.0 weight 1.0 length 0.3 >>> df . drop ( index = 'length' , level = 1 ) big small llama speed 45.0 30.0 weight 200.0 100.0 cow speed 30.0 20.0 weight 250.0 150.0 falcon speed 320.0 250.0 weight 1.0 0.8 method rename ( mapper=None , index=None , columns=None , axis=None , copy=None , inplace=False , level=None , errors='ignore' ) </> Rename columns or index labels. Function / dict values must be unique (1-to-1). Labels not contained in a dict / Series will be left as-is. Extra labels listed don't throw an error. See the :ref: user guide <basics.rename> for more. Parameters mapper (dict-like or function) \u2014 Dict-like or function transformations to apply tothat axis' values. Use either mapper and axis to specify the axis to target with mapper , or index and columns . index (dict-like or function) \u2014 Alternative to specifying axis ( mapper, axis=0 is equivalent to index=mapper ). columns (dict-like or function) \u2014 Alternative to specifying axis ( mapper, axis=1 is equivalent to columns=mapper ). axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Axis to target with mapper . Can be either the axis name('index', 'columns') or number (0, 1). The default is 'index'. copy (bool, default True) \u2014 Also copy underlying data. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` inplace (bool, default False) \u2014 Whether to modify the DataFrame rather than creating a new one.If True then value of copy is ignored. level (int or level name, default None) \u2014 In case of a MultiIndex, only rename labels in the specifiedlevel. errors ({'ignore', 'raise'}, default 'ignore') \u2014 If 'raise', raise a KeyError when a dict-like mapper , index ,or columns contains labels that are not present in the Index being transformed. If 'ignore', existing keys will be renamed and extra keys will be ignored. Returns (DataFrame or None) DataFrame with the renamed axis labels or None if inplace=True . Raises KeyError \u2014 If any of the labels is not found in the selected axis and\"errors='raise'\". See Also DataFrame.renameaxis : Set the name of the axis. Examples DataFrame.rename supports two calling conventions (index=index_mapper, columns=columns_mapper, ...) (mapper, axis={'index', 'columns'}, ...) We highly recommend using keyword arguments to clarify your intent. Rename columns using a mapping: >>> df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ]}) >>> df . rename ( columns = { \"A\" : \"a\" , \"B\" : \"c\" }) a c 0 1 4 1 2 5 2 3 6 Rename index using a mapping: >>> df . rename ( index = { 0 : \"x\" , 1 : \"y\" , 2 : \"z\" }) A B x 1 4 y 2 5 z 3 6 Cast index labels to a different type: >>> df . index RangeIndex ( start = 0 , stop = 3 , step = 1 ) >>> df . rename ( index = str ) . index Index ([ '0' , '1' , '2' ], dtype = 'object' ) >>> df . rename ( columns = { \"A\" : \"a\" , \"B\" : \"b\" , \"C\" : \"c\" }, errors = \"raise\" ) Traceback ( most recent call last ): KeyError : [ 'C' ] not found in axis Using axis-style parameters: >>> df . rename ( str . lower , axis = 'columns' ) a b 0 1 4 1 2 5 2 3 6 >>> df . rename ({ 1 : 2 , 2 : 4 }, axis = 'index' ) A B 0 1 4 2 2 5 4 3 6 method pop ( item ) </> Return item and drop from frame. Raise KeyError if not found. Parameters item (label) \u2014 Label of column to be popped. Examples >>> df = pd . DataFrame ([( 'falcon' , 'bird' , 389.0 ), ... ( 'parrot' , 'bird' , 24.0 ), ... ( 'lion' , 'mammal' , 80.5 ), ... ( 'monkey' , 'mammal' , np . nan )], ... columns = ( 'name' , 'class' , 'max_speed' )) >>> df name class max_speed 0 falcon bird 389.0 1 parrot bird 24.0 2 lion mammal 80.5 3 monkey mammal NaN >>> df . pop ( 'class' ) 0 bird 1 bird 2 mammal 3 mammal Name : class , dtype : object >>> df name max_speed 0 falcon 389.0 1 parrot 24.0 2 lion 80.5 3 monkey NaN method shift ( periods=1 , freq=None , axis=0 , fill_value=<no_default> , suffix=None ) </> Shift index by desired number of periods with an optional time freq . When freq is not passed, shift the index without realigning the data. If freq is passed (in this case, the index must be date or datetime, or it will raise a NotImplementedError ), the index will be increased using the periods and the freq . freq can be inferred when specified as \"infer\" as long as either freq or inferred_freq attribute is set in the index. Parameters periods (int or Sequence) \u2014 Number of periods to shift. Can be positive or negative.If an iterable of ints, the data will be shifted once by each int. This is equivalent to shifting by one value at a time and concatenating all resulting frames. The resulting columns will have the shift suffixed to their column names. For multiple periods, axis must not be 1. freq (DateOffset, tseries.offsets, timedelta, or str, optional) \u2014 Offset to use from the tseries module or time rule (e.g. 'EOM').If freq is specified then the index values are shifted but the data is not realigned. That is, use freq if you would like to extend the index when shifting and preserve the original data. If freq is specified as \"infer\" then it will be inferred from the freq or inferred_freq attributes of the index. If neither of those attributes exist, a ValueError is thrown. axis ({0 or 'index', 1 or 'columns', None}, default None) \u2014 Shift direction. For Series this parameter is unused and defaults to 0. fill_value (object, optional) \u2014 The scalar value to use for newly introduced missing values.the default depends on the dtype of self . For numeric data, np.nan is used. For datetime, timedelta, or period data, etc. :attr: NaT is used. For extension dtypes, self.dtype.na_value is used. suffix (str, optional) \u2014 If str and periods is an iterable, this is added after the columnname and before the shift value for each shifted column name. Returns (DataFrame) Copy of input object, shifted. See Also Index.shift : Shift values of Index.DatetimeIndex.shift : Shift values of DatetimeIndex. PeriodIndex.shift : Shift values of PeriodIndex. Examples >>> df = pd . DataFrame ({ \"Col1\" : [ 10 , 20 , 15 , 30 , 45 ], ... \"Col2\" : [ 13 , 23 , 18 , 33 , 48 ], ... \"Col3\" : [ 17 , 27 , 22 , 37 , 52 ]}, ... index = pd . date_range ( \"2020-01-01\" , \"2020-01-05\" )) >>> df Col1 Col2 Col3 2020 - 01 - 01 10 13 17 2020 - 01 - 02 20 23 27 2020 - 01 - 03 15 18 22 2020 - 01 - 04 30 33 37 2020 - 01 - 05 45 48 52 >>> df . shift ( periods = 3 ) Col1 Col2 Col3 2020 - 01 - 01 NaN NaN NaN 2020 - 01 - 02 NaN NaN NaN 2020 - 01 - 03 NaN NaN NaN 2020 - 01 - 04 10.0 13.0 17.0 2020 - 01 - 05 20.0 23.0 27.0 >>> df . shift ( periods = 1 , axis = \"columns\" ) Col1 Col2 Col3 2020 - 01 - 01 NaN 10 13 2020 - 01 - 02 NaN 20 23 2020 - 01 - 03 NaN 15 18 2020 - 01 - 04 NaN 30 33 2020 - 01 - 05 NaN 45 48 >>> df . shift ( periods = 3 , fill_value = 0 ) Col1 Col2 Col3 2020 - 01 - 01 0 0 0 2020 - 01 - 02 0 0 0 2020 - 01 - 03 0 0 0 2020 - 01 - 04 10 13 17 2020 - 01 - 05 20 23 27 >>> df . shift ( periods = 3 , freq = \"D\" ) Col1 Col2 Col3 2020 - 01 - 04 10 13 17 2020 - 01 - 05 20 23 27 2020 - 01 - 06 15 18 22 2020 - 01 - 07 30 33 37 2020 - 01 - 08 45 48 52 >>> df . shift ( periods = 3 , freq = \"infer\" ) Col1 Col2 Col3 2020 - 01 - 04 10 13 17 2020 - 01 - 05 20 23 27 2020 - 01 - 06 15 18 22 2020 - 01 - 07 30 33 37 2020 - 01 - 08 45 48 52 >>> df [ 'Col1' ] . shift ( periods = [ 0 , 1 , 2 ]) Col1_0 Col1_1 Col1_2 2020 - 01 - 01 10 NaN NaN 2020 - 01 - 02 20 10.0 NaN 2020 - 01 - 03 15 20.0 10.0 2020 - 01 - 04 30 15.0 20.0 2020 - 01 - 05 45 30.0 15.0 method set_index ( keys , drop=True , append=False , inplace=False , verify_integrity=False ) </> Set the DataFrame index using existing columns. Set the DataFrame index (row labels) using one or more existing columns or arrays (of the correct length). The index can replace the existing index or expand on it. Parameters keys (label or array-like or list of labels/arrays) \u2014 This parameter can be either a single column key, a single array ofthe same length as the calling DataFrame, or a list containing an arbitrary combination of column keys and arrays. Here, \"array\" encompasses :class: Series , :class: Index , np.ndarray , and instances of :class: ~collections.abc.Iterator . drop (bool, default True) \u2014 Delete columns to be used as the new index. append (bool, default False) \u2014 Whether to append columns to existing index. inplace (bool, default False) \u2014 Whether to modify the DataFrame rather than creating a new one. verify_integrity (bool, default False) \u2014 Check the new index for duplicates. Otherwise defer the check untilnecessary. Setting to False will improve the performance of this method. Returns (DataFrame or None) Changed row labels or None if inplace=True . See Also DataFrame.reset_index : Opposite of set_index.DataFrame.reindex : Change to new indices or expand indices. DataFrame.reindexlike : Change to same indices as other DataFrame. Examples >>> df = pd . DataFrame ({ 'month' : [ 1 , 4 , 7 , 10 ], ... 'year' : [ 2012 , 2014 , 2013 , 2014 ], ... 'sale' : [ 55 , 40 , 84 , 31 ]}) >>> df month year sale 0 1 2012 55 1 4 2014 40 2 7 2013 84 3 10 2014 31 Set the index to become the 'month' column: >>> df . set_index ( 'month' ) year sale month 1 2012 55 4 2014 40 7 2013 84 10 2014 31 Create a MultiIndex using columns 'year' and 'month': >>> df . set_index ([ 'year' , 'month' ]) sale year month 2012 1 55 2014 4 40 2013 7 84 2014 10 31 Create a MultiIndex using an Index and a column: >>> df . set_index ([ pd . Index ([ 1 , 2 , 3 , 4 ]), 'year' ]) month sale year 1 2012 1 55 2 2014 4 40 3 2013 7 84 4 2014 10 31 Create a MultiIndex using two Series: >>> s = pd . Series ([ 1 , 2 , 3 , 4 ]) >>> df . set_index ([ s , s ** 2 ]) month year sale 1 1 1 2012 55 2 4 4 2014 40 3 9 7 2013 84 4 16 10 2014 31 method reset_index ( level=None , drop=False , inplace=False , col_level=0 , col_fill='' , allow_duplicates=<no_default> , names=None ) </> Reset the index, or a level of it. Reset the index of the DataFrame, and use the default one instead. If the DataFrame has a MultiIndex, this method can remove one or more levels. Parameters level (int, str, tuple, or list, default None) \u2014 Only remove the given levels from the index. Removes all levels bydefault. drop (bool, default False) \u2014 Do not try to insert index into dataframe columns. This resetsthe index to the default integer index. inplace (bool, default False) \u2014 Whether to modify the DataFrame rather than creating a new one. col_level (int or str, default 0) \u2014 If the columns have multiple levels, determines which level thelabels are inserted into. By default it is inserted into the first level. col_fill (object, default '') \u2014 If the columns have multiple levels, determines how the otherlevels are named. If None then the index name is repeated. allow_duplicates (bool, optional, default lib.no_default) \u2014 Allow duplicate column labels to be created. .. versionadded:: 1.5.0 names (int, str or 1-dimensional list, default None) \u2014 Using the given string, rename the DataFrame column which contains theindex data. If the DataFrame has a MultiIndex, this has to be a list or tuple with length equal to the number of levels. .. versionadded:: 1.5.0 Returns (DataFrame or None) DataFrame with the new index or None if inplace=True . See Also DataFrame.set_index : Opposite of reset_index.DataFrame.reindex : Change to new indices or expand indices. DataFrame.reindexlike : Change to same indices as other DataFrame. Examples >>> df = pd . DataFrame ([( 'bird' , 389.0 ), ... ( 'bird' , 24.0 ), ... ( 'mammal' , 80.5 ), ... ( 'mammal' , np . nan )], ... index = [ 'falcon' , 'parrot' , 'lion' , 'monkey' ], ... columns = ( 'class' , 'max_speed' )) >>> df class max_speed falcon bird 389.0 parrot bird 24.0 lion mammal 80.5 monkey mammal NaN When we reset the index, the old index is added as a column, and a new sequential index is used: >>> df . reset_index () index class max_speed 0 falcon bird 389.0 1 parrot bird 24.0 2 lion mammal 80.5 3 monkey mammal NaN We can use the drop parameter to avoid the old index being added as a column: >>> df . reset_index ( drop = True ) class max_speed 0 bird 389.0 1 bird 24.0 2 mammal 80.5 3 mammal NaN You can also use reset_index with MultiIndex . >>> index = pd . MultiIndex . from_tuples ([( 'bird' , 'falcon' ), ... ( 'bird' , 'parrot' ), ... ( 'mammal' , 'lion' ), ... ( 'mammal' , 'monkey' )], ... names = [ 'class' , 'name' ]) >>> columns = pd . MultiIndex . from_tuples ([( 'speed' , 'max' ), ... ( 'species' , 'type' )]) >>> df = pd . DataFrame ([( 389.0 , 'fly' ), ... ( 24.0 , 'fly' ), ... ( 80.5 , 'run' ), ... ( np . nan , 'jump' )], ... index = index , ... columns = columns ) >>> df speed species max type class name bird falcon 389.0 fly parrot 24.0 fly mammal lion 80.5 run monkey NaN jump Using the names parameter, choose a name for the index column: >>> df . reset_index ( names = [ 'classes' , 'names' ]) classes names speed species max type 0 bird falcon 389.0 fly 1 bird parrot 24.0 fly 2 mammal lion 80.5 run 3 mammal monkey NaN jump If the index has multiple levels, we can reset a subset of them: >>> df . reset_index ( level = 'class' ) class speed species max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump If we are not dropping the index, by default, it is placed in the top level. We can place it in another level: >>> df . reset_index ( level = 'class' , col_level = 1 ) speed species class max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump When the index is inserted under another level, we can specify under which one with the parameter col_fill : >>> df . reset_index ( level = 'class' , col_level = 1 , col_fill = 'species' ) species speed species class max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump If we specify a nonexistent level for col_fill , it is created: >>> df . reset_index ( level = 'class' , col_level = 1 , col_fill = 'genus' ) genus speed species class max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump method isna ( ) </> Detect missing values. Return a boolean same-sized object indicating if the values are NA. NA values, such as None or :attr: numpy.NaN , gets mapped to True values. Everything else gets mapped to False values. Characters such as empty strings '' or :attr: numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True ). Returns (DataFrame) Mask of bool values for each element in DataFrame thatindicates whether an element is an NA value. See Also DataFrame.isnull : Alias of isna.DataFrame.notna : Boolean inverse of isna. DataFrame.dropna : Omit axes labels with missing values. isna : Top-level isna. Examples Show which entries in a DataFrame are NA. >>> df = pd . DataFrame ( dict ( age = [ 5 , 6 , np . nan ], ... born = [ pd . NaT , pd . Timestamp ( '1939-05-27' ), ... pd . Timestamp ( '1940-04-25' )], ... name = [ 'Alfred' , 'Batman' , '' ], ... toy = [ None , 'Batmobile' , 'Joker' ])) >>> df age born name toy 0 5.0 NaT Alfred None 1 6.0 1939 - 05 - 27 Batman Batmobile 2 NaN 1940 - 04 - 25 Joker >>> df . isna () age born name toy 0 False True False True 1 False False False False 2 True False False False Show which entries in a Series are NA. >>> ser = pd . Series ([ 5 , 6 , np . nan ]) >>> ser 0 5.0 1 6.0 2 NaN dtype : float64 >>> ser . isna () 0 False 1 False 2 True dtype : bool method isnull ( ) </> DataFrame.isnull is an alias for DataFrame.isna. Detect missing values. Return a boolean same-sized object indicating if the values are NA. NA values, such as None or :attr: numpy.NaN , gets mapped to True values. Everything else gets mapped to False values. Characters such as empty strings '' or :attr: numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True ). Returns (DataFrame) Mask of bool values for each element in DataFrame thatindicates whether an element is an NA value. See Also DataFrame.isnull : Alias of isna.DataFrame.notna : Boolean inverse of isna. DataFrame.dropna : Omit axes labels with missing values. isna : Top-level isna. Examples Show which entries in a DataFrame are NA. >>> df = pd . DataFrame ( dict ( age = [ 5 , 6 , np . nan ], ... born = [ pd . NaT , pd . Timestamp ( '1939-05-27' ), ... pd . Timestamp ( '1940-04-25' )], ... name = [ 'Alfred' , 'Batman' , '' ], ... toy = [ None , 'Batmobile' , 'Joker' ])) >>> df age born name toy 0 5.0 NaT Alfred None 1 6.0 1939 - 05 - 27 Batman Batmobile 2 NaN 1940 - 04 - 25 Joker >>> df . isna () age born name toy 0 False True False True 1 False False False False 2 True False False False Show which entries in a Series are NA. >>> ser = pd . Series ([ 5 , 6 , np . nan ]) >>> ser 0 5.0 1 6.0 2 NaN dtype : float64 >>> ser . isna () 0 False 1 False 2 True dtype : bool method notna ( ) </> Detect existing (non-missing) values. Return a boolean same-sized object indicating if the values are not NA. Non-missing values get mapped to True. Characters such as empty strings '' or :attr: numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True ). NA values, such as None or :attr: numpy.NaN , get mapped to False values. Returns (DataFrame) Mask of bool values for each element in DataFrame thatindicates whether an element is not an NA value. See Also DataFrame.notnull : Alias of notna.DataFrame.isna : Boolean inverse of notna. DataFrame.dropna : Omit axes labels with missing values. notna : Top-level notna. Examples Show which entries in a DataFrame are not NA. >>> df = pd . DataFrame ( dict ( age = [ 5 , 6 , np . nan ], ... born = [ pd . NaT , pd . Timestamp ( '1939-05-27' ), ... pd . Timestamp ( '1940-04-25' )], ... name = [ 'Alfred' , 'Batman' , '' ], ... toy = [ None , 'Batmobile' , 'Joker' ])) >>> df age born name toy 0 5.0 NaT Alfred None 1 6.0 1939 - 05 - 27 Batman Batmobile 2 NaN 1940 - 04 - 25 Joker >>> df . notna () age born name toy 0 True False True False 1 True True True True 2 False True True True Show which entries in a Series are not NA. >>> ser = pd . Series ([ 5 , 6 , np . nan ]) >>> ser 0 5.0 1 6.0 2 NaN dtype : float64 >>> ser . notna () 0 True 1 True 2 False dtype : bool method notnull ( ) </> DataFrame.notnull is an alias for DataFrame.notna. Detect existing (non-missing) values. Return a boolean same-sized object indicating if the values are not NA. Non-missing values get mapped to True. Characters such as empty strings '' or :attr: numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True ). NA values, such as None or :attr: numpy.NaN , get mapped to False values. Returns (DataFrame) Mask of bool values for each element in DataFrame thatindicates whether an element is not an NA value. See Also DataFrame.notnull : Alias of notna.DataFrame.isna : Boolean inverse of notna. DataFrame.dropna : Omit axes labels with missing values. notna : Top-level notna. Examples Show which entries in a DataFrame are not NA. >>> df = pd . DataFrame ( dict ( age = [ 5 , 6 , np . nan ], ... born = [ pd . NaT , pd . Timestamp ( '1939-05-27' ), ... pd . Timestamp ( '1940-04-25' )], ... name = [ 'Alfred' , 'Batman' , '' ], ... toy = [ None , 'Batmobile' , 'Joker' ])) >>> df age born name toy 0 5.0 NaT Alfred None 1 6.0 1939 - 05 - 27 Batman Batmobile 2 NaN 1940 - 04 - 25 Joker >>> df . notna () age born name toy 0 True False True False 1 True True True True 2 False True True True Show which entries in a Series are not NA. >>> ser = pd . Series ([ 5 , 6 , np . nan ]) >>> ser 0 5.0 1 6.0 2 NaN dtype : float64 >>> ser . notna () 0 True 1 True 2 False dtype : bool method dropna ( axis=0 , how=<no_default> , thresh=<no_default> , subset=None , inplace=False , ignore_index=False ) </> Remove missing values. See the :ref: User Guide <missing_data> for more on which values are considered missing, and how to work with missing data. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Determine if rows or columns which contain missing values areremoved. 0, or 'index' : Drop rows which contain missing values. 1, or 'columns' : Drop columns which contain missing value. Only a single axis is allowed. how ({'any', 'all'}, default 'any') \u2014 Determine if row or column is removed from DataFrame, when we haveat least one NA or all NA. 'any' : If any NA values are present, drop that row or column. 'all' : If all values are NA, drop that row or column. thresh (int, optional) \u2014 Require that many non-NA values. Cannot be combined with how. subset (column label or sequence of labels, optional) \u2014 Labels along other axis to consider, e.g. if you are dropping rowsthese would be a list of columns to include. inplace (bool, default False) \u2014 Whether to modify the DataFrame rather than creating a new one. ignore_index (bool, default ``False``) \u2014 If True , the resulting axis will be labeled 0, 1, \u2026, n - 1. .. versionadded:: 2.0.0 Returns (DataFrame or None) DataFrame with NA entries dropped from it or None if inplace=True . See Also DataFrame.isna: Indicate missing values.DataFrame.notna : Indicate existing (non-missing) values. DataFrame.fillna : Replace missing values. Series.dropna : Drop missing values. Index.dropna : Drop missing indices. Examples >>> df = pd . DataFrame ({ \"name\" : [ 'Alfred' , 'Batman' , 'Catwoman' ], ... \"toy\" : [ np . nan , 'Batmobile' , 'Bullwhip' ], ... \"born\" : [ pd . NaT , pd . Timestamp ( \"1940-04-25\" ), ... pd . NaT ]}) >>> df name toy born 0 Alfred NaN NaT 1 Batman Batmobile 1940 - 04 - 25 2 Catwoman Bullwhip NaT Drop the rows where at least one element is missing. >>> df . dropna () name toy born 1 Batman Batmobile 1940 - 04 - 25 Drop the columns where at least one element is missing. >>> df . dropna ( axis = 'columns' ) name 0 Alfred 1 Batman 2 Catwoman Drop the rows where all elements are missing. >>> df . dropna ( how = 'all' ) name toy born 0 Alfred NaN NaT 1 Batman Batmobile 1940 - 04 - 25 2 Catwoman Bullwhip NaT Keep only the rows with at least 2 non-NA values. >>> df . dropna ( thresh = 2 ) name toy born 1 Batman Batmobile 1940 - 04 - 25 2 Catwoman Bullwhip NaT Define in which columns to look for missing values. >>> df . dropna ( subset = [ 'name' , 'toy' ]) name toy born 1 Batman Batmobile 1940 - 04 - 25 2 Catwoman Bullwhip NaT method drop_duplicates ( subset=None , keep='first' , inplace=False , ignore_index=False ) </> Return DataFrame with duplicate rows removed. Considering certain columns is optional. Indexes, including time indexes are ignored. Parameters subset (column label or sequence of labels, optional) \u2014 Only consider certain columns for identifying duplicates, bydefault use all of the columns. keep ({'first', 'last', ``False``}, default 'first') \u2014 Determines which duplicates (if any) to keep. 'first' : Drop duplicates except for the first occurrence. 'last' : Drop duplicates except for the last occurrence. False : Drop all duplicates. inplace (bool, default ``False``) \u2014 Whether to modify the DataFrame rather than creating a new one. ignore_index (bool, default ``False``) \u2014 If True , the resulting axis will be labeled 0, 1, \u2026, n - 1. Returns (DataFrame or None) DataFrame with duplicates removed or None if inplace=True . See Also DataFrame.value_counts: Count unique combinations of columns. Examples Consider dataset containing ramen rating. >>> df = pd . DataFrame ({ ... 'brand' : [ 'Yum Yum' , 'Yum Yum' , 'Indomie' , 'Indomie' , 'Indomie' ], ... 'style' : [ 'cup' , 'cup' , 'cup' , 'pack' , 'pack' ], ... 'rating' : [ 4 , 4 , 3.5 , 15 , 5 ] ... }) >>> df brand style rating 0 Yum Yum cup 4.0 1 Yum Yum cup 4.0 2 Indomie cup 3.5 3 Indomie pack 15.0 4 Indomie pack 5.0 By default, it removes duplicate rows based on all columns. >>> df . drop_duplicates () brand style rating 0 Yum Yum cup 4.0 2 Indomie cup 3.5 3 Indomie pack 15.0 4 Indomie pack 5.0 To remove duplicates on specific column(s), use subset . >>> df . drop_duplicates ( subset = [ 'brand' ]) brand style rating 0 Yum Yum cup 4.0 2 Indomie cup 3.5 To remove duplicates and keep last occurrences, use keep . >>> df . drop_duplicates ( subset = [ 'brand' , 'style' ], keep = 'last' ) brand style rating 1 Yum Yum cup 4.0 2 Indomie cup 3.5 4 Indomie pack 5.0 method duplicated ( subset=None , keep='first' ) </> Return boolean Series denoting duplicate rows. Considering certain columns is optional. Parameters subset (column label or sequence of labels, optional) \u2014 Only consider certain columns for identifying duplicates, bydefault use all of the columns. keep ({'first', 'last', False}, default 'first') \u2014 Determines which duplicates (if any) to mark. first : Mark duplicates as True except for the first occurrence. last : Mark duplicates as True except for the last occurrence. False : Mark all duplicates as True . Returns (Series) Boolean series for each duplicated rows. See Also Index.duplicated : Equivalent method on index.Series.duplicated : Equivalent method on Series. Series.dropduplicates : Remove duplicate values from Series. DataFrame.dropduplicates : Remove duplicate values from DataFrame. Examples Consider dataset containing ramen rating. >>> df = pd . DataFrame ({ ... 'brand' : [ 'Yum Yum' , 'Yum Yum' , 'Indomie' , 'Indomie' , 'Indomie' ], ... 'style' : [ 'cup' , 'cup' , 'cup' , 'pack' , 'pack' ], ... 'rating' : [ 4 , 4 , 3.5 , 15 , 5 ] ... }) >>> df brand style rating 0 Yum Yum cup 4.0 1 Yum Yum cup 4.0 2 Indomie cup 3.5 3 Indomie pack 15.0 4 Indomie pack 5.0 By default, for each set of duplicated values, the first occurrence is set on False and all others on True. >>> df . duplicated () 0 False 1 True 2 False 3 False 4 False dtype : bool By using 'last', the last occurrence of each set of duplicated values is set on False and all others on True. >>> df . duplicated ( keep = 'last' ) 0 True 1 False 2 False 3 False 4 False dtype : bool By setting keep on False, all duplicates are True. >>> df . duplicated ( keep = False ) 0 True 1 True 2 False 3 False 4 False dtype : bool To find duplicates on specific column(s), use subset . >>> df . duplicated ( subset = [ 'brand' ]) 0 False 1 True 2 False 3 True 4 True dtype : bool method sort_values ( by , axis=0 , ascending=True , inplace=False , kind='quicksort' , na_position='last' , ignore_index=False , key=None ) </> Sort by the values along either axis. Parameters by (str or list of str) \u2014 Name or list of names to sort by. if axis is 0 or 'index' then by may contain index levels and/or column labels. if axis is 1 or 'columns' then by may contain column levels and/or index labels. axis (\"{0 or 'index', 1 or 'columns'}\", default 0) \u2014 Axis to be sorted. ascending (bool or list of bool, default True) \u2014 Sort ascending vs. descending. Specify list for multiple sortorders. If this is a list of bools, must match the length of the by. inplace (bool, default False) \u2014 If True, perform operation in-place. kind ({'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort') \u2014 Choice of sorting algorithm. See also :func: numpy.sort for moreinformation. mergesort and stable are the only stable algorithms. For DataFrames, this option is only applied when sorting on a single column or label. na_position ({'first', 'last'}, default 'last') \u2014 Puts NaNs at the beginning if first ; last puts NaNs at theend. ignore_index (bool, default False) \u2014 If True, the resulting axis will be labeled 0, 1, \u2026, n - 1. key (callable, optional) \u2014 Apply the key function to the valuesbefore sorting. This is similar to the key argument in the builtin :meth: sorted function, with the notable difference that this key function should be vectorized . It should expect a Series and return a Series with the same shape as the input. It will be applied to each column in by independently. Returns (DataFrame or None) DataFrame with sorted values or None if inplace=True . See Also DataFrame.sort_index : Sort a DataFrame by the index.Series.sort_values : Similar method for a Series. Examples >>> df = pd . DataFrame ({ ... 'col1' : [ 'A' , 'A' , 'B' , np . nan , 'D' , 'C' ], ... 'col2' : [ 2 , 1 , 9 , 8 , 7 , 4 ], ... 'col3' : [ 0 , 1 , 9 , 4 , 2 , 3 ], ... 'col4' : [ 'a' , 'B' , 'c' , 'D' , 'e' , 'F' ] ... }) >>> df col1 col2 col3 col4 0 A 2 0 a 1 A 1 1 B 2 B 9 9 c 3 NaN 8 4 D 4 D 7 2 e 5 C 4 3 F Sort by col1 >>> df . sort_values ( by = [ 'col1' ]) col1 col2 col3 col4 0 A 2 0 a 1 A 1 1 B 2 B 9 9 c 5 C 4 3 F 4 D 7 2 e 3 NaN 8 4 D Sort by multiple columns >>> df . sort_values ( by = [ 'col1' , 'col2' ]) col1 col2 col3 col4 1 A 1 1 B 0 A 2 0 a 2 B 9 9 c 5 C 4 3 F 4 D 7 2 e 3 NaN 8 4 D Sort Descending >>> df . sort_values ( by = 'col1' , ascending = False ) col1 col2 col3 col4 4 D 7 2 e 5 C 4 3 F 2 B 9 9 c 0 A 2 0 a 1 A 1 1 B 3 NaN 8 4 D Putting NAs first >>> df . sort_values ( by = 'col1' , ascending = False , na_position = 'first' ) col1 col2 col3 col4 3 NaN 8 4 D 4 D 7 2 e 5 C 4 3 F 2 B 9 9 c 0 A 2 0 a 1 A 1 1 B Sorting with a key function >>> df . sort_values ( by = 'col4' , key = lambda col : col . str . lower ()) col1 col2 col3 col4 0 A 2 0 a 1 A 1 1 B 2 B 9 9 c 3 NaN 8 4 D 4 D 7 2 e 5 C 4 3 F Natural sort with the key argument, using the natsort <https://github.com/SethMMorton/natsort> package. >>> df = pd . DataFrame ({ ... \"time\" : [ '0hr' , '128hr' , '72hr' , '48hr' , '96hr' ], ... \"value\" : [ 10 , 20 , 30 , 40 , 50 ] ... }) >>> df time value 0 0 hr 10 1 128 hr 20 2 72 hr 30 3 48 hr 40 4 96 hr 50 >>> from natsort import index_natsorted >>> df . sort_values ( ... by = \"time\" , ... key = lambda x : np . argsort ( index_natsorted ( df [ \"time\" ])) ... ) time value 0 0 hr 10 3 48 hr 40 2 72 hr 30 4 96 hr 50 1 128 hr 20 method sort_index ( axis=0 , level=None , ascending=True , inplace=False , kind='quicksort' , na_position='last' , sort_remaining=True , ignore_index=False , key=None ) </> Sort object by labels (along an axis). Returns a new DataFrame sorted by label if inplace argument is False , otherwise updates the original DataFrame and returns None. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis along which to sort. The value 0 identifies the rows,and 1 identifies the columns. level (int or level name or list of ints or list of level names) \u2014 If not None, sort on values in specified index level(s). ascending (bool or list-like of bools, default True) \u2014 Sort ascending vs. descending. When the index is a MultiIndex thesort direction can be controlled for each level individually. inplace (bool, default False) \u2014 Whether to modify the DataFrame rather than creating a new one. kind ({'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort') \u2014 Choice of sorting algorithm. See also :func: numpy.sort for moreinformation. mergesort and stable are the only stable algorithms. For DataFrames, this option is only applied when sorting on a single column or label. na_position ({'first', 'last'}, default 'last') \u2014 Puts NaNs at the beginning if first ; last puts NaNs at the end.Not implemented for MultiIndex. sort_remaining (bool, default True) \u2014 If True and sorting by level and index is multilevel, sort by otherlevels too (in order) after sorting by specified level. ignore_index (bool, default False) \u2014 If True, the resulting axis will be labeled 0, 1, \u2026, n - 1. key (callable, optional) \u2014 If not None, apply the key function to the index valuesbefore sorting. This is similar to the key argument in the builtin :meth: sorted function, with the notable difference that this key function should be vectorized . It should expect an Index and return an Index of the same shape. For MultiIndex inputs, the key is applied per level . Returns (DataFrame or None) The original DataFrame sorted by the labels or None if inplace=True . See Also Series.sort_index : Sort Series by the index.DataFrame.sort_values : Sort DataFrame by the value. Series.sort_values : Sort Series by the value. Examples >>> df = pd . DataFrame ([ 1 , 2 , 3 , 4 , 5 ], index = [ 100 , 29 , 234 , 1 , 150 ], ... columns = [ 'A' ]) >>> df . sort_index () A 1 4 29 2 100 1 150 5 234 3 By default, it sorts in ascending order, to sort in descending order, use ascending=False >>> df . sort_index ( ascending = False ) A 234 3 150 5 100 1 29 2 1 4 A key function can be specified which is applied to the index before sorting. For a MultiIndex this is applied to each level separately. >>> df = pd . DataFrame ({ \"a\" : [ 1 , 2 , 3 , 4 ]}, index = [ 'A' , 'b' , 'C' , 'd' ]) >>> df . sort_index ( key = lambda x : x . str . lower ()) a A 1 b 2 C 3 d 4 method value_counts ( subset=None , normalize=False , sort=True , ascending=False , dropna=True ) </> Return a Series containing the frequency of each distinct row in the Dataframe. Parameters subset (label or list of labels, optional) \u2014 Columns to use when counting unique combinations. normalize (bool, default False) \u2014 Return proportions rather than frequencies. sort (bool, default True) \u2014 Sort by frequencies when True. Sort by DataFrame column values when False. ascending (bool, default False) \u2014 Sort in ascending order. dropna (bool, default True) \u2014 Don't include counts of rows that contain NA values. .. versionadded:: 1.3.0 See Also Series.value_counts: Equivalent method on Series. Notes The returned Series will have a MultiIndex with one level per input column but an Index (non-multi) for a single label. By default, rows that contain any NA values are omitted from the result. By default, the resulting Series will be in descending order so that the first element is the most frequently-occurring row. Examples >>> df = pd . DataFrame ({ 'num_legs' : [ 2 , 4 , 4 , 6 ], ... 'num_wings' : [ 2 , 0 , 0 , 0 ]}, ... index = [ 'falcon' , 'dog' , 'cat' , 'ant' ]) >>> df num_legs num_wings falcon 2 2 dog 4 0 cat 4 0 ant 6 0 >>> df . value_counts () num_legs num_wings 4 0 2 2 2 1 6 0 1 Name : count , dtype : int64 >>> df . value_counts ( sort = False ) num_legs num_wings 2 2 1 4 0 2 6 0 1 Name : count , dtype : int64 >>> df . value_counts ( ascending = True ) num_legs num_wings 2 2 1 6 0 1 4 0 2 Name : count , dtype : int64 >>> df . value_counts ( normalize = True ) num_legs num_wings 4 0 0.50 2 2 0.25 6 0 0.25 Name : proportion , dtype : float64 With dropna set to False we can also count rows with NA values. >>> df = pd . DataFrame ({ 'first_name' : [ 'John' , 'Anne' , 'John' , 'Beth' ], ... 'middle_name' : [ 'Smith' , pd . NA , pd . NA , 'Louise' ]}) >>> df first_name middle_name 0 John Smith 1 Anne < NA > 2 John < NA > 3 Beth Louise >>> df . value_counts () first_name middle_name Beth Louise 1 John Smith 1 Name : count , dtype : int64 >>> df . value_counts ( dropna = False ) first_name middle_name Anne NaN 1 Beth Louise 1 John Smith 1 NaN 1 Name : count , dtype : int64 >>> df . value_counts ( \"first_name\" ) first_name John 2 Anne 1 Beth 1 Name : count , dtype : int64 method nlargest ( n , columns , keep='first' ) </> Return the first n rows ordered by columns in descending order. Return the first n rows with the largest values in columns , in descending order. The columns that are not specified are returned as well, but not used for ordering. This method is equivalent to df.sort_values(columns, ascending=False).head(n) , but more performant. Parameters n (int) \u2014 Number of rows to return. columns (label or list of labels) \u2014 Column label(s) to order by. keep ({'first', 'last', 'all'}, default 'first') \u2014 Where there are duplicate values: first : prioritize the first occurrence(s) last : prioritize the last occurrence(s) all : keep all the ties of the smallest item even if it means selecting more than n items. Returns (DataFrame) The first n rows ordered by the given columns in descendingorder. See Also DataFrame.nsmallest : Return the first n rows ordered by columns in ascending order. DataFrame.sort_values : Sort DataFrame by the values. DataFrame.head : Return the first n rows without re-ordering. Notes This function cannot be used with all column types. For example, when specifying columns with object or category dtypes, TypeError is raised. Examples >>> df = pd . DataFrame ({ 'population' : [ 59000000 , 65000000 , 434000 , ... 434000 , 434000 , 337000 , 11300 , ... 11300 , 11300 ], ... 'GDP' : [ 1937894 , 2583560 , 12011 , 4520 , 12128 , ... 17036 , 182 , 38 , 311 ], ... 'alpha-2' : [ \"IT\" , \"FR\" , \"MT\" , \"MV\" , \"BN\" , ... \"IS\" , \"NR\" , \"TV\" , \"AI\" ]}, ... index = [ \"Italy\" , \"France\" , \"Malta\" , ... \"Maldives\" , \"Brunei\" , \"Iceland\" , ... \"Nauru\" , \"Tuvalu\" , \"Anguilla\" ]) >>> df population GDP alpha - 2 Italy 59000000 1937894 IT France 65000000 2583560 FR Malta 434000 12011 MT Maldives 434000 4520 MV Brunei 434000 12128 BN Iceland 337000 17036 IS Nauru 11300 182 NR Tuvalu 11300 38 TV Anguilla 11300 311 AI In the following example, we will use nlargest to select the three rows having the largest values in column \"population\". >>> df . nlargest ( 3 , 'population' ) population GDP alpha - 2 France 65000000 2583560 FR Italy 59000000 1937894 IT Malta 434000 12011 MT When using keep='last' , ties are resolved in reverse order: >>> df . nlargest ( 3 , 'population' , keep = 'last' ) population GDP alpha - 2 France 65000000 2583560 FR Italy 59000000 1937894 IT Brunei 434000 12128 BN When using keep='all' , the number of element kept can go beyond n if there are duplicate values for the smallest element, all the ties are kept: >>> df . nlargest ( 3 , 'population' , keep = 'all' ) population GDP alpha - 2 France 65000000 2583560 FR Italy 59000000 1937894 IT Malta 434000 12011 MT Maldives 434000 4520 MV Brunei 434000 12128 BN However, nlargest does not keep n distinct largest elements: >>> df . nlargest ( 5 , 'population' , keep = 'all' ) population GDP alpha - 2 France 65000000 2583560 FR Italy 59000000 1937894 IT Malta 434000 12011 MT Maldives 434000 4520 MV Brunei 434000 12128 BN To order by the largest values in column \"population\" and then \"GDP\", we can specify multiple columns like in the next example. >>> df . nlargest ( 3 , [ 'population' , 'GDP' ]) population GDP alpha - 2 France 65000000 2583560 FR Italy 59000000 1937894 IT Brunei 434000 12128 BN method nsmallest ( n , columns , keep='first' ) </> Return the first n rows ordered by columns in ascending order. Return the first n rows with the smallest values in columns , in ascending order. The columns that are not specified are returned as well, but not used for ordering. This method is equivalent to df.sort_values(columns, ascending=True).head(n) , but more performant. Parameters n (int) \u2014 Number of items to retrieve. columns (list or str) \u2014 Column name or names to order by. keep ({'first', 'last', 'all'}, default 'first') \u2014 Where there are duplicate values: first : take the first occurrence. last : take the last occurrence. all : keep all the ties of the largest item even if it means selecting more than n items. See Also DataFrame.nlargest : Return the first n rows ordered by columns in descending order. DataFrame.sort_values : Sort DataFrame by the values. DataFrame.head : Return the first n rows without re-ordering. Examples >>> df = pd . DataFrame ({ 'population' : [ 59000000 , 65000000 , 434000 , ... 434000 , 434000 , 337000 , 337000 , ... 11300 , 11300 ], ... 'GDP' : [ 1937894 , 2583560 , 12011 , 4520 , 12128 , ... 17036 , 182 , 38 , 311 ], ... 'alpha-2' : [ \"IT\" , \"FR\" , \"MT\" , \"MV\" , \"BN\" , ... \"IS\" , \"NR\" , \"TV\" , \"AI\" ]}, ... index = [ \"Italy\" , \"France\" , \"Malta\" , ... \"Maldives\" , \"Brunei\" , \"Iceland\" , ... \"Nauru\" , \"Tuvalu\" , \"Anguilla\" ]) >>> df population GDP alpha - 2 Italy 59000000 1937894 IT France 65000000 2583560 FR Malta 434000 12011 MT Maldives 434000 4520 MV Brunei 434000 12128 BN Iceland 337000 17036 IS Nauru 337000 182 NR Tuvalu 11300 38 TV Anguilla 11300 311 AI In the following example, we will use nsmallest to select the three rows having the smallest values in column \"population\". >>> df . nsmallest ( 3 , 'population' ) population GDP alpha - 2 Tuvalu 11300 38 TV Anguilla 11300 311 AI Iceland 337000 17036 IS When using keep='last' , ties are resolved in reverse order: >>> df . nsmallest ( 3 , 'population' , keep = 'last' ) population GDP alpha - 2 Anguilla 11300 311 AI Tuvalu 11300 38 TV Nauru 337000 182 NR When using keep='all' , the number of element kept can go beyond n if there are duplicate values for the largest element, all the ties are kept. >>> df . nsmallest ( 3 , 'population' , keep = 'all' ) population GDP alpha - 2 Tuvalu 11300 38 TV Anguilla 11300 311 AI Iceland 337000 17036 IS Nauru 337000 182 NR However, nsmallest does not keep n distinct smallest elements: >>> df . nsmallest ( 4 , 'population' , keep = 'all' ) population GDP alpha - 2 Tuvalu 11300 38 TV Anguilla 11300 311 AI Iceland 337000 17036 IS Nauru 337000 182 NR To order by the smallest values in column \"population\" and then \"GDP\", we can specify multiple columns like in the next example. >>> df . nsmallest ( 3 , [ 'population' , 'GDP' ]) population GDP alpha - 2 Tuvalu 11300 38 TV Anguilla 11300 311 AI Nauru 337000 182 NR method swaplevel ( i=-2 , j=-1 , axis=0 ) </> Swap levels i and j in a :class: MultiIndex . Default is to swap the two innermost levels of the index. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to swap levels on. 0 or 'index' for row-wise, 1 or'columns' for column-wise. Returns (DataFrame) DataFrame with levels swapped in MultiIndex. Examples >>> df = pd . DataFrame ( ... { \"Grade\" : [ \"A\" , \"B\" , \"A\" , \"C\" ]}, ... index = [ ... [ \"Final exam\" , \"Final exam\" , \"Coursework\" , \"Coursework\" ], ... [ \"History\" , \"Geography\" , \"History\" , \"Geography\" ], ... [ \"January\" , \"February\" , \"March\" , \"April\" ], ... ], ... ) >>> df Grade Final exam History January A Geography February B Coursework History March A Geography April C In the following example, we will swap the levels of the indices. Here, we will swap the levels column-wise, but levels can be swapped row-wise in a similar manner. Note that column-wise is the default behaviour. By not supplying any arguments for i and j, we swap the last and second to last indices. >>> df . swaplevel () Grade Final exam January History A February Geography B Coursework March History A April Geography C By supplying one argument, we can choose which index to swap the last index with. We can for example swap the first index with the last one as follows. >>> df . swaplevel ( 0 ) Grade January History Final exam A February Geography Final exam B March History Coursework A April Geography Coursework C We can also define explicitly which indices we want to swap by supplying values for both i and j. Here, we for example swap the first and second indices. >>> df . swaplevel ( 0 , 1 ) Grade History Final exam January A Geography Final exam February B History Coursework March A Geography Coursework April C method reorder_levels ( order , axis=0 ) </> Rearrange index levels using input order. May not drop or duplicate levels. Parameters order (list of int or list of str) \u2014 List representing new level order. Reference level by number(position) or by key (label). axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Where to reorder levels. Examples >>> data = { ... \"class\" : [ \"Mammals\" , \"Mammals\" , \"Reptiles\" ], ... \"diet\" : [ \"Omnivore\" , \"Carnivore\" , \"Carnivore\" ], ... \"species\" : [ \"Humans\" , \"Dogs\" , \"Snakes\" ], ... } >>> df = pd . DataFrame ( data , columns = [ \"class\" , \"diet\" , \"species\" ]) >>> df = df . set_index ([ \"class\" , \"diet\" ]) >>> df species class diet Mammals Omnivore Humans Carnivore Dogs Reptiles Carnivore Snakes Let's reorder the levels of the index: >>> df . reorder_levels ([ \"diet\" , \"class\" ]) species diet class Omnivore Mammals Humans Carnivore Mammals Dogs Reptiles Snakes method eq ( other , axis='columns' , level=None ) </> Get Equal to of dataframe and other, element-wise (binary operator eq ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , != , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison. Parameters other (scalar, sequence, Series, or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}, default 'columns') \u2014 Whether to compare by the index (0 or 'index') or columns(1 or 'columns'). level (int or label) \u2014 Broadcast across a level, matching Index values on the passedMultiIndex level. Returns (DataFrame of bool) Result of the comparison. See Also DataFrame.eq : Compare DataFrames for equality elementwise.DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ). Examples >>> df = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 ], ... 'revenue' : [ 100 , 250 , 300 ]}, ... index = [ 'A' , 'B' , 'C' ]) >>> df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: >>> df == 100 cost revenue A False True B False False C True False >>> df . eq ( 100 ) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: >>> df != pd . Series ([ 100 , 250 ], index = [ \"cost\" , \"revenue\" ]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: >>> df . ne ( pd . Series ([ 100 , 300 ], index = [ \"A\" , \"D\" ]), axis = 'index' ) cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : >>> df == [ 250 , 100 ] cost revenue A True True B False False C False False Use the method to control the axis: >>> df . eq ([ 250 , 250 , 100 ], axis = 'index' ) cost revenue A True False B False True C True False Compare to a DataFrame of different shape. >>> other = pd . DataFrame ({ 'revenue' : [ 300 , 250 , 100 , 150 ]}, ... index = [ 'A' , 'B' , 'C' , 'D' ]) >>> other revenue A 300 B 250 C 100 D 150 >>> df . gt ( other ) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 , 150 , 300 , 220 ], ... 'revenue' : [ 100 , 250 , 300 , 200 , 175 , 225 ]}, ... index = [[ 'Q1' , 'Q1' , 'Q1' , 'Q2' , 'Q2' , 'Q2' ], ... [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ]]) >>> df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 >>> df . le ( df_multindex , level = 1 ) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False method ne ( other , axis='columns' , level=None ) </> Get Not equal to of dataframe and other, element-wise (binary operator ne ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , != , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison. Parameters other (scalar, sequence, Series, or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}, default 'columns') \u2014 Whether to compare by the index (0 or 'index') or columns(1 or 'columns'). level (int or label) \u2014 Broadcast across a level, matching Index values on the passedMultiIndex level. Returns (DataFrame of bool) Result of the comparison. See Also DataFrame.eq : Compare DataFrames for equality elementwise.DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ). Examples >>> df = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 ], ... 'revenue' : [ 100 , 250 , 300 ]}, ... index = [ 'A' , 'B' , 'C' ]) >>> df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: >>> df == 100 cost revenue A False True B False False C True False >>> df . eq ( 100 ) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: >>> df != pd . Series ([ 100 , 250 ], index = [ \"cost\" , \"revenue\" ]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: >>> df . ne ( pd . Series ([ 100 , 300 ], index = [ \"A\" , \"D\" ]), axis = 'index' ) cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : >>> df == [ 250 , 100 ] cost revenue A True True B False False C False False Use the method to control the axis: >>> df . eq ([ 250 , 250 , 100 ], axis = 'index' ) cost revenue A True False B False True C True False Compare to a DataFrame of different shape. >>> other = pd . DataFrame ({ 'revenue' : [ 300 , 250 , 100 , 150 ]}, ... index = [ 'A' , 'B' , 'C' , 'D' ]) >>> other revenue A 300 B 250 C 100 D 150 >>> df . gt ( other ) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 , 150 , 300 , 220 ], ... 'revenue' : [ 100 , 250 , 300 , 200 , 175 , 225 ]}, ... index = [[ 'Q1' , 'Q1' , 'Q1' , 'Q2' , 'Q2' , 'Q2' ], ... [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ]]) >>> df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 >>> df . le ( df_multindex , level = 1 ) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False method le ( other , axis='columns' , level=None ) </> Get Less than or equal to of dataframe and other, element-wise (binary operator le ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , != , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison. Parameters other (scalar, sequence, Series, or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}, default 'columns') \u2014 Whether to compare by the index (0 or 'index') or columns(1 or 'columns'). level (int or label) \u2014 Broadcast across a level, matching Index values on the passedMultiIndex level. Returns (DataFrame of bool) Result of the comparison. See Also DataFrame.eq : Compare DataFrames for equality elementwise.DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ). Examples >>> df = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 ], ... 'revenue' : [ 100 , 250 , 300 ]}, ... index = [ 'A' , 'B' , 'C' ]) >>> df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: >>> df == 100 cost revenue A False True B False False C True False >>> df . eq ( 100 ) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: >>> df != pd . Series ([ 100 , 250 ], index = [ \"cost\" , \"revenue\" ]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: >>> df . ne ( pd . Series ([ 100 , 300 ], index = [ \"A\" , \"D\" ]), axis = 'index' ) cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : >>> df == [ 250 , 100 ] cost revenue A True True B False False C False False Use the method to control the axis: >>> df . eq ([ 250 , 250 , 100 ], axis = 'index' ) cost revenue A True False B False True C True False Compare to a DataFrame of different shape. >>> other = pd . DataFrame ({ 'revenue' : [ 300 , 250 , 100 , 150 ]}, ... index = [ 'A' , 'B' , 'C' , 'D' ]) >>> other revenue A 300 B 250 C 100 D 150 >>> df . gt ( other ) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 , 150 , 300 , 220 ], ... 'revenue' : [ 100 , 250 , 300 , 200 , 175 , 225 ]}, ... index = [[ 'Q1' , 'Q1' , 'Q1' , 'Q2' , 'Q2' , 'Q2' ], ... [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ]]) >>> df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 >>> df . le ( df_multindex , level = 1 ) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False method lt ( other , axis='columns' , level=None ) </> Get Less than of dataframe and other, element-wise (binary operator lt ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , != , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison. Parameters other (scalar, sequence, Series, or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}, default 'columns') \u2014 Whether to compare by the index (0 or 'index') or columns(1 or 'columns'). level (int or label) \u2014 Broadcast across a level, matching Index values on the passedMultiIndex level. Returns (DataFrame of bool) Result of the comparison. See Also DataFrame.eq : Compare DataFrames for equality elementwise.DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ). Examples >>> df = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 ], ... 'revenue' : [ 100 , 250 , 300 ]}, ... index = [ 'A' , 'B' , 'C' ]) >>> df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: >>> df == 100 cost revenue A False True B False False C True False >>> df . eq ( 100 ) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: >>> df != pd . Series ([ 100 , 250 ], index = [ \"cost\" , \"revenue\" ]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: >>> df . ne ( pd . Series ([ 100 , 300 ], index = [ \"A\" , \"D\" ]), axis = 'index' ) cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : >>> df == [ 250 , 100 ] cost revenue A True True B False False C False False Use the method to control the axis: >>> df . eq ([ 250 , 250 , 100 ], axis = 'index' ) cost revenue A True False B False True C True False Compare to a DataFrame of different shape. >>> other = pd . DataFrame ({ 'revenue' : [ 300 , 250 , 100 , 150 ]}, ... index = [ 'A' , 'B' , 'C' , 'D' ]) >>> other revenue A 300 B 250 C 100 D 150 >>> df . gt ( other ) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 , 150 , 300 , 220 ], ... 'revenue' : [ 100 , 250 , 300 , 200 , 175 , 225 ]}, ... index = [[ 'Q1' , 'Q1' , 'Q1' , 'Q2' , 'Q2' , 'Q2' ], ... [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ]]) >>> df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 >>> df . le ( df_multindex , level = 1 ) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False method ge ( other , axis='columns' , level=None ) </> Get Greater than or equal to of dataframe and other, element-wise (binary operator ge ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , != , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison. Parameters other (scalar, sequence, Series, or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}, default 'columns') \u2014 Whether to compare by the index (0 or 'index') or columns(1 or 'columns'). level (int or label) \u2014 Broadcast across a level, matching Index values on the passedMultiIndex level. Returns (DataFrame of bool) Result of the comparison. See Also DataFrame.eq : Compare DataFrames for equality elementwise.DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ). Examples >>> df = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 ], ... 'revenue' : [ 100 , 250 , 300 ]}, ... index = [ 'A' , 'B' , 'C' ]) >>> df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: >>> df == 100 cost revenue A False True B False False C True False >>> df . eq ( 100 ) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: >>> df != pd . Series ([ 100 , 250 ], index = [ \"cost\" , \"revenue\" ]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: >>> df . ne ( pd . Series ([ 100 , 300 ], index = [ \"A\" , \"D\" ]), axis = 'index' ) cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : >>> df == [ 250 , 100 ] cost revenue A True True B False False C False False Use the method to control the axis: >>> df . eq ([ 250 , 250 , 100 ], axis = 'index' ) cost revenue A True False B False True C True False Compare to a DataFrame of different shape. >>> other = pd . DataFrame ({ 'revenue' : [ 300 , 250 , 100 , 150 ]}, ... index = [ 'A' , 'B' , 'C' , 'D' ]) >>> other revenue A 300 B 250 C 100 D 150 >>> df . gt ( other ) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 , 150 , 300 , 220 ], ... 'revenue' : [ 100 , 250 , 300 , 200 , 175 , 225 ]}, ... index = [[ 'Q1' , 'Q1' , 'Q1' , 'Q2' , 'Q2' , 'Q2' ], ... [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ]]) >>> df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 >>> df . le ( df_multindex , level = 1 ) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False method gt ( other , axis='columns' , level=None ) </> Get Greater than of dataframe and other, element-wise (binary operator gt ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , != , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison. Parameters other (scalar, sequence, Series, or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}, default 'columns') \u2014 Whether to compare by the index (0 or 'index') or columns(1 or 'columns'). level (int or label) \u2014 Broadcast across a level, matching Index values on the passedMultiIndex level. Returns (DataFrame of bool) Result of the comparison. See Also DataFrame.eq : Compare DataFrames for equality elementwise.DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ). Examples >>> df = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 ], ... 'revenue' : [ 100 , 250 , 300 ]}, ... index = [ 'A' , 'B' , 'C' ]) >>> df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: >>> df == 100 cost revenue A False True B False False C True False >>> df . eq ( 100 ) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: >>> df != pd . Series ([ 100 , 250 ], index = [ \"cost\" , \"revenue\" ]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: >>> df . ne ( pd . Series ([ 100 , 300 ], index = [ \"A\" , \"D\" ]), axis = 'index' ) cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : >>> df == [ 250 , 100 ] cost revenue A True True B False False C False False Use the method to control the axis: >>> df . eq ([ 250 , 250 , 100 ], axis = 'index' ) cost revenue A True False B False True C True False Compare to a DataFrame of different shape. >>> other = pd . DataFrame ({ 'revenue' : [ 300 , 250 , 100 , 150 ]}, ... index = [ 'A' , 'B' , 'C' , 'D' ]) >>> other revenue A 300 B 250 C 100 D 150 >>> df . gt ( other ) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 , 150 , 300 , 220 ], ... 'revenue' : [ 100 , 250 , 300 , 200 , 175 , 225 ]}, ... index = [[ 'Q1' , 'Q1' , 'Q1' , 'Q2' , 'Q2' , 'Q2' ], ... [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ]]) >>> df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 >>> df . le ( df_multindex , level = 1 ) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False method add ( other , axis='columns' , level=None , fill_value=None ) </> Get Addition of dataframe and other, element-wise (binary operator add ). Equivalent to dataframe + other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, radd . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method radd ( other , axis='columns' , level=None , fill_value=None ) </> Get Addition of dataframe and other, element-wise (binary operator radd ). Equivalent to other + dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, add . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method sub ( other , axis='columns' , level=None , fill_value=None ) </> Get Subtraction of dataframe and other, element-wise (binary operator sub ). Equivalent to dataframe - other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rsub . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method rsub ( other , axis='columns' , level=None , fill_value=None ) </> Get Subtraction of dataframe and other, element-wise (binary operator rsub ). Equivalent to other - dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, sub . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method mul ( other , axis='columns' , level=None , fill_value=None ) </> Get Multiplication of dataframe and other, element-wise (binary operator mul ). Equivalent to dataframe * other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rmul . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method rmul ( other , axis='columns' , level=None , fill_value=None ) </> Get Multiplication of dataframe and other, element-wise (binary operator rmul ). Equivalent to other * dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, mul . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method truediv ( other , axis='columns' , level=None , fill_value=None ) </> Get Floating division of dataframe and other, element-wise (binary operator truediv ). Equivalent to dataframe / other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rtruediv . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method rtruediv ( other , axis='columns' , level=None , fill_value=None ) </> Get Floating division of dataframe and other, element-wise (binary operator rtruediv ). Equivalent to other / dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, truediv . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method floordiv ( other , axis='columns' , level=None , fill_value=None ) </> Get Integer division of dataframe and other, element-wise (binary operator floordiv ). Equivalent to dataframe // other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rfloordiv . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method rfloordiv ( other , axis='columns' , level=None , fill_value=None ) </> Get Integer division of dataframe and other, element-wise (binary operator rfloordiv ). Equivalent to other // dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, floordiv . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method mod ( other , axis='columns' , level=None , fill_value=None ) </> Get Modulo of dataframe and other, element-wise (binary operator mod ). Equivalent to dataframe % other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rmod . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method rmod ( other , axis='columns' , level=None , fill_value=None ) </> Get Modulo of dataframe and other, element-wise (binary operator rmod ). Equivalent to other % dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, mod . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method pow ( other , axis='columns' , level=None , fill_value=None ) </> Get Exponential power of dataframe and other, element-wise (binary operator pow ). Equivalent to dataframe ** other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rpow . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method rpow ( other , axis='columns' , level=None , fill_value=None ) </> Get Exponential power of dataframe and other, element-wise (binary operator rpow ). Equivalent to other ** dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, pow . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method compare ( other , align_axis=1 , keep_shape=False , keep_equal=False , result_names=('self', 'other') ) </> Compare to another DataFrame and show the differences. Parameters other (DataFrame) \u2014 Object to compare with. align_axis ({0 or 'index', 1 or 'columns'}, default 1) \u2014 Determine which axis to align the comparison on. 0, or 'index' : Resulting differences are stacked vertically with rows drawn alternately from self and other. 1, or 'columns' : Resulting differences are aligned horizontally with columns drawn alternately from self and other. keep_shape (bool, default False) \u2014 If true, all rows and columns are kept.Otherwise, only the ones with different values are kept. keep_equal (bool, default False) \u2014 If true, the result keeps values that are equal.Otherwise, equal values are shown as NaNs. result_names (tuple, default ('self', 'other')) \u2014 Set the dataframes names in the comparison. .. versionadded:: 1.5.0 Returns (DataFrame) DataFrame that shows the differences stacked side by side. The resulting index will be a MultiIndex with 'self' and 'other' stacked alternately at the inner level. Raises ValueError \u2014 When the two DataFrames don't have identical labels or shape. See Also Series.compare : Compare with another Series and show differences.DataFrame.equals : Test whether two objects contain the same elements. Notes Matching NaNs will not appear as a difference. Can only compare identically-labeled (i.e. same shape, identical row and column labels) DataFrames Examples >>> df = pd . DataFrame ( ... { ... \"col1\" : [ \"a\" , \"a\" , \"b\" , \"b\" , \"a\" ], ... \"col2\" : [ 1.0 , 2.0 , 3.0 , np . nan , 5.0 ], ... \"col3\" : [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ] ... }, ... columns = [ \"col1\" , \"col2\" , \"col3\" ], ... ) >>> df col1 col2 col3 0 a 1.0 1.0 1 a 2.0 2.0 2 b 3.0 3.0 3 b NaN 4.0 4 a 5.0 5.0 >>> df2 = df . copy () >>> df2 . loc [ 0 , 'col1' ] = 'c' >>> df2 . loc [ 2 , 'col3' ] = 4.0 >>> df2 col1 col2 col3 0 c 1.0 1.0 1 a 2.0 2.0 2 b 3.0 4.0 3 b NaN 4.0 4 a 5.0 5.0 Align the differences on columns >>> df . compare ( df2 ) col1 col3 self other self other 0 a c NaN NaN 2 NaN NaN 3.0 4.0 Assign result_names >>> df . compare ( df2 , result_names = ( \"left\" , \"right\" )) col1 col3 left right left right 0 a c NaN NaN 2 NaN NaN 3.0 4.0 Stack the differences on rows >>> df . compare ( df2 , align_axis = 0 ) col1 col3 0 self a NaN other c NaN 2 self NaN 3.0 other NaN 4.0 Keep the equal values >>> df . compare ( df2 , keep_equal = True ) col1 col3 self other self other 0 a c 1.0 1.0 2 b b 3.0 4.0 Keep all original rows and columns >>> df . compare ( df2 , keep_shape = True ) col1 col2 col3 self other self other self other 0 a c NaN NaN NaN NaN 1 NaN NaN NaN NaN NaN NaN 2 NaN NaN NaN NaN 3.0 4.0 3 NaN NaN NaN NaN NaN NaN 4 NaN NaN NaN NaN NaN NaN Keep all original rows and columns and also all original values >>> df . compare ( df2 , keep_shape = True , keep_equal = True ) col1 col2 col3 self other self other self other 0 a c 1.0 1.0 1.0 1.0 1 a a 2.0 2.0 2.0 2.0 2 b b 3.0 3.0 3.0 4.0 3 b b NaN NaN 4.0 4.0 4 a a 5.0 5.0 5.0 5.0 method combine ( other , func , fill_value=None , overwrite=True ) </> Perform column-wise combine with another DataFrame. Combines a DataFrame with other DataFrame using func to element-wise combine columns. The row and column indexes of the resulting DataFrame will be the union of the two. Parameters other (DataFrame) \u2014 The DataFrame to merge column-wise. func (function) \u2014 Function that takes two series as inputs and return a Series or ascalar. Used to merge the two dataframes column by columns. fill_value (scalar value, default None) \u2014 The value to fill NaNs with prior to passing any column to themerge func. overwrite (bool, default True) \u2014 If True, columns in self that do not exist in other will beoverwritten with NaNs. Returns (DataFrame) Combination of the provided DataFrames. See Also DataFrame.combinefirst : Combine two DataFrame objects and default to non-null values in frame calling the method. Examples Combine using a simple function that chooses the smaller column. >>> df1 = pd . DataFrame ({ 'A' : [ 0 , 0 ], 'B' : [ 4 , 4 ]}) >>> df2 = pd . DataFrame ({ 'A' : [ 1 , 1 ], 'B' : [ 3 , 3 ]}) >>> take_smaller = lambda s1 , s2 : s1 if s1 . sum () < s2 . sum () else s2 >>> df1 . combine ( df2 , take_smaller ) A B 0 0 3 1 0 3 Example using a true element-wise combine function. >>> df1 = pd . DataFrame ({ 'A' : [ 5 , 0 ], 'B' : [ 2 , 4 ]}) >>> df2 = pd . DataFrame ({ 'A' : [ 1 , 1 ], 'B' : [ 3 , 3 ]}) >>> df1 . combine ( df2 , np . minimum ) A B 0 1 2 1 0 3 Using fill_value fills Nones prior to passing the column to the merge function. >>> df1 = pd . DataFrame ({ 'A' : [ 0 , 0 ], 'B' : [ None , 4 ]}) >>> df2 = pd . DataFrame ({ 'A' : [ 1 , 1 ], 'B' : [ 3 , 3 ]}) >>> df1 . combine ( df2 , take_smaller , fill_value =- 5 ) A B 0 0 - 5.0 1 0 4.0 However, if the same element in both dataframes is None, that None is preserved >>> df1 = pd . DataFrame ({ 'A' : [ 0 , 0 ], 'B' : [ None , 4 ]}) >>> df2 = pd . DataFrame ({ 'A' : [ 1 , 1 ], 'B' : [ None , 3 ]}) >>> df1 . combine ( df2 , take_smaller , fill_value =- 5 ) A B 0 0 - 5.0 1 0 3.0 Example that demonstrates the use of overwrite and behavior when the axis differ between the dataframes. >>> df1 = pd . DataFrame ({ 'A' : [ 0 , 0 ], 'B' : [ 4 , 4 ]}) >>> df2 = pd . DataFrame ({ 'B' : [ 3 , 3 ], 'C' : [ - 10 , 1 ], }, index = [ 1 , 2 ]) >>> df1 . combine ( df2 , take_smaller ) A B C 0 NaN NaN NaN 1 NaN 3.0 - 10.0 2 NaN 3.0 1.0 >>> df1 . combine ( df2 , take_smaller , overwrite = False ) A B C 0 0.0 NaN NaN 1 0.0 3.0 - 10.0 2 NaN 3.0 1.0 Demonstrating the preference of the passed in dataframe. >>> df2 = pd . DataFrame ({ 'B' : [ 3 , 3 ], 'C' : [ 1 , 1 ], }, index = [ 1 , 2 ]) >>> df2 . combine ( df1 , take_smaller ) A B C 0 0.0 NaN NaN 1 0.0 3.0 NaN 2 NaN 3.0 NaN >>> df2 . combine ( df1 , take_smaller , overwrite = False ) A B C 0 0.0 NaN NaN 1 0.0 3.0 1.0 2 NaN 3.0 1.0 method combine_first ( other ) </> Update null elements with value in the same location in other . Combine two DataFrame objects by filling null values in one DataFrame with non-null values from other DataFrame. The row and column indexes of the resulting DataFrame will be the union of the two. The resulting dataframe contains the 'first' dataframe values and overrides the second one values where both first.loc[index, col] and second.loc[index, col] are not missing values, upon calling first.combine_first(second). Parameters other (DataFrame) \u2014 Provided DataFrame to use to fill null values. Returns (DataFrame) The result of combining the provided DataFrame with the other object. See Also DataFrame.combine : Perform series-wise operation on two DataFrames using a given function. Examples >>> df1 = pd . DataFrame ({ 'A' : [ None , 0 ], 'B' : [ None , 4 ]}) >>> df2 = pd . DataFrame ({ 'A' : [ 1 , 1 ], 'B' : [ 3 , 3 ]}) >>> df1 . combine_first ( df2 ) A B 0 1.0 3.0 1 0.0 4.0 Null values still persist if the location of that null value does not exist in other >>> df1 = pd . DataFrame ({ 'A' : [ None , 0 ], 'B' : [ 4 , None ]}) >>> df2 = pd . DataFrame ({ 'B' : [ 3 , 3 ], 'C' : [ 1 , 1 ]}, index = [ 1 , 2 ]) >>> df1 . combine_first ( df2 ) A B C 0 NaN 4.0 NaN 1 0.0 3.0 1.0 2 NaN 3.0 1.0 method update ( other , join='left' , overwrite=True , filter_func=None , errors='ignore' ) </> Modify in place using non-NA values from another DataFrame. Aligns on indices. There is no return value. Parameters other (DataFrame, or object coercible into a DataFrame) \u2014 Should have at least one matching index/column labelwith the original DataFrame. If a Series is passed, its name attribute must be set, and that will be used as the column name to align with the original DataFrame. join ({'left'}, default 'left') \u2014 Only left join is implemented, keeping the index and columns of theoriginal object. overwrite (bool, default True) \u2014 How to handle non-NA values for overlapping keys: True: overwrite original DataFrame's values with values from other . False: only update values that are NA in the original DataFrame. filter_func (callable(1d-array) -> bool 1d-array, optional) \u2014 Can choose to replace values other than NA. Return True for valuesthat should be updated. errors ({'raise', 'ignore'}, default 'ignore') \u2014 If 'raise', will raise a ValueError if the DataFrame and other both contain non-NA data in the same place. Returns (None) This method directly changes calling object. Raises NotImplementedError \u2014 If join != 'left' ValueError \u2014 When errors='raise' and there's overlapping non-NA data. When errors is not either 'ignore' or 'raise' See Also dict.update : Similar method for dictionaries.DataFrame.merge : For column(s)-on-column(s) operations. Examples >>> df = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 ], ... 'B' : [ 400 , 500 , 600 ]}) >>> new_df = pd . DataFrame ({ 'B' : [ 4 , 5 , 6 ], ... 'C' : [ 7 , 8 , 9 ]}) >>> df . update ( new_df ) >>> df A B 0 1 4 1 2 5 2 3 6 The DataFrame's length does not increase as a result of the update, only values at matching index/column labels are updated. >>> df = pd . DataFrame ({ 'A' : [ 'a' , 'b' , 'c' ], ... 'B' : [ 'x' , 'y' , 'z' ]}) >>> new_df = pd . DataFrame ({ 'B' : [ 'd' , 'e' , 'f' , 'g' , 'h' , 'i' ]}) >>> df . update ( new_df ) >>> df A B 0 a d 1 b e 2 c f >>> df = pd . DataFrame ({ 'A' : [ 'a' , 'b' , 'c' ], ... 'B' : [ 'x' , 'y' , 'z' ]}) >>> new_df = pd . DataFrame ({ 'B' : [ 'd' , 'f' ]}, index = [ 0 , 2 ]) >>> df . update ( new_df ) >>> df A B 0 a d 1 b y 2 c f For Series, its name attribute must be set. >>> df = pd . DataFrame ({ 'A' : [ 'a' , 'b' , 'c' ], ... 'B' : [ 'x' , 'y' , 'z' ]}) >>> new_column = pd . Series ([ 'd' , 'e' , 'f' ], name = 'B' ) >>> df . update ( new_column ) >>> df A B 0 a d 1 b e 2 c f If other contains NaNs the corresponding values are not updated in the original dataframe. >>> df = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 ], ... 'B' : [ 400. , 500. , 600. ]}) >>> new_df = pd . DataFrame ({ 'B' : [ 4 , np . nan , 6 ]}) >>> df . update ( new_df ) >>> df A B 0 1 4.0 1 2 500.0 2 3 6.0 method groupby ( by=None , axis=<no_default> , level=None , as_index=True , sort=True , group_keys=True , observed=<no_default> , dropna=True ) </> Group DataFrame using a mapper or by a Series of columns. A groupby operation involves some combination of splitting the object, applying a function, and combining the results. This can be used to group large amounts of data and compute operations on these groups. Parameters by (mapping, function, label, pd.Grouper or list of such) \u2014 Used to determine the groups for the groupby.If by is a function, it's called on each value of the object's index. If a dict or Series is passed, the Series or dict VALUES will be used to determine the groups (the Series' values are first aligned; see .align() method). If a list or ndarray of length equal to the selected axis is passed (see the groupby user guide <https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#splitting-an-object-into-groups> _), the values are used as-is to determine the groups. A label or list of labels may be passed to group by the columns in self . Notice that a tuple is interpreted as a (single) key. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Split along rows (0) or columns (1). For Series this parameteris unused and defaults to 0. .. deprecated:: 2.1.0 Will be removed and behave like axis=0 in a future version. For ``axis=1``, do ``frame.T.groupby(...)`` instead. level (int, level name, or sequence of such, default None) \u2014 If the axis is a MultiIndex (hierarchical), group by a particularlevel or levels. Do not specify both by and level . as_index (bool, default True) \u2014 Return object with group labels as theindex. Only relevant for DataFrame input. as_index=False is effectively \"SQL-style\" grouped output. This argument has no effect on filtrations (see the filtrations in the user guide <https://pandas.pydata.org/docs/dev/user_guide/groupby.html#filtration> ), such as head() , tail() , nth() and in transformations (see the transformations in the user guide <https://pandas.pydata.org/docs/dev/user_guide/groupby.html#transformation> ). sort (bool, default True) \u2014 Sort group keys. Get better performance by turning this off.Note this does not influence the order of observations within each group. Groupby preserves the order of rows within each group. If False, the groups will appear in the same order as they did in the original DataFrame. This argument has no effect on filtrations (see the filtrations in the user guide <https://pandas.pydata.org/docs/dev/user_guide/groupby.html#filtration> ), such as head() , tail() , nth() and in transformations (see the transformations in the user guide <https://pandas.pydata.org/docs/dev/user_guide/groupby.html#transformation> ). .. versionchanged:: 2.0.0 Specifying ``sort=False`` with an ordered categorical grouper will no longer sort the values. group_keys (bool, default True) \u2014 When calling apply and the by argument produces a like-indexed(i.e. :ref: a transform <groupby.transform> ) result, add group keys to index to identify pieces. By default group keys are not included when the result's index (and column) labels match the inputs, and are included otherwise. .. versionchanged:: 1.5.0 Warns that group_keys will no longer be ignored when the result from apply is a like-indexed Series or DataFrame. Specify group_keys explicitly to include the group keys or not. .. versionchanged:: 2.0.0 group_keys now defaults to True . observed (bool, default False) \u2014 This only applies if any of the groupers are Categoricals.If True: only show observed values for categorical groupers. If False: show all values for categorical groupers. .. deprecated:: 2.1.0 The default value will change to True in a future version of pandas. dropna (bool, default True) \u2014 If True, and if group keys contain NA values, NA values togetherwith row/column will be dropped. If False, NA values will also be treated as the key in groups. Returns (pandas.api.typing.DataFrameGroupBy) Returns a groupby object that contains information about the groups. See Also resample : Convenience method for frequency conversion and resampling of time series. Notes See the user guide <https://pandas.pydata.org/pandas-docs/stable/groupby.html> __ for more detailed usage and examples, including splitting an object into groups, iterating through groups, selecting a group, aggregation, and more. Examples >>> df = pd . DataFrame ({ 'Animal' : [ 'Falcon' , 'Falcon' , ... 'Parrot' , 'Parrot' ], ... 'Max Speed' : [ 380. , 370. , 24. , 26. ]}) >>> df Animal Max Speed 0 Falcon 380.0 1 Falcon 370.0 2 Parrot 24.0 3 Parrot 26.0 >>> df . groupby ([ 'Animal' ]) . mean () Max Speed Animal Falcon 375.0 Parrot 25.0 Hierarchical Indexes We can groupby different levels of a hierarchical index using the level parameter: >>> arrays = [[ 'Falcon' , 'Falcon' , 'Parrot' , 'Parrot' ], ... [ 'Captive' , 'Wild' , 'Captive' , 'Wild' ]] >>> index = pd . MultiIndex . from_arrays ( arrays , names = ( 'Animal' , 'Type' )) >>> df = pd . DataFrame ({ 'Max Speed' : [ 390. , 350. , 30. , 20. ]}, ... index = index ) >>> df Max Speed Animal Type Falcon Captive 390.0 Wild 350.0 Parrot Captive 30.0 Wild 20.0 >>> df . groupby ( level = 0 ) . mean () Max Speed Animal Falcon 370.0 Parrot 25.0 >>> df . groupby ( level = \"Type\" ) . mean () Max Speed Type Captive 210.0 Wild 185.0 We can also choose to include NA in group keys or not by setting dropna parameter, the default setting is True . >>> l = [[ 1 , 2 , 3 ], [ 1 , None , 4 ], [ 2 , 1 , 3 ], [ 1 , 2 , 2 ]] >>> df = pd . DataFrame ( l , columns = [ \"a\" , \"b\" , \"c\" ]) >>> df . groupby ( by = [ \"b\" ]) . sum () a c b 1.0 2 3 2.0 2 5 >>> df . groupby ( by = [ \"b\" ], dropna = False ) . sum () a c b 1.0 2 3 2.0 2 5 NaN 1 4 >>> l = [[ \"a\" , 12 , 12 ], [ None , 12.3 , 33. ], [ \"b\" , 12.3 , 123 ], [ \"a\" , 1 , 1 ]] >>> df = pd . DataFrame ( l , columns = [ \"a\" , \"b\" , \"c\" ]) >>> df . groupby ( by = \"a\" ) . sum () b c a a 13.0 13.0 b 12.3 123.0 >>> df . groupby ( by = \"a\" , dropna = False ) . sum () b c a a 13.0 13.0 b 12.3 123.0 NaN 12.3 33.0 When using .apply() , use group_keys to include or exclude the group keys. The group_keys argument defaults to True (include). >>> df = pd . DataFrame ({ 'Animal' : [ 'Falcon' , 'Falcon' , ... 'Parrot' , 'Parrot' ], ... 'Max Speed' : [ 380. , 370. , 24. , 26. ]}) >>> df . groupby ( \"Animal\" , group_keys = True )[[ 'Max Speed' ]] . apply ( lambda x : x ) Max Speed Animal Falcon 0 380.0 1 370.0 Parrot 2 24.0 3 26.0 >>> df . groupby ( \"Animal\" , group_keys = False )[[ 'Max Speed' ]] . apply ( lambda x : x ) Max Speed 0 380.0 1 370.0 2 24.0 3 26.0 method pivot ( columns , index=<no_default> , values=<no_default> ) </> Return reshaped DataFrame organized by given index / column values. Reshape data (produce a \"pivot\" table) based on column values. Uses unique values from specified index / columns to form axes of the resulting DataFrame. This function does not support data aggregation, multiple values will result in a MultiIndex in the columns. See the :ref: User Guide <reshaping> for more on reshaping. Parameters columns (str or object or a list of str) \u2014 Column to use to make new frame's columns. index (str or object or a list of str, optional) \u2014 Column to use to make new frame's index. If not given, uses existing index. values (str, object or a list of the previous, optional) \u2014 Column(s) to use for populating new frame's values. If notspecified, all remaining columns will be used and the result will have hierarchically indexed columns. Returns (DataFrame) Returns reshaped DataFrame. Raises ValueError \u2014 When there are any index , columns combinations with multiplevalues. DataFrame.pivot_table when you need to aggregate. See Also DataFrame.pivottable : Generalization of pivot that can handle duplicate values for one index/column pair. DataFrame.unstack : Pivot based on the index values instead of a column. wide_to_long : Wide panel to long format. Less flexible but more user-friendly than melt. Notes For finer-tuned control, see hierarchical indexing documentation along with the related stack/unstack methods. Reference :ref: the user guide <reshaping.pivot> for more examples. Examples >>> df = pd . DataFrame ({ 'foo' : [ 'one' , 'one' , 'one' , 'two' , 'two' , ... 'two' ], ... 'bar' : [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ], ... 'baz' : [ 1 , 2 , 3 , 4 , 5 , 6 ], ... 'zoo' : [ 'x' , 'y' , 'z' , 'q' , 'w' , 't' ]}) >>> df foo bar baz zoo 0 one A 1 x 1 one B 2 y 2 one C 3 z 3 two A 4 q 4 two B 5 w 5 two C 6 t >>> df . pivot ( index = 'foo' , columns = 'bar' , values = 'baz' ) bar A B C foo one 1 2 3 two 4 5 6 >>> df . pivot ( index = 'foo' , columns = 'bar' )[ 'baz' ] bar A B C foo one 1 2 3 two 4 5 6 >>> df . pivot ( index = 'foo' , columns = 'bar' , values = [ 'baz' , 'zoo' ]) baz zoo bar A B C A B C foo one 1 2 3 x y z two 4 5 6 q w t You could also assign a list of column names or a list of index names. >>> df = pd . DataFrame ({ ... \"lev1\" : [ 1 , 1 , 1 , 2 , 2 , 2 ], ... \"lev2\" : [ 1 , 1 , 2 , 1 , 1 , 2 ], ... \"lev3\" : [ 1 , 2 , 1 , 2 , 1 , 2 ], ... \"lev4\" : [ 1 , 2 , 3 , 4 , 5 , 6 ], ... \"values\" : [ 0 , 1 , 2 , 3 , 4 , 5 ]}) >>> df lev1 lev2 lev3 lev4 values 0 1 1 1 1 0 1 1 1 2 2 1 2 1 2 1 3 2 3 2 1 2 4 3 4 2 1 1 5 4 5 2 2 2 6 5 >>> df . pivot ( index = \"lev1\" , columns = [ \"lev2\" , \"lev3\" ], values = \"values\" ) lev2 1 2 lev3 1 2 1 2 lev1 1 0.0 1.0 2.0 NaN 2 4.0 3.0 NaN 5.0 >>> df . pivot ( index = [ \"lev1\" , \"lev2\" ], columns = [ \"lev3\" ], values = \"values\" ) lev3 1 2 lev1 lev2 1 1 0.0 1.0 2 2.0 NaN 2 1 4.0 3.0 2 NaN 5.0 A ValueError is raised if there are any duplicates. >>> df = pd . DataFrame ({ \"foo\" : [ 'one' , 'one' , 'two' , 'two' ], ... \"bar\" : [ 'A' , 'A' , 'B' , 'C' ], ... \"baz\" : [ 1 , 2 , 3 , 4 ]}) >>> df foo bar baz 0 one A 1 1 one A 2 2 two B 3 3 two C 4 Notice that the first two rows are the same for our index and columns arguments. >>> df . pivot ( index = 'foo' , columns = 'bar' , values = 'baz' ) Traceback ( most recent call last ): ... ValueError : Index contains duplicate entries , cannot reshape method pivot_table ( values=None , index=None , columns=None , aggfunc='mean' , fill_value=None , margins=False , dropna=True , margins_name='All' , observed=<no_default> , sort=True ) </> Create a spreadsheet-style pivot table as a DataFrame. The levels in the pivot table will be stored in MultiIndex objects (hierarchical indexes) on the index and columns of the result DataFrame. Parameters values (list-like or scalar, optional) \u2014 Column or columns to aggregate. index (column, Grouper, array, or list of the previous) \u2014 Keys to group by on the pivot table index. If a list is passed,it can contain any of the other types (except list). If an array is passed, it must be the same length as the data and will be used in the same manner as column values. columns (column, Grouper, array, or list of the previous) \u2014 Keys to group by on the pivot table column. If a list is passed,it can contain any of the other types (except list). If an array is passed, it must be the same length as the data and will be used in the same manner as column values. aggfunc (function, list of functions, dict, default \"mean\") \u2014 If a list of functions is passed, the resulting pivot table will havehierarchical columns whose top level are the function names (inferred from the function objects themselves). If a dict is passed, the key is column to aggregate and the value is function or list of functions. If margin=True , aggfunc will be used to calculate the partial aggregates. fill_value (scalar, default None) \u2014 Value to replace missing values with (in the resulting pivot table,after aggregation). margins (bool, default False) \u2014 If margins=True , special All columns and rowswill be added with partial group aggregates across the categories on the rows and columns. dropna (bool, default True) \u2014 Do not include columns whose entries are all NaN. If True,rows with a NaN value in any column will be omitted before computing margins. margins_name (str, default 'All') \u2014 Name of the row / column that will contain the totalswhen margins is True. observed (bool, default False) \u2014 This only applies if any of the groupers are Categoricals.If True: only show observed values for categorical groupers. If False: show all values for categorical groupers. .. deprecated:: 2.2.0 The default value of ``False`` is deprecated and will change to ``True`` in a future version of pandas. sort (bool, default True) \u2014 Specifies if the result should be sorted. .. versionadded:: 1.3.0 Returns (DataFrame) An Excel style pivot table. See Also DataFrame.pivot : Pivot without aggregation that can handle non-numeric data. DataFrame.melt: Unpivot a DataFrame from wide to long format, optionally leaving identifiers set. wide_to_long : Wide panel to long format. Less flexible but more user-friendly than melt. Notes Reference :ref: the user guide <reshaping.pivot> for more examples. Examples >>> df = pd . DataFrame ({ \"A\" : [ \"foo\" , \"foo\" , \"foo\" , \"foo\" , \"foo\" , ... \"bar\" , \"bar\" , \"bar\" , \"bar\" ], ... \"B\" : [ \"one\" , \"one\" , \"one\" , \"two\" , \"two\" , ... \"one\" , \"one\" , \"two\" , \"two\" ], ... \"C\" : [ \"small\" , \"large\" , \"large\" , \"small\" , ... \"small\" , \"large\" , \"small\" , \"small\" , ... \"large\" ], ... \"D\" : [ 1 , 2 , 2 , 3 , 3 , 4 , 5 , 6 , 7 ], ... \"E\" : [ 2 , 4 , 5 , 5 , 6 , 6 , 8 , 9 , 9 ]}) >>> df A B C D E 0 foo one small 1 2 1 foo one large 2 4 2 foo one large 2 5 3 foo two small 3 5 4 foo two small 3 6 5 bar one large 4 6 6 bar one small 5 8 7 bar two small 6 9 8 bar two large 7 9 This first example aggregates values by taking the sum. >>> table = pd . pivot_table ( df , values = 'D' , index = [ 'A' , 'B' ], ... columns = [ 'C' ], aggfunc = \"sum\" ) >>> table C large small A B bar one 4.0 5.0 two 7.0 6.0 foo one 4.0 1.0 two NaN 6.0 We can also fill missing values using the fill_value parameter. >>> table = pd . pivot_table ( df , values = 'D' , index = [ 'A' , 'B' ], ... columns = [ 'C' ], aggfunc = \"sum\" , fill_value = 0 ) >>> table C large small A B bar one 4 5 two 7 6 foo one 4 1 two 0 6 The next example aggregates by taking the mean across multiple columns. >>> table = pd . pivot_table ( df , values = [ 'D' , 'E' ], index = [ 'A' , 'C' ], ... aggfunc = { 'D' : \"mean\" , 'E' : \"mean\" }) >>> table D E A C bar large 5.500000 7.500000 small 5.500000 8.500000 foo large 2.000000 4.500000 small 2.333333 4.333333 We can also calculate multiple types of aggregations for any given value column. >>> table = pd . pivot_table ( df , values = [ 'D' , 'E' ], index = [ 'A' , 'C' ], ... aggfunc = { 'D' : \"mean\" , ... 'E' : [ \"min\" , \"max\" , \"mean\" ]}) >>> table D E mean max mean min A C bar large 5.500000 9 7.500000 6 small 5.500000 9 8.500000 8 foo large 2.000000 5 4.500000 4 small 2.333333 6 4.333333 2 method stack ( level=-1 , dropna=<no_default> , sort=<no_default> , future_stack=False ) </> Stack the prescribed level(s) from columns to index. Return a reshaped DataFrame or Series having a multi-level index with one or more new inner-most levels compared to the current DataFrame. The new inner-most levels are created by pivoting the columns of the current dataframe: if the columns have a single level, the output is a Series; if the columns have multiple levels, the new index level(s) is (are) taken from the prescribed level(s) and the output is a DataFrame. Parameters level (int, str, list, default -1) \u2014 Level(s) to stack from the column axis onto the indexaxis, defined as one index or label, or a list of indices or labels. dropna (bool, default True) \u2014 Whether to drop rows in the resulting Frame/Series withmissing values. Stacking a column level onto the index axis can create combinations of index and column values that are missing from the original dataframe. See Examples section. sort (bool, default True) \u2014 Whether to sort the levels of the resulting MultiIndex. future_stack (bool, default False) \u2014 Whether to use the new implementation that will replace the currentimplementation in pandas 3.0. When True, dropna and sort have no impact on the result and must remain unspecified. See :ref: pandas 2.1.0 Release notes <whatsnew_210.enhancements.new_stack> for more details. Returns (DataFrame or Series) Stacked dataframe or series. See Also DataFrame.unstack : Unstack prescribed level(s) from index axis onto column axis. DataFrame.pivot : Reshape dataframe from long format to wide format. DataFrame.pivottable : Create a spreadsheet-style pivot table as a DataFrame. Notes The function is named by analogy with a collection of books being reorganized from being side by side on a horizontal position (the columns of the dataframe) to being stacked vertically on top of each other (in the index of the dataframe). Reference :ref: the user guide <reshaping.stacking> for more examples. Examples Single level columns >>> df_single_level_cols = pd . DataFrame ([[ 0 , 1 ], [ 2 , 3 ]], ... index = [ 'cat' , 'dog' ], ... columns = [ 'weight' , 'height' ]) Stacking a dataframe with a single level column axis returns a Series: >>> df_single_level_cols weight height cat 0 1 dog 2 3 >>> df_single_level_cols . stack ( future_stack = True ) cat weight 0 height 1 dog weight 2 height 3 dtype : int64 Multi level columns: simple case >>> multicol1 = pd . MultiIndex . from_tuples ([( 'weight' , 'kg' ), ... ( 'weight' , 'pounds' )]) >>> df_multi_level_cols1 = pd . DataFrame ([[ 1 , 2 ], [ 2 , 4 ]], ... index = [ 'cat' , 'dog' ], ... columns = multicol1 ) Stacking a dataframe with a multi-level column axis: >>> df_multi_level_cols1 weight kg pounds cat 1 2 dog 2 4 >>> df_multi_level_cols1 . stack ( future_stack = True ) weight cat kg 1 pounds 2 dog kg 2 pounds 4 Missing values >>> multicol2 = pd . MultiIndex . from_tuples ([( 'weight' , 'kg' ), ... ( 'height' , 'm' )]) >>> df_multi_level_cols2 = pd . DataFrame ([[ 1.0 , 2.0 ], [ 3.0 , 4.0 ]], ... index = [ 'cat' , 'dog' ], ... columns = multicol2 ) It is common to have missing values when stacking a dataframe with multi-level columns, as the stacked dataframe typically has more values than the original dataframe. Missing values are filled with NaNs: >>> df_multi_level_cols2 weight height kg m cat 1.0 2.0 dog 3.0 4.0 >>> df_multi_level_cols2 . stack ( future_stack = True ) weight height cat kg 1.0 NaN m NaN 2.0 dog kg 3.0 NaN m NaN 4.0 Prescribing the level(s) to be stacked The first parameter controls which level or levels are stacked: >>> df_multi_level_cols2 . stack ( 0 , future_stack = True ) kg m cat weight 1.0 NaN height NaN 2.0 dog weight 3.0 NaN height NaN 4.0 >>> df_multi_level_cols2 . stack ([ 0 , 1 ], future_stack = True ) cat weight kg 1.0 height m 2.0 dog weight kg 3.0 height m 4.0 dtype : float64 method explode ( column , ignore_index=False ) </> Transform each element of a list-like to a row, replicating index values. Parameters column (IndexLabel) \u2014 Column(s) to explode.For multiple columns, specify a non-empty list with each element be str or tuple, and all specified columns their list-like data on same row of the frame must have matching length. .. versionadded:: 1.3.0 Multi-column explode ignore_index (bool, default False) \u2014 If True, the resulting index will be labeled 0, 1, \u2026, n - 1. Returns (DataFrame) Exploded lists to rows of the subset columns;index will be duplicated for these rows. Raises ValueError \u2014 If columns of the frame are not unique. If specified columns to explode is empty list. If specified columns to explode have not matching count of elements rowwise in the frame. See Also DataFrame.unstack : Pivot a level of the (necessarily hierarchical) index labels. DataFrame.melt : Unpivot a DataFrame from wide format to long format. Series.explode : Explode a DataFrame from list-like columns to long format. Notes This routine will explode list-likes including lists, tuples, sets, Series, and np.ndarray. The result dtype of the subset rows will be object. Scalars will be returned unchanged, and empty list-likes will result in a np.nan for that row. In addition, the ordering of rows in the output will be non-deterministic when exploding sets. Reference :ref: the user guide <reshaping.explode> for more examples. Examples >>> df = pd . DataFrame ({ 'A' : [[ 0 , 1 , 2 ], 'foo' , [], [ 3 , 4 ]], ... 'B' : 1 , ... 'C' : [[ 'a' , 'b' , 'c' ], np . nan , [], [ 'd' , 'e' ]]}) >>> df A B C 0 [ 0 , 1 , 2 ] 1 [ a , b , c ] 1 foo 1 NaN 2 [] 1 [] 3 [ 3 , 4 ] 1 [ d , e ] Single-column explode. >>> df . explode ( 'A' ) A B C 0 0 1 [ a , b , c ] 0 1 1 [ a , b , c ] 0 2 1 [ a , b , c ] 1 foo 1 NaN 2 NaN 1 [] 3 3 1 [ d , e ] 3 4 1 [ d , e ] Multi-column explode. >>> df . explode ( list ( 'AC' )) A B C 0 0 1 a 0 1 1 b 0 2 1 c 1 foo 1 NaN 2 NaN 1 NaN 3 3 1 d 3 4 1 e method unstack ( level=-1 , fill_value=None , sort=True ) </> Pivot a level of the (necessarily hierarchical) index labels. Returns a DataFrame having a new level of column labels whose inner-most level consists of the pivoted index labels. If the index is not a MultiIndex, the output will be a Series (the analogue of stack when the columns are not a MultiIndex). Parameters level (int, str, or list of these, default -1 (last level)) \u2014 Level(s) of index to unstack, can pass level name. fill_value (int, str or dict) \u2014 Replace NaN with this value if the unstack produces missing values. sort (bool, default True) \u2014 Sort the level(s) in the resulting MultiIndex columns. See Also DataFrame.pivot : Pivot a table based on column values.DataFrame.stack : Pivot a level of the column labels (inverse operation from unstack ). Notes Reference :ref: the user guide <reshaping.stacking> for more examples. Examples >>> index = pd . MultiIndex . from_tuples ([( 'one' , 'a' ), ( 'one' , 'b' ), ... ( 'two' , 'a' ), ( 'two' , 'b' )]) >>> s = pd . Series ( np . arange ( 1.0 , 5.0 ), index = index ) >>> s one a 1.0 b 2.0 two a 3.0 b 4.0 dtype : float64 >>> s . unstack ( level =- 1 ) a b one 1.0 2.0 two 3.0 4.0 >>> s . unstack ( level = 0 ) one two a 1.0 3.0 b 2.0 4.0 >>> df = s . unstack ( level = 0 ) >>> df . unstack () one a 1.0 b 2.0 two a 3.0 b 4.0 dtype : float64 method melt ( id_vars=None , value_vars=None , var_name=None , value_name='value' , col_level=None , ignore_index=True ) </> Unpivot a DataFrame from wide to long format, optionally leaving identifiers set. This function is useful to massage a DataFrame into a format where one or more columns are identifier variables ( id_vars ), while all other columns, considered measured variables ( value_vars ), are \"unpivoted\" to the row axis, leaving just two non-identifier columns, 'variable' and 'value'. Parameters id_vars (scalar, tuple, list, or ndarray, optional) \u2014 Column(s) to use as identifier variables. value_vars (scalar, tuple, list, or ndarray, optional) \u2014 Column(s) to unpivot. If not specified, uses all columns thatare not set as id_vars . var_name (scalar, default None) \u2014 Name to use for the 'variable' column. If None it uses frame.columns.name or 'variable'. value_name (scalar, default 'value') \u2014 Name to use for the 'value' column, can't be an existing column label. col_level (scalar, optional) \u2014 If columns are a MultiIndex then use this level to melt. ignore_index (bool, default True) \u2014 If True, original index is ignored. If False, the original index is retained.Index labels will be repeated as necessary. Returns (DataFrame) Unpivoted DataFrame. See Also melt : Identical method.pivot_table : Create a spreadsheet-style pivot table as a DataFrame. DataFrame.pivot : Return reshaped DataFrame organized by given index / column values. DataFrame.explode : Explode a DataFrame from list-like columns to long format. Notes Reference :ref: the user guide <reshaping.melt> for more examples. Examples >>> df = pd . DataFrame ({ 'A' : { 0 : 'a' , 1 : 'b' , 2 : 'c' }, ... 'B' : { 0 : 1 , 1 : 3 , 2 : 5 }, ... 'C' : { 0 : 2 , 1 : 4 , 2 : 6 }}) >>> df A B C 0 a 1 2 1 b 3 4 2 c 5 6 >>> df . melt ( id_vars = [ 'A' ], value_vars = [ 'B' ]) A variable value 0 a B 1 1 b B 3 2 c B 5 >>> df . melt ( id_vars = [ 'A' ], value_vars = [ 'B' , 'C' ]) A variable value 0 a B 1 1 b B 3 2 c B 5 3 a C 2 4 b C 4 5 c C 6 The names of 'variable' and 'value' columns can be customized: >>> df . melt ( id_vars = [ 'A' ], value_vars = [ 'B' ], ... var_name = 'myVarname' , value_name = 'myValname' ) A myVarname myValname 0 a B 1 1 b B 3 2 c B 5 Original index values can be kept around: >>> df . melt ( id_vars = [ 'A' ], value_vars = [ 'B' , 'C' ], ignore_index = False ) A variable value 0 a B 1 1 b B 3 2 c B 5 0 a C 2 1 b C 4 2 c C 6 If you have multi-index columns: >>> df . columns = [ list ( 'ABC' ), list ( 'DEF' )] >>> df A B C D E F 0 a 1 2 1 b 3 4 2 c 5 6 >>> df . melt ( col_level = 0 , id_vars = [ 'A' ], value_vars = [ 'B' ]) A variable value 0 a B 1 1 b B 3 2 c B 5 >>> df . melt ( id_vars = [( 'A' , 'D' )], value_vars = [( 'B' , 'E' )]) ( A , D ) variable_0 variable_1 value 0 a B E 1 1 b B E 3 2 c B E 5 method diff ( periods=1 , axis=0 ) </> First discrete difference of element. Calculates the difference of a DataFrame element compared with another element in the DataFrame (default is element in previous row). Parameters periods (int, default 1) \u2014 Periods to shift for calculating difference, accepts negativevalues. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Take difference over rows (0) or columns (1). Returns (DataFrame) First differences of the Series. See Also DataFrame.pct_change: Percent change over given number of periods.DataFrame.shift: Shift index by desired number of periods with an optional time freq. Series.diff: First discrete difference of object. Notes For boolean dtypes, this uses :meth: operator.xor rather than :meth: operator.sub . The result is calculated according to current dtype in DataFrame, however dtype of the result is always float64. Examples w , , ) f c 1 4 9 6 5 6 ) c N 0 0 0 0 0 n ) c 0 3 7 3 0 8 w ) c N N N 0 0 0 w ) c 0 0 0 0 0 N e ) ) a N 0 method aggregate ( func=None , axis=0 , *args , **kwargs ) </> Aggregate using one or more operations over the specified axis. Parameters func (function, str, list or dict) \u2014 Function to use for aggregating the data. If a function, must eitherwork when passed a DataFrame or when passed to DataFrame.apply. Accepted combinations are: function string function name list of functions and/or function names, e.g. [np.sum, 'mean'] dict of axis labels -> functions, function names or list of such. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 If 0 or 'index': apply function to each column.If 1 or 'columns': apply function to each row. *args \u2014 Positional arguments to pass to func . **kwargs \u2014 Keyword arguments to pass to func . Returns (scalar, Series or DataFrame) : n n s See Also DataFrame.apply : Perform any type of operations.DataFrame.transform : Perform transformation type operations. pandas.DataFrame.groupby : Perform operations over groups. pandas.DataFrame.resample : Perform operations over resampled bins. pandas.DataFrame.rolling : Perform operations over rolling window. pandas.DataFrame.expanding : Perform operations over expanding window. pandas.core.window.ewm.ExponentialMovingWindow : Perform operation over exponential weighted window. Notes The aggregation operations are always performed over an axis, either the index (default) or the column axis. This behavior is different from numpy aggregation functions ( mean , median , prod , sum , std , var ), where the default is to compute the aggregation of the flattened array, e.g., numpy.mean(arr_2d) as opposed to numpy.mean(arr_2d, axis=0) . agg is an alias for aggregate . Use the alias. Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See :ref: gotchas.udf-mutation for more details. A passed user-defined-function will be passed a Series for evaluation. Examples >>> df = pd . DataFrame ([[ 1 , 2 , 3 ], ... [ 4 , 5 , 6 ], ... [ 7 , 8 , 9 ], ... [ np . nan , np . nan , np . nan ]], ... columns = [ 'A' , 'B' , 'C' ]) Aggregate these functions over the rows. >>> df . agg ([ 'sum' , 'min' ]) A B C sum 12.0 15.0 18.0 min 1.0 2.0 3.0 Different aggregations per column. >>> df . agg ({ 'A' : [ 'sum' , 'min' ], 'B' : [ 'min' , 'max' ]}) A B sum 12.0 NaN min 1.0 2.0 max NaN 8.0 Aggregate different functions over the columns and rename the index of the resulting DataFrame. >>> df . agg ( x = ( 'A' , 'max' ), y = ( 'B' , 'min' ), z = ( 'C' , 'mean' )) A B C x 7.0 NaN NaN y NaN 2.0 NaN z NaN NaN 6.0 Aggregate over the columns. >>> df . agg ( \"mean\" , axis = \"columns\" ) 0 2.0 1 5.0 2 8.0 3 NaN dtype : float64 method transform ( func , axis=0 , *args , **kwargs ) </> Call func on self producing a DataFrame with the same axis shape as self. Parameters func (function, str, list-like or dict-like) \u2014 Function to use for transforming the data. If a function, must eitherwork when passed a DataFrame or when passed to DataFrame.apply. If func is both list-like and dict-like, dict-like behavior takes precedence. Accepted combinations are: function string function name list-like of functions and/or function names, e.g. [np.exp, 'sqrt'] dict-like of axis labels -> functions, function names or list-like of such. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 If 0 or 'index': apply function to each column.If 1 or 'columns': apply function to each row. *args \u2014 Positional arguments to pass to func . **kwargs \u2014 Keyword arguments to pass to func . Returns (DataFrame) A DataFrame that must have the same length as self. See Also DataFrame.agg : Only perform aggregating type operations.DataFrame.apply : Invoke function on a DataFrame. Notes Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See :ref: gotchas.udf-mutation for more details. Examples >>> df = pd . DataFrame ({ 'A' : range ( 3 ), 'B' : range ( 1 , 4 )}) >>> df A B 0 0 1 1 1 2 2 2 3 >>> df . transform ( lambda x : x + 1 ) A B 0 1 2 1 2 3 2 3 4 Even though the resulting DataFrame must have the same length as the input DataFrame, it is possible to provide several input functions: >>> s = pd . Series ( range ( 3 )) >>> s 0 0 1 1 2 2 dtype : int64 >>> s . transform ([ np . sqrt , np . exp ]) sqrt exp 0 0.000000 1.000000 1 1.000000 2.718282 2 1.414214 7.389056 You can call transform on a GroupBy object: >>> df = pd . DataFrame ({ ... \"Date\" : [ ... \"2015-05-08\" , \"2015-05-07\" , \"2015-05-06\" , \"2015-05-05\" , ... \"2015-05-08\" , \"2015-05-07\" , \"2015-05-06\" , \"2015-05-05\" ], ... \"Data\" : [ 5 , 8 , 6 , 1 , 50 , 100 , 60 , 120 ], ... }) >>> df Date Data 0 2015 - 05 - 08 5 1 2015 - 05 - 07 8 2 2015 - 05 - 06 6 3 2015 - 05 - 05 1 4 2015 - 05 - 08 50 5 2015 - 05 - 07 100 6 2015 - 05 - 06 60 7 2015 - 05 - 05 120 >>> df . groupby ( 'Date' )[ 'Data' ] . transform ( 'sum' ) 0 55 1 108 2 66 3 121 4 55 5 108 6 66 7 121 Name : Data , dtype : int64 >>> df = pd . DataFrame ({ ... \"c\" : [ 1 , 1 , 1 , 2 , 2 , 2 , 2 ], ... \"type\" : [ \"m\" , \"n\" , \"o\" , \"m\" , \"m\" , \"n\" , \"n\" ] ... }) >>> df c type 0 1 m 1 1 n 2 1 o 3 2 m 4 2 m 5 2 n 6 2 n >>> df [ 'size' ] = df . groupby ( 'c' )[ 'type' ] . transform ( len ) >>> df c type size 0 1 m 3 1 1 n 3 2 1 o 3 3 2 m 4 4 2 m 4 5 2 n 4 6 2 n 4 method apply ( func , axis=0 , raw=False , result_type=None , args=() , by_row='compat' , engine='python' , engine_kwargs=None , **kwargs ) </> Apply a function along an axis of the DataFrame. Objects passed to the function are Series objects whose index is either the DataFrame's index ( axis=0 ) or the DataFrame's columns ( axis=1 ). By default ( result_type=None ), the final return type is inferred from the return type of the applied function. Otherwise, it depends on the result_type argument. Parameters func (function) \u2014 Function to apply to each column or row. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Axis along which the function is applied: 0 or 'index': apply function to each column. 1 or 'columns': apply function to each row. raw (bool, default False) \u2014 Determines if row or column is passed as a Series or ndarray object: False : passes each row or column as a Series to the function. True : the passed function will receive ndarray objects instead. If you are just applying a NumPy reduction function this will achieve much better performance. result_type ({'expand', 'reduce', 'broadcast', None}, default None) \u2014 These only act when axis=1 (columns): 'expand' : list-like results will be turned into columns. 'reduce' : returns a Series if possible rather than expanding list-like results. This is the opposite of 'expand'. 'broadcast' : results will be broadcast to the original shape of the DataFrame, the original index and columns will be retained. The default behaviour (None) depends on the return value of the applied function: list-like results will be returned as a Series of those. However if the apply function returns a Series these are expanded to columns. args (tuple) \u2014 Positional arguments to pass to func in addition to thearray/series. by_row (False or \"compat\", default \"compat\") \u2014 Only has an effect when func is a listlike or dictlike of funcsand the func isn't a string. If \"compat\", will if possible first translate the func into pandas methods (e.g. Series().apply(np.sum) will be translated to Series().sum() ). If that doesn't work, will try call to apply again with by_row=True and if that fails, will call apply again with by_row=False (backward compatible). If False, the funcs will be passed the whole Series at once. .. versionadded:: 2.1.0 engine ({'python', 'numba'}, default 'python') \u2014 Choose between the python (default) engine or the numba engine in apply. The numba engine will attempt to JIT compile the passed function, which may result in speedups for large DataFrames. It also supports the following engine_kwargs : nopython (compile the function in nopython mode) nogil (release the GIL inside the JIT compiled function) parallel (try to apply the function in parallel over the DataFrame) Note: Due to limitations within numba/how pandas interfaces with numba, you should only use this if raw=True Note: The numba compiler only supports a subset of valid Python/numpy operations. Please read more about the supported python features <https://numba.pydata.org/numba-doc/dev/reference/pysupported.html> and supported numpy features <https://numba.pydata.org/numba-doc/dev/reference/numpysupported.html> in numba to learn what you can or cannot use in the passed function. .. versionadded:: 2.2.0 engine_kwargs (dict) \u2014 Pass keyword arguments to the engine.This is currently only used by the numba engine, see the documentation for the engine argument for more information. **kwargs \u2014 Additional keyword arguments to pass as keywords arguments to func . Returns (Series or DataFrame) Result of applying func along the given axis of theDataFrame. See Also DataFrame.map: For elementwise operations.DataFrame.aggregate: Only perform aggregating type operations. DataFrame.transform: Only perform transforming type operations. Notes Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See :ref: gotchas.udf-mutation for more details. Examples >>> df = pd . DataFrame ([[ 4 , 9 ]] * 3 , columns = [ 'A' , 'B' ]) >>> df A B 0 4 9 1 4 9 2 4 9 Using a numpy universal function (in this case the same as np.sqrt(df) ): >>> df . apply ( np . sqrt ) A B 0 2.0 3.0 1 2.0 3.0 2 2.0 3.0 Using a reducing function on either axis >>> df . apply ( np . sum , axis = 0 ) A 12 B 27 dtype : int64 >>> df . apply ( np . sum , axis = 1 ) 0 13 1 13 2 13 dtype : int64 Returning a list-like will result in a Series >>> df . apply ( lambda x : [ 1 , 2 ], axis = 1 ) 0 [ 1 , 2 ] 1 [ 1 , 2 ] 2 [ 1 , 2 ] dtype : object Passing result_type='expand' will expand list-like results to columns of a Dataframe >>> df . apply ( lambda x : [ 1 , 2 ], axis = 1 , result_type = 'expand' ) 0 1 0 1 2 1 1 2 2 1 2 Returning a Series inside the function is similar to passing result_type='expand' . The resulting column names will be the Series index. >>> df . apply ( lambda x : pd . Series ([ 1 , 2 ], index = [ 'foo' , 'bar' ]), axis = 1 ) foo bar 0 1 2 1 1 2 2 1 2 Passing result_type='broadcast' will ensure the same shape result, whether list-like or scalar is returned by the function, and broadcast it along the axis. The resulting column names will be the originals. >>> df . apply ( lambda x : [ 1 , 2 ], axis = 1 , result_type = 'broadcast' ) A B 0 1 2 1 1 2 2 1 2 method map ( func , na_action=None , **kwargs ) </> Apply a function to a Dataframe elementwise. .. versionadded:: 2.1.0 DataFrame.applymap was deprecated and renamed to DataFrame.map. This method applies a function that accepts and returns a scalar to every element of a DataFrame. Parameters func (callable) \u2014 Python function, returns a single value from a single value. na_action ({None, 'ignore'}, default None) \u2014 If 'ignore', propagate NaN values, without passing them to func. **kwargs \u2014 Additional keyword arguments to pass as keywords arguments to func . Returns (DataFrame) Transformed DataFrame. See Also DataFrame.apply : Apply a function along input axis of DataFrame.DataFrame.replace: Replace values given in to_replace with value . Series.map : Apply a function elementwise on a Series. Examples >>> df = pd . DataFrame ([[ 1 , 2.12 ], [ 3.356 , 4.567 ]]) >>> df 0 1 0 1.000 2.120 1 3.356 4.567 >>> df . map ( lambda x : len ( str ( x ))) 0 1 0 3 4 1 5 5 Like Series.map, NA values can be ignored: >>> df_copy = df . copy () >>> df_copy . iloc [ 0 , 0 ] = pd . NA >>> df_copy . map ( lambda x : len ( str ( x )), na_action = 'ignore' ) 0 1 0 NaN 4 1 5.0 5 It is also possible to use map with functions that are not lambda functions: >>> df . map ( round , ndigits = 1 ) 0 1 0 1.0 2.1 1 3.4 4.6 Note that a vectorized version of func often exists, which will be much faster. You could square each number elementwise. >>> df . map ( lambda x : x ** 2 ) 0 1 0 1.000000 4.494400 1 11.262736 20.857489 But it's better to avoid map in that case. >>> df ** 2 0 1 0 1.000000 4.494400 1 11.262736 20.857489 method applymap ( func , na_action=None , **kwargs ) </> Apply a function to a Dataframe elementwise. .. deprecated:: 2.1.0 DataFrame.applymap has been deprecated. Use DataFrame.map instead. This method applies a function that accepts and returns a scalar to every element of a DataFrame. Parameters func (callable) \u2014 Python function, returns a single value from a single value. na_action ({None, 'ignore'}, default None) \u2014 If 'ignore', propagate NaN values, without passing them to func. **kwargs \u2014 Additional keyword arguments to pass as keywords arguments to func . Returns (DataFrame) Transformed DataFrame. See Also DataFrame.apply : Apply a function along input axis of DataFrame.DataFrame.map : Apply a function along input axis of DataFrame. DataFrame.replace: Replace values given in to_replace with value . Examples >>> df = pd . DataFrame ([[ 1 , 2.12 ], [ 3.356 , 4.567 ]]) >>> df 0 1 0 1.000 2.120 1 3.356 4.567 >>> df . map ( lambda x : len ( str ( x ))) 0 1 0 3 4 1 5 5 method join ( other , on=None , how='left' , lsuffix='' , rsuffix='' , sort=False , validate=None ) </> Join columns of another DataFrame. Join columns with other DataFrame either on index or on a key column. Efficiently join multiple DataFrame objects by index at once by passing a list. Parameters other (DataFrame, Series, or a list containing any combination of them) \u2014 Index should be similar to one of the columns in this one. If aSeries is passed, its name attribute must be set, and that will be used as the column name in the resulting joined DataFrame. on (str, list of str, or array-like, optional) \u2014 Column or index level name(s) in the caller to join on the indexin other , otherwise joins index-on-index. If multiple values given, the other DataFrame must have a MultiIndex. Can pass an array as the join key if it is not already contained in the calling DataFrame. Like an Excel VLOOKUP operation. how ({'left', 'right', 'outer', 'inner', 'cross'}, default 'left') \u2014 How to handle the operation of the two objects. left: use calling frame's index (or column if on is specified) right: use other 's index. outer: form union of calling frame's index (or column if on is specified) with other 's index, and sort it lexicographically. inner: form intersection of calling frame's index (or column if on is specified) with other 's index, preserving the order of the calling's one. cross: creates the cartesian product from both frames, preserves the order of the left keys. lsuffix (str, default '') \u2014 Suffix to use from left frame's overlapping columns. rsuffix (str, default '') \u2014 Suffix to use from right frame's overlapping columns. sort (bool, default False) \u2014 Order result DataFrame lexicographically by the join key. If False,the order of the join key depends on the join type (how keyword). validate (str, optional) \u2014 If specified, checks if join is of specified type. \"one_to_one\" or \"1:1\": check if join keys are unique in both left and right datasets. \"one_to_many\" or \"1:m\": check if join keys are unique in left dataset. \"many_to_one\" or \"m:1\": check if join keys are unique in right dataset. \"many_to_many\" or \"m:m\": allowed, but does not result in checks. .. versionadded:: 1.5.0 Returns (DataFrame) A dataframe containing columns from both the caller and other . See Also DataFrame.merge : For column(s)-on-column(s) operations. Notes Parameters on , lsuffix , and rsuffix are not supported when passing a list of DataFrame objects. Examples >>> df = pd . DataFrame ({ 'key' : [ 'K0' , 'K1' , 'K2' , 'K3' , 'K4' , 'K5' ], ... 'A' : [ 'A0' , 'A1' , 'A2' , 'A3' , 'A4' , 'A5' ]}) >>> df key A 0 K0 A0 1 K1 A1 2 K2 A2 3 K3 A3 4 K4 A4 5 K5 A5 >>> other = pd . DataFrame ({ 'key' : [ 'K0' , 'K1' , 'K2' ], ... 'B' : [ 'B0' , 'B1' , 'B2' ]}) >>> other key B 0 K0 B0 1 K1 B1 2 K2 B2 Join DataFrames using their indexes. >>> df . join ( other , lsuffix = '_caller' , rsuffix = '_other' ) key_caller A key_other B 0 K0 A0 K0 B0 1 K1 A1 K1 B1 2 K2 A2 K2 B2 3 K3 A3 NaN NaN 4 K4 A4 NaN NaN 5 K5 A5 NaN NaN If we want to join using the key columns, we need to set key to be the index in both df and other . The joined DataFrame will have key as its index. >>> df . set_index ( 'key' ) . join ( other . set_index ( 'key' )) A B key K0 A0 B0 K1 A1 B1 K2 A2 B2 K3 A3 NaN K4 A4 NaN K5 A5 NaN Another option to join using the key columns is to use the on parameter. DataFrame.join always uses other 's index but we can use any column in df . This method preserves the original DataFrame's index in the result. >>> df . join ( other . set_index ( 'key' ), on = 'key' ) key A B 0 K0 A0 B0 1 K1 A1 B1 2 K2 A2 B2 3 K3 A3 NaN 4 K4 A4 NaN 5 K5 A5 NaN Using non-unique key values shows how they are matched. >>> df = pd . DataFrame ({ 'key' : [ 'K0' , 'K1' , 'K1' , 'K3' , 'K0' , 'K1' ], ... 'A' : [ 'A0' , 'A1' , 'A2' , 'A3' , 'A4' , 'A5' ]}) >>> df key A 0 K0 A0 1 K1 A1 2 K1 A2 3 K3 A3 4 K0 A4 5 K1 A5 >>> df . join ( other . set_index ( 'key' ), on = 'key' , validate = 'm:1' ) key A B 0 K0 A0 B0 1 K1 A1 B1 2 K1 A2 B1 3 K3 A3 NaN 4 K0 A4 B0 5 K1 A5 B1 method merge ( right , how='inner' , on=None , left_on=None , right_on=None , left_index=False , right_index=False , sort=False , suffixes=('_x', '_y') , copy=None , indicator=False , validate=None ) </> Merge DataFrame or named Series objects with a database-style join. A named Series object is treated as a DataFrame with a single named column. The join is done on columns or indexes. If joining columns on columns, the DataFrame indexes will be ignored . Otherwise if joining indexes on indexes or indexes on a column or columns, the index will be passed on. When performing a cross merge, no column specifications to merge on are allowed. .. warning:: If both key columns contain rows where the key is a null value, those rows will be matched against each other. This is different from usual SQL join behaviour and can lead to unexpected results. Parameters right (DataFrame or named Series) \u2014 Object to merge with. how ({'left', 'right', 'outer', 'inner', 'cross'}, default 'inner') \u2014 Type of merge to be performed. left: use only keys from left frame, similar to a SQL left outer join; preserve key order. right: use only keys from right frame, similar to a SQL right outer join; preserve key order. outer: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically. inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys. cross: creates the cartesian product from both frames, preserves the order of the left keys. on (label or list) \u2014 Column or index level names to join on. These must be found in bothDataFrames. If on is None and not merging on indexes then this defaults to the intersection of the columns in both DataFrames. left_on (label or list, or array-like) \u2014 Column or index level names to join on in the left DataFrame. Can alsobe an array or list of arrays of the length of the left DataFrame. These arrays are treated as if they are columns. right_on (label or list, or array-like) \u2014 Column or index level names to join on in the right DataFrame. Can alsobe an array or list of arrays of the length of the right DataFrame. These arrays are treated as if they are columns. left_index (bool, default False) \u2014 Use the index from the left DataFrame as the join key(s). If it is aMultiIndex, the number of keys in the other DataFrame (either the index or a number of columns) must match the number of levels. right_index (bool, default False) \u2014 Use the index from the right DataFrame as the join key. Same caveats asleft_index. sort (bool, default False) \u2014 Sort the join keys lexicographically in the result DataFrame. If False,the order of the join keys depends on the join type (how keyword). suffixes (list-like, default is (\"_x\", \"_y\")) \u2014 A length-2 sequence where each element is optionally a stringindicating the suffix to add to overlapping column names in left and right respectively. Pass a value of None instead of a string to indicate that the column name from left or right should be left as-is, with no suffix. At least one of the values must not be None. copy (bool, default True) \u2014 If False, avoid copy if possible. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` indicator (bool or str, default False) \u2014 If True, adds a column to the output DataFrame called \"_merge\" withinformation on the source of each row. The column can be given a different name by providing a string argument. The column will have a Categorical type with the value of \"left_only\" for observations whose merge key only appears in the left DataFrame, \"right_only\" for observations whose merge key only appears in the right DataFrame, and \"both\" if the observation's merge key is found in both DataFrames. validate (str, optional) \u2014 If specified, checks if merge is of specified type. \"one_to_one\" or \"1:1\": check if merge keys are unique in both left and right datasets. \"one_to_many\" or \"1:m\": check if merge keys are unique in left dataset. \"many_to_one\" or \"m:1\": check if merge keys are unique in right dataset. \"many_to_many\" or \"m:m\": allowed, but does not result in checks. Returns (DataFrame) A DataFrame of the two merged objects. See Also merge_ordered : Merge with optional filling/interpolation.merge_asof : Merge on nearest keys. DataFrame.join : Similar method using indices. Examples >>> df1 = pd . DataFrame ({ 'lkey' : [ 'foo' , 'bar' , 'baz' , 'foo' ], ... 'value' : [ 1 , 2 , 3 , 5 ]}) >>> df2 = pd . DataFrame ({ 'rkey' : [ 'foo' , 'bar' , 'baz' , 'foo' ], ... 'value' : [ 5 , 6 , 7 , 8 ]}) >>> df1 lkey value 0 foo 1 1 bar 2 2 baz 3 3 foo 5 >>> df2 rkey value 0 foo 5 1 bar 6 2 baz 7 3 foo 8 Merge df1 and df2 on the lkey and rkey columns. The value columns have the default suffixes, _x and _y, appended. >>> df1 . merge ( df2 , left_on = 'lkey' , right_on = 'rkey' ) lkey value_x rkey value_y 0 foo 1 foo 5 1 foo 1 foo 8 2 bar 2 bar 6 3 baz 3 baz 7 4 foo 5 foo 5 5 foo 5 foo 8 Merge DataFrames df1 and df2 with specified left and right suffixes appended to any overlapping columns. >>> df1 . merge ( df2 , left_on = 'lkey' , right_on = 'rkey' , ... suffixes = ( '_left' , '_right' )) lkey value_left rkey value_right 0 foo 1 foo 5 1 foo 1 foo 8 2 bar 2 bar 6 3 baz 3 baz 7 4 foo 5 foo 5 5 foo 5 foo 8 Merge DataFrames df1 and df2, but raise an exception if the DataFrames have any overlapping columns. >>> df1 . merge ( df2 , left_on = 'lkey' , right_on = 'rkey' , suffixes = ( False , False )) Traceback ( most recent call last ): ... ValueError : columns overlap but no suffix specified : Index ([ 'value' ], dtype = 'object' ) >>> df1 = pd . DataFrame ({ 'a' : [ 'foo' , 'bar' ], 'b' : [ 1 , 2 ]}) >>> df2 = pd . DataFrame ({ 'a' : [ 'foo' , 'baz' ], 'c' : [ 3 , 4 ]}) >>> df1 a b 0 foo 1 1 bar 2 >>> df2 a c 0 foo 3 1 baz 4 >>> df1 . merge ( df2 , how = 'inner' , on = 'a' ) a b c 0 foo 1 3 >>> df1 . merge ( df2 , how = 'left' , on = 'a' ) a b c 0 foo 1 3.0 1 bar 2 NaN >>> df1 = pd . DataFrame ({ 'left' : [ 'foo' , 'bar' ]}) >>> df2 = pd . DataFrame ({ 'right' : [ 7 , 8 ]}) >>> df1 left 0 foo 1 bar >>> df2 right 0 7 1 8 >>> df1 . merge ( df2 , how = 'cross' ) left right 0 foo 7 1 foo 8 2 bar 7 3 bar 8 method round ( decimals=0 , *args , **kwargs ) </> Round a DataFrame to a variable number of decimal places. Parameters decimals (int, dict, Series) \u2014 Number of decimal places to round each column to. If an int isgiven, round each column to the same number of places. Otherwise dict and Series round to variable numbers of places. Column names should be in the keys if decimals is a dict-like, or in the index if decimals is a Series. Any columns not included in decimals will be left as is. Elements of decimals which are not columns of the input will be ignored. *args \u2014 Additional keywords have no effect but might be accepted forcompatibility with numpy. **kwargs \u2014 Additional keywords have no effect but might be accepted forcompatibility with numpy. Returns (DataFrame) A DataFrame with the affected columns rounded to the specifiednumber of decimal places. See Also numpy.around : Round a numpy array to the given number of decimals.Series.round : Round a Series to the given number of decimals. Examples >>> df = pd . DataFrame ([( .21 , .32 ), ( .01 , .67 ), ( .66 , .03 ), ( .21 , .18 )], ... columns = [ 'dogs' , 'cats' ]) >>> df dogs cats 0 0.21 0.32 1 0.01 0.67 2 0.66 0.03 3 0.21 0.18 By providing an integer each column is rounded to the same number of decimal places >>> df . round ( 1 ) dogs cats 0 0.2 0.3 1 0.0 0.7 2 0.7 0.0 3 0.2 0.2 With a dict, the number of places for specific columns can be specified with the column names as key and the number of decimal places as value >>> df . round ({ 'dogs' : 1 , 'cats' : 0 }) dogs cats 0 0.2 0.0 1 0.0 1.0 2 0.7 0.0 3 0.2 0.0 Using a Series, the number of places for specific columns can be specified with the column names as index and the number of decimal places as value >>> decimals = pd . Series ([ 0 , 1 ], index = [ 'cats' , 'dogs' ]) >>> df . round ( decimals ) dogs cats 0 0.2 0.0 1 0.0 1.0 2 0.7 0.0 3 0.2 0.0 method corr ( method='pearson' , min_periods=1 , numeric_only=False ) </> Compute pairwise correlation of columns, excluding NA/null values. Parameters method ({'pearson', 'kendall', 'spearman'} or callable) \u2014 Method of correlation: pearson : standard correlation coefficient kendall : Kendall Tau correlation coefficient spearman : Spearman rank correlation callable: callable with input two 1d ndarrays and returning a float. Note that the returned matrix from corr will have 1 along the diagonals and will be symmetric regardless of the callable's behavior. min_periods (int, optional) \u2014 Minimum number of observations required per pair of columnsto have a valid result. Currently only available for Pearson and Spearman correlation. numeric_only (bool, default False) \u2014 Include only float , int or boolean data. .. versionadded:: 1.5.0 .. versionchanged:: 2.0.0 The default value of numeric_only is now False . Returns (DataFrame) Correlation matrix. See Also DataFrame.corrwith : Compute pairwise correlation with another DataFrame or Series. Series.corr : Compute the correlation between two Series. Notes Pearson, Kendall and Spearman correlation are currently computed using pairwise complete observations. Pearson correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient> _ Kendall rank correlation coefficient <https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient> _ Spearman's rank correlation coefficient <https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient> _ Examples >>> def histogram_intersection ( a , b ): ... v = np . minimum ( a , b ) . sum () . round ( decimals = 1 ) ... return v >>> df = pd . DataFrame ([( .2 , .3 ), ( .0 , .6 ), ( .6 , .0 ), ( .2 , .1 )], ... columns = [ 'dogs' , 'cats' ]) >>> df . corr ( method = histogram_intersection ) dogs cats dogs 1.0 0.3 cats 0.3 1.0 >>> df = pd . DataFrame ([( 1 , 1 ), ( 2 , np . nan ), ( np . nan , 3 ), ( 4 , 4 )], ... columns = [ 'dogs' , 'cats' ]) >>> df . corr ( min_periods = 3 ) dogs cats dogs 1.0 NaN cats NaN 1.0 method cov ( min_periods=None , ddof=1 , numeric_only=False ) </> Compute pairwise covariance of columns, excluding NA/null values. Compute the pairwise covariance among the series of a DataFrame. The returned data frame is the covariance matrix <https://en.wikipedia.org/wiki/Covariance_matrix> __ of the columns of the DataFrame. Both NA and null values are automatically excluded from the calculation. (See the note below about bias from missing values.) A threshold can be set for the minimum number of observations for each value created. Comparisons with observations below this threshold will be returned as NaN . This method is generally used for the analysis of time series data to understand the relationship between different measures across time. Parameters min_periods (int, optional) \u2014 Minimum number of observations required per pair of columnsto have a valid result. ddof (int, default 1) \u2014 Delta degrees of freedom. The divisor used in calculationsis N - ddof , where N represents the number of elements. This argument is applicable only when no nan is in the dataframe. numeric_only (bool, default False) \u2014 Include only float , int or boolean data. .. versionadded:: 1.5.0 .. versionchanged:: 2.0.0 The default value of numeric_only is now False . Returns (DataFrame) The covariance matrix of the series of the DataFrame. See Also Series.cov : Compute covariance with another Series.core.window.ewm.ExponentialMovingWindow.cov : Exponential weighted sample covariance. core.window.expanding.Expanding.cov : Expanding sample covariance. core.window.rolling.Rolling.cov : Rolling sample covariance. Notes Returns the covariance matrix of the DataFrame's time series. The covariance is normalized by N-ddof. For DataFrames that have Series that are missing data (assuming that data is missing at random <https://en.wikipedia.org/wiki/Missing_data#Missing_at_random> __) the returned covariance matrix will be an unbiased estimate of the variance and covariance between the member Series. However, for many applications this estimate may not be acceptable because the estimate covariance matrix is not guaranteed to be positive semi-definite. This could lead to estimate correlations having absolute values which are greater than one, and/or a non-invertible covariance matrix. See Estimation of covariance matrices <https://en.wikipedia.org/w/index.php?title=Estimation_of_covariance_ matrices> __ for more details. Examples >>> df = pd . DataFrame ([( 1 , 2 ), ( 0 , 3 ), ( 2 , 0 ), ( 1 , 1 )], ... columns = [ 'dogs' , 'cats' ]) >>> df . cov () dogs cats dogs 0.666667 - 1.000000 cats - 1.000000 1.666667 >>> np . random . seed ( 42 ) >>> df = pd . DataFrame ( np . random . randn ( 1000 , 5 ), ... columns = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) >>> df . cov () a b c d e a 0.998438 - 0.020161 0.059277 - 0.008943 0.014144 b - 0.020161 1.059352 - 0.008543 - 0.024738 0.009826 c 0.059277 - 0.008543 1.010670 - 0.001486 - 0.000271 d - 0.008943 - 0.024738 - 0.001486 0.921297 - 0.013692 e 0.014144 0.009826 - 0.000271 - 0.013692 0.977795 Minimum number of periods This method also supports an optional min_periods keyword that specifies the required minimum number of non-NA observations for each column pair in order to have a valid result: >>> np . random . seed ( 42 ) >>> df = pd . DataFrame ( np . random . randn ( 20 , 3 ), ... columns = [ 'a' , 'b' , 'c' ]) >>> df . loc [ df . index [: 5 ], 'a' ] = np . nan >>> df . loc [ df . index [ 5 : 10 ], 'b' ] = np . nan >>> df . cov ( min_periods = 12 ) a b c a 0.316741 NaN - 0.150812 b NaN 1.248003 0.191417 c - 0.150812 0.191417 0.895202 method corrwith ( other , axis=0 , drop=False , method='pearson' , numeric_only=False ) </> Compute pairwise correlation. Pairwise correlation is computed between rows or columns of DataFrame with rows or columns of Series or DataFrame. DataFrames are first aligned along both axes before computing the correlations. Parameters other (DataFrame, Series) \u2014 Object with which to compute correlations. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to use. 0 or 'index' to compute row-wise, 1 or 'columns' forcolumn-wise. drop (bool, default False) \u2014 Drop missing indices from result. method ({'pearson', 'kendall', 'spearman'} or callable) \u2014 Method of correlation: pearson : standard correlation coefficient kendall : Kendall Tau correlation coefficient spearman : Spearman rank correlation callable: callable with input two 1d ndarrays and returning a float. numeric_only (bool, default False) \u2014 Include only float , int or boolean data. .. versionadded:: 1.5.0 .. versionchanged:: 2.0.0 The default value of numeric_only is now False . Returns (Series) Pairwise correlations. See Also DataFrame.corr : Compute pairwise correlation of columns. Examples >>> index = [ \"a\" , \"b\" , \"c\" , \"d\" , \"e\" ] >>> columns = [ \"one\" , \"two\" , \"three\" , \"four\" ] >>> df1 = pd . DataFrame ( np . arange ( 20 ) . reshape ( 5 , 4 ), index = index , columns = columns ) >>> df2 = pd . DataFrame ( np . arange ( 16 ) . reshape ( 4 , 4 ), index = index [: 4 ], columns = columns ) >>> df1 . corrwith ( df2 ) one 1.0 two 1.0 three 1.0 four 1.0 dtype : float64 >>> df2 . corrwith ( df1 , axis = 1 ) a 1.0 b 1.0 c 1.0 d 1.0 e NaN dtype : float64 method count ( axis=0 , numeric_only=False ) </> Count non-NA cells for each column or row. The values None , NaN , NaT , pandas.NA are considered NA. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 If 0 or 'index' counts are generated for each column.If 1 or 'columns' counts are generated for each row. numeric_only (bool, default False) \u2014 Include only float , int or boolean data. Returns (Series) For each column/row the number of non-NA/null entries. See Also Series.count: Number of non-NA elements in a Series.DataFrame.value_counts: Count unique combinations of columns. DataFrame.shape: Number of DataFrame rows and columns (including NA elements). DataFrame.isna: Boolean same-sized DataFrame showing places of NA elements. Examples Constructing DataFrame from a dictionary: >>> df = pd . DataFrame ({ \"Person\" : ... [ \"John\" , \"Myla\" , \"Lewis\" , \"John\" , \"Myla\" ], ... \"Age\" : [ 24. , np . nan , 21. , 33 , 26 ], ... \"Single\" : [ False , True , True , True , False ]}) >>> df Person Age Single 0 John 24.0 False 1 Myla NaN True 2 Lewis 21.0 True 3 John 33.0 True 4 Myla 26.0 False Notice the uncounted NA values: >>> df . count () Person 5 Age 4 Single 5 dtype : int64 Counts for each row : >>> df . count ( axis = 'columns' ) 0 3 1 2 2 3 3 3 4 3 dtype : int64 method any ( axis=0 , bool_only=False , skipna=True , **kwargs ) </> Return whether any element is True, potentially over an axis. Returns False unless there is at least one element within a series or along a Dataframe axis that is True or equivalent (e.g. non-zero or non-empty). Parameters axis ({0 or 'index', 1 or 'columns', None}, default 0) \u2014 Indicate which axis or axes should be reduced. For Series this parameteris unused and defaults to 0. 0 / 'index' : reduce the index, return a Series whose index is the original column labels. 1 / 'columns' : reduce the columns, return a Series whose index is the original index. None : reduce all axes, return a scalar. bool_only (bool, default False) \u2014 Include only boolean columns. Not implemented for Series. skipna (bool, default True) \u2014 Exclude NA/null values. If the entire row/column is NA and skipna isTrue, then the result will be False, as for an empty row/column. If skipna is False, then NA are treated as True, because these are not equal to zero. **kwargs (any, default None) \u2014 Additional keywords have no effect but might be accepted forcompatibility with NumPy. Returns (Series or DataFrame) If level is specified, then, DataFrame is returned; otherwise, Seriesis returned. See Also numpy.any : Numpy version of this method.Series.any : Return whether any element is True. Series.all : Return whether all elements are True. DataFrame.any : Return whether any element is True over requested axis. DataFrame.all : Return whether all elements are True over requested axis. Examples Series For Series input, the output is a scalar indicating whether any element is True. >>> pd . Series ([ False , False ]) . any () False >>> pd . Series ([ True , False ]) . any () True >>> pd . Series ([], dtype = \"float64\" ) . any () False >>> pd . Series ([ np . nan ]) . any () False >>> pd . Series ([ np . nan ]) . any ( skipna = False ) True DataFrame Whether each column contains at least one True element (the default). >>> df = pd . DataFrame ({ \"A\" : [ 1 , 2 ], \"B\" : [ 0 , 2 ], \"C\" : [ 0 , 0 ]}) >>> df A B C 0 1 0 0 1 2 2 0 >>> df . any () A True B True C False dtype : bool Aggregating over the columns. >>> df = pd . DataFrame ({ \"A\" : [ True , False ], \"B\" : [ 1 , 2 ]}) >>> df A B 0 True 1 1 False 2 >>> df . any ( axis = 'columns' ) 0 True 1 True dtype : bool >>> df = pd . DataFrame ({ \"A\" : [ True , False ], \"B\" : [ 1 , 0 ]}) >>> df A B 0 True 1 1 False 0 >>> df . any ( axis = 'columns' ) 0 True 1 False dtype : bool Aggregating over the entire DataFrame with axis=None . >>> df . any ( axis = None ) True any for an empty DataFrame is an empty Series. >>> pd . DataFrame ([]) . any () Series ([], dtype : bool ) method all ( axis=0 , bool_only=False , skipna=True , **kwargs ) </> Return whether all elements are True, potentially over an axis. Returns True unless there at least one element within a series or along a Dataframe axis that is False or equivalent (e.g. zero or empty). Parameters axis ({0 or 'index', 1 or 'columns', None}, default 0) \u2014 Indicate which axis or axes should be reduced. For Series this parameteris unused and defaults to 0. 0 / 'index' : reduce the index, return a Series whose index is the original column labels. 1 / 'columns' : reduce the columns, return a Series whose index is the original index. None : reduce all axes, return a scalar. bool_only (bool, default False) \u2014 Include only boolean columns. Not implemented for Series. skipna (bool, default True) \u2014 Exclude NA/null values. If the entire row/column is NA and skipna isTrue, then the result will be True, as for an empty row/column. If skipna is False, then NA are treated as True, because these are not equal to zero. **kwargs (any, default None) \u2014 Additional keywords have no effect but might be accepted forcompatibility with NumPy. Returns (Series or DataFrame) If level is specified, then, DataFrame is returned; otherwise, Seriesis returned. See Also Series.all : Return True if all elements are True.DataFrame.any : Return True if one (or more) elements are True. Examples Series >>> pd . Series ([ True , True ]) . all () True >>> pd . Series ([ True , False ]) . all () False >>> pd . Series ([], dtype = \"float64\" ) . all () True >>> pd . Series ([ np . nan ]) . all () True >>> pd . Series ([ np . nan ]) . all ( skipna = False ) True DataFrames Create a dataframe from a dictionary. >>> df = pd . DataFrame ({ 'col1' : [ True , True ], 'col2' : [ True , False ]}) >>> df col1 col2 0 True True 1 True False Default behaviour checks if values in each column all return True. >>> df . all () col1 True col2 False dtype : bool Specify axis='columns' to check if values in each row all return True. >>> df . all ( axis = 'columns' ) 0 True 1 False dtype : bool Or axis=None for whether every value is True. >>> df . all ( axis = None ) False method min ( axis=0 , skipna=True , numeric_only=False , **kwargs ) </> Return the minimum of the values over the requested axis. If you want the index of the minimum, use idxmin . This is the equivalent of the numpy.ndarray method argmin . Parameters axis ({index (0), columns (1)}) \u2014 Axis for the function to be applied on.For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. .. versionadded:: 2.0.0 skipna (bool, default True) \u2014 Exclude NA/null values when computing the result. numeric_only (bool, default False) \u2014 Include only float, int, boolean columns. Not implemented for Series. **kwargs \u2014 Additional keyword arguments to be passed to the function. See Also Series.sum : Return the sum.Series.min : Return the minimum. Series.max : Return the maximum. Series.idxmin : Return the index of the minimum. Series.idxmax : Return the index of the maximum. DataFrame.sum : Return the sum over the requested axis. DataFrame.min : Return the minimum over the requested axis. DataFrame.max : Return the maximum over the requested axis. DataFrame.idxmin : Return the index of the minimum over the requested axis. DataFrame.idxmax : Return the index of the maximum over the requested axis. Examples >>> idx = pd . MultiIndex . from_arrays ([ ... [ 'warm' , 'warm' , 'cold' , 'cold' ], ... [ 'dog' , 'falcon' , 'fish' , 'spider' ]], ... names = [ 'blooded' , 'animal' ]) >>> s = pd . Series ([ 4 , 2 , 0 , 8 ], name = 'legs' , index = idx ) >>> s blooded animal warm dog 4 falcon 2 cold fish 0 spider 8 Name : legs , dtype : int64 >>> s . min () 0 method max ( axis=0 , skipna=True , numeric_only=False , **kwargs ) </> Return the maximum of the values over the requested axis. If you want the index of the maximum, use idxmax . This is the equivalent of the numpy.ndarray method argmax . Parameters axis ({index (0), columns (1)}) \u2014 Axis for the function to be applied on.For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. .. versionadded:: 2.0.0 skipna (bool, default True) \u2014 Exclude NA/null values when computing the result. numeric_only (bool, default False) \u2014 Include only float, int, boolean columns. Not implemented for Series. **kwargs \u2014 Additional keyword arguments to be passed to the function. See Also Series.sum : Return the sum.Series.min : Return the minimum. Series.max : Return the maximum. Series.idxmin : Return the index of the minimum. Series.idxmax : Return the index of the maximum. DataFrame.sum : Return the sum over the requested axis. DataFrame.min : Return the minimum over the requested axis. DataFrame.max : Return the maximum over the requested axis. DataFrame.idxmin : Return the index of the minimum over the requested axis. DataFrame.idxmax : Return the index of the maximum over the requested axis. Examples >>> idx = pd . MultiIndex . from_arrays ([ ... [ 'warm' , 'warm' , 'cold' , 'cold' ], ... [ 'dog' , 'falcon' , 'fish' , 'spider' ]], ... names = [ 'blooded' , 'animal' ]) >>> s = pd . Series ([ 4 , 2 , 0 , 8 ], name = 'legs' , index = idx ) >>> s blooded animal warm dog 4 falcon 2 cold fish 0 spider 8 Name : legs , dtype : int64 >>> s . max () 8 method sum ( axis=0 , skipna=True , numeric_only=False , min_count=0 , **kwargs ) </> Return the sum of the values over the requested axis. This is equivalent to the method numpy.sum . Parameters axis ({index (0), columns (1)}) \u2014 Axis for the function to be applied on.For Series this parameter is unused and defaults to 0. .. warning:: The behavior of DataFrame.sum with ``axis=None`` is deprecated, in a future version this will reduce over both axes and return a scalar To retain the old behavior, pass axis=0 (or do not pass axis). .. versionadded:: 2.0.0 skipna (bool, default True) \u2014 Exclude NA/null values when computing the result. numeric_only (bool, default False) \u2014 Include only float, int, boolean columns. Not implemented for Series. min_count (int, default 0) \u2014 The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. **kwargs \u2014 Additional keyword arguments to be passed to the function. See Also Series.sum : Return the sum.Series.min : Return the minimum. Series.max : Return the maximum. Series.idxmin : Return the index of the minimum. Series.idxmax : Return the index of the maximum. DataFrame.sum : Return the sum over the requested axis. DataFrame.min : Return the minimum over the requested axis. DataFrame.max : Return the maximum over the requested axis. DataFrame.idxmin : Return the index of the minimum over the requested axis. DataFrame.idxmax : Return the index of the maximum over the requested axis. Examples >>> idx = pd . MultiIndex . from_arrays ([ ... [ 'warm' , 'warm' , 'cold' , 'cold' ], ... [ 'dog' , 'falcon' , 'fish' , 'spider' ]], ... names = [ 'blooded' , 'animal' ]) >>> s = pd . Series ([ 4 , 2 , 0 , 8 ], name = 'legs' , index = idx ) >>> s blooded animal warm dog 4 falcon 2 cold fish 0 spider 8 Name : legs , dtype : int64 >>> s . sum () 14 By default, the sum of an empty or all-NA Series is 0 . >>> pd . Series ([], dtype = \"float64\" ) . sum () # min_count=0 is the default 0.0 This can be controlled with the min_count parameter. For example, if you'd like the sum of an empty series to be NaN, pass min_count=1 . >>> pd . Series ([], dtype = \"float64\" ) . sum ( min_count = 1 ) nan Thanks to the skipna parameter, min_count handles all-NA and empty series identically. >>> pd . Series ([ np . nan ]) . sum () 0.0 >>> pd . Series ([ np . nan ]) . sum ( min_count = 1 ) nan method prod ( axis=0 , skipna=True , numeric_only=False , min_count=0 , **kwargs ) </> Return the product of the values over the requested axis. Parameters axis ({index (0), columns (1)}) \u2014 Axis for the function to be applied on.For Series this parameter is unused and defaults to 0. .. warning:: The behavior of DataFrame.prod with ``axis=None`` is deprecated, in a future version this will reduce over both axes and return a scalar To retain the old behavior, pass axis=0 (or do not pass axis). .. versionadded:: 2.0.0 skipna (bool, default True) \u2014 Exclude NA/null values when computing the result. numeric_only (bool, default False) \u2014 Include only float, int, boolean columns. Not implemented for Series. min_count (int, default 0) \u2014 The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. **kwargs \u2014 Additional keyword arguments to be passed to the function. See Also Series.sum : Return the sum.Series.min : Return the minimum. Series.max : Return the maximum. Series.idxmin : Return the index of the minimum. Series.idxmax : Return the index of the maximum. DataFrame.sum : Return the sum over the requested axis. DataFrame.min : Return the minimum over the requested axis. DataFrame.max : Return the maximum over the requested axis. DataFrame.idxmin : Return the index of the minimum over the requested axis. DataFrame.idxmax : Return the index of the maximum over the requested axis. Examples By default, the product of an empty or all-NA Series is 1 >>> pd . Series ([], dtype = \"float64\" ) . prod () 1.0 This can be controlled with the min_count parameter >>> pd . Series ([], dtype = \"float64\" ) . prod ( min_count = 1 ) nan Thanks to the skipna parameter, min_count handles all-NA and empty series identically. >>> pd . Series ([ np . nan ]) . prod () 1.0 >>> pd . Series ([ np . nan ]) . prod ( min_count = 1 ) nan method mean ( axis=0 , skipna=True , numeric_only=False , **kwargs ) </> Return the mean of the values over the requested axis. Parameters axis ({index (0), columns (1)}) \u2014 Axis for the function to be applied on.For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. .. versionadded:: 2.0.0 skipna (bool, default True) \u2014 Exclude NA/null values when computing the result. numeric_only (bool, default False) \u2014 Include only float, int, boolean columns. Not implemented for Series. **kwargs \u2014 Additional keyword arguments to be passed to the function. Returns (Series or scalar) s \u00b6 ) ) 0 e ) f b 2 3 ) 5 5 4 1 ) 5 5 4 d . , ) ) 5 4 method median ( axis=0 , skipna=True , numeric_only=False , **kwargs ) </> Return the median of the values over the requested axis. Parameters axis ({index (0), columns (1)}) \u2014 Axis for the function to be applied on.For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. .. versionadded:: 2.0.0 skipna (bool, default True) \u2014 Exclude NA/null values when computing the result. numeric_only (bool, default False) \u2014 Include only float, int, boolean columns. Not implemented for Series. **kwargs \u2014 Additional keyword arguments to be passed to the function. Returns (Series or scalar) s \u00b6 ) ) 0 e ) f b 2 3 ) 5 5 4 1 ) 5 5 4 ` . , ) ) 5 4 method sem ( axis=0 , skipna=True , ddof=1 , numeric_only=False , **kwargs ) </> Return unbiased standard error of the mean over requested axis. Normalized by N-1 by default. This can be changed using the ddof argument Parameters axis ({index (0), columns (1)}) \u2014 For Series this parameter is unused and defaults to 0. .. warning:: The behavior of DataFrame.sem with ``axis=None`` is deprecated, in a future version this will reduce over both axes and return a scalar To retain the old behavior, pass axis=0 (or do not pass axis). skipna (bool, default True) \u2014 Exclude NA/null values. If an entire row/column is NA, the resultwill be NA. ddof (int, default 1) \u2014 Delta Degrees of Freedom. The divisor used in calculations is N - ddof,where N represents the number of elements. numeric_only (bool, default False) \u2014 Include only float, int, boolean columns. Not implemented for Series. Returns (Series or DataFrame (if level specified)) s \u00b6 ) ) 5 e ) f b 2 3 ) 5 5 4 1 ) 5 5 4 ` . , ) ) 5 4 method var ( axis=0 , skipna=True , ddof=1 , numeric_only=False , **kwargs ) </> Return unbiased variance over requested axis. Normalized by N-1 by default. This can be changed using the ddof argument. Parameters axis ({index (0), columns (1)}) \u2014 For Series this parameter is unused and defaults to 0. .. warning:: The behavior of DataFrame.var with ``axis=None`` is deprecated, in a future version this will reduce over both axes and return a scalar To retain the old behavior, pass axis=0 (or do not pass axis). skipna (bool, default True) \u2014 Exclude NA/null values. If an entire row/column is NA, the resultwill be NA. ddof (int, default 1) \u2014 Delta Degrees of Freedom. The divisor used in calculations is N - ddof,where N represents the number of elements. numeric_only (bool, default False) \u2014 Include only float, int, boolean columns. Not implemented for Series. Examples >>> df = pd . DataFrame ({ 'person_id' : [ 0 , 1 , 2 , 3 ], ... 'age' : [ 21 , 25 , 62 , 43 ], ... 'height' : [ 1.61 , 1.87 , 1.49 , 2.01 ]} ... ) . set_index ( 'person_id' ) >>> df age height person_id 0 21 1.61 1 25 1.87 2 62 1.49 3 43 2.01 >>> df . var () age 352.916667 height 0.056367 dtype : float64 Alternatively, ddof=0 can be set to normalize by N instead of N-1: >>> df . var ( ddof = 0 ) age 264.687500 height 0.042275 dtype : float64 method std ( axis=0 , skipna=True , ddof=1 , numeric_only=False , **kwargs ) </> Return sample standard deviation over requested axis. Normalized by N-1 by default. This can be changed using the ddof argument. Parameters axis ({index (0), columns (1)}) \u2014 For Series this parameter is unused and defaults to 0. .. warning:: The behavior of DataFrame.std with ``axis=None`` is deprecated, in a future version this will reduce over both axes and return a scalar To retain the old behavior, pass axis=0 (or do not pass axis). skipna (bool, default True) \u2014 Exclude NA/null values. If an entire row/column is NA, the resultwill be NA. ddof (int, default 1) \u2014 Delta Degrees of Freedom. The divisor used in calculations is N - ddof,where N represents the number of elements. numeric_only (bool, default False) \u2014 Include only float, int, boolean columns. Not implemented for Series. Notes To have the same behaviour as numpy.std , use ddof=0 (instead of the default ddof=1 ) Examples >>> df = pd . DataFrame ({ 'person_id' : [ 0 , 1 , 2 , 3 ], ... 'age' : [ 21 , 25 , 62 , 43 ], ... 'height' : [ 1.61 , 1.87 , 1.49 , 2.01 ]} ... ) . set_index ( 'person_id' ) >>> df age height person_id 0 21 1.61 1 25 1.87 2 62 1.49 3 43 2.01 The standard deviation of the columns can be found as follows: >>> df . std () age 18.786076 height 0.237417 dtype : float64 Alternatively, ddof=0 can be set to normalize by N instead of N-1: >>> df . std ( ddof = 0 ) age 16.269219 height 0.205609 dtype : float64 method skew ( axis=0 , skipna=True , numeric_only=False , **kwargs ) </> Return unbiased skew over requested axis. Normalized by N-1. Parameters axis ({index (0), columns (1)}) \u2014 Axis for the function to be applied on.For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. .. versionadded:: 2.0.0 skipna (bool, default True) \u2014 Exclude NA/null values when computing the result. numeric_only (bool, default False) \u2014 Include only float, int, boolean columns. Not implemented for Series. **kwargs \u2014 Additional keyword arguments to be passed to the function. Returns (Series or scalar) s \u00b6 ) ) 0 e , ) f c 1 3 5 ) 0 0 0 4 1 ) 1 1 0 4 d . , ) ) 0 4 method kurt ( axis=0 , skipna=True , numeric_only=False , **kwargs ) </> Return unbiased kurtosis over requested axis. Kurtosis obtained using Fisher's definition of kurtosis (kurtosis of normal == 0.0). Normalized by N-1. Parameters axis ({index (0), columns (1)}) \u2014 Axis for the function to be applied on.For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. .. versionadded:: 2.0.0 skipna (bool, default True) \u2014 Exclude NA/null values when computing the result. numeric_only (bool, default False) \u2014 Include only float, int, boolean columns. Not implemented for Series. **kwargs \u2014 Additional keyword arguments to be passed to the function. Returns (Series or scalar) s \u00b6 ) s 1 2 2 3 4 ) 5 e , ) f b 3 4 4 4 ) 5 0 4 e ) 3 1 , ) ) 0 0 4 method cummin ( axis=None , skipna=True , *args , **kwargs ) </> Return cumulative minimum over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative minimum. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The index or the name of the axis. 0 is equivalent to None or 'index'.For Series this parameter is unused and defaults to 0. skipna (bool, default True) \u2014 Exclude NA/null values. If an entire row/column is NA, the resultwill be NA. Returns (Series or DataFrame) Return cumulative minimum of Series or DataFrame. See Also core.window.expanding.Expanding.min : Similar functionality but ignores NaN values. DataFrame.min : Return the minimum over DataFrame axis. DataFrame.cummax : Return cumulative maximum over DataFrame axis. DataFrame.cummin : Return cumulative minimum over DataFrame axis. DataFrame.cumsum : Return cumulative sum over DataFrame axis. DataFrame.cumprod : Return cumulative product over DataFrame axis. Examples Series >>> s = pd . Series ([ 2 , np . nan , 5 , - 1 , 0 ]) >>> s 0 2.0 1 NaN 2 5.0 3 - 1.0 4 0.0 dtype : float64 By default, NA values are ignored. >>> s . cummin () 0 2.0 1 NaN 2 2.0 3 - 1.0 4 - 1.0 dtype : float64 To include NA values in the operation, use skipna=False >>> s . cummin ( skipna = False ) 0 2.0 1 NaN 2 NaN 3 NaN 4 NaN dtype : float64 DataFrame >>> df = pd . DataFrame ([[ 2.0 , 1.0 ], ... [ 3.0 , np . nan ], ... [ 1.0 , 0.0 ]], ... columns = list ( 'AB' )) >>> df A B 0 2.0 1.0 1 3.0 NaN 2 1.0 0.0 By default, iterates over rows and finds the minimum in each column. This is equivalent to axis=None or axis='index' . >>> df . cummin () A B 0 2.0 1.0 1 2.0 NaN 2 1.0 0.0 To iterate over columns and find the minimum in each row, use axis=1 >>> df . cummin ( axis = 1 ) A B 0 2.0 1.0 1 3.0 NaN 2 1.0 0.0 method cummax ( axis=None , skipna=True , *args , **kwargs ) </> Return cumulative maximum over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative maximum. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The index or the name of the axis. 0 is equivalent to None or 'index'.For Series this parameter is unused and defaults to 0. skipna (bool, default True) \u2014 Exclude NA/null values. If an entire row/column is NA, the resultwill be NA. Returns (Series or DataFrame) Return cumulative maximum of Series or DataFrame. See Also core.window.expanding.Expanding.max : Similar functionality but ignores NaN values. DataFrame.max : Return the maximum over DataFrame axis. DataFrame.cummax : Return cumulative maximum over DataFrame axis. DataFrame.cummin : Return cumulative minimum over DataFrame axis. DataFrame.cumsum : Return cumulative sum over DataFrame axis. DataFrame.cumprod : Return cumulative product over DataFrame axis. Examples Series >>> s = pd . Series ([ 2 , np . nan , 5 , - 1 , 0 ]) >>> s 0 2.0 1 NaN 2 5.0 3 - 1.0 4 0.0 dtype : float64 By default, NA values are ignored. >>> s . cummax () 0 2.0 1 NaN 2 5.0 3 5.0 4 5.0 dtype : float64 To include NA values in the operation, use skipna=False >>> s . cummax ( skipna = False ) 0 2.0 1 NaN 2 NaN 3 NaN 4 NaN dtype : float64 DataFrame >>> df = pd . DataFrame ([[ 2.0 , 1.0 ], ... [ 3.0 , np . nan ], ... [ 1.0 , 0.0 ]], ... columns = list ( 'AB' )) >>> df A B 0 2.0 1.0 1 3.0 NaN 2 1.0 0.0 By default, iterates over rows and finds the maximum in each column. This is equivalent to axis=None or axis='index' . >>> df . cummax () A B 0 2.0 1.0 1 3.0 NaN 2 3.0 1.0 To iterate over columns and find the maximum in each row, use axis=1 >>> df . cummax ( axis = 1 ) A B 0 2.0 2.0 1 3.0 NaN 2 1.0 1.0 method cumsum ( axis=None , skipna=True , *args , **kwargs ) </> Return cumulative sum over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative sum. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The index or the name of the axis. 0 is equivalent to None or 'index'.For Series this parameter is unused and defaults to 0. skipna (bool, default True) \u2014 Exclude NA/null values. If an entire row/column is NA, the resultwill be NA. Returns (Series or DataFrame) Return cumulative sum of Series or DataFrame. See Also core.window.expanding.Expanding.sum : Similar functionality but ignores NaN values. DataFrame.sum : Return the sum over DataFrame axis. DataFrame.cummax : Return cumulative maximum over DataFrame axis. DataFrame.cummin : Return cumulative minimum over DataFrame axis. DataFrame.cumsum : Return cumulative sum over DataFrame axis. DataFrame.cumprod : Return cumulative product over DataFrame axis. Examples Series >>> s = pd . Series ([ 2 , np . nan , 5 , - 1 , 0 ]) >>> s 0 2.0 1 NaN 2 5.0 3 - 1.0 4 0.0 dtype : float64 By default, NA values are ignored. >>> s . cumsum () 0 2.0 1 NaN 2 7.0 3 6.0 4 6.0 dtype : float64 To include NA values in the operation, use skipna=False >>> s . cumsum ( skipna = False ) 0 2.0 1 NaN 2 NaN 3 NaN 4 NaN dtype : float64 DataFrame >>> df = pd . DataFrame ([[ 2.0 , 1.0 ], ... [ 3.0 , np . nan ], ... [ 1.0 , 0.0 ]], ... columns = list ( 'AB' )) >>> df A B 0 2.0 1.0 1 3.0 NaN 2 1.0 0.0 By default, iterates over rows and finds the sum in each column. This is equivalent to axis=None or axis='index' . >>> df . cumsum () A B 0 2.0 1.0 1 5.0 NaN 2 6.0 1.0 To iterate over columns and find the sum in each row, use axis=1 >>> df . cumsum ( axis = 1 ) A B 0 2.0 3.0 1 3.0 NaN 2 1.0 1.0 method cumprod ( axis=None , skipna=True , *args , **kwargs ) </> Return cumulative product over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative product. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The index or the name of the axis. 0 is equivalent to None or 'index'.For Series this parameter is unused and defaults to 0. skipna (bool, default True) \u2014 Exclude NA/null values. If an entire row/column is NA, the resultwill be NA. Returns (Series or DataFrame) Return cumulative product of Series or DataFrame. See Also core.window.expanding.Expanding.prod : Similar functionality but ignores NaN values. DataFrame.prod : Return the product over DataFrame axis. DataFrame.cummax : Return cumulative maximum over DataFrame axis. DataFrame.cummin : Return cumulative minimum over DataFrame axis. DataFrame.cumsum : Return cumulative sum over DataFrame axis. DataFrame.cumprod : Return cumulative product over DataFrame axis. Examples Series >>> s = pd . Series ([ 2 , np . nan , 5 , - 1 , 0 ]) >>> s 0 2.0 1 NaN 2 5.0 3 - 1.0 4 0.0 dtype : float64 By default, NA values are ignored. >>> s . cumprod () 0 2.0 1 NaN 2 10.0 3 - 10.0 4 - 0.0 dtype : float64 To include NA values in the operation, use skipna=False >>> s . cumprod ( skipna = False ) 0 2.0 1 NaN 2 NaN 3 NaN 4 NaN dtype : float64 DataFrame >>> df = pd . DataFrame ([[ 2.0 , 1.0 ], ... [ 3.0 , np . nan ], ... [ 1.0 , 0.0 ]], ... columns = list ( 'AB' )) >>> df A B 0 2.0 1.0 1 3.0 NaN 2 1.0 0.0 By default, iterates over rows and finds the product in each column. This is equivalent to axis=None or axis='index' . >>> df . cumprod () A B 0 2.0 1.0 1 6.0 NaN 2 6.0 0.0 To iterate over columns and find the product in each row, use axis=1 >>> df . cumprod ( axis = 1 ) A B 0 2.0 2.0 1 3.0 NaN 2 1.0 0.0 method nunique ( axis=0 , dropna=True ) </> Count number of distinct elements in specified axis. Return Series with number of distinct elements. Can ignore NaN values. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to use. 0 or 'index' for row-wise, 1 or 'columns' forcolumn-wise. dropna (bool, default True) \u2014 Don't include NaN in the counts. See Also Series.nunique: Method nunique for Series.DataFrame.count: Count non-NA cells for each column or row. Examples >>> df = pd . DataFrame ({ 'A' : [ 4 , 5 , 6 ], 'B' : [ 4 , 1 , 1 ]}) >>> df . nunique () A 3 B 2 dtype : int64 >>> df . nunique ( axis = 1 ) 0 1 1 2 2 2 dtype : int64 method idxmin ( axis=0 , skipna=True , numeric_only=False ) </> Return index of first occurrence of minimum over requested axis. NA/null values are excluded. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise. skipna (bool, default True) \u2014 Exclude NA/null values. If an entire row/column is NA, the resultwill be NA. numeric_only (bool, default False) \u2014 Include only float , int or boolean data. .. versionadded:: 1.5.0 Returns (Series) Indexes of minima along the specified axis. Raises ValueError \u2014 If the row/column is empty See Also Series.idxmin : Return index of the minimum element. Notes This method is the DataFrame version of ndarray.argmin . Examples Consider a dataset containing food consumption in Argentina. >>> df = pd . DataFrame ({ 'consumption' : [ 10.51 , 103.11 , 55.48 ], ... 'co2_emissions' : [ 37.2 , 19.66 , 1712 ]}, ... index = [ 'Pork' , 'Wheat Products' , 'Beef' ]) >>> df consumption co2_emissions Pork 10.51 37.20 Wheat Products 103.11 19.66 Beef 55.48 1712.00 By default, it returns the index for the minimum value in each column. >>> df . idxmin () consumption Pork co2_emissions Wheat Products dtype : object To return the index for the minimum value in each row, use axis=\"columns\" . >>> df . idxmin ( axis = \"columns\" ) Pork consumption Wheat Products co2_emissions Beef consumption dtype : object method idxmax ( axis=0 , skipna=True , numeric_only=False ) </> Return index of first occurrence of maximum over requested axis. NA/null values are excluded. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise. skipna (bool, default True) \u2014 Exclude NA/null values. If an entire row/column is NA, the resultwill be NA. numeric_only (bool, default False) \u2014 Include only float , int or boolean data. .. versionadded:: 1.5.0 Returns (Series) Indexes of maxima along the specified axis. Raises ValueError \u2014 If the row/column is empty See Also Series.idxmax : Return index of the maximum element. Notes This method is the DataFrame version of ndarray.argmax . Examples Consider a dataset containing food consumption in Argentina. >>> df = pd . DataFrame ({ 'consumption' : [ 10.51 , 103.11 , 55.48 ], ... 'co2_emissions' : [ 37.2 , 19.66 , 1712 ]}, ... index = [ 'Pork' , 'Wheat Products' , 'Beef' ]) >>> df consumption co2_emissions Pork 10.51 37.20 Wheat Products 103.11 19.66 Beef 55.48 1712.00 By default, it returns the index for the maximum value in each column. >>> df . idxmax () consumption Wheat Products co2_emissions Beef dtype : object To return the index for the maximum value in each row, use axis=\"columns\" . >>> df . idxmax ( axis = \"columns\" ) Pork co2_emissions Wheat Products consumption Beef co2_emissions dtype : object method mode ( axis=0 , numeric_only=False , dropna=True ) </> Get the mode(s) of each element along the selected axis. The mode of a set of values is the value that appears most often. It can be multiple values. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to iterate over while searching for the mode: 0 or 'index' : get mode of each column 1 or 'columns' : get mode of each row. numeric_only (bool, default False) \u2014 If True, only apply to numeric columns. dropna (bool, default True) \u2014 Don't consider counts of NaN/NaT. Returns (DataFrame) The modes of each column or row. See Also Series.mode : Return the highest frequency value in a Series.Series.value_counts : Return the counts of values in a Series. Examples >>> df = pd . DataFrame ([( 'bird' , 2 , 2 ), ... ( 'mammal' , 4 , np . nan ), ... ( 'arthropod' , 8 , 0 ), ... ( 'bird' , 2 , np . nan )], ... index = ( 'falcon' , 'horse' , 'spider' , 'ostrich' ), ... columns = ( 'species' , 'legs' , 'wings' )) >>> df species legs wings falcon bird 2 2.0 horse mammal 4 NaN spider arthropod 8 0.0 ostrich bird 2 NaN By default, missing values are not considered, and the mode of wings are both 0 and 2. Because the resulting DataFrame has two rows, the second row of species and legs contains NaN . >>> df . mode () species legs wings 0 bird 2.0 0.0 1 NaN NaN 2.0 Setting dropna=False NaN values are considered and they can be the mode (like for wings). >>> df . mode ( dropna = False ) species legs wings 0 bird 2 NaN Setting numeric_only=True , only the mode of numeric columns is computed, and columns of other types are ignored. >>> df . mode ( numeric_only = True ) legs wings 0 2.0 0.0 1 NaN 2.0 To compute the mode over columns and not rows, use the axis parameter: >>> df . mode ( axis = 'columns' , numeric_only = True ) 0 1 falcon 2.0 NaN horse 4.0 NaN spider 0.0 8.0 ostrich 2.0 NaN method quantile ( q=0.5 , axis=0 , numeric_only=False , interpolation='linear' , method='single' ) </> Return values at the given quantile over requested axis. Parameters q (float or array-like, default 0.5 (50% quantile)) \u2014 Value between 0 <= q <= 1, the quantile(s) to compute. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Equals 0 or 'index' for row-wise, 1 or 'columns' for column-wise. numeric_only (bool, default False) \u2014 Include only float , int or boolean data. .. versionchanged:: 2.0.0 The default value of numeric_only is now False . interpolation ({'linear', 'lower', 'higher', 'midpoint', 'nearest'}) \u2014 This optional parameter specifies the interpolation method to use,when the desired quantile lies between two data points i and j : linear: i + (j - i) * fraction , where fraction is the fractional part of the index surrounded by i and j . lower: i . higher: j . nearest: i or j whichever is nearest. midpoint: ( i + j ) / 2. method ({'single', 'table'}, default 'single') \u2014 Whether to compute quantiles per-column ('single') or over all columns('table'). When 'table', the only allowed interpolation methods are 'nearest', 'lower', and 'higher'. Returns (Series or DataFrame) ee . e . See Also core.window.rolling.Rolling.quantile: Rolling quantile.numpy.percentile: Numpy function to compute the percentile. Examples >>> df = pd . DataFrame ( np . array ([[ 1 , 1 ], [ 2 , 10 ], [ 3 , 100 ], [ 4 , 100 ]]), ... columns = [ 'a' , 'b' ]) >>> df . quantile ( .1 ) a 1.3 b 3.7 Name : 0.1 , dtype : float64 >>> df . quantile ([ .1 , .5 ]) a b 0.1 1.3 3.7 0.5 2.5 55.0 Specifying method='table' will compute the quantile over all columns. >>> df . quantile ( .1 , method = \"table\" , interpolation = \"nearest\" ) a 1 b 1 Name : 0.1 , dtype : int64 >>> df . quantile ([ .1 , .5 ], method = \"table\" , interpolation = \"nearest\" ) a b 0.1 1 1 0.5 3 100 Specifying numeric_only=False will also compute the quantile of datetime and timedelta data. >>> df = pd . DataFrame ({ 'A' : [ 1 , 2 ], ... 'B' : [ pd . Timestamp ( '2010' ), ... pd . Timestamp ( '2011' )], ... 'C' : [ pd . Timedelta ( '1 days' ), ... pd . Timedelta ( '2 days' )]}) >>> df . quantile ( 0.5 , numeric_only = False ) A 1.5 B 2010 - 07 - 02 12 : 00 : 00 C 1 days 12 : 00 : 00 Name : 0.5 , dtype : object method to_timestamp ( freq=None , how='start' , axis=0 , copy=None ) </> Cast to DatetimeIndex of timestamps, at beginning of period. Parameters freq (str, default frequency of PeriodIndex) \u2014 Desired frequency. how ({'s', 'e', 'start', 'end'}) \u2014 Convention for converting period to timestamp; start of periodvs. end. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to convert (the index by default). copy (bool, default True) \u2014 If False then underlying input data is not copied. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` Returns (DataFrame) The DataFrame has a DatetimeIndex. Examples >>> idx = pd . PeriodIndex ([ '2023' , '2024' ], freq = 'Y' ) >>> d = { 'col1' : [ 1 , 2 ], 'col2' : [ 3 , 4 ]} >>> df1 = pd . DataFrame ( data = d , index = idx ) >>> df1 col1 col2 2023 1 3 2024 2 4 The resulting timestamps will be at the beginning of the year in this case >>> df1 = df1 . to_timestamp () >>> df1 col1 col2 2023 - 01 - 01 1 3 2024 - 01 - 01 2 4 >>> df1 . index DatetimeIndex ([ '2023-01-01' , '2024-01-01' ], dtype = 'datetime64[ns]' , freq = None ) Using freq which is the offset that the Timestamps will have >>> df2 = pd . DataFrame ( data = d , index = idx ) >>> df2 = df2 . to_timestamp ( freq = 'M' ) >>> df2 col1 col2 2023 - 01 - 31 1 3 2024 - 01 - 31 2 4 >>> df2 . index DatetimeIndex ([ '2023-01-31' , '2024-01-31' ], dtype = 'datetime64[ns]' , freq = None ) method to_period ( freq=None , axis=0 , copy=None ) </> Convert DataFrame from DatetimeIndex to PeriodIndex. Convert DataFrame from DatetimeIndex to PeriodIndex with desired frequency (inferred from index if not passed). Parameters freq (str, default) \u2014 Frequency of the PeriodIndex. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to convert (the index by default). copy (bool, default True) \u2014 If False then underlying input data is not copied. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` Returns (DataFrame) The DataFrame has a PeriodIndex. Examples >>> idx = pd . to_datetime ( ... [ ... \"2001-03-31 00:00:00\" , ... \"2002-05-31 00:00:00\" , ... \"2003-08-31 00:00:00\" , ... ] ... ) >>> idx DatetimeIndex ([ '2001-03-31' , '2002-05-31' , '2003-08-31' ], dtype = 'datetime64[ns]' , freq = None ) >>> idx . to_period ( \"M\" ) PeriodIndex ([ '2001-03' , '2002-05' , '2003-08' ], dtype = 'period[M]' ) For the yearly frequency >>> idx . to_period ( \"Y\" ) PeriodIndex ([ '2001' , '2002' , '2003' ], dtype = 'period[Y-DEC]' ) method isin ( values ) </> Whether each element in the DataFrame is contained in values. Parameters values (iterable, Series, DataFrame or dict) \u2014 The result will only be true at a location if all thelabels match. If values is a Series, that's the index. If values is a dict, the keys must be the column names, which must match. If values is a DataFrame, then both the index and column labels must match. Returns (DataFrame) DataFrame of booleans showing whether each element in the DataFrameis contained in values. See Also DataFrame.eq: Equality test for DataFrame.Series.isin: Equivalent method on Series. Series.str.contains: Test if pattern or regex is contained within a string of a Series or Index. Examples >>> df = pd . DataFrame ({ 'num_legs' : [ 2 , 4 ], 'num_wings' : [ 2 , 0 ]}, ... index = [ 'falcon' , 'dog' ]) >>> df num_legs num_wings falcon 2 2 dog 4 0 When values is a list check whether every value in the DataFrame is present in the list (which animals have 0 or 2 legs or wings) >>> df . isin ([ 0 , 2 ]) num_legs num_wings falcon True True dog False True To check if values is not in the DataFrame, use the ~ operator: >>> ~ df . isin ([ 0 , 2 ]) num_legs num_wings falcon False False dog True False When values is a dict, we can pass values to check for each column separately: >>> df . isin ({ 'num_wings' : [ 0 , 3 ]}) num_legs num_wings falcon False False dog False True When values is a Series or DataFrame the index and column must match. Note that 'falcon' does not match based on the number of legs in other. >>> other = pd . DataFrame ({ 'num_legs' : [ 8 , 3 ], 'num_wings' : [ 0 , 2 ]}, ... index = [ 'spider' , 'falcon' ]) >>> df . isin ( other ) num_legs num_wings falcon False True dog False False classmethod create ( value ) </> Create a channel from a list. The second dimension is identified by tuple. if all elements are tuple, then a channel is created directly. Otherwise, elements are converted to tuples first and channels are created then. Examples >>> Channel . create ([ 1 , 2 , 3 ]) # 3 rows, 1 column >>> Channel . create ([( 1 , 2 , 3 )]) # 1 row, 3 columns Parameters value (Union) \u2014 The value to create a channel Returns (DataFrame) A channel (dataframe) classmethod from_glob ( pattern , ftype='any' , sortby='name' , reverse=False ) </> Create a channel with a glob pattern Parameters ftype (str, optional) \u2014 The file type, one of any, link, dir and file sortby (str, optional) \u2014 How the files should be sorted. One of name, mtime and size reverse (bool, optional) \u2014 Whether sort them in a reversed way. Returns (DataFrame) The channel classmethod a_from_glob ( pattern , ftype='any' , sortby='name' , reverse=False ) </> Create a channel with a glob pattern asynchronously Parameters pattern (str) \u2014 The glob pattern, supported: \"dir1/dir2/*.txt\" ftype (str, optional) \u2014 The file type, one of any, link, dir and file sortby (str, optional) \u2014 How the files should be sorted. One of name, mtime and size reverse (bool, optional) \u2014 Whether sort them in a reversed way. Returns (DataFrame) The channel classmethod from_pairs ( pattern , ftype='any' , sortby='name' , reverse=False ) </> Create a width=2 channel with a glob pattern Parameters ftype (str, optional) \u2014 The file type, one of any, link, dir and file sortby (str, optional) \u2014 How the files should be sorted. One of name, mtime and size reverse (bool, optional) \u2014 Whether sort them in a reversed way. Returns (DataFrame) The channel classmethod a_from_pairs ( pattern , ftype='any' , sortby='name' , reverse=False ) </> Create a width=2 channel with a glob pattern Parameters ftype (str, optional) \u2014 The file type, one of any, link, dir and file sortby (str, optional) \u2014 How the files should be sorted. One of name, mtime and size reverse (bool, optional) \u2014 Whether sort them in a reversed way. Returns (DataFrame) The channel classmethod from_csv ( *args , **kwargs ) </> Create a channel from a csv file Uses pandas.read_csv() to create a channel Parameters *args \u2014 and **kwargs \u2014 Arguments passing to pandas.read_csv() classmethod from_excel ( *args , **kwargs ) </> Create a channel from an excel file. Uses pandas.read_excel() to create a channel Parameters *args \u2014 and **kwargs \u2014 Arguments passing to pandas.read_excel() classmethod from_table ( *args , **kwargs ) </> Create a channel from a table file. Uses pandas.read_table() to create a channel Parameters *args \u2014 and **kwargs \u2014 Arguments passing to pandas.read_table() function pipen.channel. expand_dir ( data , col=0 , pattern='*' , ftype='any' , sortby='name' , reverse=False ) </> Expand a Channel according to the files in ,other cols will keep the same. This is only applicable to a 1-row channel. Examples >>> ch = channel . create ([( './' , 1 )]) >>> ch >> expand () >>> [[ './a' , 1 ], [ './b' , 1 ], [ './c' , 1 ]] Parameters col (str | int, optional) \u2014 the index or name of the column used to expand pattern (str, optional) \u2014 use a pattern to filter the files/dirs, default: * ftype (str, optional) \u2014 the type of the files/dirs to include - 'dir', 'file', 'link' or 'any' (default) sortby (str, optional) \u2014 how the list is sorted - 'name' (default), 'mtime', 'size' reverse (bool, optional) \u2014 reverse sort. Returns (DataFrame) The expanded channel function pipen.channel. collapse_files ( data , col=0 ) </> Collapse a Channel according to the files in ,other cols will use the values in row 0. Note that other values in other rows will be discarded. Examples >>> ch = channel . create ([[ './a' , 1 ], [ './b' , 1 ], [ './c' , 1 ]]) >>> ch >> collapse () >>> [[ '.' , 1 ]] Parameters data (DataFrame) \u2014 The original channel col (str | int, optional) \u2014 the index or name of the column used to collapse on Returns (DataFrame) The collapsed channel","title":"pipen.channel"},{"location":"api/pipen.channel/#pipenchannel","text":"</> Provide some function for creating and modifying channels (dataframes) Classes Channel \u2014 A DataFrame wrapper with creators </> Functions collapse_files ( data , col ) (DataFrame) \u2014 Collapse a Channel according to the files in ,other cols will use the values in row 0. </> expand_dir ( data , col , pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Expand a Channel according to the files in ,other cols will keep the same. </> class","title":"pipen.channel"},{"location":"api/pipen.channel/#pipenchannelchannel","text":"</> Bases pandas.core.frame.DataFrame pandas.core.generic.NDFrame pandas.core.base.PandasObject pandas.core.accessor.DirNamesMixin pandas.core.indexing.IndexingMixin pandas.core.arraylike.OpsMixin A DataFrame wrapper with creators Parameters data (optional) \u2014 Dict can contain Series, arrays, constants, dataclass or list-like objects. Ifdata is a dict, column order follows insertion-order. If a dict contains Series which have an index defined, it is aligned by its index. This alignment also occurs if data is a Series or a DataFrame itself. Alignment is done on Series/DataFrame inputs. If data is a list of dicts, column order follows insertion-order. index (Axes | None, optional) \u2014 Index to use for resulting frame. Will default to RangeIndex ifno indexing information part of input data and no index provided. columns (Axes | None, optional) \u2014 Column labels to use for resulting frame when data does not have them,defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels, will perform column selection instead. dtype (Dtype | None, optional) \u2014 Data type to force. Only a single dtype is allowed. If None, infer. copy (bool | none, optional) \u2014 Copy data from inputs.For dict data, the default of None behaves like copy=True . For DataFrame or 2d ndarray input, the default of None behaves like copy=False . If data is a dict containing one or more Series (possibly of different dtypes), copy=False will ensure that these inputs are not copied. .. versionchanged:: 1.3.0 Attributes T \u2014 The transpose of the DataFrame. </> at (_AtIndexer) \u2014 Access a single value for a row/column label pair. Similar to loc , in that both provide label-based lookups. Use at if you only need to get or set a single value in a DataFrame or Series. </> attrs (dict[Hashable, Any]) \u2014 Dictionary of global attributes of this dataset. .. warning:: attrs is experimental and may change without warning. </> axes (list) \u2014 Return a list representing the axes of the DataFrame. It has the row axis labels and column axis labels as the only members. They are returned in that order. </> dtypes \u2014 Return the dtypes in the DataFrame. This returns a Series with the data type of each column. The result's index is the original DataFrame's columns. Columns with mixed types are stored with the object dtype. See :ref: the User Guide <basics.dtypes> for more. </> empty \u2014 Indicator whether Series/DataFrame is empty. True if Series/DataFrame is entirely empty (no items), meaning any of the axes are of length 0. </> flags (Flags) \u2014 Get the properties associated with this pandas object. The available flags are :attr: Flags.allows_duplicate_labels </> iat (_iAtIndexer) \u2014 Access a single value for a row/column pair by integer position. Similar to iloc , in that both provide integer-based lookups. Use iat if you only need to get or set a single value in a DataFrame or Series. </> iloc (_iLocIndexer) \u2014 Purely integer-location based indexing for selection by position. .. deprecated:: 2.2.0 Returning a tuple from a callable is deprecated. .iloc[] is primarily integer position based (from 0 to length-1 of the axis), but may also be used with a boolean array. Allowed inputs are: An integer, e.g. 5 . A list or array of integers, e.g. [4, 3, 0] . A slice object with ints, e.g. 1:7 . A boolean array. A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above). This is useful in method chains, when you don't have a reference to the calling object, but would like to base your selection on some value. A tuple of row and column indexes. The tuple elements consist of one of the above inputs, e.g. (0, 1) . .iloc will raise IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing (this conforms with python/numpy slice semantics). See more at :ref: Selection by Position <indexing.integer> . </> loc (_LocIndexer) \u2014 Access a group of rows and columns by label(s) or a boolean array. .loc[] is primarily label based, but may also be used with a boolean array. Allowed inputs are: A single label, e.g. 5 or 'a' , (note that 5 is interpreted as a label of the index, and never as an integer position along the index). A list or array of labels, e.g. ['a', 'b', 'c'] . A slice object with labels, e.g. 'a':'f' . .. warning:: Note that contrary to usual python slices, both the start and the stop are included A boolean array of the same length as the axis being sliced, e.g. [True, False, True] . An alignable boolean Series. The index of the key will be aligned before masking. An alignable Index. The Index of the returned selection will be the input. A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above) See more at :ref: Selection by Label <indexing.label> . </> ndim (int) \u2014 Return an int representing the number of axes / array dimensions. Return 1 if Series. Otherwise return 2 if DataFrame. </> shape (tuple) \u2014 Return a tuple representing the dimensionality of the DataFrame. </> size (int) \u2014 Return an int representing the number of elements in this object. Return the number of rows if Series. Otherwise return the number of rows times number of columns if DataFrame. </> style (Styler) \u2014 Returns a Styler object. Contains methods for building a styled HTML representation of the DataFrame. </> values \u2014 Return a Numpy representation of the DataFrame. .. warning:: We recommend using :meth: DataFrame.to_numpy instead. Only the values in the DataFrame will be returned, the axes labels will be removed. </> Methods __add__ ( other ) (DataFrame) \u2014 Get Addition of DataFrame and other, column-wise. </> __arrow_c_stream__ ( requested_schema ) (PyCapsule) \u2014 Export the pandas DataFrame as an Arrow C stream PyCapsule. </> __contains__ ( key ) (bool) \u2014 True if the key is in the info axis </> __dataframe__ ( nan_as_null , allow_copy ) (DataFrame interchange object) \u2014 Return the dataframe interchange object implementing the interchange protocol. </> __dataframe_consortium_standard__ ( api_version ) (Any) \u2014 Provide entry point to the Consortium DataFrame Standard API. </> __delitem__ ( key ) \u2014 Delete item </> __dir__ ( ) (list) \u2014 Provide method name lookup and completion. </> __finalize__ ( other , method , **kwargs ) \u2014 Propagate metadata from other to self. </> __getattr__ ( name ) \u2014 After regular attribute access, try looking up the nameThis allows simpler access to columns for interactive use. </> __iter__ ( ) (iterator) \u2014 Iterate over info axis. </> __len__ ( ) (int) \u2014 Returns length of info axis, but here we use the index. </> __matmul__ ( other ) (pandas.core.frame.dataframe | pandas.core.series.series) \u2014 Matrix multiplication using binary @ operator. </> __repr__ ( ) (str) \u2014 Return a string representation for a particular DataFrame. </> __rmatmul__ ( other ) (DataFrame) \u2014 Matrix multiplication using binary @ operator. </> __setattr__ ( name , value ) \u2014 After regular attribute access, try setting the nameThis allows simpler access to columns for interactive use. </> __sizeof__ ( ) (int) \u2014 Generates the total memory usage for an object that returnseither a value or Series of values </> a_from_glob ( pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Create a channel with a glob pattern asynchronously </> a_from_pairs ( pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Create a width=2 channel with a glob pattern </> abs ( ) (abs) \u2014 Return a Series/DataFrame with absolute numeric value of each element. </> add ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Addition of dataframe and other, element-wise (binary operator add ). </> add_prefix ( prefix , axis ) (Series or DataFrame) \u2014 Prefix labels with string prefix . </> add_suffix ( suffix , axis ) (Series or DataFrame) \u2014 Suffix labels with string suffix . </> aggregate ( func , axis , *args , **kwargs ) (scalar, Series or DataFrame) \u2014 Aggregate using one or more operations over the specified axis. </> align ( other , join , axis , level , copy , fill_value , method , limit , fill_axis , broadcast_axis ) (tuple of (Series/DataFrame, type of other)) \u2014 Align two objects on their axes with the specified join method. </> all ( axis , bool_only , skipna , **kwargs ) (Series or DataFrame) \u2014 Return whether all elements are True, potentially over an axis. </> any ( axis , bool_only , skipna , **kwargs ) (Series or DataFrame) \u2014 Return whether any element is True, potentially over an axis. </> apply ( func , axis , raw , result_type , args , by_row , engine , engine_kwargs , **kwargs ) (Series or DataFrame) \u2014 Apply a function along an axis of the DataFrame. </> applymap ( func , na_action , **kwargs ) (DataFrame) \u2014 Apply a function to a Dataframe elementwise. </> asfreq ( freq , method , how , normalize , fill_value ) (Series/DataFrame) \u2014 Convert time series to specified frequency. </> asof ( where , subset ) (scalar, Series, or DataFrame) \u2014 Return the last row(s) without any NaNs before where . </> assign ( **kwargs ) (DataFrame) \u2014 Assign new columns to a DataFrame. </> astype ( dtype , copy , errors ) (same type as caller) \u2014 Cast a pandas object to a specified dtype dtype . </> at_time ( time , asof , axis ) (Series or DataFrame) \u2014 Select values at particular time of day (e.g., 9:30AM). </> backfill ( axis , inplace , limit , downcast ) (Series/DataFrame or None) \u2014 Fill NA/NaN values by using the next valid observation to fill the gap. </> between_time ( start_time , end_time , inclusive , axis ) (Series or DataFrame) \u2014 Select values between particular times of the day (e.g., 9:00-9:30 AM). </> bfill ( axis , inplace , limit , limit_area , downcast ) (Series/DataFrame or None) \u2014 Fill NA/NaN values by using the next valid observation to fill the gap. </> bool ( ) (bool) \u2014 Return the bool of a single element Series or DataFrame. </> clip ( lower , upper , axis , inplace , **kwargs ) (Series or DataFrame or None) \u2014 Trim values at input threshold(s). </> combine ( other , func , fill_value , overwrite ) (DataFrame) \u2014 Perform column-wise combine with another DataFrame. </> combine_first ( other ) (DataFrame) \u2014 Update null elements with value in the same location in other . </> compare ( other , align_axis , keep_shape , keep_equal , result_names ) (DataFrame) \u2014 Compare to another DataFrame and show the differences. </> convert_dtypes ( infer_objects , convert_string , convert_integer , convert_boolean , convert_floating , dtype_backend ) (Series or DataFrame) \u2014 Convert columns to the best possible dtypes using dtypes supporting pd.NA . </> copy ( deep ) (Series or DataFrame) \u2014 Make a copy of this object's indices and data. </> corr ( method , min_periods , numeric_only ) (DataFrame) \u2014 Compute pairwise correlation of columns, excluding NA/null values. </> corrwith ( other , axis , drop , method , numeric_only ) (Series) \u2014 Compute pairwise correlation. </> count ( axis , numeric_only ) (Series) \u2014 Count non-NA cells for each column or row. </> cov ( min_periods , ddof , numeric_only ) (DataFrame) \u2014 Compute pairwise covariance of columns, excluding NA/null values. </> create ( value ) (DataFrame) \u2014 Create a channel from a list. </> cummax ( axis , skipna , *args , **kwargs ) (Series or DataFrame) \u2014 Return cumulative maximum over a DataFrame or Series axis. </> cummin ( axis , skipna , *args , **kwargs ) (Series or DataFrame) \u2014 Return cumulative minimum over a DataFrame or Series axis. </> cumprod ( axis , skipna , *args , **kwargs ) (Series or DataFrame) \u2014 Return cumulative product over a DataFrame or Series axis. </> cumsum ( axis , skipna , *args , **kwargs ) (Series or DataFrame) \u2014 Return cumulative sum over a DataFrame or Series axis. </> describe ( percentiles , include , exclude ) (Series or DataFrame) \u2014 Generate descriptive statistics. </> diff ( periods , axis ) (DataFrame) \u2014 First discrete difference of element. </> dot ( other ) (Series or DataFrame) \u2014 Compute the matrix multiplication between the DataFrame and other. </> drop ( labels , axis , index , columns , level , inplace , errors ) (DataFrame or None) \u2014 Drop specified labels from rows or columns. </> drop_duplicates ( subset , keep , inplace , ignore_index ) (DataFrame or None) \u2014 Return DataFrame with duplicate rows removed. </> droplevel ( level , axis ) (Series/DataFrame) \u2014 Return Series/DataFrame with requested index / column level(s) removed. </> dropna ( axis , how , thresh , subset , inplace , ignore_index ) (DataFrame or None) \u2014 Remove missing values. </> duplicated ( subset , keep ) (Series) \u2014 Return boolean Series denoting duplicate rows. </> eq ( other , axis , level ) (DataFrame of bool) \u2014 Get Equal to of dataframe and other, element-wise (binary operator eq ). </> equals ( other ) (bool) \u2014 Test whether two objects contain the same elements. </> eval ( expr , inplace , **kwargs ) (ndarray, scalar, pandas object, or None) \u2014 Evaluate a string describing operations on DataFrame columns. </> ewm ( com , span , halflife , alpha , min_periods , adjust , ignore_na , axis , times , method ) (pandas.api.typing.ExponentialMovingWindow) \u2014 Provide exponentially weighted (EW) calculations. </> expanding ( min_periods , axis , method ) (pandas.api.typing.Expanding) \u2014 Provide expanding window calculations. </> explode ( column , ignore_index ) (DataFrame) \u2014 Transform each element of a list-like to a row, replicating index values. </> ffill ( axis , inplace , limit , limit_area , downcast ) (Series/DataFrame or None) \u2014 Fill NA/NaN values by propagating the last valid observation to next valid. </> fillna ( value , method , axis , inplace , limit , downcast ) (Series/DataFrame or None) \u2014 Fill NA/NaN values using the specified method. </> filter ( items , like , regex , axis ) (same type as input object) \u2014 Subset the dataframe rows or columns according to the specified index labels. </> first ( offset ) (Series or DataFrame) \u2014 Select initial periods of time series data based on a date offset. </> first_valid_index ( ) (type of index) \u2014 Return index for first non-NA value or None, if no non-NA value is found. </> floordiv ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Integer division of dataframe and other, element-wise (binary operator floordiv ). </> from_csv ( *args , **kwargs ) \u2014 Create a channel from a csv file </> from_dict ( data , orient , dtype , columns ) (DataFrame) \u2014 Construct DataFrame from dict of array-like or dicts. </> from_excel ( *args , **kwargs ) \u2014 Create a channel from an excel file. </> from_glob ( pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Create a channel with a glob pattern </> from_pairs ( pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Create a width=2 channel with a glob pattern </> from_records ( data , index , exclude , columns , coerce_float , nrows ) (DataFrame) \u2014 Convert structured or record ndarray to DataFrame. </> from_table ( *args , **kwargs ) \u2014 Create a channel from a table file. </> ge ( other , axis , level ) (DataFrame of bool) \u2014 Get Greater than or equal to of dataframe and other, element-wise (binary operator ge ). </> get ( key , default ) (same type as items contained in object) \u2014 Get item from object for given key (ex: DataFrame column). </> groupby ( by , axis , level , as_index , sort , group_keys , observed , dropna ) (pandas.api.typing.DataFrameGroupBy) \u2014 Group DataFrame using a mapper or by a Series of columns. </> gt ( other , axis , level ) (DataFrame of bool) \u2014 Get Greater than of dataframe and other, element-wise (binary operator gt ). </> head ( n ) (same type as caller) \u2014 Return the first n rows. </> idxmax ( axis , skipna , numeric_only ) (Series) \u2014 Return index of first occurrence of maximum over requested axis. </> idxmin ( axis , skipna , numeric_only ) (Series) \u2014 Return index of first occurrence of minimum over requested axis. </> infer_objects ( copy ) (same type as input object) \u2014 Attempt to infer better dtypes for object columns. </> info ( verbose , buf , max_cols , memory_usage , show_counts ) (None) \u2014 Print a concise summary of a DataFrame. </> insert ( loc , column , value , allow_duplicates ) \u2014 Insert column into DataFrame at specified location. </> interpolate ( method , axis , limit , inplace , limit_direction , limit_area , downcast , **kwargs ) (Series or DataFrame or None) \u2014 Fill NaN values using an interpolation method. </> isetitem ( loc , value ) \u2014 Set the given value in the column with position loc . </> isin ( values ) (DataFrame) \u2014 Whether each element in the DataFrame is contained in values. </> isna ( ) (DataFrame) \u2014 Detect missing values. </> isnull ( ) (DataFrame) \u2014 DataFrame.isnull is an alias for DataFrame.isna. </> items ( ) (label : object) \u2014 Iterate over (column name, Series) pairs. </> iterrows ( ) (index : label or tuple of label) \u2014 Iterate over DataFrame rows as (index, Series) pairs. </> itertuples ( index , name ) (iterator) \u2014 Iterate over DataFrame rows as namedtuples. </> join ( other , on , how , lsuffix , rsuffix , sort , validate ) (DataFrame) \u2014 Join columns of another DataFrame. </> keys ( ) (Index) \u2014 Get the 'info axis' (see Indexing for more). </> kurt ( axis , skipna , numeric_only , **kwargs ) (Series or scalar) \u2014 Return unbiased kurtosis over requested axis. </> last ( offset ) (Series or DataFrame) \u2014 Select final periods of time series data based on a date offset. </> last_valid_index ( ) (type of index) \u2014 Return index for last non-NA value or None, if no non-NA value is found. </> le ( other , axis , level ) (DataFrame of bool) \u2014 Get Less than or equal to of dataframe and other, element-wise (binary operator le ). </> lt ( other , axis , level ) (DataFrame of bool) \u2014 Get Less than of dataframe and other, element-wise (binary operator lt ). </> map ( func , na_action , **kwargs ) (DataFrame) \u2014 Apply a function to a Dataframe elementwise. </> mask ( cond , other , inplace , axis , level ) (Same type as caller or None if ``inplace=True``.) \u2014 Replace values where the condition is True. </> max ( axis , skipna , numeric_only , **kwargs ) (Series or scalar) \u2014 Return the maximum of the values over the requested axis. </> mean ( axis , skipna , numeric_only , **kwargs ) (Series or scalar) \u2014 Return the mean of the values over the requested axis. </> median ( axis , skipna , numeric_only , **kwargs ) (Series or scalar) \u2014 Return the median of the values over the requested axis. </> melt ( id_vars , value_vars , var_name , value_name , col_level , ignore_index ) (DataFrame) \u2014 Unpivot a DataFrame from wide to long format, optionally leaving identifiers set. </> memory_usage ( index , deep ) (Series) \u2014 Return the memory usage of each column in bytes. </> merge ( right , how , on , left_on , right_on , left_index , right_index , sort , suffixes , copy , indicator , validate ) (DataFrame) \u2014 Merge DataFrame or named Series objects with a database-style join. </> min ( axis , skipna , numeric_only , **kwargs ) (Series or scalar) \u2014 Return the minimum of the values over the requested axis. </> mod ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Modulo of dataframe and other, element-wise (binary operator mod ). </> mode ( axis , numeric_only , dropna ) (DataFrame) \u2014 Get the mode(s) of each element along the selected axis. </> mul ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Multiplication of dataframe and other, element-wise (binary operator mul ). </> ne ( other , axis , level ) (DataFrame of bool) \u2014 Get Not equal to of dataframe and other, element-wise (binary operator ne ). </> nlargest ( n , columns , keep ) (DataFrame) \u2014 Return the first n rows ordered by columns in descending order. </> notna ( ) (DataFrame) \u2014 Detect existing (non-missing) values. </> notnull ( ) (DataFrame) \u2014 DataFrame.notnull is an alias for DataFrame.notna. </> nsmallest ( n , columns , keep ) (DataFrame) \u2014 Return the first n rows ordered by columns in ascending order. </> nunique ( axis , dropna ) (Series) \u2014 Count number of distinct elements in specified axis. </> pad ( axis , inplace , limit , downcast ) (Series/DataFrame or None) \u2014 Fill NA/NaN values by propagating the last valid observation to next valid. </> pct_change ( periods , fill_method , limit , freq , **kwargs ) (Series or DataFrame) \u2014 Fractional change between the current and a prior element. </> pipe ( func , *args , **kwargs ) (the return type of ``func``.) \u2014 Apply chainable functions that expect Series or DataFrames. </> pivot ( columns , index , values ) (DataFrame) \u2014 Return reshaped DataFrame organized by given index / column values. </> pivot_table ( values , index , columns , aggfunc , fill_value , margins , dropna , margins_name , observed , sort ) (DataFrame) \u2014 Create a spreadsheet-style pivot table as a DataFrame. </> pop ( item ) (Series) \u2014 Return item and drop from frame. Raise KeyError if not found. </> pow ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Exponential power of dataframe and other, element-wise (binary operator pow ). </> prod ( axis , skipna , numeric_only , min_count , **kwargs ) (Series or scalar) \u2014 Return the product of the values over the requested axis. </> quantile ( q , axis , numeric_only , interpolation , method ) (Series or DataFrame) \u2014 Return values at the given quantile over requested axis. </> query ( expr , inplace , **kwargs ) (DataFrame or None) \u2014 Query the columns of a DataFrame with a boolean expression. </> radd ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Addition of dataframe and other, element-wise (binary operator radd ). </> rank ( axis , method , numeric_only , na_option , ascending , pct ) (same type as caller) \u2014 Compute numerical data ranks (1 through n) along axis. </> reindex ( labels , index , columns , axis , method , copy , level , fill_value , limit , tolerance ) (DataFrame with changed index.) \u2014 Conform DataFrame to new index with optional filling logic. </> reindex_like ( other , method , copy , limit , tolerance ) (Series or DataFrame) \u2014 Return an object with matching indices as other object. </> rename ( mapper , index , columns , axis , copy , inplace , level , errors ) (DataFrame or None) \u2014 Rename columns or index labels. </> rename_axis ( mapper , index , columns , axis , copy , inplace ) (Series, DataFrame, or None) \u2014 Set the name of the axis for the index or columns. </> reorder_levels ( order , axis ) (DataFrame) \u2014 Rearrange index levels using input order. May not drop or duplicate levels. </> replace ( to_replace , value , inplace , limit , regex , method ) (Series/DataFrame) \u2014 Replace values given in to_replace with value . </> resample ( rule , axis , closed , label , convention , kind , on , level , origin , offset , group_keys ) (pandas.api.typing.Resampler) \u2014 Resample time-series data. </> reset_index ( level , drop , inplace , col_level , col_fill , allow_duplicates , names ) (DataFrame or None) \u2014 Reset the index, or a level of it. </> rfloordiv ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Integer division of dataframe and other, element-wise (binary operator rfloordiv ). </> rmod ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Modulo of dataframe and other, element-wise (binary operator rmod ). </> rmul ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Multiplication of dataframe and other, element-wise (binary operator rmul ). </> rolling ( window , min_periods , center , win_type , on , axis , closed , step , method ) (pandas.api.typing.Window or pandas.api.typing.Rolling) \u2014 Provide rolling window calculations. </> round ( decimals , *args , **kwargs ) (DataFrame) \u2014 Round a DataFrame to a variable number of decimal places. </> rpow ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Exponential power of dataframe and other, element-wise (binary operator rpow ). </> rsub ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Subtraction of dataframe and other, element-wise (binary operator rsub ). </> rtruediv ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Floating division of dataframe and other, element-wise (binary operator rtruediv ). </> sample ( n , frac , replace , weights , random_state , axis , ignore_index ) (Series or DataFrame) \u2014 Return a random sample of items from an axis of object. </> select_dtypes ( include , exclude ) (DataFrame) \u2014 Return a subset of the DataFrame's columns based on the column dtypes. </> sem ( axis , skipna , ddof , numeric_only , **kwargs ) (Series or DataFrame (if level specified)) \u2014 Return unbiased standard error of the mean over requested axis. </> set_axis ( labels , axis , copy ) (DataFrame) \u2014 Assign desired index to given axis. </> set_flags ( copy , allows_duplicate_labels ) (Series or DataFrame) \u2014 Return a new object with updated flags. </> set_index ( keys , drop , append , inplace , verify_integrity ) (DataFrame or None) \u2014 Set the DataFrame index using existing columns. </> shift ( periods , freq , axis , fill_value , suffix ) (DataFrame) \u2014 Shift index by desired number of periods with an optional time freq . </> skew ( axis , skipna , numeric_only , **kwargs ) (Series or scalar) \u2014 Return unbiased skew over requested axis. </> sort_index ( axis , level , ascending , inplace , kind , na_position , sort_remaining , ignore_index , key ) (DataFrame or None) \u2014 Sort object by labels (along an axis). </> sort_values ( by , axis , ascending , inplace , kind , na_position , ignore_index , key ) (DataFrame or None) \u2014 Sort by the values along either axis. </> squeeze ( axis ) (DataFrame, Series, or scalar) \u2014 Squeeze 1 dimensional axis objects into scalars. </> stack ( level , dropna , sort , future_stack ) (DataFrame or Series) \u2014 Stack the prescribed level(s) from columns to index. </> std ( axis , skipna , ddof , numeric_only , **kwargs ) (Series or DataFrame (if level specified)) \u2014 Return sample standard deviation over requested axis. </> sub ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Subtraction of dataframe and other, element-wise (binary operator sub ). </> sum ( axis , skipna , numeric_only , min_count , **kwargs ) (Series or scalar) \u2014 Return the sum of the values over the requested axis. </> swapaxes ( axis1 , axis2 , copy ) (same as input) \u2014 Interchange axes and swap values axes appropriately. </> swaplevel ( i , j , axis ) (DataFrame) \u2014 Swap levels i and j in a :class: MultiIndex . </> tail ( n ) (type of caller) \u2014 Return the last n rows. </> take ( indices , axis , **kwargs ) (same type as caller) \u2014 Return the elements in the given positional indices along an axis. </> to_clipboard ( excel , sep , **kwargs ) \u2014 Copy object to the system clipboard. </> to_csv ( path_or_buf , sep , na_rep , float_format , columns , header , index , index_label , mode , encoding , compression , quoting , quotechar , lineterminator , chunksize , date_format , doublequote , escapechar , decimal , errors , storage_options ) (None or str) \u2014 Write object to a comma-separated values (csv) file. </> to_dict ( orient , into , index ) (dict, list or collections.abc.MutableMapping) \u2014 Convert the DataFrame to a dictionary. </> to_excel ( excel_writer , sheet_name , na_rep , float_format , columns , header , index , index_label , startrow , startcol , engine , merge_cells , inf_rep , freeze_panes , storage_options , engine_kwargs ) \u2014 Write object to an Excel sheet. </> to_feather ( path , **kwargs ) \u2014 Write a DataFrame to the binary Feather format. </> to_gbq ( destination_table , project_id , chunksize , reauth , if_exists , auth_local_webserver , table_schema , location , progress_bar , credentials ) \u2014 Write a DataFrame to a Google BigQuery table. </> to_hdf ( path_or_buf , key , mode , complevel , complib , append , format , index , min_itemsize , nan_rep , dropna , data_columns , errors , encoding ) \u2014 Write the contained data to an HDF5 file using HDFStore. </> to_html ( buf , columns , col_space , header , index , na_rep , formatters , float_format , sparsify , index_names , justify , max_rows , max_cols , show_dimensions , decimal , bold_rows , classes , escape , notebook , border , table_id , render_links , encoding ) (str or None) \u2014 Render a DataFrame as an HTML table. </> to_json ( path_or_buf , orient , date_format , double_precision , force_ascii , date_unit , default_handler , lines , compression , index , indent , storage_options , mode ) (None or str) \u2014 Convert the object to a JSON string. </> to_latex ( buf , columns , header , index , na_rep , formatters , float_format , sparsify , index_names , bold_rows , column_format , longtable , escape , encoding , decimal , multicolumn , multicolumn_format , multirow , caption , label , position ) (str or None) \u2014 Render object to a LaTeX tabular, longtable, or nested table. </> to_markdown ( buf , mode , index , storage_options , **kwargs ) (str) \u2014 Print DataFrame in Markdown-friendly format. </> to_numpy ( dtype , copy , na_value ) (numpy.ndarray) \u2014 Convert the DataFrame to a NumPy array. </> to_orc ( path , engine , index , engine_kwargs ) (bytes if no path argument is provided else None) \u2014 Write a DataFrame to the ORC format. </> to_parquet ( path , engine , compression , index , partition_cols , storage_options , **kwargs ) (bytes if no path argument is provided else None) \u2014 Write a DataFrame to the binary parquet format. </> to_period ( freq , axis , copy ) (DataFrame) \u2014 Convert DataFrame from DatetimeIndex to PeriodIndex. </> to_pickle ( path , compression , protocol , storage_options ) \u2014 Pickle (serialize) object to file. </> to_records ( index , column_dtypes , index_dtypes ) (numpy.rec.recarray) \u2014 Convert DataFrame to a NumPy record array. </> to_sql ( name , con , schema , if_exists , index , index_label , chunksize , dtype , method ) (None or int) \u2014 Write records stored in a DataFrame to a SQL database. </> to_stata ( path , convert_dates , write_index , byteorder , time_stamp , data_label , variable_labels , version , convert_strl , compression , storage_options , value_labels ) \u2014 Export DataFrame object to Stata dta format. </> to_string ( buf , columns , col_space , header , index , na_rep , formatters , float_format , sparsify , index_names , justify , max_rows , max_cols , show_dimensions , decimal , line_width , min_rows , max_colwidth , encoding ) (str or None) \u2014 Render a DataFrame to a console-friendly tabular output. </> to_timestamp ( freq , how , axis , copy ) (DataFrame) \u2014 Cast to DatetimeIndex of timestamps, at beginning of period. </> to_xarray ( ) (xarray.DataArray or xarray.Dataset) \u2014 Return an xarray object from the pandas object. </> to_xml ( path_or_buffer , index , root_name , row_name , na_rep , attr_cols , elem_cols , namespaces , prefix , encoding , xml_declaration , pretty_print , parser , stylesheet , compression , storage_options ) (None or str) \u2014 Render a DataFrame to an XML document. </> transform ( func , axis , *args , **kwargs ) (DataFrame) \u2014 Call func on self producing a DataFrame with the same axis shape as self. </> transpose ( *args , copy ) (DataFrame) \u2014 Transpose index and columns. </> truediv ( other , axis , level , fill_value ) (DataFrame) \u2014 Get Floating division of dataframe and other, element-wise (binary operator truediv ). </> truncate ( before , after , axis , copy ) (type of caller) \u2014 Truncate a Series or DataFrame before and after some index value. </> tz_convert ( tz , axis , level , copy ) (Series/DataFrame) \u2014 Convert tz-aware axis to target time zone. </> tz_localize ( tz , axis , level , copy , ambiguous , nonexistent ) (Series/DataFrame) \u2014 Localize tz-naive index of a Series or DataFrame to target time zone. </> unstack ( level , fill_value , sort ) (Series or DataFrame) \u2014 Pivot a level of the (necessarily hierarchical) index labels. </> update ( other , join , overwrite , filter_func , errors ) (None) \u2014 Modify in place using non-NA values from another DataFrame. </> value_counts ( subset , normalize , sort , ascending , dropna ) (Series) \u2014 Return a Series containing the frequency of each distinct row in the Dataframe. </> var ( axis , skipna , ddof , numeric_only , **kwargs ) (Series or DataFrame (if level specified)) \u2014 Return unbiased variance over requested axis. </> where ( cond , other , inplace , axis , level ) (Same type as caller or None if ``inplace=True``.) \u2014 Replace values where the condition is False. </> xs ( key , axis , level , drop_level ) (Series or DataFrame) \u2014 Return cross-section from the Series/DataFrame. </> method","title":"pipen.channel.Channel"},{"location":"api/pipen.channel/#pandascorearraylikeopsmixinadd","text":"</> Get Addition of DataFrame and other, column-wise. Equivalent to DataFrame.add(other) . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Object to be added to the DataFrame. Returns (DataFrame) The result of adding other to DataFrame. See Also DataFrame.add : Add a DataFrame and another object, with option for index- or column-oriented addition. Examples >>> df = pd . DataFrame ({ 'height' : [ 1.5 , 2.6 ], 'weight' : [ 500 , 800 ]}, ... index = [ 'elk' , 'moose' ]) >>> df height weight elk 1.5 500 moose 2.6 800 Adding a scalar affects all rows and columns. >>> df [[ 'height' , 'weight' ]] + 1.5 height weight elk 3.0 501.5 moose 4.1 801.5 Each element of a list is added to a column of the DataFrame, in order. >>> df [[ 'height' , 'weight' ]] + [ 0.5 , 1.5 ] height weight elk 2.0 501.5 moose 3.1 801.5 Keys of a dictionary are aligned to the DataFrame, based on column names; each value in the dictionary is added to the corresponding column. >>> df [[ 'height' , 'weight' ]] + { 'height' : 0.5 , 'weight' : 1.5 } height weight elk 2.0 501.5 moose 3.1 801.5 When other is a :class: Series , the index of other is aligned with the columns of the DataFrame. >>> s1 = pd . Series ([ 0.5 , 1.5 ], index = [ 'weight' , 'height' ]) >>> df [[ 'height' , 'weight' ]] + s1 height weight elk 3.0 500.5 moose 4.1 800.5 Even when the index of other is the same as the index of the DataFrame, the :class: Series will not be reoriented. If index-wise alignment is desired, :meth: DataFrame.add should be used with axis='index' . >>> s2 = pd . Series ([ 0.5 , 1.5 ], index = [ 'elk' , 'moose' ]) >>> df [[ 'height' , 'weight' ]] + s2 elk height moose weight elk NaN NaN NaN NaN moose NaN NaN NaN NaN >>> df [[ 'height' , 'weight' ]] . add ( s2 , axis = 'index' ) height weight elk 2.0 500.5 moose 4.1 801.5 When other is a :class: DataFrame , both columns names and the index are aligned. >>> other = pd . DataFrame ({ 'height' : [ 0.2 , 0.4 , 0.6 ]}, ... index = [ 'elk' , 'moose' , 'deer' ]) >>> df [[ 'height' , 'weight' ]] + other height weight deer NaN NaN elk 1.7 NaN moose 3.0 NaN method","title":"pandas.core.arraylike.OpsMixin.add"},{"location":"api/pipen.channel/#pandascoreaccessordirnamesmixindir","text":"</> Provide method name lookup and completion. Notes Only provide 'public' methods. method","title":"pandas.core.accessor.DirNamesMixin.dir"},{"location":"api/pipen.channel/#pandascorebasepandasobjectsizeof","text":"</> Generates the total memory usage for an object that returnseither a value or Series of values method","title":"pandas.core.base.PandasObject.sizeof"},{"location":"api/pipen.channel/#pandascoregenericndframeset_flags","text":"</> Return a new object with updated flags. Parameters copy (bool, default False) \u2014 Specify if a copy of the object should be made. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` allows_duplicate_labels (bool, optional) \u2014 Whether the returned object allows duplicate labels. Returns (Series or DataFrame) The same type as the caller. See Also DataFrame.attrs : Global metadata applying to this dataset.DataFrame.flags : Global flags applying to this object. Notes This method returns a new object that's a view on the same data as the input. Mutating the input or the output values will be reflected in the other. This method is intended to be used in method chains. \"Flags\" differ from \"metadata\". Flags reflect properties of the pandas object (the Series or DataFrame). Metadata refer to properties of the dataset, and should be stored in :attr: DataFrame.attrs . Examples >>> df = pd . DataFrame ({ \"A\" : [ 1 , 2 ]}) >>> df . flags . allows_duplicate_labels True >>> df2 = df . set_flags ( allows_duplicate_labels = False ) >>> df2 . flags . allows_duplicate_labels False method","title":"pandas.core.generic.NDFrame.set_flags"},{"location":"api/pipen.channel/#pandascoregenericndframeswapaxes","text":"</> Interchange axes and swap values axes appropriately. .. deprecated:: 2.1.0 swapaxes is deprecated and will be removed. Please use transpose instead. Examples Please see examples for :meth: DataFrame.transpose . method","title":"pandas.core.generic.NDFrame.swapaxes"},{"location":"api/pipen.channel/#pandascoregenericndframedroplevel","text":"</> Return Series/DataFrame with requested index / column level(s) removed. Parameters level (int, str, or list-like) \u2014 If a string is given, must be the name of a levelIf list-like, elements must be names or positional indexes of levels. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Axis along which the level(s) is removed: 0 or 'index': remove level(s) in column. 1 or 'columns': remove level(s) in row. For Series this parameter is unused and defaults to 0. Returns (Series/DataFrame) Series/DataFrame with requested index / column level(s) removed. Examples >>> df = pd . DataFrame ([ ... [ 1 , 2 , 3 , 4 ], ... [ 5 , 6 , 7 , 8 ], ... [ 9 , 10 , 11 , 12 ] ... ]) . set_index ([ 0 , 1 ]) . rename_axis ([ 'a' , 'b' ]) >>> df . columns = pd . MultiIndex . from_tuples ([ ... ( 'c' , 'e' ), ( 'd' , 'f' ) ... ], names = [ 'level_1' , 'level_2' ]) >>> df level_1 c d level_2 e f a b 1 2 3 4 5 6 7 8 9 10 11 12 >>> df . droplevel ( 'a' ) level_1 c d level_2 e f b 2 3 4 6 7 8 10 11 12 >>> df . droplevel ( 'level_2' , axis = 1 ) level_1 c d a b 1 2 3 4 5 6 7 8 9 10 11 12 method","title":"pandas.core.generic.NDFrame.droplevel"},{"location":"api/pipen.channel/#pandascoregenericndframesqueeze","text":"</> Squeeze 1 dimensional axis objects into scalars. Series or DataFrames with a single element are squeezed to a scalar. DataFrames with a single column or a single row are squeezed to a Series. Otherwise the object is unchanged. This method is most useful when you don't know if your object is a Series or DataFrame, but you do know it has just a single column. In that case you can safely call squeeze to ensure you have a Series. Parameters axis ({0 or 'index', 1 or 'columns', None}, default None) \u2014 A specific axis to squeeze. By default, all length-1 axes aresqueezed. For Series this parameter is unused and defaults to None . Returns (DataFrame, Series, or scalar) The projection after squeezing axis or all the axes. See Also Series.iloc : Integer-location based indexing for selecting scalars.DataFrame.iloc : Integer-location based indexing for selecting Series. Series.to_frame : Inverse of DataFrame.squeeze for a single-column DataFrame. Examples >>> primes = pd . Series ([ 2 , 3 , 5 , 7 ]) Slicing might produce a Series with a single value: >>> even_primes = primes [ primes % 2 == 0 ] >>> even_primes 0 2 dtype : int64 >>> even_primes . squeeze () 2 Squeezing objects with more than one value in every axis does nothing: >>> odd_primes = primes [ primes % 2 == 1 ] >>> odd_primes 1 3 2 5 3 7 dtype : int64 >>> odd_primes . squeeze () 1 3 2 5 3 7 dtype : int64 Squeezing is even more effective when used with DataFrames. >>> df = pd . DataFrame ([[ 1 , 2 ], [ 3 , 4 ]], columns = [ 'a' , 'b' ]) >>> df a b 0 1 2 1 3 4 Slicing a single column will produce a DataFrame with the columns having only one value: >>> df_a = df [[ 'a' ]] >>> df_a a 0 1 1 3 So the columns can be squeezed down, resulting in a Series: >>> df_a . squeeze ( 'columns' ) 0 1 1 3 Name : a , dtype : int64 Slicing a single row from a single column will produce a single scalar DataFrame: >>> df_0a = df . loc [ df . index < 1 , [ 'a' ]] >>> df_0a a 0 1 Squeezing the rows produces a single scalar Series: >>> df_0a . squeeze ( 'rows' ) a 1 Name : 0 , dtype : int64 Squeezing all axes will project directly into a scalar: >>> df_0a . squeeze () 1 method","title":"pandas.core.generic.NDFrame.squeeze"},{"location":"api/pipen.channel/#pandascoregenericndframerename_axis","text":"</> Set the name of the axis for the index or columns. Parameters mapper (scalar, list-like, optional) \u2014 Value to set the axis name attribute. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to rename. For Series this parameter is unused and defaults to 0. copy (bool, default None) \u2014 Also copy underlying data. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` inplace (bool, default False) \u2014 Modifies the object directly, instead of creating a new Seriesor DataFrame. Returns (Series, DataFrame, or None) The same type as the caller or None if inplace=True . See Also Series.rename : Alter Series index labels or name.DataFrame.rename : Alter DataFrame index labels or name. Index.rename : Set new names on index. Notes DataFrame.rename_axis supports two calling conventions (index=index_mapper, columns=columns_mapper, ...) (mapper, axis={'index', 'columns'}, ...) The first calling convention will only modify the names of the index and/or the names of the Index object that is the columns. In this case, the parameter copy is ignored. The second calling convention will modify the names of the corresponding index if mapper is a list or a scalar. However, if mapper is dict-like or a function, it will use the deprecated behavior of modifying the axis labels . We highly recommend using keyword arguments to clarify your intent. Examples Series >>> s = pd . Series ([ \"dog\" , \"cat\" , \"monkey\" ]) >>> s 0 dog 1 cat 2 monkey dtype : object >>> s . rename_axis ( \"animal\" ) animal 0 dog 1 cat 2 monkey dtype : object DataFrame >>> df = pd . DataFrame ({ \"num_legs\" : [ 4 , 4 , 2 ], ... \"num_arms\" : [ 0 , 0 , 2 ]}, ... [ \"dog\" , \"cat\" , \"monkey\" ]) >>> df num_legs num_arms dog 4 0 cat 4 0 monkey 2 2 >>> df = df . rename_axis ( \"animal\" ) >>> df num_legs num_arms animal dog 4 0 cat 4 0 monkey 2 2 >>> df = df . rename_axis ( \"limbs\" , axis = \"columns\" ) >>> df limbs num_legs num_arms animal dog 4 0 cat 4 0 monkey 2 2 MultiIndex >>> df . index = pd . MultiIndex . from_product ([[ 'mammal' ], ... [ 'dog' , 'cat' , 'monkey' ]], ... names = [ 'type' , 'name' ]) >>> df limbs num_legs num_arms type name mammal dog 4 0 cat 4 0 monkey 2 2 >>> df . rename_axis ( index = { 'type' : 'class' }) limbs num_legs num_arms class name mammal dog 4 0 cat 4 0 monkey 2 2 >>> df . rename_axis ( columns = str . upper ) LIMBS num_legs num_arms type name mammal dog 4 0 cat 4 0 monkey 2 2 method","title":"pandas.core.generic.NDFrame.rename_axis"},{"location":"api/pipen.channel/#pandascoregenericndframeequals","text":"</> Test whether two objects contain the same elements. This function allows two Series or DataFrames to be compared against each other to see if they have the same shape and elements. NaNs in the same location are considered equal. The row/column index do not need to have the same type, as long as the values are considered equal. Corresponding columns and index must be of the same dtype. Parameters other (Series or DataFrame) \u2014 The other Series or DataFrame to be compared with the first. Returns (bool) True if all elements are the same in both objects, Falseotherwise. See Also Series.eq : Compare two Series objects of the same length and return a Series where each element is True if the element in each Series is equal, False otherwise. DataFrame.eq : Compare two DataFrame objects of the same shape and return a DataFrame where each element is True if the respective element in each DataFrame is equal, False otherwise. testing.assert_series_equal : Raises an AssertionError if left and right are not equal. Provides an easy interface to ignore inequality in dtypes, indexes and precision among others. testing.assert_frame_equal : Like assert_series_equal, but targets DataFrames. numpy.array_equal : Return True if two arrays have the same shape and elements, False otherwise. Examples >>> df = pd . DataFrame ({ 1 : [ 10 ], 2 : [ 20 ]}) >>> df 1 2 0 10 20 DataFrames df and exactly_equal have the same types and values for their elements and column labels, which will return True. >>> exactly_equal = pd . DataFrame ({ 1 : [ 10 ], 2 : [ 20 ]}) >>> exactly_equal 1 2 0 10 20 >>> df . equals ( exactly_equal ) True DataFrames df and different_column_type have the same element types and values, but have different types for the column labels, which will still return True. >>> different_column_type = pd . DataFrame ({ 1.0 : [ 10 ], 2.0 : [ 20 ]}) >>> different_column_type 1.0 2.0 0 10 20 >>> df . equals ( different_column_type ) True DataFrames df and different_data_type have different types for the same values for their elements, and will return False even though their column labels are the same values and types. >>> different_data_type = pd . DataFrame ({ 1 : [ 10.0 ], 2 : [ 20.0 ]}) >>> different_data_type 1 2 0 10.0 20.0 >>> df . equals ( different_data_type ) False method","title":"pandas.core.generic.NDFrame.equals"},{"location":"api/pipen.channel/#pandascoregenericndframebool","text":"</> Return the bool of a single element Series or DataFrame. .. deprecated:: 2.1.0 bool is deprecated and will be removed in future version of pandas. For Series use pandas.Series.item . This must be a boolean scalar value, either True or False. It will raise a ValueError if the Series or DataFrame does not have exactly 1 element, or that element is not boolean (integer values 0 and 1 will also raise an exception). Returns (bool) The value in the Series or DataFrame. See Also Series.astype : Change the data type of a Series, including to boolean.DataFrame.astype : Change the data type of a DataFrame, including to boolean. numpy.bool_ : NumPy boolean data type, used by pandas for boolean values. Examples The method will only work for single element objects with a boolean value: >>> pd . Series ([ True ]) . bool () # doctest: +SKIP True >>> pd . Series ([ False ]) . bool () # doctest: +SKIP False >>> pd . DataFrame ({ 'col' : [ True ]}) . bool () # doctest: +SKIP True >>> pd . DataFrame ({ 'col' : [ False ]}) . bool () # doctest: +SKIP False This is an alternative method and will only work for single element objects with a boolean value: >>> pd . Series ([ True ]) . item () # doctest: +SKIP True >>> pd . Series ([ False ]) . item () # doctest: +SKIP False method","title":"pandas.core.generic.NDFrame.bool"},{"location":"api/pipen.channel/#pandascoregenericndframeabs","text":"</> Return a Series/DataFrame with absolute numeric value of each element. This function only applies to elements that are all numeric. Returns (abs) Series/DataFrame containing the absolute value of each element. See Also numpy.absolute : Calculate the absolute value element-wise. Notes For complex inputs, 1.2 + 1j , the absolute value is :math: \\sqrt{ a^2 + b^2 } . Examples Absolute numeric values in a Series. >>> s = pd . Series ([ - 1.10 , 2 , - 3.33 , 4 ]) >>> s . abs () 0 1.10 1 2.00 2 3.33 3 4.00 dtype : float64 Absolute numeric values in a Series with complex numbers. >>> s = pd . Series ([ 1.2 + 1 j ]) >>> s . abs () 0 1.56205 dtype : float64 Absolute numeric values in a Series with a Timedelta element. >>> s = pd . Series ([ pd . Timedelta ( '1 days' )]) >>> s . abs () 0 1 days dtype : timedelta64 [ ns ] Select rows with data closest to certain value using argsort (from StackOverflow <https://stackoverflow.com/a/17758115> __). >>> df = pd . DataFrame ({ ... 'a' : [ 4 , 5 , 6 , 7 ], ... 'b' : [ 10 , 20 , 30 , 40 ], ... 'c' : [ 100 , 50 , - 30 , - 50 ] ... }) >>> df a b c 0 4 10 100 1 5 20 50 2 6 30 - 30 3 7 40 - 50 >>> df . loc [( df . c - 43 ) . abs () . argsort ()] a b c 1 5 20 50 0 4 10 100 2 6 30 - 30 3 7 40 - 50 method","title":"pandas.core.generic.NDFrame.abs"},{"location":"api/pipen.channel/#pandascoregenericndframeiter","text":"</> Iterate over info axis. Returns (iterator) Info axis as iterator. Examples >>> df = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 ], 'B' : [ 4 , 5 , 6 ]}) >>> for x in df : ... print ( x ) A B method","title":"pandas.core.generic.NDFrame.iter"},{"location":"api/pipen.channel/#pandascoregenericndframekeys","text":"</> Get the 'info axis' (see Indexing for more). This is index for Series, columns for DataFrame. Returns (Index) Info axis. Examples >>> d = pd . DataFrame ( data = { 'A' : [ 1 , 2 , 3 ], 'B' : [ 0 , 4 , 8 ]}, ... index = [ 'a' , 'b' , 'c' ]) >>> d A B a 1 0 b 2 4 c 3 8 >>> d . keys () Index ([ 'A' , 'B' ], dtype = 'object' ) method","title":"pandas.core.generic.NDFrame.keys"},{"location":"api/pipen.channel/#pandascoregenericndframecontains","text":"</> True if the key is in the info axis method","title":"pandas.core.generic.NDFrame.contains"},{"location":"api/pipen.channel/#pandascoregenericndframeto_excel","text":"</> Write object to an Excel sheet. To write a single object to an Excel .xlsx file it is only necessary to specify a target file name. To write to multiple sheets it is necessary to create an ExcelWriter object with a target file name, and specify a sheet in the file to write to. Multiple sheets may be written to by specifying unique sheet_name . With all data written to the file it is necessary to save the changes. Note that creating an ExcelWriter object with a file name that already exists will result in the contents of the existing file being erased. Parameters excel_writer (path-like, file-like, or ExcelWriter object) \u2014 File path or existing ExcelWriter. sheet_name (str, default 'Sheet1') \u2014 Name of sheet which will contain DataFrame. na_rep (str, default '') \u2014 Missing data representation. float_format (str, optional) \u2014 Format string for floating point numbers. For example float_format=\"%.2f\" will format 0.1234 to 0.12. columns (sequence or list of str, optional) \u2014 Columns to write. header (bool or list of str, default True) \u2014 Write out the column names. If a list of string is given it isassumed to be aliases for the column names. index (bool, default True) \u2014 Write row names (index). index_label (str or sequence, optional) \u2014 Column label for index column(s) if desired. If not specified, and header and index are True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. startrow (int, default 0) \u2014 Upper left cell row to dump data frame. startcol (int, default 0) \u2014 Upper left cell column to dump data frame. engine (str, optional) \u2014 Write engine to use, 'openpyxl' or 'xlsxwriter'. You can also set thisvia the options io.excel.xlsx.writer or io.excel.xlsm.writer . merge_cells (bool, default True) \u2014 Write MultiIndex and Hierarchical Rows as merged cells. inf_rep (str, default 'inf') \u2014 Representation for infinity (there is no native representation forinfinity in Excel). freeze_panes (tuple of int (length 2), optional) \u2014 Specifies the one-based bottommost row and rightmost column thatis to be frozen. storage_options (dict, optional) \u2014 Extra options that make sense for a particular storage connection, e.g.host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to fsspec.open . Please see fsspec and urllib for more details, and for more examples on storage options refer here <https://pandas.pydata.org/docs/user_guide/io.html? highlight=storage_options#reading-writing-remote-files> _. .. versionadded:: 1.2.0 engine_kwargs (dict, optional) \u2014 Arbitrary keyword arguments passed to excel engine. See Also to_csv : Write DataFrame to a comma-separated values (csv) file.ExcelWriter : Class for writing DataFrame objects into excel sheets. read_excel : Read an Excel file into a pandas DataFrame. read_csv : Read a comma-separated values (csv) file into DataFrame. io.formats.style.Styler.to_excel : Add styles to Excel sheet. Notes For compatibility with :meth: ~DataFrame.to_csv , to_excel serializes lists and dicts to strings before writing. Once a workbook has been saved it is not possible to write further data without rewriting the whole workbook. Examples : , , ) P : , P s : ) P ) ) : , P ) , s : P method","title":"pandas.core.generic.NDFrame.to_excel"},{"location":"api/pipen.channel/#pandascoregenericndframeto_json","text":"</> Convert the object to a JSON string. Note NaN's and None will be converted to null and datetime objects will be converted to UNIX timestamps. Parameters path_or_buf (str, path object, file-like object, or None, default None) \u2014 String, path object (implementing os.PathLike[str]), or file-likeobject implementing a write() function. If None, the result is returned as a string. orient (str) \u2014 Indication of expected JSON string format. Series: default is 'index' allowed values are: {'split', 'records', 'index', 'table'}. DataFrame: default is 'columns' allowed values are: {'split', 'records', 'index', 'columns', 'values', 'table'}. The format of the JSON string: 'split' : dict like {'index' -> [index], 'columns' -> [columns], 'data' -> [values]} 'records' : list like [{column -> value}, ... , {column -> value}] 'index' : dict like {index -> {column -> value}} 'columns' : dict like {column -> {index -> value}} 'values' : just the values array 'table' : dict like {'schema': {schema}, 'data': {data}} Describing the data, where data component is like orient='records' . date_format ({None, 'epoch', 'iso'}) \u2014 Type of date conversion. 'epoch' = epoch milliseconds,'iso' = ISO8601. The default depends on the orient . For orient='table' , the default is 'iso'. For all other orients, the default is 'epoch'. double_precision (int, default 10) \u2014 The number of decimal places to use when encodingfloating point values. The possible maximal value is 15. Passing double_precision greater than 15 will raise a ValueError. force_ascii (bool, default True) \u2014 Force encoded string to be ASCII. date_unit (str, default 'ms' (milliseconds)) \u2014 The time unit to encode to, governs timestamp and ISO8601precision. One of 's', 'ms', 'us', 'ns' for second, millisecond, microsecond, and nanosecond respectively. default_handler (callable, default None) \u2014 Handler to call if object cannot otherwise be converted to asuitable format for JSON. Should receive a single argument which is the object to convert and return a serialisable object. lines (bool, default False) \u2014 If 'orient' is 'records' write out line-delimited json format. Willthrow ValueError if incorrect 'orient' since others are not list-like. compression (str or dict, default 'infer') \u2014 For on-the-fly compression of the output data. If 'infer' and 'path_or_buf' ispath-like, then detect compression from the following extensions: '.gz', '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2' (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of { 'zip' , 'gzip' , 'bz2' , 'zstd' , 'xz' , 'tar' } and other key-value pairs are forwarded to zipfile.ZipFile , gzip.GzipFile , bz2.BZ2File , zstandard.ZstdCompressor , lzma.LZMAFile or tarfile.TarFile , respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1} . .. versionadded:: 1.5.0 Added support for .tar files. .. versionchanged:: 1.4.0 Zstandard support. index (bool or None, default None) \u2014 The index is only used when 'orient' is 'split', 'index', 'column',or 'table'. Of these, 'index' and 'column' do not support index=False . indent (int, optional) \u2014 Length of whitespace used to indent each record. storage_options (dict, optional) \u2014 Extra options that make sense for a particular storage connection, e.g.host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to fsspec.open . Please see fsspec and urllib for more details, and for more examples on storage options refer here <https://pandas.pydata.org/docs/user_guide/io.html? highlight=storage_options#reading-writing-remote-files> _. mode (str, default 'w' (writing)) \u2014 Specify the IO mode for output when supplying a path_or_buf.Accepted args are 'w' (writing) and 'a' (append) only. mode='a' is only supported when lines is True and orient is 'records'. Returns (None or str) If path_or_buf is None, returns the resulting json format as astring. Otherwise returns None. See Also read_json : Convert a JSON string to pandas object. Notes The behavior of indent=0 varies from the stdlib, which does not indent the output but does insert newlines. Currently, indent=0 and the default indent=None are equivalent in pandas, though this may change in a future release. orient='table' contains a 'pandas_version' field under 'schema'. This stores the version of pandas used in the latest revision of the schema. Examples >>> from json import loads , dumps >>> df = pd . DataFrame ( ... [[ \"a\" , \"b\" ], [ \"c\" , \"d\" ]], ... index = [ \"row 1\" , \"row 2\" ], ... columns = [ \"col 1\" , \"col 2\" ], ... ) >>> result = df . to_json ( orient = \"split\" ) >>> parsed = loads ( result ) >>> dumps ( parsed , indent = 4 ) # doctest: +SKIP { \"columns\" : [ \"col 1\" , \"col 2\" ], \"index\" : [ \"row 1\" , \"row 2\" ], \"data\" : [ [ \"a\" , \"b\" ], [ \"c\" , \"d\" ] ] } Encoding/decoding a Dataframe using 'records' formatted JSON. Note that index labels are not preserved with this encoding. >>> result = df . to_json ( orient = \"records\" ) >>> parsed = loads ( result ) >>> dumps ( parsed , indent = 4 ) # doctest: +SKIP [ { \"col 1\" : \"a\" , \"col 2\" : \"b\" }, { \"col 1\" : \"c\" , \"col 2\" : \"d\" } ] Encoding/decoding a Dataframe using 'index' formatted JSON: >>> result = df . to_json ( orient = \"index\" ) >>> parsed = loads ( result ) >>> dumps ( parsed , indent = 4 ) # doctest: +SKIP { \"row 1\" : { \"col 1\" : \"a\" , \"col 2\" : \"b\" }, \"row 2\" : { \"col 1\" : \"c\" , \"col 2\" : \"d\" } } Encoding/decoding a Dataframe using 'columns' formatted JSON: >>> result = df . to_json ( orient = \"columns\" ) >>> parsed = loads ( result ) >>> dumps ( parsed , indent = 4 ) # doctest: +SKIP { \"col 1\" : { \"row 1\" : \"a\" , \"row 2\" : \"c\" }, \"col 2\" : { \"row 1\" : \"b\" , \"row 2\" : \"d\" } } Encoding/decoding a Dataframe using 'values' formatted JSON: >>> result = df . to_json ( orient = \"values\" ) >>> parsed = loads ( result ) >>> dumps ( parsed , indent = 4 ) # doctest: +SKIP [ [ \"a\" , \"b\" ], [ \"c\" , \"d\" ] ] Encoding with Table Schema: >>> result = df . to_json ( orient = \"table\" ) >>> parsed = loads ( result ) >>> dumps ( parsed , indent = 4 ) # doctest: +SKIP { \"schema\" : { \"fields\" : [ { \"name\" : \"index\" , \"type\" : \"string\" }, { \"name\" : \"col 1\" , \"type\" : \"string\" }, { \"name\" : \"col 2\" , \"type\" : \"string\" } ], \"primaryKey\" : [ \"index\" ], \"pandas_version\" : \"1.4.0\" }, \"data\" : [ { \"index\" : \"row 1\" , \"col 1\" : \"a\" , \"col 2\" : \"b\" }, { \"index\" : \"row 2\" , \"col 1\" : \"c\" , \"col 2\" : \"d\" } ] } method","title":"pandas.core.generic.NDFrame.to_json"},{"location":"api/pipen.channel/#pandascoregenericndframeto_hdf","text":"</> Write the contained data to an HDF5 file using HDFStore. Hierarchical Data Format (HDF) is self-describing, allowing an application to interpret the structure and contents of a file with no outside information. One HDF file can hold a mix of related objects which can be accessed as a group or as individual objects. In order to add another DataFrame or Series to an existing HDF file please use append mode and a different a key. .. warning:: One can store a subclass of DataFrame or Series to HDF5, but the type of the subclass is lost upon storing. For more information see the :ref: user guide <io.hdf5> . Parameters path_or_buf (str or pandas.HDFStore) \u2014 File path or HDFStore object. key (str) \u2014 Identifier for the group in the store. mode ({'a', 'w', 'r+'}, default 'a') \u2014 Mode to open file: 'w': write, a new file is created (an existing file with the same name would be deleted). 'a': append, an existing file is opened for reading and writing, and if the file does not exist it is created. 'r+': similar to 'a', but the file must already exist. complevel ({0-9}, default None) \u2014 Specifies a compression level for data.A value of 0 or None disables compression. complib ({'zlib', 'lzo', 'bzip2', 'blosc'}, default 'zlib') \u2014 Specifies the compression library to be used.These additional compressors for Blosc are supported (default if no compressor specified: 'blosc:blosclz'): {'blosc:blosclz', 'blosc:lz4', 'blosc:lz4hc', 'blosc:snappy', 'blosc:zlib', 'blosc:zstd'}. Specifying a compression library which is not available issues a ValueError. append (bool, default False) \u2014 For Table formats, append the input data to the existing. format ({'fixed', 'table', None}, default 'fixed') \u2014 Possible values: 'fixed': Fixed format. Fast writing/reading. Not-appendable, nor searchable. 'table': Table format. Write as a PyTables Table structure which may perform worse but allow more flexible operations like searching / selecting subsets of the data. If None, pd.get_option('io.hdf.default_format') is checked, followed by fallback to \"fixed\". index (bool, default True) \u2014 Write DataFrame index as a column. min_itemsize (dict or int, optional) \u2014 Map column names to minimum string sizes for columns. nan_rep (Any, optional) \u2014 How to represent null values as str.Not allowed with append=True. dropna (bool, default False, optional) \u2014 Remove missing values. data_columns (list of columns or True, optional) \u2014 List of columns to create as indexed data columns for on-diskqueries, or True to use all columns. By default only the axes of the object are indexed. See :ref: Query via data columns<io.hdf5-query-data-columns> . for more information. Applicable only to format='table'. errors (str, default 'strict') \u2014 Specifies how encoding and decoding errors are to be handled.See the errors argument for :func: open for a full list of options. See Also read_hdf : Read from HDF file.DataFrame.to_orc : Write a DataFrame to the binary orc format. DataFrame.to_parquet : Write a DataFrame to the binary parquet format. DataFrame.to_sql : Write to a SQL table. DataFrame.to_feather : Write out feather-format for DataFrames. DataFrame.to_csv : Write out to a csv file. Examples >>> df = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 ], 'B' : [ 4 , 5 , 6 ]}, ... index = [ 'a' , 'b' , 'c' ]) # doctest: +SKIP >>> df . to_hdf ( 'data.h5' , key = 'df' , mode = 'w' ) # doctest: +SKIP We can add another object to the same file: >>> s = pd . Series ([ 1 , 2 , 3 , 4 ]) # doctest: +SKIP >>> s . to_hdf ( 'data.h5' , key = 's' ) # doctest: +SKIP Reading from HDF file: >>> pd . read_hdf ( 'data.h5' , 'df' ) # doctest: +SKIP A B a 1 4 b 2 5 c 3 6 >>> pd . read_hdf ( 'data.h5' , 's' ) # doctest: +SKIP 0 1 1 2 2 3 3 4 dtype : int64 method","title":"pandas.core.generic.NDFrame.to_hdf"},{"location":"api/pipen.channel/#pandascoregenericndframeto_sql","text":"</> Write records stored in a DataFrame to a SQL database. Databases supported by SQLAlchemy [1]_ are supported. Tables can be newly created, appended to, or overwritten. Parameters name (str) \u2014 Name of SQL table. con (sqlalchemy.engine.(Engine or Connection) or sqlite3.Connection) \u2014 Using SQLAlchemy makes it possible to use any DB supported by thatlibrary. Legacy support is provided for sqlite3.Connection objects. The user is responsible for engine disposal and connection closure for the SQLAlchemy connectable. See here <https://docs.sqlalchemy.org/en/20/core/connections.html> _. If passing a sqlalchemy.engine.Connection which is already in a transaction, the transaction will not be committed. If passing a sqlite3.Connection, it will not be possible to roll back the record insertion. schema (str, optional) \u2014 Specify the schema (if database flavor supports this). If None, usedefault schema. if_exists ({'fail', 'replace', 'append'}, default 'fail') \u2014 How to behave if the table already exists. fail: Raise a ValueError. replace: Drop the table before inserting new values. append: Insert new values to the existing table. index (bool, default True) \u2014 Write DataFrame index as a column. Uses index_label as the columnname in the table. Creates a table index for this column. index_label (str or sequence, default None) \u2014 Column label for index column(s). If None is given (default) and index is True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. chunksize (int, optional) \u2014 Specify the number of rows in each batch to be written at a time.By default, all rows will be written at once. dtype (dict or scalar, optional) \u2014 Specifying the datatype for columns. If a dictionary is used, thekeys should be the column names and the values should be the SQLAlchemy types or strings for the sqlite3 legacy mode. If a scalar is provided, it will be applied to all columns. method ({None, 'multi', callable}, optional) \u2014 Controls the SQL insertion clause used: None : Uses standard SQL INSERT clause (one per row). 'multi': Pass multiple values in a single INSERT clause. callable with signature (pd_table, conn, keys, data_iter) . Details and a sample callable implementation can be found in the section :ref: insert method <io.sql.method> . Returns (None or int) Number of rows affected by to_sql. None is returned if the callablepassed into method does not return an integer number of rows. The number of returned rows affected is the sum of the rowcount attribute of sqlite3.Cursor or SQLAlchemy connectable which may not reflect the exact number of written rows as stipulated in the sqlite3 <https://docs.python.org/3/library/sqlite3.html#sqlite3.Cursor.rowcount> or SQLAlchemy <https://docs.sqlalchemy.org/en/20/core/connections.html#sqlalchemy.engine.CursorResult.rowcount> . .. versionadded:: 1.4.0 Raises ValueError \u2014 When the table already exists and if_exists is 'fail' (thedefault). See Also read_sql : Read a DataFrame from a table. Notes Timezone aware datetime columns will be written as Timestamp with timezone type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone. Not all datastores support method=\"multi\" . Oracle, for example, does not support multi-value insert. References .. [1] https://docs.sqlalchemy.org.. [2] https://www.python.org/dev/peps/pep-0249/ Examples Create an in-memory SQLite database. >>> from sqlalchemy import create_engine >>> engine = create_engine ( 'sqlite://' , echo = False ) Create a table from scratch with 3 rows. >>> df = pd . DataFrame ({ 'name' : [ 'User 1' , 'User 2' , 'User 3' ]}) >>> df name 0 User 1 1 User 2 2 User 3 >>> df . to_sql ( name = 'users' , con = engine ) 3 >>> from sqlalchemy import text >>> with engine . connect () as conn : ... conn . execute ( text ( \"SELECT * FROM users\" )) . fetchall () [( 0 , 'User 1' ), ( 1 , 'User 2' ), ( 2 , 'User 3' )] An sqlalchemy.engine.Connection can also be passed to con : >>> with engine . begin () as connection : ... df1 = pd . DataFrame ({ 'name' : [ 'User 4' , 'User 5' ]}) ... df1 . to_sql ( name = 'users' , con = connection , if_exists = 'append' ) 2 This is allowed to support operations that require that the same DBAPI connection is used for the entire operation. >>> df2 = pd . DataFrame ({ 'name' : [ 'User 6' , 'User 7' ]}) >>> df2 . to_sql ( name = 'users' , con = engine , if_exists = 'append' ) 2 >>> with engine . connect () as conn : ... conn . execute ( text ( \"SELECT * FROM users\" )) . fetchall () [( 0 , 'User 1' ), ( 1 , 'User 2' ), ( 2 , 'User 3' ), ( 0 , 'User 4' ), ( 1 , 'User 5' ), ( 0 , 'User 6' ), ( 1 , 'User 7' )] Overwrite the table with just df2 . >>> df2 . to_sql ( name = 'users' , con = engine , if_exists = 'replace' , ... index_label = 'id' ) 2 >>> with engine . connect () as conn : ... conn . execute ( text ( \"SELECT * FROM users\" )) . fetchall () [( 0 , 'User 6' ), ( 1 , 'User 7' )] Use method to define a callable insertion method to do nothing if there's a primary key conflict on a table in a PostgreSQL database. >>> from sqlalchemy.dialects.postgresql import insert >>> def insert_on_conflict_nothing ( table , conn , keys , data_iter ): ... # \"a\" is the primary key in \"conflict_table\" ... data = [ dict ( zip ( keys , row )) for row in data_iter ] ... stmt = insert ( table . table ) . values ( data ) . on_conflict_do_nothing ( index_elements = [ \"a\" ]) ... result = conn . execute ( stmt ) ... return result . rowcount >>> df_conflict . to_sql ( name = \"conflict_table\" , con = conn , if_exists = \"append\" , method = insert_on_conflict_nothing ) # doctest: +SKIP 0 For MySQL, a callable to update columns b and c if there's a conflict on a primary key. >>> from sqlalchemy.dialects.mysql import insert >>> def insert_on_conflict_update ( table , conn , keys , data_iter ): ... # update columns \"b\" and \"c\" on primary key conflict ... data = [ dict ( zip ( keys , row )) for row in data_iter ] ... stmt = ( ... insert ( table . table ) ... . values ( data ) ... ) ... stmt = stmt . on_duplicate_key_update ( b = stmt . inserted . b , c = stmt . inserted . c ) ... result = conn . execute ( stmt ) ... return result . rowcount >>> df_conflict . to_sql ( name = \"conflict_table\" , con = conn , if_exists = \"append\" , method = insert_on_conflict_update ) # doctest: +SKIP 2 Specify the dtype (especially useful for integers with missing values). Notice that while pandas is forced to store the data as floating point, the database supports nullable integers. When fetching the data with Python, we get back integer scalars. >>> df = pd . DataFrame ({ \"A\" : [ 1 , None , 2 ]}) >>> df A 0 1.0 1 NaN 2 2.0 >>> from sqlalchemy.types import Integer >>> df . to_sql ( name = 'integers' , con = engine , index = False , ... dtype = { \"A\" : Integer ()}) 3 >>> with engine . connect () as conn : ... conn . execute ( text ( \"SELECT * FROM integers\" )) . fetchall () [( 1 ,), ( None ,), ( 2 ,)] method","title":"pandas.core.generic.NDFrame.to_sql"},{"location":"api/pipen.channel/#pandascoregenericndframeto_pickle","text":"</> Pickle (serialize) object to file. Parameters path (str, path object, or file-like object) \u2014 String, path object (implementing os.PathLike[str] ), or file-likeobject implementing a binary write() function. File path where the pickled object will be stored. compression (str or dict, default 'infer') \u2014 For on-the-fly compression of the output data. If 'infer' and 'path' ispath-like, then detect compression from the following extensions: '.gz', '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2' (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of { 'zip' , 'gzip' , 'bz2' , 'zstd' , 'xz' , 'tar' } and other key-value pairs are forwarded to zipfile.ZipFile , gzip.GzipFile , bz2.BZ2File , zstandard.ZstdCompressor , lzma.LZMAFile or tarfile.TarFile , respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1} . .. versionadded:: 1.5.0 Added support for .tar files. protocol (int) \u2014 Int which indicates which protocol should be used by the pickler,default HIGHEST_PROTOCOL (see [1]_ paragraph 12.1.2). The possible values are 0, 1, 2, 3, 4, 5. A negative value for the protocol parameter is equivalent to setting its value to HIGHEST_PROTOCOL. .. [1] https://docs.python.org/3/library/pickle.html. storage_options (dict, optional) \u2014 Extra options that make sense for a particular storage connection, e.g.host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to fsspec.open . Please see fsspec and urllib for more details, and for more examples on storage options refer here <https://pandas.pydata.org/docs/user_guide/io.html? highlight=storage_options#reading-writing-remote-files> _. See Also read_pickle : Load pickled pandas object (or any object) from file.DataFrame.to_hdf : Write DataFrame to an HDF5 file. DataFrame.to_sql : Write DataFrame to a SQL database. DataFrame.to_parquet : Write a DataFrame to the binary parquet format. Examples >>> original_df = pd . DataFrame ({ \"foo\" : range ( 5 ), \"bar\" : range ( 5 , 10 )}) # doctest: +SKIP >>> original_df # doctest: +SKIP foo bar 0 0 5 1 1 6 2 2 7 3 3 8 4 4 9 >>> original_df . to_pickle ( \"./dummy.pkl\" ) # doctest: +SKIP >>> unpickled_df = pd . read_pickle ( \"./dummy.pkl\" ) # doctest: +SKIP >>> unpickled_df # doctest: +SKIP foo bar 0 0 5 1 1 6 2 2 7 3 3 8 4 4 9 method","title":"pandas.core.generic.NDFrame.to_pickle"},{"location":"api/pipen.channel/#pandascoregenericndframeto_clipboard","text":"</> Copy object to the system clipboard. Write a text representation of object to the system clipboard. This can be pasted into Excel, for example. Parameters excel (bool, default True) \u2014 Produce output in a csv format for easy pasting into excel. True, use the provided separator for csv pasting. False, write a string representation of the object to the clipboard. sep (str, default ``'\\t'``) \u2014 Field delimiter. **kwargs \u2014 These parameters will be passed to DataFrame.to_csv. See Also DataFrame.to_csv : Write a DataFrame to a comma-separated values (csv) file. read_clipboard : Read text from clipboard and pass to read_csv. Notes Requirements for your platform. Linux : xclip , or xsel (with PyQt4 modules) Windows : none macOS : none This method uses the processes developed for the package pyperclip . A solution to render any output string format is given in the examples. Examples Copy the contents of a DataFrame to the clipboard. >>> df = pd . DataFrame ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]], columns = [ 'A' , 'B' , 'C' ]) >>> df . to_clipboard ( sep = ',' ) # doctest: +SKIP ... # Wrote the following to the system clipboard: ... # ,A,B,C ... # 0,1,2,3 ... # 1,4,5,6 We can omit the index by passing the keyword index and setting it to false. >>> df . to_clipboard ( sep = ',' , index = False ) # doctest: +SKIP ... # Wrote the following to the system clipboard: ... # A,B,C ... # 1,2,3 ... # 4,5,6 Using the original pyperclip package for any string output format. .. code-block:: python import pyperclip html = df.style.to_html() pyperclip.copy(html) method","title":"pandas.core.generic.NDFrame.to_clipboard"},{"location":"api/pipen.channel/#pandascoregenericndframeto_xarray","text":"</> Return an xarray object from the pandas object. Returns (xarray.DataArray or xarray.Dataset) Data in the pandas structure converted to Dataset if the object isa DataFrame, or a DataArray if the object is a Series. See Also DataFrame.to_hdf : Write DataFrame to an HDF5 file.DataFrame.to_parquet : Write a DataFrame to the binary parquet format. Notes See the xarray docs <https://xarray.pydata.org/en/stable/> __ Examples >>> df = pd . DataFrame ([( 'falcon' , 'bird' , 389.0 , 2 ), ... ( 'parrot' , 'bird' , 24.0 , 2 ), ... ( 'lion' , 'mammal' , 80.5 , 4 ), ... ( 'monkey' , 'mammal' , np . nan , 4 )], ... columns = [ 'name' , 'class' , 'max_speed' , ... 'num_legs' ]) >>> df name class max_speed num_legs 0 falcon bird 389.0 2 1 parrot bird 24.0 2 2 lion mammal 80.5 4 3 monkey mammal NaN 4 >>> df . to_xarray () # doctest: +SKIP < xarray . Dataset > Dimensions : ( index : 4 ) Coordinates : * index ( index ) int64 32 B 0 1 2 3 Data variables : name ( index ) object 32 B 'falcon' 'parrot' 'lion' 'monkey' class ( index ) object 32 B 'bird' 'bird' 'mammal' 'mammal' max_speed ( index ) float64 32 B 389.0 24.0 80.5 nan num_legs ( index ) int64 32 B 2 2 4 4 >>> df [ 'max_speed' ] . to_xarray () # doctest: +SKIP < xarray . DataArray 'max_speed' ( index : 4 ) > array ([ 389. , 24. , 80.5 , nan ]) Coordinates : * index ( index ) int64 0 1 2 3 >>> dates = pd . to_datetime ([ '2018-01-01' , '2018-01-01' , ... '2018-01-02' , '2018-01-02' ]) >>> df_multiindex = pd . DataFrame ({ 'date' : dates , ... 'animal' : [ 'falcon' , 'parrot' , ... 'falcon' , 'parrot' ], ... 'speed' : [ 350 , 18 , 361 , 15 ]}) >>> df_multiindex = df_multiindex . set_index ([ 'date' , 'animal' ]) >>> df_multiindex speed date animal 2018 - 01 - 01 falcon 350 parrot 18 2018 - 01 - 02 falcon 361 parrot 15 >>> df_multiindex . to_xarray () # doctest: +SKIP < xarray . Dataset > Dimensions : ( date : 2 , animal : 2 ) Coordinates : * date ( date ) datetime64 [ ns ] 2018 - 01 - 01 2018 - 01 - 02 * animal ( animal ) object 'falcon' 'parrot' Data variables : speed ( date , animal ) int64 350 18 361 15 method","title":"pandas.core.generic.NDFrame.to_xarray"},{"location":"api/pipen.channel/#pandascoregenericndframeto_latex","text":"</> Render object to a LaTeX tabular, longtable, or nested table. Requires \\usepackage{{booktabs}} . The output can be copy/pasted into a main LaTeX document or read from an external file with \\input{{table.tex}} . .. versionchanged:: 2.0.0 Refactored to use the Styler implementation via jinja2 templating. Parameters buf (str, Path or StringIO-like, optional, default None) \u2014 Buffer to write to. If None, the output is returned as a string. columns (list of label, optional) \u2014 The subset of columns to write. Writes all columns by default. header (bool or list of str, default True) \u2014 Write out the column names. If a list of strings is given,it is assumed to be aliases for the column names. index (bool, default True) \u2014 Write row names (index). na_rep (str, default 'NaN') \u2014 Missing data representation. formatters (list of functions or dict of {{str: function}}, optional) \u2014 Formatter functions to apply to columns' elements by position orname. The result of each function must be a unicode string. List must be of length equal to the number of columns. float_format (one-parameter function or str, optional, default None) \u2014 Formatter for floating point numbers. For example float_format=\"%.2f\" and float_format=\"{{:0.2f}}\".format will both result in 0.1234 being formatted as 0.12. sparsify (bool, optional) \u2014 Set to False for a DataFrame with a hierarchical index to printevery multiindex key at each row. By default, the value will be read from the config module. index_names (bool, default True) \u2014 Prints the names of the indexes. bold_rows (bool, default False) \u2014 Make the row labels bold in the output. column_format (str, optional) \u2014 The columns format as specified in LaTeX table format<https://en.wikibooks.org/wiki/LaTeX/Tables> __ e.g. 'rcl' for 3 columns. By default, 'l' will be used for all columns except columns of numbers, which default to 'r'. longtable (bool, optional) \u2014 Use a longtable environment instead of tabular. Requiresadding a \\usepackage{{longtable}} to your LaTeX preamble. By default, the value will be read from the pandas config module, and set to True if the option styler.latex.environment is \"longtable\" . .. versionchanged:: 2.0.0 The pandas option affecting this argument has changed. escape (bool, optional) \u2014 By default, the value will be read from the pandas configmodule and set to True if the option styler.format.escape is \"latex\" . When set to False prevents from escaping latex special characters in column names. .. versionchanged:: 2.0.0 The pandas option affecting this argument has changed, as has the default value to False . encoding (str, optional) \u2014 A string representing the encoding to use in the output file,defaults to 'utf-8'. decimal (str, default '.') \u2014 Character recognized as decimal separator, e.g. ',' in Europe. multicolumn (bool, default True) \u2014 Use \\multicolumn to enhance MultiIndex columns.The default will be read from the config module, and is set as the option styler.sparse.columns . .. versionchanged:: 2.0.0 The pandas option affecting this argument has changed. multicolumn_format (str, default 'r') \u2014 The alignment for multicolumns, similar to column_format The default will be read from the config module, and is set as the option styler.latex.multicol_align . .. versionchanged:: 2.0.0 The pandas option affecting this argument has changed, as has the default value to \"r\". multirow (bool, default True) \u2014 Use \\multirow to enhance MultiIndex rows. Requires adding a\\usepackage{{multirow}} to your LaTeX preamble. Will print centered labels (instead of top-aligned) across the contained rows, separating groups via clines. The default will be read from the pandas config module, and is set as the option styler.sparse.index . .. versionchanged:: 2.0.0 The pandas option affecting this argument has changed, as has the default value to True . caption (str or tuple, optional) \u2014 Tuple (full_caption, short_caption),which results in \\caption[short_caption]{{full_caption}} ; if a single string is passed, no short caption will be set. label (str, optional) \u2014 The LaTeX label to be placed inside \\label{{}} in the output.This is used with \\ref{{}} in the main .tex file. position (str, optional) \u2014 The LaTeX positional argument for tables, to be placed after \\begin{{}} in the output. Returns (str or None) If buf is None, returns the result as a string. Otherwise returns None. See Also io.formats.style.Styler.to_latex : Render a DataFrame to LaTeX with conditional formatting. DataFrame.to_string : Render a DataFrame to a console-friendly tabular output. DataFrame.to_html : Render a DataFrame as an HTML table. Notes As of v2.0.0 this method has changed to use the Styler implementation as part of :meth: .Styler.to_latex via jinja2 templating. This means that jinja2 is a requirement, and needs to be installed, for this method to function. It is advised that users switch to using Styler, since that implementation is more frequently updated and contains much more flexibility with the output. Examples Convert a general DataFrame to LaTeX with formatting: >>> df = pd . DataFrame ( dict ( name = [ 'Raphael' , 'Donatello' ], ... age = [ 26 , 45 ], ... height = [ 181.23 , 177.65 ])) >>> print ( df . to_latex ( index = False , ... formatters = { \"name\" : str . upper }, ... float_format = \" {:.1f} \" . format , ... )) # doctest: +SKIP \\ begin { tabular }{ lrr } \\ toprule name & age & height \\\\ \\ midrule RAPHAEL & 26 & 181.2 \\\\ DONATELLO & 45 & 177.7 \\\\ \\ bottomrule \\ end { tabular } method","title":"pandas.core.generic.NDFrame.to_latex"},{"location":"api/pipen.channel/#pandascoregenericndframeto_csv","text":"</> Write object to a comma-separated values (csv) file. Parameters path_or_buf (str, path object, file-like object, or None, default None) \u2014 String, path object (implementing os.PathLike[str]), or file-likeobject implementing a write() function. If None, the result is returned as a string. If a non-binary file object is passed, it should be opened with newline='' , disabling universal newlines. If a binary file object is passed, mode might need to contain a 'b' . sep (str, default ',') \u2014 String of length 1. Field delimiter for the output file. na_rep (str, default '') \u2014 Missing data representation. float_format (str, Callable, default None) \u2014 Format string for floating point numbers. If a Callable is given, it takesprecedence over other numeric formatting parameters, like decimal. columns (sequence, optional) \u2014 Columns to write. header (bool or list of str, default True) \u2014 Write out the column names. If a list of strings is given it isassumed to be aliases for the column names. index (bool, default True) \u2014 Write row names (index). index_label (str or sequence, or False, default None) \u2014 Column label for index column(s) if desired. If None is given, and header and index are True, then the index names are used. A sequence should be given if the object uses MultiIndex. If False do not print fields for index names. Use index_label=False for easier importing in R. mode ({'w', 'x', 'a'}, default 'w') \u2014 Forwarded to either open(mode=) or fsspec.open(mode=) to controlthe file opening. Typical values include: 'w', truncate the file first. 'x', exclusive creation, failing if the file already exists. 'a', append to the end of file if it exists. encoding (str, optional) \u2014 A string representing the encoding to use in the output file,defaults to 'utf-8'. encoding is not supported if path_or_buf is a non-binary file object. compression (str or dict, default 'infer') \u2014 For on-the-fly compression of the output data. If 'infer' and 'path_or_buf' ispath-like, then detect compression from the following extensions: '.gz', '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2' (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of { 'zip' , 'gzip' , 'bz2' , 'zstd' , 'xz' , 'tar' } and other key-value pairs are forwarded to zipfile.ZipFile , gzip.GzipFile , bz2.BZ2File , zstandard.ZstdCompressor , lzma.LZMAFile or tarfile.TarFile , respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1} . .. versionadded:: 1.5.0 Added support for .tar files. May be a dict with key 'method' as compression mode and other entries as additional compression options if compression mode is 'zip'. Passing compression options as keys in dict is supported for compression modes 'gzip', 'bz2', 'zstd', and 'zip'. quoting (optional constant from csv module) \u2014 Defaults to csv.QUOTE_MINIMAL. If you have set a float_format then floats are converted to strings and thus csv.QUOTE_NONNUMERIC will treat them as non-numeric. quotechar (str, default '\\\"') \u2014 String of length 1. Character used to quote fields. lineterminator (str, optional) \u2014 The newline character or character sequence to use in the outputfile. Defaults to os.linesep , which depends on the OS in which this method is called ('\\n' for linux, '\\r\\n' for Windows, i.e.). .. versionchanged:: 1.5.0 Previously was line_terminator, changed for consistency with read_csv and the standard library 'csv' module. chunksize (int or None) \u2014 Rows to write at a time. date_format (str, default None) \u2014 Format string for datetime objects. doublequote (bool, default True) \u2014 Control quoting of quotechar inside a field. escapechar (str, default None) \u2014 String of length 1. Character used to escape sep and quotechar when appropriate. decimal (str, default '.') \u2014 Character recognized as decimal separator. E.g. use ',' forEuropean data. errors (str, default 'strict') \u2014 Specifies how encoding and decoding errors are to be handled.See the errors argument for :func: open for a full list of options. storage_options (dict, optional) \u2014 Extra options that make sense for a particular storage connection, e.g.host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to fsspec.open . Please see fsspec and urllib for more details, and for more examples on storage options refer here <https://pandas.pydata.org/docs/user_guide/io.html? highlight=storage_options#reading-writing-remote-files> _. Returns (None or str) If path_or_buf is None, returns the resulting csv format as astring. Otherwise returns None. See Also read_csv : Load a CSV file into a DataFrame.to_excel : Write DataFrame to an Excel file. Examples Create 'out.csv' containing 'df' without indices >>> df = pd . DataFrame ({ 'name' : [ 'Raphael' , 'Donatello' ], ... 'mask' : [ 'red' , 'purple' ], ... 'weapon' : [ 'sai' , 'bo staff' ]}) >>> df . to_csv ( 'out.csv' , index = False ) # doctest: +SKIP Create 'out.zip' containing 'out.csv' >>> df . to_csv ( index = False ) 'name,mask,weapon \\n Raphael,red,sai \\n Donatello,purple,bo staff \\n ' >>> compression_opts = dict ( method = 'zip' , ... archive_name = 'out.csv' ) # doctest: +SKIP >>> df . to_csv ( 'out.zip' , index = False , ... compression = compression_opts ) # doctest: +SKIP To write a csv file to a new folder or nested folder you will first need to create it using either Pathlib or os: >>> from pathlib import Path # doctest: +SKIP >>> filepath = Path ( 'folder/subfolder/out.csv' ) # doctest: +SKIP >>> filepath . parent . mkdir ( parents = True , exist_ok = True ) # doctest: +SKIP >>> df . to_csv ( filepath ) # doctest: +SKIP >>> import os # doctest: +SKIP >>> os . makedirs ( 'folder/subfolder' , exist_ok = True ) # doctest: +SKIP >>> df . to_csv ( 'folder/subfolder/out.csv' ) # doctest: +SKIP method","title":"pandas.core.generic.NDFrame.to_csv"},{"location":"api/pipen.channel/#pandascoregenericndframetake","text":"</> Return the elements in the given positional indices along an axis. This means that we are not indexing according to actual values in the index attribute of the object. We are indexing according to the actual position of the element in the object. Parameters indices (array-like) \u2014 An array of ints indicating which positions to take. axis ({0 or 'index', 1 or 'columns', None}, default 0) \u2014 The axis on which to select elements. 0 means that we areselecting rows, 1 means that we are selecting columns. For Series this parameter is unused and defaults to 0. **kwargs \u2014 For compatibility with :meth: numpy.take . Has no effect on theoutput. Returns (same type as caller) An array-like containing the elements taken from the object. See Also DataFrame.loc : Select a subset of a DataFrame by labels.DataFrame.iloc : Select a subset of a DataFrame by positions. numpy.take : Take elements from an array along an axis. Examples >>> df = pd . DataFrame ([( 'falcon' , 'bird' , 389.0 ), ... ( 'parrot' , 'bird' , 24.0 ), ... ( 'lion' , 'mammal' , 80.5 ), ... ( 'monkey' , 'mammal' , np . nan )], ... columns = [ 'name' , 'class' , 'max_speed' ], ... index = [ 0 , 2 , 3 , 1 ]) >>> df name class max_speed 0 falcon bird 389.0 2 parrot bird 24.0 3 lion mammal 80.5 1 monkey mammal NaN Take elements at positions 0 and 3 along the axis 0 (default). Note how the actual indices selected (0 and 1) do not correspond to our selected indices 0 and 3. That's because we are selecting the 0th and 3rd rows, not rows whose indices equal 0 and 3. >>> df . take ([ 0 , 3 ]) name class max_speed 0 falcon bird 389.0 1 monkey mammal NaN Take elements at indices 1 and 2 along the axis 1 (column selection). >>> df . take ([ 1 , 2 ], axis = 1 ) class max_speed 0 bird 389.0 2 bird 24.0 3 mammal 80.5 1 mammal NaN We may take elements using negative integers for positive indices, starting from the end of the object, just like with Python lists. >>> df . take ([ - 1 , - 2 ]) name class max_speed 1 monkey mammal NaN 3 lion mammal 80.5 method","title":"pandas.core.generic.NDFrame.take"},{"location":"api/pipen.channel/#pandascoregenericndframexs","text":"</> Return cross-section from the Series/DataFrame. This method takes a key argument to select data at a particular level of a MultiIndex. Parameters key (label or tuple of label) \u2014 Label contained in the index, or partially in a MultiIndex. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Axis to retrieve cross-section on. level (object, defaults to first n levels (n=1 or len(key))) \u2014 In case of a key partially contained in a MultiIndex, indicatewhich levels are used. Levels can be referred by label or position. drop_level (bool, default True) \u2014 If False, returns object with same levels as self. Returns (Series or DataFrame) Cross-section from the original Series or DataFramecorresponding to the selected index levels. See Also DataFrame.loc : Access a group of rows and columns by label(s) or a boolean array. DataFrame.iloc : Purely integer-location based indexing for selection by position. Notes xs can not be used to set values. MultiIndex Slicers is a generic way to get/set values on any level or levels. It is a superset of xs functionality, see :ref: MultiIndex Slicers <advanced.mi_slicers> . Examples >>> d = { 'num_legs' : [ 4 , 4 , 2 , 2 ], ... 'num_wings' : [ 0 , 0 , 2 , 2 ], ... 'class' : [ 'mammal' , 'mammal' , 'mammal' , 'bird' ], ... 'animal' : [ 'cat' , 'dog' , 'bat' , 'penguin' ], ... 'locomotion' : [ 'walks' , 'walks' , 'flies' , 'walks' ]} >>> df = pd . DataFrame ( data = d ) >>> df = df . set_index ([ 'class' , 'animal' , 'locomotion' ]) >>> df num_legs num_wings class animal locomotion mammal cat walks 4 0 dog walks 4 0 bat flies 2 2 bird penguin walks 2 2 Get values at specified index >>> df . xs ( 'mammal' ) num_legs num_wings animal locomotion cat walks 4 0 dog walks 4 0 bat flies 2 2 Get values at several indexes >>> df . xs (( 'mammal' , 'dog' , 'walks' )) num_legs 4 num_wings 0 Name : ( mammal , dog , walks ), dtype : int64 Get values at specified index and level >>> df . xs ( 'cat' , level = 1 ) num_legs num_wings class locomotion mammal walks 4 0 Get values at several indexes and levels >>> df . xs (( 'bird' , 'walks' ), ... level = [ 0 , 'locomotion' ]) num_legs num_wings animal penguin 2 2 Get values at specified column and axis >>> df . xs ( 'num_wings' , axis = 1 ) class animal locomotion mammal cat walks 0 dog walks 0 bat flies 2 bird penguin walks 2 Name : num_wings , dtype : int64 method","title":"pandas.core.generic.NDFrame.xs"},{"location":"api/pipen.channel/#pandascoregenericndframedelitem","text":"</> Delete item method","title":"pandas.core.generic.NDFrame.delitem"},{"location":"api/pipen.channel/#pandascoregenericndframeget","text":"</> Get item from object for given key (ex: DataFrame column). Returns default value if not found. Examples >>> df = pd . DataFrame ( ... [ ... [ 24.3 , 75.7 , \"high\" ], ... [ 31 , 87.8 , \"high\" ], ... [ 22 , 71.6 , \"medium\" ], ... [ 35 , 95 , \"medium\" ], ... ], ... columns = [ \"temp_celsius\" , \"temp_fahrenheit\" , \"windspeed\" ], ... index = pd . date_range ( start = \"2014-02-12\" , end = \"2014-02-15\" , freq = \"D\" ), ... ) >>> df temp_celsius temp_fahrenheit windspeed 2014 - 02 - 12 24.3 75.7 high 2014 - 02 - 13 31.0 87.8 high 2014 - 02 - 14 22.0 71.6 medium 2014 - 02 - 15 35.0 95.0 medium >>> df . get ([ \"temp_celsius\" , \"windspeed\" ]) temp_celsius windspeed 2014 - 02 - 12 24.3 high 2014 - 02 - 13 31.0 high 2014 - 02 - 14 22.0 medium 2014 - 02 - 15 35.0 medium >>> ser = df [ 'windspeed' ] >>> ser . get ( '2014-02-13' ) 'high' If the key isn't found, the default value will be used. >>> df . get ([ \"temp_celsius\" , \"temp_kelvin\" ], default = \"default_value\" ) 'default_value' >>> ser . get ( '2014-02-10' , '[unknown]' ) '[unknown]' method","title":"pandas.core.generic.NDFrame.get"},{"location":"api/pipen.channel/#pandascoregenericndframereindex_like","text":"</> Return an object with matching indices as other object. Conform the object to the same index on all axes. Optional filling logic, placing NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False. Parameters other (Object of the same data type) \u2014 Its row and column indices are used to define the new indicesof this object. method ({None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'}) \u2014 Method to use for filling holes in reindexed DataFrame.Please note: this is only applicable to DataFrames/Series with a monotonically increasing/decreasing index. None (default): don't fill gaps pad / ffill: propagate last valid observation forward to next valid backfill / bfill: use next valid observation to fill gap nearest: use nearest valid observations to fill gap. copy (bool, default True) \u2014 Return a new object, even if the passed indexes are the same. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` limit (int, default None) \u2014 Maximum number of consecutive labels to fill for inexact matches. tolerance (optional) \u2014 Maximum distance between original and new labels for inexactmatches. The values of the index at the matching locations must satisfy the equation abs(index[indexer] - target) <= tolerance . Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index's type. Returns (Series or DataFrame) Same type as caller, but with changed indices on each axis. See Also DataFrame.set_index : Set row labels.DataFrame.reset_index : Remove row labels or move them to new columns. DataFrame.reindex : Change to new indices or expand indices. Notes Same as calling .reindex(index=other.index, columns=other.columns,...) . Examples >>> df1 = pd . DataFrame ([[ 24.3 , 75.7 , 'high' ], ... [ 31 , 87.8 , 'high' ], ... [ 22 , 71.6 , 'medium' ], ... [ 35 , 95 , 'medium' ]], ... columns = [ 'temp_celsius' , 'temp_fahrenheit' , ... 'windspeed' ], ... index = pd . date_range ( start = '2014-02-12' , ... end = '2014-02-15' , freq = 'D' )) >>> df1 temp_celsius temp_fahrenheit windspeed 2014 - 02 - 12 24.3 75.7 high 2014 - 02 - 13 31.0 87.8 high 2014 - 02 - 14 22.0 71.6 medium 2014 - 02 - 15 35.0 95.0 medium >>> df2 = pd . DataFrame ([[ 28 , 'low' ], ... [ 30 , 'low' ], ... [ 35.1 , 'medium' ]], ... columns = [ 'temp_celsius' , 'windspeed' ], ... index = pd . DatetimeIndex ([ '2014-02-12' , '2014-02-13' , ... '2014-02-15' ])) >>> df2 temp_celsius windspeed 2014 - 02 - 12 28.0 low 2014 - 02 - 13 30.0 low 2014 - 02 - 15 35.1 medium >>> df2 . reindex_like ( df1 ) temp_celsius temp_fahrenheit windspeed 2014 - 02 - 12 28.0 NaN low 2014 - 02 - 13 30.0 NaN low 2014 - 02 - 14 NaN NaN NaN 2014 - 02 - 15 35.1 NaN medium method","title":"pandas.core.generic.NDFrame.reindex_like"},{"location":"api/pipen.channel/#pandascoregenericndframeadd_prefix","text":"</> Prefix labels with string prefix . For Series, the row labels are prefixed. For DataFrame, the column labels are prefixed. Parameters prefix (str) \u2014 The string to add before each label. axis ({0 or 'index', 1 or 'columns', None}, default None) \u2014 Axis to add prefix on .. versionadded:: 2.0.0 Returns (Series or DataFrame) New Series or DataFrame with updated labels. See Also Series.add_suffix: Suffix row labels with string suffix .DataFrame.add_suffix: Suffix column labels with string suffix . Examples >>> s = pd . Series ([ 1 , 2 , 3 , 4 ]) >>> s 0 1 1 2 2 3 3 4 dtype : int64 >>> s . add_prefix ( 'item_' ) item_0 1 item_1 2 item_2 3 item_3 4 dtype : int64 >>> df = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 , 4 ], 'B' : [ 3 , 4 , 5 , 6 ]}) >>> df A B 0 1 3 1 2 4 2 3 5 3 4 6 >>> df . add_prefix ( 'col_' ) col_A col_B 0 1 3 1 2 4 2 3 5 3 4 6 method","title":"pandas.core.generic.NDFrame.add_prefix"},{"location":"api/pipen.channel/#pandascoregenericndframeadd_suffix","text":"</> Suffix labels with string suffix . For Series, the row labels are suffixed. For DataFrame, the column labels are suffixed. Parameters suffix (str) \u2014 The string to add after each label. axis ({0 or 'index', 1 or 'columns', None}, default None) \u2014 Axis to add suffix on .. versionadded:: 2.0.0 Returns (Series or DataFrame) New Series or DataFrame with updated labels. See Also Series.add_prefix: Prefix row labels with string prefix .DataFrame.add_prefix: Prefix column labels with string prefix . Examples >>> s = pd . Series ([ 1 , 2 , 3 , 4 ]) >>> s 0 1 1 2 2 3 3 4 dtype : int64 >>> s . add_suffix ( '_item' ) 0 _item 1 1 _item 2 2 _item 3 3 _item 4 dtype : int64 >>> df = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 , 4 ], 'B' : [ 3 , 4 , 5 , 6 ]}) >>> df A B 0 1 3 1 2 4 2 3 5 3 4 6 >>> df . add_suffix ( '_col' ) A_col B_col 0 1 3 1 2 4 2 3 5 3 4 6 method","title":"pandas.core.generic.NDFrame.add_suffix"},{"location":"api/pipen.channel/#pandascoregenericndframefilter","text":"</> Subset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index. Parameters items (list-like) \u2014 Keep labels from axis which are in items. like (str) \u2014 Keep labels from axis for which \"like in label == True\". regex (str (regular expression)) \u2014 Keep labels from axis for which re.search(regex, label) == True. axis ({0 or 'index', 1 or 'columns', None}, default None) \u2014 The axis to filter on, expressed either as an index (int)or axis name (str). By default this is the info axis, 'columns' for DataFrame. For Series this parameter is unused and defaults to None . See Also DataFrame.loc : Access a group of rows and columns by label(s) or a boolean array. Notes The items , like , and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with [] . Examples >>> df = pd . DataFrame ( np . array (([ 1 , 2 , 3 ], [ 4 , 5 , 6 ])), ... index = [ 'mouse' , 'rabbit' ], ... columns = [ 'one' , 'two' , 'three' ]) >>> df one two three mouse 1 2 3 rabbit 4 5 6 >>> # select columns by name >>> df . filter ( items = [ 'one' , 'three' ]) one three mouse 1 3 rabbit 4 6 >>> # select columns by regular expression >>> df . filter ( regex = 'e$' , axis = 1 ) one three mouse 1 3 rabbit 4 6 >>> # select rows containing 'bbi' >>> df . filter ( like = 'bbi' , axis = 0 ) one two three rabbit 4 5 6 method","title":"pandas.core.generic.NDFrame.filter"},{"location":"api/pipen.channel/#pandascoregenericndframehead","text":"</> Return the first n rows. This function returns the first n rows for the object based on position. It is useful for quickly testing if your object has the right type of data in it. For negative values of n , this function returns all rows except the last |n| rows, equivalent to df[:n] . If n is larger than the number of rows, this function returns all rows. Parameters n (int, default 5) \u2014 Number of rows to select. Returns (same type as caller) The first n rows of the caller object. See Also DataFrame.tail: Returns the last n rows. Examples >>> df = pd . DataFrame ({ 'animal' : [ 'alligator' , 'bee' , 'falcon' , 'lion' , ... 'monkey' , 'parrot' , 'shark' , 'whale' , 'zebra' ]}) >>> df animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey 5 parrot 6 shark 7 whale 8 zebra Viewing the first 5 lines >>> df . head () animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey Viewing the first n lines (three in this case) >>> df . head ( 3 ) animal 0 alligator 1 bee 2 falcon For negative values of n >>> df . head ( - 3 ) animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey 5 parrot method","title":"pandas.core.generic.NDFrame.head"},{"location":"api/pipen.channel/#pandascoregenericndframetail","text":"</> Return the last n rows. This function returns last n rows from the object based on position. It is useful for quickly verifying data, for example, after sorting or appending rows. For negative values of n , this function returns all rows except the first |n| rows, equivalent to df[|n|:] . If n is larger than the number of rows, this function returns all rows. Parameters n (int, default 5) \u2014 Number of rows to select. Returns (type of caller) The last n rows of the caller object. See Also DataFrame.head : The first n rows of the caller object. Examples >>> df = pd . DataFrame ({ 'animal' : [ 'alligator' , 'bee' , 'falcon' , 'lion' , ... 'monkey' , 'parrot' , 'shark' , 'whale' , 'zebra' ]}) >>> df animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey 5 parrot 6 shark 7 whale 8 zebra Viewing the last 5 lines >>> df . tail () animal 4 monkey 5 parrot 6 shark 7 whale 8 zebra Viewing the last n lines (three in this case) >>> df . tail ( 3 ) animal 6 shark 7 whale 8 zebra For negative values of n >>> df . tail ( - 3 ) animal 3 lion 4 monkey 5 parrot 6 shark 7 whale 8 zebra method","title":"pandas.core.generic.NDFrame.tail"},{"location":"api/pipen.channel/#pandascoregenericndframesample","text":"</> Return a random sample of items from an axis of object. You can use random_state for reproducibility. Parameters n (int, optional) \u2014 Number of items from axis to return. Cannot be used with frac .Default = 1 if frac = None. frac (float, optional) \u2014 Fraction of axis items to return. Cannot be used with n . replace (bool, default False) \u2014 Allow or disallow sampling of the same row more than once. weights (str or ndarray-like, optional) \u2014 Default 'None' results in equal probability weighting.If passed a Series, will align with target object on index. Index values in weights not found in sampled object will be ignored and index values in sampled object not in weights will be assigned weights of zero. If called on a DataFrame, will accept the name of a column when axis = 0. Unless weights are a Series, weights must be same length as axis being sampled. If weights do not sum to 1, they will be normalized to sum to 1. Missing values in the weights column will be treated as zero. Infinite values not allowed. random_state (int, array-like, BitGenerator, np.random.RandomState, np.random.Generator, optional) \u2014 If int, array-like, or BitGenerator, seed for random number generator.If np.random.RandomState or np.random.Generator, use as given. .. versionchanged:: 1.4.0 np.random.Generator objects now accepted axis ({0 or 'index', 1 or 'columns', None}, default None) \u2014 Axis to sample. Accepts axis number or name. Default is stat axisfor given data type. For Series this parameter is unused and defaults to None . ignore_index (bool, default False) \u2014 If True, the resulting index will be labeled 0, 1, \u2026, n - 1. .. versionadded:: 1.3.0 Returns (Series or DataFrame) A new object of same type as caller containing n items randomlysampled from the caller object. See Also DataFrameGroupBy.sample: Generates random samples from each group of a DataFrame object. SeriesGroupBy.sample: Generates random samples from each group of a Series object. numpy.random.choice: Generates a random sample from a given 1-D numpy array. Notes If frac > 1, replacement should be set to True . Examples >>> df = pd . DataFrame ({ 'num_legs' : [ 2 , 4 , 8 , 0 ], ... 'num_wings' : [ 2 , 0 , 0 , 0 ], ... 'num_specimen_seen' : [ 10 , 2 , 1 , 8 ]}, ... index = [ 'falcon' , 'dog' , 'spider' , 'fish' ]) >>> df num_legs num_wings num_specimen_seen falcon 2 2 10 dog 4 0 2 spider 8 0 1 fish 0 0 8 Extract 3 random elements from the Series df['num_legs'] : Note that we use random_state to ensure the reproducibility of the examples. >>> df [ 'num_legs' ] . sample ( n = 3 , random_state = 1 ) fish 0 spider 8 falcon 2 Name : num_legs , dtype : int64 A random 50% sample of the DataFrame with replacement: >>> df . sample ( frac = 0.5 , replace = True , random_state = 1 ) num_legs num_wings num_specimen_seen dog 4 0 2 fish 0 0 8 An upsample sample of the DataFrame with replacement: Note that replace parameter has to be True for frac parameter > 1. >>> df . sample ( frac = 2 , replace = True , random_state = 1 ) num_legs num_wings num_specimen_seen dog 4 0 2 fish 0 0 8 falcon 2 2 10 falcon 2 2 10 fish 0 0 8 dog 4 0 2 fish 0 0 8 dog 4 0 2 Using a DataFrame column as weights. Rows with larger value in the num_specimen_seen column are more likely to be sampled. >>> df . sample ( n = 2 , weights = 'num_specimen_seen' , random_state = 1 ) num_legs num_wings num_specimen_seen falcon 2 2 10 fish 0 0 8 method","title":"pandas.core.generic.NDFrame.sample"},{"location":"api/pipen.channel/#pandascoregenericndframepipe","text":"</> Apply chainable functions that expect Series or DataFrames. Parameters func (function) \u2014 Function to apply to the Series/DataFrame. args , and kwargs are passed into func . Alternatively a (callable, data_keyword) tuple where data_keyword is a string indicating the keyword of callable that expects the Series/DataFrame. *args (iterable, optional) \u2014 Positional arguments passed into func . **kwargs (mapping, optional) \u2014 A dictionary of keyword arguments passed into func . See Also DataFrame.apply : Apply a function along input axis of DataFrame.DataFrame.map : Apply a function elementwise on a whole DataFrame. Series.map : Apply a mapping correspondence on a :class: ~pandas.Series . Notes Use .pipe when chaining together functions that expect Series, DataFrames or GroupBy objects. Examples Constructing a income DataFrame from a dictionary. >>> data = [[ 8000 , 1000 ], [ 9500 , np . nan ], [ 5000 , 2000 ]] >>> df = pd . DataFrame ( data , columns = [ 'Salary' , 'Others' ]) >>> df Salary Others 0 8000 1000.0 1 9500 NaN 2 5000 2000.0 Functions that perform tax reductions on an income DataFrame. >>> def subtract_federal_tax ( df ): ... return df * 0.9 >>> def subtract_state_tax ( df , rate ): ... return df * ( 1 - rate ) >>> def subtract_national_insurance ( df , rate , rate_increase ): ... new_rate = rate + rate_increase ... return df * ( 1 - new_rate ) Instead of writing >>> subtract_national_insurance ( ... subtract_state_tax ( subtract_federal_tax ( df ), rate = 0.12 ), ... rate = 0.05 , ... rate_increase = 0.02 ) # doctest: +SKIP You can write >>> ( ... df . pipe ( subtract_federal_tax ) ... . pipe ( subtract_state_tax , rate = 0.12 ) ... . pipe ( subtract_national_insurance , rate = 0.05 , rate_increase = 0.02 ) ... ) Salary Others 0 5892.48 736.56 1 6997.32 NaN 2 3682.80 1473.12 If you have a function that takes the data as (say) the second argument, pass a tuple indicating which keyword expects the data. For example, suppose national_insurance takes its data as df in the second argument: >>> def subtract_national_insurance ( rate , df , rate_increase ): ... new_rate = rate + rate_increase ... return df * ( 1 - new_rate ) >>> ( ... df . pipe ( subtract_federal_tax ) ... . pipe ( subtract_state_tax , rate = 0.12 ) ... . pipe ( ... ( subtract_national_insurance , 'df' ), ... rate = 0.05 , ... rate_increase = 0.02 ... ) ... ) Salary Others 0 5892.48 736.56 1 6997.32 NaN 2 3682.80 1473.12 method","title":"pandas.core.generic.NDFrame.pipe"},{"location":"api/pipen.channel/#pandascoregenericndframefinalize","text":"</> Propagate metadata from other to self. Parameters other (the object from which to get the attributes that we are going) \u2014 to propagate method (str, optional) \u2014 A passed method name providing context on where __finalize__ was called. .. warning:: The value passed as method are not currently considered stable across pandas releases. method","title":"pandas.core.generic.NDFrame.finalize"},{"location":"api/pipen.channel/#pandascoregenericndframegetattr","text":"</> After regular attribute access, try looking up the nameThis allows simpler access to columns for interactive use. method","title":"pandas.core.generic.NDFrame.getattr"},{"location":"api/pipen.channel/#pandascoregenericndframesetattr","text":"</> After regular attribute access, try setting the nameThis allows simpler access to columns for interactive use. method","title":"pandas.core.generic.NDFrame.setattr"},{"location":"api/pipen.channel/#pandascoregenericndframeastype","text":"</> Cast a pandas object to a specified dtype dtype . Parameters dtype (str, data type, Series or Mapping of column name -> data type) \u2014 Use a str, numpy.dtype, pandas.ExtensionDtype or Python type tocast entire pandas object to the same type. Alternatively, use a mapping, e.g. {col: dtype, ...}, where col is a column label and dtype is a numpy.dtype or Python type to cast one or more of the DataFrame's columns to column-specific types. copy (bool, default True) \u2014 Return a copy when copy=True (be very careful setting copy=False as changes to values then may propagate to other pandas objects). .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` errors ({'raise', 'ignore'}, default 'raise') \u2014 Control raising of exceptions on invalid data for provided dtype. raise : allow exceptions to be raised ignore : suppress exceptions. On error return original object. See Also to_datetime : Convert argument to datetime.to_timedelta : Convert argument to timedelta. to_numeric : Convert argument to a numeric type. numpy.ndarray.astype : Cast a numpy array to a specified type. Notes .. versionchanged:: 2.0.0 Using ``astype`` to convert from timezone-naive dtype to timezone-aware dtype will raise an exception. Use :meth:`Series.dt.tz_localize` instead. Examples Create a DataFrame: >>> d = { 'col1' : [ 1 , 2 ], 'col2' : [ 3 , 4 ]} >>> df = pd . DataFrame ( data = d ) >>> df . dtypes col1 int64 col2 int64 dtype : object Cast all columns to int32: >>> df . astype ( 'int32' ) . dtypes col1 int32 col2 int32 dtype : object Cast col1 to int32 using a dictionary: >>> df . astype ({ 'col1' : 'int32' }) . dtypes col1 int32 col2 int64 dtype : object Create a series: >>> ser = pd . Series ([ 1 , 2 ], dtype = 'int32' ) >>> ser 0 1 1 2 dtype : int32 >>> ser . astype ( 'int64' ) 0 1 1 2 dtype : int64 Convert to categorical type: >>> ser . astype ( 'category' ) 0 1 1 2 dtype : category Categories ( 2 , int32 ): [ 1 , 2 ] Convert to ordered categorical type with custom ordering: >>> from pandas.api.types import CategoricalDtype >>> cat_dtype = CategoricalDtype ( ... categories = [ 2 , 1 ], ordered = True ) >>> ser . astype ( cat_dtype ) 0 1 1 2 dtype : category Categories ( 2 , int64 ): [ 2 < 1 ] Create a series of dates: >>> ser_date = pd . Series ( pd . date_range ( '20200101' , periods = 3 )) >>> ser_date 0 2020 - 01 - 01 1 2020 - 01 - 02 2 2020 - 01 - 03 dtype : datetime64 [ ns ] method","title":"pandas.core.generic.NDFrame.astype"},{"location":"api/pipen.channel/#pandascoregenericndframecopy","text":"</> Make a copy of this object's indices and data. When deep=True (default), a new object will be created with a copy of the calling object's data and indices. Modifications to the data or indices of the copy will not be reflected in the original object (see notes below). When deep=False , a new object will be created without copying the calling object's data or index (only references to the data and index are copied). Any changes to the data of the original will be reflected in the shallow copy (and vice versa). .. note:: The deep=False behaviour as described above will change in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that the \"shallow\" copy is that is returned with deep=False will still avoid making an eager copy, but changes to the data of the original will no longer be reflected in the shallow copy (or vice versa). Instead, it makes use of a lazy (deferred) copy mechanism that will copy the data only when any changes to the original or shallow copy is made. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` Parameters deep (bool, default True) \u2014 Make a deep copy, including a copy of the data and the indices.With deep=False neither the indices nor the data are copied. Returns (Series or DataFrame) Object type matches caller. Notes When deep=True , data is copied but actual Python objects will not be copied recursively, only the reference to the object. This is in contrast to copy.deepcopy in the Standard Library, which recursively copies object data (see examples below). While Index objects are copied when deep=True , the underlying numpy array is not copied for performance reasons. Since Index is immutable, the underlying data can be safely shared and a copy is not needed. Since pandas is not thread safe, see the :ref: gotchas <gotchas.thread-safety> when copying in a threading environment. When copy_on_write in pandas config is set to True , the copy_on_write config takes effect even when deep=False . This means that any changes to the copied data would make a new copy of the data upon write (and vice versa). Changes made to either the original or copied variable would not be reflected in the counterpart. See :ref: Copy_on_Write <copy_on_write> for more information. Examples >>> s = pd . Series ([ 1 , 2 ], index = [ \"a\" , \"b\" ]) >>> s a 1 b 2 dtype : int64 >>> s_copy = s . copy () >>> s_copy a 1 b 2 dtype : int64 Shallow copy versus default (deep) copy: >>> s = pd . Series ([ 1 , 2 ], index = [ \"a\" , \"b\" ]) >>> deep = s . copy () >>> shallow = s . copy ( deep = False ) Shallow copy shares data and index with original. >>> s is shallow False >>> s . values is shallow . values and s . index is shallow . index True Deep copy has own copy of data and index. >>> s is deep False >>> s . values is deep . values or s . index is deep . index False Updates to the data shared by shallow copy and original is reflected in both (NOTE: this will no longer be true for pandas >= 3.0); deep copy remains unchanged. >>> s . iloc [ 0 ] = 3 >>> shallow . iloc [ 1 ] = 4 >>> s a 3 b 4 dtype : int64 >>> shallow a 3 b 4 dtype : int64 >>> deep a 1 b 2 dtype : int64 Note that when copying an object containing Python objects, a deep copy will copy the data, but will not do so recursively. Updating a nested data object will be reflected in the deep copy. >>> s = pd . Series ([[ 1 , 2 ], [ 3 , 4 ]]) >>> deep = s . copy () >>> s [ 0 ][ 0 ] = 10 >>> s 0 [ 10 , 2 ] 1 [ 3 , 4 ] dtype : object >>> deep 0 [ 10 , 2 ] 1 [ 3 , 4 ] dtype : object Copy-on-Write is set to true , the shallow copy is not modified when the original data is changed: >>> with pd . option_context ( \"mode.copy_on_write\" , True ): ... s = pd . Series ([ 1 , 2 ], index = [ \"a\" , \"b\" ]) ... copy = s . copy ( deep = False ) ... s . iloc [ 0 ] = 100 ... s a 100 b 2 dtype : int64 >>> copy a 1 b 2 dtype : int64 method","title":"pandas.core.generic.NDFrame.copy"},{"location":"api/pipen.channel/#pandascoregenericndframedeepcopy","text":"</> method","title":"pandas.core.generic.NDFrame.deepcopy"},{"location":"api/pipen.channel/#pandascoregenericndframeinfer_objects","text":"</> Attempt to infer better dtypes for object columns. Attempts soft conversion of object-dtyped columns, leaving non-object and unconvertible columns unchanged. The inference rules are the same as during normal Series/DataFrame construction. Parameters copy (bool, default True) \u2014 Whether to make a copy for non-object or non-inferable columnsor Series. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` See Also to_datetime : Convert argument to datetime.to_timedelta : Convert argument to timedelta. to_numeric : Convert argument to numeric type. convert_dtypes : Convert argument to best possible dtype. Examples >>> df = pd . DataFrame ({ \"A\" : [ \"a\" , 1 , 2 , 3 ]}) >>> df = df . iloc [ 1 :] >>> df A 1 1 2 2 3 3 >>> df . dtypes A object dtype : object >>> df . infer_objects () . dtypes A int64 dtype : object method","title":"pandas.core.generic.NDFrame.infer_objects"},{"location":"api/pipen.channel/#pandascoregenericndframeconvert_dtypes","text":"</> Convert columns to the best possible dtypes using dtypes supporting pd.NA . Parameters infer_objects (bool, default True) \u2014 Whether object dtypes should be converted to the best possible types. convert_string (bool, default True) \u2014 Whether object dtypes should be converted to StringDtype() . convert_integer (bool, default True) \u2014 Whether, if possible, conversion can be done to integer extension types. convert_boolean (bool, defaults True) \u2014 Whether object dtypes should be converted to BooleanDtypes() . convert_floating (bool, defaults True) \u2014 Whether, if possible, conversion can be done to floating extension types.If convert_integer is also True, preference will be give to integer dtypes if the floats can be faithfully casted to integers. dtype_backend ({'numpy_nullable', 'pyarrow'}, default 'numpy_nullable') \u2014 Back-end data type applied to the resultant :class: DataFrame (still experimental). Behaviour is as follows: \"numpy_nullable\" : returns nullable-dtype-backed :class: DataFrame (default). \"pyarrow\" : returns pyarrow-backed nullable :class: ArrowDtype DataFrame. .. versionadded:: 2.0 Returns (Series or DataFrame) Copy of input object with new dtype. See Also infer_objects : Infer dtypes of objects.to_datetime : Convert argument to datetime. to_timedelta : Convert argument to timedelta. to_numeric : Convert argument to a numeric type. Notes By default, convert_dtypes will attempt to convert a Series (or each Series in a DataFrame) to dtypes that support pd.NA . By using the options convert_string , convert_integer , convert_boolean and convert_floating , it is possible to turn off individual conversions to StringDtype , the integer extension types, BooleanDtype or floating extension types, respectively. For object-dtyped columns, if infer_objects is True , use the inference rules as during normal Series/DataFrame construction. Then, if possible, convert to StringDtype , BooleanDtype or an appropriate integer or floating extension type, otherwise leave as object . If the dtype is integer, convert to an appropriate integer extension type. If the dtype is numeric, and consists of all integers, convert to an appropriate integer extension type. Otherwise, convert to an appropriate floating extension type. In the future, as new dtypes are added that support pd.NA , the results of this method will change to support those new dtypes. Examples >>> df = pd . DataFrame ( ... { ... \"a\" : pd . Series ([ 1 , 2 , 3 ], dtype = np . dtype ( \"int32\" )), ... \"b\" : pd . Series ([ \"x\" , \"y\" , \"z\" ], dtype = np . dtype ( \"O\" )), ... \"c\" : pd . Series ([ True , False , np . nan ], dtype = np . dtype ( \"O\" )), ... \"d\" : pd . Series ([ \"h\" , \"i\" , np . nan ], dtype = np . dtype ( \"O\" )), ... \"e\" : pd . Series ([ 10 , np . nan , 20 ], dtype = np . dtype ( \"float\" )), ... \"f\" : pd . Series ([ np . nan , 100.5 , 200 ], dtype = np . dtype ( \"float\" )), ... } ... ) Start with a DataFrame with default dtypes. >>> df a b c d e f 0 1 x True h 10.0 NaN 1 2 y False i NaN 100.5 2 3 z NaN NaN 20.0 200.0 >>> df . dtypes a int32 b object c object d object e float64 f float64 dtype : object Convert the DataFrame to use best possible dtypes. >>> dfn = df . convert_dtypes () >>> dfn a b c d e f 0 1 x True h 10 < NA > 1 2 y False i < NA > 100.5 2 3 z < NA > < NA > 20 200.0 >>> dfn . dtypes a Int32 b string [ python ] c boolean d string [ python ] e Int64 f Float64 dtype : object Start with a Series of strings and missing data represented by np.nan . >>> s = pd . Series ([ \"a\" , \"b\" , np . nan ]) >>> s 0 a 1 b 2 NaN dtype : object Obtain a Series with dtype StringDtype . >>> s . convert_dtypes () 0 a 1 b 2 < NA > dtype : string method","title":"pandas.core.generic.NDFrame.convert_dtypes"},{"location":"api/pipen.channel/#pandascoregenericndframefillna","text":"</> Fill NA/NaN values using the specified method. Parameters value (scalar, dict, Series, or DataFrame) \u2014 Value to use to fill holes (e.g. 0), alternately adict/Series/DataFrame of values specifying which value to use for each index (for a Series) or column (for a DataFrame). Values not in the dict/Series/DataFrame will not be filled. This value cannot be a list. method ({'backfill', 'bfill', 'ffill', None}, default None) \u2014 Method to use for filling holes in reindexed Series: ffill: propagate last valid observation forward to next valid. backfill / bfill: use next valid observation to fill gap. .. deprecated:: 2.1.0 Use ffill or bfill instead. axis ({0 or 'index'} for Series, {0 or 'index', 1 or 'columns'} for DataFrame) \u2014 Axis along which to fill missing values. For Series this parameter is unused and defaults to 0. inplace (bool, default False) \u2014 If True, fill in-place. Note: this will modify anyother views on this object (e.g., a no-copy slice for a column in a DataFrame). limit (int, default None) \u2014 If method is specified, this is the maximum number of consecutiveNaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None. downcast (dict, default is None) \u2014 A dict of item->dtype of what to downcast if possible,or the string 'infer' which will try to downcast to an appropriate equal type (e.g. float64 to int64 if possible). .. deprecated:: 2.2.0 Returns (Series/DataFrame or None) Object with missing values filled or None if inplace=True . See Also ffill : Fill values by propagating the last valid observation to next valid.bfill : Fill values by using the next valid observation to fill the gap. interpolate : Fill NaN values using interpolation. reindex : Conform object to new index. asfreq : Convert TimeSeries to specified frequency. Examples >>> df = pd . DataFrame ([[ np . nan , 2 , np . nan , 0 ], ... [ 3 , 4 , np . nan , 1 ], ... [ np . nan , np . nan , np . nan , np . nan ], ... [ np . nan , 3 , np . nan , 4 ]], ... columns = list ( \"ABCD\" )) >>> df A B C D 0 NaN 2.0 NaN 0.0 1 3.0 4.0 NaN 1.0 2 NaN NaN NaN NaN 3 NaN 3.0 NaN 4.0 Replace all NaN elements with 0s. >>> df . fillna ( 0 ) A B C D 0 0.0 2.0 0.0 0.0 1 3.0 4.0 0.0 1.0 2 0.0 0.0 0.0 0.0 3 0.0 3.0 0.0 4.0 Replace all NaN elements in column 'A', 'B', 'C', and 'D', with 0, 1, 2, and 3 respectively. >>> values = { \"A\" : 0 , \"B\" : 1 , \"C\" : 2 , \"D\" : 3 } >>> df . fillna ( value = values ) A B C D 0 0.0 2.0 2.0 0.0 1 3.0 4.0 2.0 1.0 2 0.0 1.0 2.0 3.0 3 0.0 3.0 2.0 4.0 Only replace the first NaN element. >>> df . fillna ( value = values , limit = 1 ) A B C D 0 0.0 2.0 2.0 0.0 1 3.0 4.0 NaN 1.0 2 NaN 1.0 NaN 3.0 3 NaN 3.0 NaN 4.0 When filling using a DataFrame, replacement happens along the same column names and same indices >>> df2 = pd . DataFrame ( np . zeros (( 4 , 4 )), columns = list ( \"ABCE\" )) >>> df . fillna ( df2 ) A B C D 0 0.0 2.0 0.0 0.0 1 3.0 4.0 0.0 1.0 2 0.0 0.0 0.0 NaN 3 0.0 3.0 0.0 4.0 Note that column D is not affected since it is not present in df2. method","title":"pandas.core.generic.NDFrame.fillna"},{"location":"api/pipen.channel/#pandascoregenericndframeffill","text":"</> Fill NA/NaN values by propagating the last valid observation to next valid. Parameters axis ({0 or 'index'} for Series, {0 or 'index', 1 or 'columns'} for DataFrame) \u2014 Axis along which to fill missing values. For Series this parameter is unused and defaults to 0. inplace (bool, default False) \u2014 If True, fill in-place. Note: this will modify anyother views on this object (e.g., a no-copy slice for a column in a DataFrame). limit (int, default None) \u2014 If method is specified, this is the maximum number of consecutiveNaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None. limit_area ({`None`, 'inside', 'outside'}, default None) \u2014 If limit is specified, consecutive NaNs will be filled with thisrestriction. None : No fill restriction. 'inside': Only fill NaNs surrounded by valid values (interpolate). 'outside': Only fill NaNs outside valid values (extrapolate). .. versionadded:: 2.2.0 downcast (dict, default is None) \u2014 A dict of item->dtype of what to downcast if possible,or the string 'infer' which will try to downcast to an appropriate equal type (e.g. float64 to int64 if possible). .. deprecated:: 2.2.0 Returns (Series/DataFrame or None) Object with missing values filled or None if inplace=True . Examples >>> df = pd . DataFrame ([[ np . nan , 2 , np . nan , 0 ], ... [ 3 , 4 , np . nan , 1 ], ... [ np . nan , np . nan , np . nan , np . nan ], ... [ np . nan , 3 , np . nan , 4 ]], ... columns = list ( \"ABCD\" )) >>> df A B C D 0 NaN 2.0 NaN 0.0 1 3.0 4.0 NaN 1.0 2 NaN NaN NaN NaN 3 NaN 3.0 NaN 4.0 >>> df . ffill () A B C D 0 NaN 2.0 NaN 0.0 1 3.0 4.0 NaN 1.0 2 3.0 4.0 NaN 1.0 3 3.0 3.0 NaN 4.0 >>> ser = pd . Series ([ 1 , np . nan , 2 , 3 ]) >>> ser . ffill () 0 1.0 1 1.0 2 2.0 3 3.0 dtype : float64 method","title":"pandas.core.generic.NDFrame.ffill"},{"location":"api/pipen.channel/#pandascoregenericndframepad","text":"</> Fill NA/NaN values by propagating the last valid observation to next valid. .. deprecated:: 2.0 Series/DataFrame.pad is deprecated. Use Series/DataFrame.ffill instead. Returns (Series/DataFrame or None) Object with missing values filled or None if inplace=True . Examples Please see examples for :meth: DataFrame.ffill or :meth: Series.ffill . method","title":"pandas.core.generic.NDFrame.pad"},{"location":"api/pipen.channel/#pandascoregenericndframebfill","text":"</> Fill NA/NaN values by using the next valid observation to fill the gap. Parameters axis ({0 or 'index'} for Series, {0 or 'index', 1 or 'columns'} for DataFrame) \u2014 Axis along which to fill missing values. For Series this parameter is unused and defaults to 0. inplace (bool, default False) \u2014 If True, fill in-place. Note: this will modify anyother views on this object (e.g., a no-copy slice for a column in a DataFrame). limit (int, default None) \u2014 If method is specified, this is the maximum number of consecutiveNaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None. limit_area ({`None`, 'inside', 'outside'}, default None) \u2014 If limit is specified, consecutive NaNs will be filled with thisrestriction. None : No fill restriction. 'inside': Only fill NaNs surrounded by valid values (interpolate). 'outside': Only fill NaNs outside valid values (extrapolate). .. versionadded:: 2.2.0 downcast (dict, default is None) \u2014 A dict of item->dtype of what to downcast if possible,or the string 'infer' which will try to downcast to an appropriate equal type (e.g. float64 to int64 if possible). .. deprecated:: 2.2.0 Returns (Series/DataFrame or None) Object with missing values filled or None if inplace=True . Examples For Series: >>> s = pd . Series ([ 1 , None , None , 2 ]) >>> s . bfill () 0 1.0 1 2.0 2 2.0 3 2.0 dtype : float64 >>> s . bfill ( limit = 1 ) 0 1.0 1 NaN 2 2.0 3 2.0 dtype : float64 With DataFrame: >>> df = pd . DataFrame ({ 'A' : [ 1 , None , None , 4 ], 'B' : [ None , 5 , None , 7 ]}) >>> df A B 0 1.0 NaN 1 NaN 5.0 2 NaN NaN 3 4.0 7.0 >>> df . bfill () A B 0 1.0 5.0 1 4.0 5.0 2 4.0 7.0 3 4.0 7.0 >>> df . bfill ( limit = 1 ) A B 0 1.0 5.0 1 NaN 5.0 2 4.0 7.0 3 4.0 7.0 method","title":"pandas.core.generic.NDFrame.bfill"},{"location":"api/pipen.channel/#pandascoregenericndframebackfill","text":"</> Fill NA/NaN values by using the next valid observation to fill the gap. .. deprecated:: 2.0 Series/DataFrame.backfill is deprecated. Use Series/DataFrame.bfill instead. Returns (Series/DataFrame or None) Object with missing values filled or None if inplace=True . Examples Please see examples for :meth: DataFrame.bfill or :meth: Series.bfill . method","title":"pandas.core.generic.NDFrame.backfill"},{"location":"api/pipen.channel/#pandascoregenericndframereplace","text":"</> Replace values given in to_replace with value . Values of the Series/DataFrame are replaced with other values dynamically. This differs from updating with .loc or .iloc , which require you to specify a location to update with some value. Parameters to_replace (str, regex, list, dict, Series, int, float, or None) \u2014 How to find the values that will be replaced. numeric, str or regex: numeric: numeric values equal to to_replace will be replaced with value str: string exactly matching to_replace will be replaced with value regex: regexs matching to_replace will be replaced with value list of str, regex, or numeric: First, if to_replace and value are both lists, they must be the same length. Second, if regex=True then all of the strings in both lists will be interpreted as regexs otherwise they will match directly. This doesn't matter much for value since there are only a few possible substitution regexes you can use. str, regex and numeric rules apply as above. dict: Dicts can be used to specify different replacement values for different existing values. For example, {'a': 'b', 'y': 'z'} replaces the value 'a' with 'b' and 'y' with 'z'. To use a dict in this way, the optional value parameter should not be given. For a DataFrame a dict can specify that different values should be replaced in different columns. For example, {'a': 1, 'b': 'z'} looks for the value 1 in column 'a' and the value 'z' in column 'b' and replaces these values with whatever is specified in value . The value parameter should not be None in this case. You can treat this as a special case of passing two lists except that you are specifying the column to search in. For a DataFrame nested dictionaries, e.g., {'a': {'b': np.nan}} , are read as follows: look in column 'a' for the value 'b' and replace it with NaN. The optional value parameter should not be specified to use a nested dict in this way. You can nest regular expressions as well. Note that column names (the top-level dictionary keys in a nested dictionary) cannot be regular expressions. None: This means that the regex argument must be a string, compiled regular expression, or list, dict, ndarray or Series of such elements. If value is also None then this must be a nested dictionary or Series. See the examples section for examples of each of these. value (scalar, dict, list, str, regex, default None) \u2014 Value to replace any values matching to_replace with.For a DataFrame a dict of values can be used to specify which value to use for each column (columns not in the dict will not be filled). Regular expressions, strings and lists or dicts of such objects are also allowed. inplace (bool, default False) \u2014 If True, performs operation inplace and returns None. limit (int, default None) \u2014 Maximum size gap to forward or backward fill. .. deprecated:: 2.1.0 regex (bool or same types as `to_replace`, default False) \u2014 Whether to interpret to_replace and/or value as regularexpressions. Alternatively, this could be a regular expression or a list, dict, or array of regular expressions in which case to_replace must be None . method ({'pad', 'ffill', 'bfill'}) \u2014 The method to use when for replacement, when to_replace is ascalar, list or tuple and value is None . .. deprecated:: 2.1.0 Returns (Series/DataFrame) Object after replacement. Raises AssertionError \u2014 If regex is not a bool and to_replace is not None . TypeError \u2014 If to_replace is not a scalar, array-like, dict , or None If to_replace is a dict and value is not a list , dict , ndarray , or Series If to_replace is None and regex is not compilable into a regular expression or is a list, dict, ndarray, or Series. When replacing multiple bool or datetime64 objects and the arguments to to_replace does not match the type of the value being replaced ValueError \u2014 If a list or an ndarray is passed to to_replace and value but they are not the same length. See Also Series.fillna : Fill NA values.DataFrame.fillna : Fill NA values. Series.where : Replace values based on boolean condition. DataFrame.where : Replace values based on boolean condition. DataFrame.map: Apply a function to a Dataframe elementwise. Series.map: Map values of Series according to an input mapping or function. Series.str.replace : Simple string replacement. Notes Regex substitution is performed under the hood with re.sub . The rules for substitution for re.sub are the same. Regular expressions will only substitute on strings, meaning you cannot provide, for example, a regular expression matching floating point numbers and expect the columns in your frame that have a numeric dtype to be matched. However, if those floating point numbers are strings, then you can do this. This method has a lot of options. You are encouraged to experiment and play with this method to gain intuition about how it works. When dict is used as the to_replace value, it is like key(s) in the dict are the to_replace part and value(s) in the dict are the value parameter. Examples * ) ) 5 2 3 4 5 4 , , ) ) C a b c d e * ) C a b c d e ) C a b c d e ) 3 3 3 4 5 4 * ) C a b c d e ) C a b c d e ) C a b c d e * , ) ) B c w z ) B c r z ) B c w z ) B c w z ) B c w z d s : ) e . o : ) 0 e e b e t t e 0 . ) 0 0 0 b b t 0 . l : ) 0 e e b e t 0 . , . , , ) ) C e e h i j y . ) C f g e e e method","title":"pandas.core.generic.NDFrame.replace"},{"location":"api/pipen.channel/#pandascoregenericndframeinterpolate","text":"</> Fill NaN values using an interpolation method. Please note that only method='linear' is supported for DataFrame/Series with a MultiIndex. Parameters method (str, default 'linear') \u2014 Interpolation technique to use. One of: 'linear': Ignore the index and treat the values as equally spaced. This is the only method supported on MultiIndexes. 'time': Works on daily and higher resolution data to interpolate given length of interval. 'index', 'values': use the actual numerical values of the index. 'pad': Fill in NaNs using existing values. 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'barycentric', 'polynomial': Passed to scipy.interpolate.interp1d , whereas 'spline' is passed to scipy.interpolate.UnivariateSpline . These methods use the numerical values of the index. Both 'polynomial' and 'spline' require that you also specify an order (int), e.g. df.interpolate(method='polynomial', order=5) . Note that, slinear method in Pandas refers to the Scipy first order spline instead of Pandas first order spline . 'krogh', 'piecewise_polynomial', 'spline', 'pchip', 'akima', 'cubicspline': Wrappers around the SciPy interpolation methods of similar names. See Notes . 'from_derivatives': Refers to scipy.interpolate.BPoly.from_derivatives . axis ({{0 or 'index', 1 or 'columns', None}}, default None) \u2014 Axis to interpolate along. For Series this parameter is unusedand defaults to 0. limit (int, optional) \u2014 Maximum number of consecutive NaNs to fill. Must be greater than0. inplace (bool, default False) \u2014 Update the data in place if possible. limit_direction ({{'forward', 'backward', 'both'}}, Optional) \u2014 Consecutive NaNs will be filled in this direction. If limit is specified: * If 'method' is 'pad' or 'ffill', 'limit_direction' must be 'forward'. * If 'method' is 'backfill' or 'bfill', 'limit_direction' must be 'backwards'. If 'limit' is not specified: * If 'method' is 'backfill' or 'bfill', the default is 'backward' * else the default is 'forward' raises ValueError if limit_direction is 'forward' or 'both' and method is 'backfill' or 'bfill'. raises ValueError if limit_direction is 'backward' or 'both' and method is 'pad' or 'ffill'. limit_area ({{`None`, 'inside', 'outside'}}, default None) \u2014 If limit is specified, consecutive NaNs will be filled with thisrestriction. None : No fill restriction. 'inside': Only fill NaNs surrounded by valid values (interpolate). 'outside': Only fill NaNs outside valid values (extrapolate). downcast (optional, 'infer' or None, defaults to None) \u2014 Downcast dtypes if possible. .. deprecated:: 2.1.0 Returns (Series or DataFrame or None) Returns the same object type as the caller, interpolated atsome or all NaN values or None if inplace=True . See Also fillna : Fill missing values using different methods.scipy.interpolate.Akima1DInterpolator : Piecewise cubic polynomials (Akima interpolator). scipy.interpolate.BPoly.from_derivatives : Piecewise polynomial in the Bernstein basis. scipy.interpolate.interp1d : Interpolate a 1-D function. scipy.interpolate.KroghInterpolator : Interpolate polynomial (Krogh interpolator). scipy.interpolate.PchipInterpolator : PCHIP 1-d monotonic cubic interpolation. scipy.interpolate.CubicSpline : Cubic spline data interpolator. Notes The 'krogh', 'piecewise_polynomial', 'spline', 'pchip' and 'akima' methods are wrappers around the respective SciPy implementations of similar names. These use the actual numerical values of the index. For more information on their behavior, see the SciPy documentation <https://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation> __. Examples Filling in NaN in a :class: ~pandas.Series via linearinterpolation. >>> s = pd . Series ([ 0 , 1 , np . nan , 3 ]) >>> s 0 0.0 1 1.0 2 NaN 3 3.0 dtype : float64 >>> s . interpolate () 0 0.0 1 1.0 2 2.0 3 3.0 dtype : float64 Filling in NaN in a Series via polynomial interpolation or splines: Both 'polynomial' and 'spline' methods require that you also specify an order (int). >>> s = pd . Series ([ 0 , 2 , np . nan , 8 ]) >>> s . interpolate ( method = 'polynomial' , order = 2 ) 0 0.000000 1 2.000000 2 4.666667 3 8.000000 dtype : float64 Fill the DataFrame forward (that is, going down) along each column using linear interpolation. Note how the last entry in column 'a' is interpolated differently, because there is no entry after it to use for interpolation. Note how the first entry in column 'b' remains NaN , because there is no entry before it to use for interpolation. >>> df = pd . DataFrame ([( 0.0 , np . nan , - 1.0 , 1.0 ), ... ( np . nan , 2.0 , np . nan , np . nan ), ... ( 2.0 , 3.0 , np . nan , 9.0 ), ... ( np . nan , 4.0 , - 4.0 , 16.0 )], ... columns = list ( 'abcd' )) >>> df a b c d 0 0.0 NaN - 1.0 1.0 1 NaN 2.0 NaN NaN 2 2.0 3.0 NaN 9.0 3 NaN 4.0 - 4.0 16.0 >>> df . interpolate ( method = 'linear' , limit_direction = 'forward' , axis = 0 ) a b c d 0 0.0 NaN - 1.0 1.0 1 1.0 2.0 - 2.0 5.0 2 2.0 3.0 - 3.0 9.0 3 2.0 4.0 - 4.0 16.0 Using polynomial interpolation. >>> df [ 'd' ] . interpolate ( method = 'polynomial' , order = 2 ) 0 1.0 1 4.0 2 9.0 3 16.0 Name : d , dtype : float64 method","title":"pandas.core.generic.NDFrame.interpolate"},{"location":"api/pipen.channel/#pandascoregenericndframeasof","text":"</> Return the last row(s) without any NaNs before where . The last row (for each element in where , if list) without any NaN is taken. In case of a :class: ~pandas.DataFrame , the last row without NaN considering only the subset of columns (if not None ) If there is no good value, NaN is returned for a Series or a Series of NaN values for a DataFrame Parameters where (date or array-like of dates) \u2014 Date(s) before which the last row(s) are returned. subset (str or array-like of str, default `None`) \u2014 For DataFrame, if not None , only use these columns tocheck for NaNs. Returns (scalar, Series, or DataFrame) : r , r n e See Also merge_asof : Perform an asof merge. Similar to left join. Notes Dates are assumed to be sorted. Raises if this is not the case. Examples A Series and a scalar where . >>> s = pd . Series ([ 1 , 2 , np . nan , 4 ], index = [ 10 , 20 , 30 , 40 ]) >>> s 10 1.0 20 2.0 30 NaN 40 4.0 dtype : float64 >>> s . asof ( 20 ) 2.0 For a sequence where , a Series is returned. The first value is NaN, because the first element of where is before the first index value. >>> s . asof ([ 5 , 20 ]) 5 NaN 20 2.0 dtype : float64 Missing values are not considered. The following is 2.0 , not NaN, even though NaN is at the index location for 30 . >>> s . asof ( 30 ) 2.0 Take all columns into consideration >>> df = pd . DataFrame ({ 'a' : [ 10. , 20. , 30. , 40. , 50. ], ... 'b' : [ None , None , None , None , 500 ]}, ... index = pd . DatetimeIndex ([ '2018-02-27 09:01:00' , ... '2018-02-27 09:02:00' , ... '2018-02-27 09:03:00' , ... '2018-02-27 09:04:00' , ... '2018-02-27 09:05:00' ])) >>> df . asof ( pd . DatetimeIndex ([ '2018-02-27 09:03:30' , ... '2018-02-27 09:04:30' ])) a b 2018 - 02 - 27 09 : 03 : 30 NaN NaN 2018 - 02 - 27 09 : 04 : 30 NaN NaN Take a single column into consideration >>> df . asof ( pd . DatetimeIndex ([ '2018-02-27 09:03:30' , ... '2018-02-27 09:04:30' ]), ... subset = [ 'a' ]) a b 2018 - 02 - 27 09 : 03 : 30 30.0 NaN 2018 - 02 - 27 09 : 04 : 30 40.0 NaN method","title":"pandas.core.generic.NDFrame.asof"},{"location":"api/pipen.channel/#pandascoregenericndframeclip","text":"</> Trim values at input threshold(s). Assigns values outside boundary to boundary values. Thresholds can be singular values or array like, and in the latter case the clipping is performed element-wise in the specified axis. Parameters lower (float or array-like, default None) \u2014 Minimum threshold value. All values below thisthreshold will be set to it. A missing threshold (e.g NA ) will not clip the value. upper (float or array-like, default None) \u2014 Maximum threshold value. All values above thisthreshold will be set to it. A missing threshold (e.g NA ) will not clip the value. axis ({{0 or 'index', 1 or 'columns', None}}, default None) \u2014 Align object with lower and upper along the given axis.For Series this parameter is unused and defaults to None . inplace (bool, default False) \u2014 Whether to perform the operation in place on the data. Returns (Series or DataFrame or None) Same type as calling object with the values outside theclip boundaries replaced or None if inplace=True . See Also Series.clip : Trim values at input threshold in series.DataFrame.clip : Trim values at input threshold in dataframe. numpy.clip : Clip (limit) the values in an array. Examples >>> data = { 'col_0' : [ 9 , - 3 , 0 , - 1 , 5 ], 'col_1' : [ - 2 , - 7 , 6 , 8 , - 5 ]} >>> df = pd . DataFrame ( data ) >>> df col_0 col_1 0 9 - 2 1 - 3 - 7 2 0 6 3 - 1 8 4 5 - 5 Clips per column using lower and upper thresholds: >>> df . clip ( - 4 , 6 ) col_0 col_1 0 6 - 2 1 - 3 - 4 2 0 6 3 - 1 6 4 5 - 4 Clips using specific lower and upper thresholds per column: >>> df . clip ([ - 2 , - 1 ], [ 4 , 5 ]) col_0 col_1 0 4 - 1 1 - 2 - 1 2 0 5 3 - 1 5 4 4 - 1 Clips using specific lower and upper thresholds per column element: >>> t = pd . Series ([ 2 , - 4 , - 1 , 6 , 3 ]) >>> t 0 2 1 - 4 2 - 1 3 6 4 3 dtype : int64 >>> df . clip ( t , t + 4 , axis = 0 ) col_0 col_1 0 6 2 1 - 3 - 4 2 0 3 3 6 8 4 5 3 Clips using specific lower threshold per column element, with missing values: >>> t = pd . Series ([ 2 , - 4 , np . nan , 6 , 3 ]) >>> t 0 2.0 1 - 4.0 2 NaN 3 6.0 4 3.0 dtype : float64 >>> df . clip ( t , axis = 0 ) col_0 col_1 0 9 2 1 - 3 - 4 2 0 6 3 6 8 4 5 3 method","title":"pandas.core.generic.NDFrame.clip"},{"location":"api/pipen.channel/#pandascoregenericndframeasfreq","text":"</> Convert time series to specified frequency. Returns the original data conformed to a new index with the specified frequency. If the index of this Series/DataFrame is a :class: ~pandas.PeriodIndex , the new index is the result of transforming the original index with :meth: PeriodIndex.asfreq <pandas.PeriodIndex.asfreq> (so the original index will map one-to-one to the new index). Otherwise, the new index will be equivalent to pd.date_range(start, end, freq=freq) where start and end are, respectively, the first and last entries in the original index (see :func: pandas.date_range ). The values corresponding to any timesteps in the new index which were not present in the original index will be null ( NaN ), unless a method for filling such unknowns is provided (see the method parameter below). The :meth: resample method is more appropriate if an operation on each group of timesteps (such as an aggregate) is necessary to represent the data at the new frequency. Parameters freq (DateOffset or str) \u2014 Frequency DateOffset or string. method ({'backfill'/'bfill', 'pad'/'ffill'}, default None) \u2014 Method to use for filling holes in reindexed Series (note thisdoes not fill NaNs that already were present): 'pad' / 'ffill': propagate last valid observation forward to next valid 'backfill' / 'bfill': use NEXT valid observation to fill. how ({'start', 'end'}, default end) \u2014 For PeriodIndex only (see PeriodIndex.asfreq). normalize (bool, default False) \u2014 Whether to reset output index to midnight. fill_value (scalar, optional) \u2014 Value to use for missing values, applied during upsampling (notethis does not fill NaNs that already were present). Returns (Series/DataFrame) Series/DataFrame object reindexed to the specified frequency. See Also reindex : Conform DataFrame to new index with optional filling logic. Notes To learn more about the frequency strings, please see this link <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases> __. Examples Start by creating a series with 4 one minute timestamps. >>> index = pd . date_range ( '1/1/2000' , periods = 4 , freq = 'min' ) >>> series = pd . Series ([ 0.0 , None , 2.0 , 3.0 ], index = index ) >>> df = pd . DataFrame ({ 's' : series }) >>> df s 2000 - 01 - 01 00 : 00 : 00 0.0 2000 - 01 - 01 00 : 01 : 00 NaN 2000 - 01 - 01 00 : 02 : 00 2.0 2000 - 01 - 01 00 : 03 : 00 3.0 Upsample the series into 30 second bins. >>> df . asfreq ( freq = '30s' ) s 2000 - 01 - 01 00 : 00 : 00 0.0 2000 - 01 - 01 00 : 00 : 30 NaN 2000 - 01 - 01 00 : 01 : 00 NaN 2000 - 01 - 01 00 : 01 : 30 NaN 2000 - 01 - 01 00 : 02 : 00 2.0 2000 - 01 - 01 00 : 02 : 30 NaN 2000 - 01 - 01 00 : 03 : 00 3.0 Upsample again, providing a fill value . >>> df . asfreq ( freq = '30s' , fill_value = 9.0 ) s 2000 - 01 - 01 00 : 00 : 00 0.0 2000 - 01 - 01 00 : 00 : 30 9.0 2000 - 01 - 01 00 : 01 : 00 NaN 2000 - 01 - 01 00 : 01 : 30 9.0 2000 - 01 - 01 00 : 02 : 00 2.0 2000 - 01 - 01 00 : 02 : 30 9.0 2000 - 01 - 01 00 : 03 : 00 3.0 Upsample again, providing a method . >>> df . asfreq ( freq = '30s' , method = 'bfill' ) s 2000 - 01 - 01 00 : 00 : 00 0.0 2000 - 01 - 01 00 : 00 : 30 NaN 2000 - 01 - 01 00 : 01 : 00 NaN 2000 - 01 - 01 00 : 01 : 30 2.0 2000 - 01 - 01 00 : 02 : 00 2.0 2000 - 01 - 01 00 : 02 : 30 3.0 2000 - 01 - 01 00 : 03 : 00 3.0 method","title":"pandas.core.generic.NDFrame.asfreq"},{"location":"api/pipen.channel/#pandascoregenericndframeat_time","text":"</> Select values at particular time of day (e.g., 9:30AM). Parameters time (datetime.time or str) \u2014 The values to select. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 For Series this parameter is unused and defaults to 0. Raises TypeError \u2014 If the index is not a :class: DatetimeIndex See Also between_time : Select values between particular times of the day.first : Select initial periods of time series based on a date offset. last : Select final periods of time series based on a date offset. DatetimeIndex.indexer_at_time : Get just the index locations for values at particular time of the day. Examples >>> i = pd . date_range ( '2018-04-09' , periods = 4 , freq = '12h' ) >>> ts = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 , 4 ]}, index = i ) >>> ts A 2018 - 04 - 09 00 : 00 : 00 1 2018 - 04 - 09 12 : 00 : 00 2 2018 - 04 - 10 00 : 00 : 00 3 2018 - 04 - 10 12 : 00 : 00 4 >>> ts . at_time ( '12:00' ) A 2018 - 04 - 09 12 : 00 : 00 2 2018 - 04 - 10 12 : 00 : 00 4 method","title":"pandas.core.generic.NDFrame.at_time"},{"location":"api/pipen.channel/#pandascoregenericndframebetween_time","text":"</> Select values between particular times of the day (e.g., 9:00-9:30 AM). By setting start_time to be later than end_time , you can get the times that are not between the two times. Parameters start_time (datetime.time or str) \u2014 Initial time as a time filter limit. end_time (datetime.time or str) \u2014 End time as a time filter limit. inclusive ({\"both\", \"neither\", \"left\", \"right\"}, default \"both\") \u2014 Include boundaries; whether to set each bound as closed or open. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Determine range time on index or columns value.For Series this parameter is unused and defaults to 0. Returns (Series or DataFrame) Data from the original object filtered to the specified dates range. Raises TypeError \u2014 If the index is not a :class: DatetimeIndex See Also at_time : Select values at a particular time of the day.first : Select initial periods of time series based on a date offset. last : Select final periods of time series based on a date offset. DatetimeIndex.indexer_between_time : Get just the index locations for values between particular times of the day. Examples >>> i = pd . date_range ( '2018-04-09' , periods = 4 , freq = '1D20min' ) >>> ts = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 , 4 ]}, index = i ) >>> ts A 2018 - 04 - 09 00 : 00 : 00 1 2018 - 04 - 10 00 : 20 : 00 2 2018 - 04 - 11 00 : 40 : 00 3 2018 - 04 - 12 01 : 00 : 00 4 >>> ts . between_time ( '0:15' , '0:45' ) A 2018 - 04 - 10 00 : 20 : 00 2 2018 - 04 - 11 00 : 40 : 00 3 You get the times that are not between two times by setting start_time later than end_time : >>> ts . between_time ( '0:45' , '0:15' ) A 2018 - 04 - 09 00 : 00 : 00 1 2018 - 04 - 12 01 : 00 : 00 4 method","title":"pandas.core.generic.NDFrame.between_time"},{"location":"api/pipen.channel/#pandascoregenericndframeresample","text":"</> Resample time-series data. Convenience method for frequency conversion and resampling of time series. The object must have a datetime-like index ( DatetimeIndex , PeriodIndex , or TimedeltaIndex ), or the caller must pass the label of a datetime-like series/index to the on / level keyword parameter. Parameters rule (DateOffset, Timedelta or str) \u2014 The offset string or object representing target conversion. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Which axis to use for up- or down-sampling. For Series this parameteris unused and defaults to 0. Must be DatetimeIndex , TimedeltaIndex or PeriodIndex . .. deprecated:: 2.0.0 Use frame.T.resample(...) instead. closed ({'right', 'left'}, default None) \u2014 Which side of bin interval is closed. The default is 'left'for all frequency offsets except for 'ME', 'YE', 'QE', 'BME', 'BA', 'BQE', and 'W' which all have a default of 'right'. label ({'right', 'left'}, default None) \u2014 Which bin edge label to label bucket with. The default is 'left'for all frequency offsets except for 'ME', 'YE', 'QE', 'BME', 'BA', 'BQE', and 'W' which all have a default of 'right'. convention ({'start', 'end', 's', 'e'}, default 'start') \u2014 For PeriodIndex only, controls whether to use the start orend of rule . kind ({'timestamp', 'period'}, optional, default None) \u2014 Pass 'timestamp' to convert the resulting index to a DateTimeIndex or 'period' to convert it to a PeriodIndex . By default the input representation is retained. .. deprecated:: 2.2.0 Convert index to desired type explicitly instead. on (str, optional) \u2014 For a DataFrame, column to use instead of index for resampling.Column must be datetime-like. level (str or int, optional) \u2014 For a MultiIndex, level (name or number) to use forresampling. level must be datetime-like. origin (Timestamp or str, default 'start_day') \u2014 The timestamp on which to adjust the grouping. The timezone of originmust match the timezone of the index. If string, must be one of the following: 'epoch': origin is 1970-01-01 'start': origin is the first value of the timeseries 'start_day': origin is the first day at midnight of the timeseries 'end': origin is the last value of the timeseries 'end_day': origin is the ceiling midnight of the last day .. versionadded:: 1.3.0 .. note:: Only takes effect for Tick-frequencies (i.e. fixed frequencies like days, hours, and minutes, rather than months or quarters). offset (Timedelta or str, default is None) \u2014 An offset timedelta added to the origin. group_keys (bool, default False) \u2014 Whether to include the group keys in the result index when using .apply() on the resampled object. .. versionadded:: 1.5.0 Not specifying ``group_keys`` will retain values-dependent behavior from pandas 1.4 and earlier (see :ref:`pandas 1.5.0 Release notes <whatsnew_150.enhancements.resample_group_keys>` for examples). .. versionchanged:: 2.0.0 ``group_keys`` now defaults to ``False``. Returns (pandas.api.typing.Resampler) :class: ~pandas.core.Resampler object. See Also Series.resample : Resample a Series.DataFrame.resample : Resample a DataFrame. groupby : Group Series/DataFrame by mapping, function, label, or list of labels. asfreq : Reindex a Series/DataFrame with the given frequency without grouping. Notes See the user guide <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#resampling> __ for more. To learn more about the offset strings, please see this link <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects> __. Examples Start by creating a series with 9 one minute timestamps. >>> index = pd . date_range ( '1/1/2000' , periods = 9 , freq = 'min' ) >>> series = pd . Series ( range ( 9 ), index = index ) >>> series 2000 - 01 - 01 00 : 00 : 00 0 2000 - 01 - 01 00 : 01 : 00 1 2000 - 01 - 01 00 : 02 : 00 2 2000 - 01 - 01 00 : 03 : 00 3 2000 - 01 - 01 00 : 04 : 00 4 2000 - 01 - 01 00 : 05 : 00 5 2000 - 01 - 01 00 : 06 : 00 6 2000 - 01 - 01 00 : 07 : 00 7 2000 - 01 - 01 00 : 08 : 00 8 Freq : min , dtype : int64 Downsample the series into 3 minute bins and sum the values of the timestamps falling into a bin. >>> series . resample ( '3min' ) . sum () 2000 - 01 - 01 00 : 00 : 00 3 2000 - 01 - 01 00 : 03 : 00 12 2000 - 01 - 01 00 : 06 : 00 21 Freq : 3 min , dtype : int64 Downsample the series into 3 minute bins as above, but label each bin using the right edge instead of the left. Please note that the value in the bucket used as the label is not included in the bucket, which it labels. For example, in the original series the bucket 2000-01-01 00:03:00 contains the value 3, but the summed value in the resampled bucket with the label 2000-01-01 00:03:00 does not include 3 (if it did, the summed value would be 6, not 3). >>> series . resample ( '3min' , label = 'right' ) . sum () 2000 - 01 - 01 00 : 03 : 00 3 2000 - 01 - 01 00 : 06 : 00 12 2000 - 01 - 01 00 : 09 : 00 21 Freq : 3 min , dtype : int64 To include this value close the right side of the bin interval, as shown below. >>> series . resample ( '3min' , label = 'right' , closed = 'right' ) . sum () 2000 - 01 - 01 00 : 00 : 00 0 2000 - 01 - 01 00 : 03 : 00 6 2000 - 01 - 01 00 : 06 : 00 15 2000 - 01 - 01 00 : 09 : 00 15 Freq : 3 min , dtype : int64 Upsample the series into 30 second bins. >>> series . resample ( '30s' ) . asfreq ()[ 0 : 5 ] # Select first 5 rows 2000 - 01 - 01 00 : 00 : 00 0.0 2000 - 01 - 01 00 : 00 : 30 NaN 2000 - 01 - 01 00 : 01 : 00 1.0 2000 - 01 - 01 00 : 01 : 30 NaN 2000 - 01 - 01 00 : 02 : 00 2.0 Freq : 30 s , dtype : float64 Upsample the series into 30 second bins and fill the NaN values using the ffill method. >>> series . resample ( '30s' ) . ffill ()[ 0 : 5 ] 2000 - 01 - 01 00 : 00 : 00 0 2000 - 01 - 01 00 : 00 : 30 0 2000 - 01 - 01 00 : 01 : 00 1 2000 - 01 - 01 00 : 01 : 30 1 2000 - 01 - 01 00 : 02 : 00 2 Freq : 30 s , dtype : int64 Upsample the series into 30 second bins and fill the NaN values using the bfill method. >>> series . resample ( '30s' ) . bfill ()[ 0 : 5 ] 2000 - 01 - 01 00 : 00 : 00 0 2000 - 01 - 01 00 : 00 : 30 1 2000 - 01 - 01 00 : 01 : 00 1 2000 - 01 - 01 00 : 01 : 30 2 2000 - 01 - 01 00 : 02 : 00 2 Freq : 30 s , dtype : int64 Pass a custom function via apply >>> def custom_resampler ( arraylike ): ... return np . sum ( arraylike ) + 5 ... >>> series . resample ( '3min' ) . apply ( custom_resampler ) 2000 - 01 - 01 00 : 00 : 00 8 2000 - 01 - 01 00 : 03 : 00 17 2000 - 01 - 01 00 : 06 : 00 26 Freq : 3 min , dtype : int64 For a Series with a PeriodIndex, the keyword convention can be used to control whether to use the start or end of rule . Resample a year by quarter using 'start' convention . Values are assigned to the first quarter of the period. >>> s = pd . Series ( ... [ 1 , 2 ], index = pd . period_range ( \"2012-01-01\" , freq = \"Y\" , periods = 2 ) ... ) >>> s 2012 1 2013 2 Freq : Y - DEC , dtype : int64 >>> s . resample ( \"Q\" , convention = \"start\" ) . asfreq () 2012 Q1 1.0 2012 Q2 NaN 2012 Q3 NaN 2012 Q4 NaN 2013 Q1 2.0 2013 Q2 NaN 2013 Q3 NaN 2013 Q4 NaN Freq : Q - DEC , dtype : float64 Resample quarters by month using 'end' convention . Values are assigned to the last month of the period. >>> q = pd . Series ( ... [ 1 , 2 , 3 , 4 ], index = pd . period_range ( \"2018-01-01\" , freq = \"Q\" , periods = 4 ) ... ) >>> q 2018 Q1 1 2018 Q2 2 2018 Q3 3 2018 Q4 4 Freq : Q - DEC , dtype : int64 >>> q . resample ( \"M\" , convention = \"end\" ) . asfreq () 2018 - 03 1.0 2018 - 04 NaN 2018 - 05 NaN 2018 - 06 2.0 2018 - 07 NaN 2018 - 08 NaN 2018 - 09 3.0 2018 - 10 NaN 2018 - 11 NaN 2018 - 12 4.0 Freq : M , dtype : float64 For DataFrame objects, the keyword on can be used to specify the column instead of the index for resampling. >>> d = { 'price' : [ 10 , 11 , 9 , 13 , 14 , 18 , 17 , 19 ], ... 'volume' : [ 50 , 60 , 40 , 100 , 50 , 100 , 40 , 50 ]} >>> df = pd . DataFrame ( d ) >>> df [ 'week_starting' ] = pd . date_range ( '01/01/2018' , ... periods = 8 , ... freq = 'W' ) >>> df price volume week_starting 0 10 50 2018 - 01 - 07 1 11 60 2018 - 01 - 14 2 9 40 2018 - 01 - 21 3 13 100 2018 - 01 - 28 4 14 50 2018 - 02 - 04 5 18 100 2018 - 02 - 11 6 17 40 2018 - 02 - 18 7 19 50 2018 - 02 - 25 >>> df . resample ( 'ME' , on = 'week_starting' ) . mean () price volume week_starting 2018 - 01 - 31 10.75 62.5 2018 - 02 - 28 17.00 60.0 For a DataFrame with MultiIndex, the keyword level can be used to specify on which level the resampling needs to take place. >>> days = pd . date_range ( '1/1/2000' , periods = 4 , freq = 'D' ) >>> d2 = { 'price' : [ 10 , 11 , 9 , 13 , 14 , 18 , 17 , 19 ], ... 'volume' : [ 50 , 60 , 40 , 100 , 50 , 100 , 40 , 50 ]} >>> df2 = pd . DataFrame ( ... d2 , ... index = pd . MultiIndex . from_product ( ... [ days , [ 'morning' , 'afternoon' ]] ... ) ... ) >>> df2 price volume 2000 - 01 - 01 morning 10 50 afternoon 11 60 2000 - 01 - 02 morning 9 40 afternoon 13 100 2000 - 01 - 03 morning 14 50 afternoon 18 100 2000 - 01 - 04 morning 17 40 afternoon 19 50 >>> df2 . resample ( 'D' , level = 0 ) . sum () price volume 2000 - 01 - 01 21 110 2000 - 01 - 02 22 140 2000 - 01 - 03 32 150 2000 - 01 - 04 36 90 If you want to adjust the start of the bins based on a fixed timestamp: >>> start , end = '2000-10-01 23:30:00' , '2000-10-02 00:30:00' >>> rng = pd . date_range ( start , end , freq = '7min' ) >>> ts = pd . Series ( np . arange ( len ( rng )) * 3 , index = rng ) >>> ts 2000 - 10 - 01 23 : 30 : 00 0 2000 - 10 - 01 23 : 37 : 00 3 2000 - 10 - 01 23 : 44 : 00 6 2000 - 10 - 01 23 : 51 : 00 9 2000 - 10 - 01 23 : 58 : 00 12 2000 - 10 - 02 00 : 05 : 00 15 2000 - 10 - 02 00 : 12 : 00 18 2000 - 10 - 02 00 : 19 : 00 21 2000 - 10 - 02 00 : 26 : 00 24 Freq : 7 min , dtype : int64 >>> ts . resample ( '17min' ) . sum () 2000 - 10 - 01 23 : 14 : 00 0 2000 - 10 - 01 23 : 31 : 00 9 2000 - 10 - 01 23 : 48 : 00 21 2000 - 10 - 02 00 : 05 : 00 54 2000 - 10 - 02 00 : 22 : 00 24 Freq : 17 min , dtype : int64 >>> ts . resample ( '17min' , origin = 'epoch' ) . sum () 2000 - 10 - 01 23 : 18 : 00 0 2000 - 10 - 01 23 : 35 : 00 18 2000 - 10 - 01 23 : 52 : 00 27 2000 - 10 - 02 00 : 09 : 00 39 2000 - 10 - 02 00 : 26 : 00 24 Freq : 17 min , dtype : int64 >>> ts . resample ( '17min' , origin = '2000-01-01' ) . sum () 2000 - 10 - 01 23 : 24 : 00 3 2000 - 10 - 01 23 : 41 : 00 15 2000 - 10 - 01 23 : 58 : 00 45 2000 - 10 - 02 00 : 15 : 00 45 Freq : 17 min , dtype : int64 If you want to adjust the start of the bins with an offset Timedelta, the two following lines are equivalent: >>> ts . resample ( '17min' , origin = 'start' ) . sum () 2000 - 10 - 01 23 : 30 : 00 9 2000 - 10 - 01 23 : 47 : 00 21 2000 - 10 - 02 00 : 04 : 00 54 2000 - 10 - 02 00 : 21 : 00 24 Freq : 17 min , dtype : int64 >>> ts . resample ( '17min' , offset = '23h30min' ) . sum () 2000 - 10 - 01 23 : 30 : 00 9 2000 - 10 - 01 23 : 47 : 00 21 2000 - 10 - 02 00 : 04 : 00 54 2000 - 10 - 02 00 : 21 : 00 24 Freq : 17 min , dtype : int64 If you want to take the largest Timestamp as the end of the bins: >>> ts . resample ( '17min' , origin = 'end' ) . sum () 2000 - 10 - 01 23 : 35 : 00 0 2000 - 10 - 01 23 : 52 : 00 18 2000 - 10 - 02 00 : 09 : 00 27 2000 - 10 - 02 00 : 26 : 00 63 Freq : 17 min , dtype : int64 In contrast with the start_day , you can use end_day to take the ceiling midnight of the largest Timestamp as the end of the bins and drop the bins not containing data: >>> ts . resample ( '17min' , origin = 'end_day' ) . sum () 2000 - 10 - 01 23 : 38 : 00 3 2000 - 10 - 01 23 : 55 : 00 15 2000 - 10 - 02 00 : 12 : 00 45 2000 - 10 - 02 00 : 29 : 00 45 Freq : 17 min , dtype : int64 method","title":"pandas.core.generic.NDFrame.resample"},{"location":"api/pipen.channel/#pandascoregenericndframefirst","text":"</> Select initial periods of time series data based on a date offset. .. deprecated:: 2.1 :meth: .first is deprecated and will be removed in a future version. Please create a mask and filter using .loc instead. For a DataFrame with a sorted DatetimeIndex, this function can select the first few rows based on a date offset. Parameters offset (str, DateOffset or dateutil.relativedelta) \u2014 The offset length of the data that will be selected. For instance,'1ME' will display all the rows having their index within the first month. Returns (Series or DataFrame) A subset of the caller. Raises TypeError \u2014 If the index is not a :class: DatetimeIndex See Also last : Select final periods of time series based on a date offset.at_time : Select values at a particular time of the day. between_time : Select values between particular times of the day. Examples >>> i = pd . date_range ( '2018-04-09' , periods = 4 , freq = '2D' ) >>> ts = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 , 4 ]}, index = i ) >>> ts A 2018 - 04 - 09 1 2018 - 04 - 11 2 2018 - 04 - 13 3 2018 - 04 - 15 4 Get the rows for the first 3 days: >>> ts . first ( '3D' ) A 2018 - 04 - 09 1 2018 - 04 - 11 2 Notice the data for 3 first calendar days were returned, not the first 3 days observed in the dataset, and therefore data for 2018-04-13 was not returned. method","title":"pandas.core.generic.NDFrame.first"},{"location":"api/pipen.channel/#pandascoregenericndframelast","text":"</> Select final periods of time series data based on a date offset. .. deprecated:: 2.1 :meth: .last is deprecated and will be removed in a future version. Please create a mask and filter using .loc instead. For a DataFrame with a sorted DatetimeIndex, this function selects the last few rows based on a date offset. Parameters offset (str, DateOffset, dateutil.relativedelta) \u2014 The offset length of the data that will be selected. For instance,'3D' will display all the rows having their index within the last 3 days. Returns (Series or DataFrame) A subset of the caller. Raises TypeError \u2014 If the index is not a :class: DatetimeIndex See Also first : Select initial periods of time series based on a date offset.at_time : Select values at a particular time of the day. between_time : Select values between particular times of the day. Notes .. deprecated:: 2.1.0 Please create a mask and filter using .loc instead Examples >>> i = pd . date_range ( '2018-04-09' , periods = 4 , freq = '2D' ) >>> ts = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 , 4 ]}, index = i ) >>> ts A 2018 - 04 - 09 1 2018 - 04 - 11 2 2018 - 04 - 13 3 2018 - 04 - 15 4 Get the rows for the last 3 days: >>> ts . last ( '3D' ) # doctest: +SKIP A 2018 - 04 - 13 3 2018 - 04 - 15 4 Notice the data for 3 last calendar days were returned, not the last 3 observed days in the dataset, and therefore data for 2018-04-11 was not returned. method","title":"pandas.core.generic.NDFrame.last"},{"location":"api/pipen.channel/#pandascoregenericndframerank","text":"</> Compute numerical data ranks (1 through n) along axis. By default, equal values are assigned a rank that is the average of the ranks of those values. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Index to direct ranking.For Series this parameter is unused and defaults to 0. method ({'average', 'min', 'max', 'first', 'dense'}, default 'average') \u2014 How to rank the group of records that have the same value (i.e. ties): average: average rank of the group min: lowest rank in the group max: highest rank in the group first: ranks assigned in order they appear in the array dense: like 'min', but rank always increases by 1 between groups. numeric_only (bool, default False) \u2014 For DataFrame objects, rank only numeric columns if set to True. .. versionchanged:: 2.0.0 The default value of numeric_only is now False . na_option ({'keep', 'top', 'bottom'}, default 'keep') \u2014 How to rank NaN values: keep: assign NaN rank to NaN values top: assign lowest rank to NaN values bottom: assign highest rank to NaN values ascending (bool, default True) \u2014 Whether or not the elements should be ranked in ascending order. pct (bool, default False) \u2014 Whether or not to display the returned rankings in percentileform. Returns (same type as caller) Return a Series or DataFrame with data ranks as values. See Also core.groupby.DataFrameGroupBy.rank : Rank of values within each group.core.groupby.SeriesGroupBy.rank : Rank of values within each group. Examples >>> df = pd . DataFrame ( data = { 'Animal' : [ 'cat' , 'penguin' , 'dog' , ... 'spider' , 'snake' ], ... 'Number_legs' : [ 4 , 2 , 4 , 8 , np . nan ]}) >>> df Animal Number_legs 0 cat 4.0 1 penguin 2.0 2 dog 4.0 3 spider 8.0 4 snake NaN Ties are assigned the mean of the ranks (by default) for the group. >>> s = pd . Series ( range ( 5 ), index = list ( \"abcde\" )) >>> s [ \"d\" ] = s [ \"b\" ] >>> s . rank () a 1.0 b 2.5 c 4.0 d 2.5 e 5.0 dtype : float64 The following example shows how the method behaves with the above parameters: default_rank: this is the default behaviour obtained without using any parameter. max_rank: setting method = 'max' the records that have the same values are ranked using the highest rank (e.g.: since 'cat' and 'dog' are both in the 2nd and 3rd position, rank 3 is assigned.) NA_bottom: choosing na_option = 'bottom' , if there are records with NaN values they are placed at the bottom of the ranking. pct_rank: when setting pct = True , the ranking is expressed as percentile rank. >>> df [ 'default_rank' ] = df [ 'Number_legs' ] . rank () >>> df [ 'max_rank' ] = df [ 'Number_legs' ] . rank ( method = 'max' ) >>> df [ 'NA_bottom' ] = df [ 'Number_legs' ] . rank ( na_option = 'bottom' ) >>> df [ 'pct_rank' ] = df [ 'Number_legs' ] . rank ( pct = True ) >>> df Animal Number_legs default_rank max_rank NA_bottom pct_rank 0 cat 4.0 2.5 3.0 2.5 0.625 1 penguin 2.0 1.0 1.0 1.0 0.250 2 dog 4.0 2.5 3.0 2.5 0.625 3 spider 8.0 4.0 4.0 4.0 1.000 4 snake NaN NaN NaN 5.0 NaN method","title":"pandas.core.generic.NDFrame.rank"},{"location":"api/pipen.channel/#pandascoregenericndframealign","text":"</> Align two objects on their axes with the specified join method. Join method is specified for each axis Index. Parameters join ({'outer', 'inner', 'left', 'right'}, default 'outer') \u2014 Type of alignment to be performed. left: use only keys from left frame, preserve key order. right: use only keys from right frame, preserve key order. outer: use union of keys from both frames, sort keys lexicographically. inner: use intersection of keys from both frames, preserve the order of the left keys. axis (allowed axis of the other object, default None) \u2014 Align on index (0), columns (1), or both (None). level (int or level name, default None) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. copy (bool, default True) \u2014 Always returns new objects. If copy=False and no reindexing isrequired then original objects are returned. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` fill_value (scalar, default np.nan) \u2014 Value to use for missing values. Defaults to NaN, but can be any\"compatible\" value. method ({'backfill', 'bfill', 'pad', 'ffill', None}, default None) \u2014 Method to use for filling holes in reindexed Series: pad / ffill: propagate last valid observation forward to next valid. backfill / bfill: use NEXT valid observation to fill gap. .. deprecated:: 2.1 limit (int, default None) \u2014 If method is specified, this is the maximum number of consecutiveNaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None. .. deprecated:: 2.1 fill_axis ({0 or 'index'} for Series, {0 or 'index', 1 or 'columns'} for DataFrame, default 0) \u2014 Filling axis, method and limit. .. deprecated:: 2.1 broadcast_axis ({0 or 'index'} for Series, {0 or 'index', 1 or 'columns'} for DataFrame, default None) \u2014 Broadcast values along this axis, if aligning two objects ofdifferent dimensions. .. deprecated:: 2.1 Returns (tuple of (Series/DataFrame, type of other)) Aligned objects. Examples >>> df = pd . DataFrame ( ... [[ 1 , 2 , 3 , 4 ], [ 6 , 7 , 8 , 9 ]], columns = [ \"D\" , \"B\" , \"E\" , \"A\" ], index = [ 1 , 2 ] ... ) >>> other = pd . DataFrame ( ... [[ 10 , 20 , 30 , 40 ], [ 60 , 70 , 80 , 90 ], [ 600 , 700 , 800 , 900 ]], ... columns = [ \"A\" , \"B\" , \"C\" , \"D\" ], ... index = [ 2 , 3 , 4 ], ... ) >>> df D B E A 1 1 2 3 4 2 6 7 8 9 >>> other A B C D 2 10 20 30 40 3 60 70 80 90 4 600 700 800 900 Align on columns: >>> left , right = df . align ( other , join = \"outer\" , axis = 1 ) >>> left A B C D E 1 4 2 NaN 1 3 2 9 7 NaN 6 8 >>> right A B C D E 2 10 20 30 40 NaN 3 60 70 80 90 NaN 4 600 700 800 900 NaN We can also align on the index: >>> left , right = df . align ( other , join = \"outer\" , axis = 0 ) >>> left D B E A 1 1.0 2.0 3.0 4.0 2 6.0 7.0 8.0 9.0 3 NaN NaN NaN NaN 4 NaN NaN NaN NaN >>> right A B C D 1 NaN NaN NaN NaN 2 10.0 20.0 30.0 40.0 3 60.0 70.0 80.0 90.0 4 600.0 700.0 800.0 900.0 Finally, the default axis=None will align on both index and columns: >>> left , right = df . align ( other , join = \"outer\" , axis = None ) >>> left A B C D E 1 4.0 2.0 NaN 1.0 3.0 2 9.0 7.0 NaN 6.0 8.0 3 NaN NaN NaN NaN NaN 4 NaN NaN NaN NaN NaN >>> right A B C D E 1 NaN NaN NaN NaN NaN 2 10.0 20.0 30.0 40.0 NaN 3 60.0 70.0 80.0 90.0 NaN 4 600.0 700.0 800.0 900.0 NaN method","title":"pandas.core.generic.NDFrame.align"},{"location":"api/pipen.channel/#pandascoregenericndframewhere","text":"</> Replace values where the condition is False. Parameters cond (bool Series/DataFrame, array-like, or callable) \u2014 Where cond is True, keep the original value. WhereFalse, replace with corresponding value from other . If cond is callable, it is computed on the Series/DataFrame and should return boolean Series/DataFrame or array. The callable must not change input Series/DataFrame (though pandas doesn't check it). other (scalar, Series/DataFrame, or callable) \u2014 Entries where cond is False are replaced withcorresponding value from other . If other is callable, it is computed on the Series/DataFrame and should return scalar or Series/DataFrame. The callable must not change input Series/DataFrame (though pandas doesn't check it). If not specified, entries will be filled with the corresponding NULL value ( np.nan for numpy dtypes, pd.NA for extension dtypes). inplace (bool, default False) \u2014 Whether to perform the operation in place on the data. axis (int, default None) \u2014 Alignment axis if needed. For Series this parameter isunused and defaults to 0. level (int, default None) \u2014 Alignment level if needed. See Also :func: DataFrame.mask : Return an object of same shape as self. Notes The where method is an application of the if-then idiom. For each element in the calling DataFrame, if cond is True the element is used; otherwise the corresponding element from the DataFrame other is used. If the axis of other does not align with axis of cond Series/DataFrame, the misaligned index positions will be filled with False. The signature for :func: DataFrame.where differs from :func: numpy.where . Roughly df1.where(m, df2) is equivalent to np.where(m, df1, df2) . For further details and examples see the where documentation in :ref: indexing <indexing.where_mask> . The dtype of the object takes precedence. The fill value is casted to the object's dtype, if this can be done losslessly. Examples >>> s = pd . Series ( range ( 5 )) >>> s . where ( s > 0 ) 0 NaN 1 1.0 2 2.0 3 3.0 4 4.0 dtype : float64 >>> s . mask ( s > 0 ) 0 0.0 1 NaN 2 NaN 3 NaN 4 NaN dtype : float64 >>> s = pd . Series ( range ( 5 )) >>> t = pd . Series ([ True , False ]) >>> s . where ( t , 99 ) 0 0 1 99 2 99 3 99 4 99 dtype : int64 >>> s . mask ( t , 99 ) 0 99 1 1 2 99 3 99 4 99 dtype : int64 >>> s . where ( s > 1 , 10 ) 0 10 1 10 2 2 3 3 4 4 dtype : int64 >>> s . mask ( s > 1 , 10 ) 0 0 1 1 2 10 3 10 4 10 dtype : int64 >>> df = pd . DataFrame ( np . arange ( 10 ) . reshape ( - 1 , 2 ), columns = [ 'A' , 'B' ]) >>> df A B 0 0 1 1 2 3 2 4 5 3 6 7 4 8 9 >>> m = df % 3 == 0 >>> df . where ( m , - df ) A B 0 0 - 1 1 - 2 3 2 - 4 - 5 3 6 - 7 4 - 8 9 >>> df . where ( m , - df ) == np . where ( m , df , - df ) A B 0 True True 1 True True 2 True True 3 True True 4 True True >>> df . where ( m , - df ) == df . mask ( ~ m , - df ) A B 0 True True 1 True True 2 True True 3 True True 4 True True method","title":"pandas.core.generic.NDFrame.where"},{"location":"api/pipen.channel/#pandascoregenericndframemask","text":"</> Replace values where the condition is True. Parameters cond (bool Series/DataFrame, array-like, or callable) \u2014 Where cond is False, keep the original value. WhereTrue, replace with corresponding value from other . If cond is callable, it is computed on the Series/DataFrame and should return boolean Series/DataFrame or array. The callable must not change input Series/DataFrame (though pandas doesn't check it). other (scalar, Series/DataFrame, or callable) \u2014 Entries where cond is True are replaced withcorresponding value from other . If other is callable, it is computed on the Series/DataFrame and should return scalar or Series/DataFrame. The callable must not change input Series/DataFrame (though pandas doesn't check it). If not specified, entries will be filled with the corresponding NULL value ( np.nan for numpy dtypes, pd.NA for extension dtypes). inplace (bool, default False) \u2014 Whether to perform the operation in place on the data. axis (int, default None) \u2014 Alignment axis if needed. For Series this parameter isunused and defaults to 0. level (int, default None) \u2014 Alignment level if needed. See Also :func: DataFrame.where : Return an object of same shape as self. Notes The mask method is an application of the if-then idiom. For each element in the calling DataFrame, if cond is False the element is used; otherwise the corresponding element from the DataFrame other is used. If the axis of other does not align with axis of cond Series/DataFrame, the misaligned index positions will be filled with True. The signature for :func: DataFrame.where differs from :func: numpy.where . Roughly df1.where(m, df2) is equivalent to np.where(m, df1, df2) . For further details and examples see the mask documentation in :ref: indexing <indexing.where_mask> . The dtype of the object takes precedence. The fill value is casted to the object's dtype, if this can be done losslessly. Examples >>> s = pd . Series ( range ( 5 )) >>> s . where ( s > 0 ) 0 NaN 1 1.0 2 2.0 3 3.0 4 4.0 dtype : float64 >>> s . mask ( s > 0 ) 0 0.0 1 NaN 2 NaN 3 NaN 4 NaN dtype : float64 >>> s = pd . Series ( range ( 5 )) >>> t = pd . Series ([ True , False ]) >>> s . where ( t , 99 ) 0 0 1 99 2 99 3 99 4 99 dtype : int64 >>> s . mask ( t , 99 ) 0 99 1 1 2 99 3 99 4 99 dtype : int64 >>> s . where ( s > 1 , 10 ) 0 10 1 10 2 2 3 3 4 4 dtype : int64 >>> s . mask ( s > 1 , 10 ) 0 0 1 1 2 10 3 10 4 10 dtype : int64 >>> df = pd . DataFrame ( np . arange ( 10 ) . reshape ( - 1 , 2 ), columns = [ 'A' , 'B' ]) >>> df A B 0 0 1 1 2 3 2 4 5 3 6 7 4 8 9 >>> m = df % 3 == 0 >>> df . where ( m , - df ) A B 0 0 - 1 1 - 2 3 2 - 4 - 5 3 6 - 7 4 - 8 9 >>> df . where ( m , - df ) == np . where ( m , df , - df ) A B 0 True True 1 True True 2 True True 3 True True 4 True True >>> df . where ( m , - df ) == df . mask ( ~ m , - df ) A B 0 True True 1 True True 2 True True 3 True True 4 True True method","title":"pandas.core.generic.NDFrame.mask"},{"location":"api/pipen.channel/#pandascoregenericndframetruncate","text":"</> Truncate a Series or DataFrame before and after some index value. This is a useful shorthand for boolean indexing based on index values above or below certain thresholds. Parameters before (date, str, int) \u2014 Truncate all rows before this index value. after (date, str, int) \u2014 Truncate all rows after this index value. axis ({0 or 'index', 1 or 'columns'}, optional) \u2014 Axis to truncate. Truncates the index (rows) by default.For Series this parameter is unused and defaults to 0. copy (bool, default is True,) \u2014 Return a copy of the truncated section. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` Returns (type of caller) The truncated Series or DataFrame. See Also DataFrame.loc : Select a subset of a DataFrame by label.DataFrame.iloc : Select a subset of a DataFrame by position. Notes If the index being truncated contains only datetime values, before and after may be specified as strings instead of Timestamps. Examples >>> df = pd . DataFrame ({ 'A' : [ 'a' , 'b' , 'c' , 'd' , 'e' ], ... 'B' : [ 'f' , 'g' , 'h' , 'i' , 'j' ], ... 'C' : [ 'k' , 'l' , 'm' , 'n' , 'o' ]}, ... index = [ 1 , 2 , 3 , 4 , 5 ]) >>> df A B C 1 a f k 2 b g l 3 c h m 4 d i n 5 e j o >>> df . truncate ( before = 2 , after = 4 ) A B C 2 b g l 3 c h m 4 d i n The columns of a DataFrame can be truncated. >>> df . truncate ( before = \"A\" , after = \"B\" , axis = \"columns\" ) A B 1 a f 2 b g 3 c h 4 d i 5 e j For Series, only rows can be truncated. >>> df [ 'A' ] . truncate ( before = 2 , after = 4 ) 2 b 3 c 4 d Name : A , dtype : object The index values in truncate can be datetimes or string dates. >>> dates = pd . date_range ( '2016-01-01' , '2016-02-01' , freq = 's' ) >>> df = pd . DataFrame ( index = dates , data = { 'A' : 1 }) >>> df . tail () A 2016 - 01 - 31 23 : 59 : 56 1 2016 - 01 - 31 23 : 59 : 57 1 2016 - 01 - 31 23 : 59 : 58 1 2016 - 01 - 31 23 : 59 : 59 1 2016 - 02 - 01 00 : 00 : 00 1 >>> df . truncate ( before = pd . Timestamp ( '2016-01-05' ), ... after = pd . Timestamp ( '2016-01-10' )) . tail () A 2016 - 01 - 09 23 : 59 : 56 1 2016 - 01 - 09 23 : 59 : 57 1 2016 - 01 - 09 23 : 59 : 58 1 2016 - 01 - 09 23 : 59 : 59 1 2016 - 01 - 10 00 : 00 : 00 1 Because the index is a DatetimeIndex containing only dates, we can specify before and after as strings. They will be coerced to Timestamps before truncation. >>> df . truncate ( '2016-01-05' , '2016-01-10' ) . tail () A 2016 - 01 - 09 23 : 59 : 56 1 2016 - 01 - 09 23 : 59 : 57 1 2016 - 01 - 09 23 : 59 : 58 1 2016 - 01 - 09 23 : 59 : 59 1 2016 - 01 - 10 00 : 00 : 00 1 Note that truncate assumes a 0 value for any unspecified time component (midnight). This differs from partial string slicing, which returns any partially matching dates. >>> df . loc [ '2016-01-05' : '2016-01-10' , :] . tail () A 2016 - 01 - 10 23 : 59 : 55 1 2016 - 01 - 10 23 : 59 : 56 1 2016 - 01 - 10 23 : 59 : 57 1 2016 - 01 - 10 23 : 59 : 58 1 2016 - 01 - 10 23 : 59 : 59 1 method","title":"pandas.core.generic.NDFrame.truncate"},{"location":"api/pipen.channel/#pandascoregenericndframetz_convert","text":"</> Convert tz-aware axis to target time zone. Parameters tz (str or tzinfo object or None) \u2014 Target time zone. Passing None will convert toUTC and remove the timezone information. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to convert level (int, str, default None) \u2014 If axis is a MultiIndex, convert a specific level. Otherwisemust be None. copy (bool, default True) \u2014 Also make a copy of the underlying data. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` Returns (Series/DataFrame) Object with time zone converted axis. Raises TypeError \u2014 If the axis is tz-naive. Examples Change to another time zone: >>> s = pd . Series ( ... [ 1 ], ... index = pd . DatetimeIndex ([ '2018-09-15 01:30:00+02:00' ]), ... ) >>> s . tz_convert ( 'Asia/Shanghai' ) 2018 - 09 - 15 07 : 30 : 00 + 08 : 00 1 dtype : int64 Pass None to convert to UTC and get a tz-naive index: >>> s = pd . Series ([ 1 ], ... index = pd . DatetimeIndex ([ '2018-09-15 01:30:00+02:00' ])) >>> s . tz_convert ( None ) 2018 - 09 - 14 23 : 30 : 00 1 dtype : int64 method","title":"pandas.core.generic.NDFrame.tz_convert"},{"location":"api/pipen.channel/#pandascoregenericndframetz_localize","text":"</> Localize tz-naive index of a Series or DataFrame to target time zone. This operation localizes the Index. To localize the values in a timezone-naive Series, use :meth: Series.dt.tz_localize . Parameters tz (str or tzinfo or None) \u2014 Time zone to localize. Passing None will remove thetime zone information and preserve local time. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to localize level (int, str, default None) \u2014 If axis ia a MultiIndex, localize a specific level. Otherwisemust be None. copy (bool, default True) \u2014 Also make a copy of the underlying data. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` ambiguous ('infer', bool-ndarray, 'NaT', default 'raise') \u2014 When clocks moved backward due to DST, ambiguous times may arise.For example in Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a situation, the ambiguous parameter dictates how ambiguous times should be handled. 'infer' will attempt to infer fall dst-transition hours based on order bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times) 'NaT' will return NaT where there are ambiguous times 'raise' will raise an AmbiguousTimeError if there are ambiguous times. nonexistent (str, default 'raise') \u2014 A nonexistent time does not exist in a particular timezonewhere clocks moved forward due to DST. Valid values are: 'shift_forward' will shift the nonexistent time forward to the closest existing time 'shift_backward' will shift the nonexistent time backward to the closest existing time 'NaT' will return NaT where there are nonexistent times timedelta objects will shift nonexistent times by the timedelta 'raise' will raise an NonExistentTimeError if there are nonexistent times. Returns (Series/DataFrame) Same type as the input. Raises TypeError \u2014 If the TimeSeries is tz-aware and tz is not None. Examples Localize local times: >>> s = pd . Series ( ... [ 1 ], ... index = pd . DatetimeIndex ([ '2018-09-15 01:30:00' ]), ... ) >>> s . tz_localize ( 'CET' ) 2018 - 09 - 15 01 : 30 : 00 + 02 : 00 1 dtype : int64 Pass None to convert to tz-naive index and preserve local time: >>> s = pd . Series ([ 1 ], ... index = pd . DatetimeIndex ([ '2018-09-15 01:30:00+02:00' ])) >>> s . tz_localize ( None ) 2018 - 09 - 15 01 : 30 : 00 1 dtype : int64 Be careful with DST changes. When there is sequential data, pandas can infer the DST time: >>> s = pd . Series ( range ( 7 ), ... index = pd . DatetimeIndex ([ '2018-10-28 01:30:00' , ... '2018-10-28 02:00:00' , ... '2018-10-28 02:30:00' , ... '2018-10-28 02:00:00' , ... '2018-10-28 02:30:00' , ... '2018-10-28 03:00:00' , ... '2018-10-28 03:30:00' ])) >>> s . tz_localize ( 'CET' , ambiguous = 'infer' ) 2018 - 10 - 28 01 : 30 : 00 + 02 : 00 0 2018 - 10 - 28 02 : 00 : 00 + 02 : 00 1 2018 - 10 - 28 02 : 30 : 00 + 02 : 00 2 2018 - 10 - 28 02 : 00 : 00 + 01 : 00 3 2018 - 10 - 28 02 : 30 : 00 + 01 : 00 4 2018 - 10 - 28 03 : 00 : 00 + 01 : 00 5 2018 - 10 - 28 03 : 30 : 00 + 01 : 00 6 dtype : int64 In some cases, inferring the DST is impossible. In such cases, you can pass an ndarray to the ambiguous parameter to set the DST explicitly >>> s = pd . Series ( range ( 3 ), ... index = pd . DatetimeIndex ([ '2018-10-28 01:20:00' , ... '2018-10-28 02:36:00' , ... '2018-10-28 03:46:00' ])) >>> s . tz_localize ( 'CET' , ambiguous = np . array ([ True , True , False ])) 2018 - 10 - 28 01 : 20 : 00 + 02 : 00 0 2018 - 10 - 28 02 : 36 : 00 + 02 : 00 1 2018 - 10 - 28 03 : 46 : 00 + 01 : 00 2 dtype : int64 If the DST transition causes nonexistent times, you can shift these dates forward or backward with a timedelta object or 'shift_forward' or 'shift_backward' . >>> s = pd . Series ( range ( 2 ), ... index = pd . DatetimeIndex ([ '2015-03-29 02:30:00' , ... '2015-03-29 03:30:00' ])) >>> s . tz_localize ( 'Europe/Warsaw' , nonexistent = 'shift_forward' ) 2015 - 03 - 29 03 : 00 : 00 + 02 : 00 0 2015 - 03 - 29 03 : 30 : 00 + 02 : 00 1 dtype : int64 >>> s . tz_localize ( 'Europe/Warsaw' , nonexistent = 'shift_backward' ) 2015 - 03 - 29 01 : 59 : 59.999999999 + 01 : 00 0 2015 - 03 - 29 03 : 30 : 00 + 02 : 00 1 dtype : int64 >>> s . tz_localize ( 'Europe/Warsaw' , nonexistent = pd . Timedelta ( '1h' )) 2015 - 03 - 29 03 : 30 : 00 + 02 : 00 0 2015 - 03 - 29 03 : 30 : 00 + 02 : 00 1 dtype : int64 method","title":"pandas.core.generic.NDFrame.tz_localize"},{"location":"api/pipen.channel/#pandascoregenericndframedescribe","text":"</> Generate descriptive statistics. Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset's distribution, excluding NaN values. Analyzes both numeric and object series, as well as DataFrame column sets of mixed data types. The output will vary depending on what is provided. Refer to the notes below for more detail. Parameters percentiles (list-like of numbers, optional) \u2014 The percentiles to include in the output. All shouldfall between 0 and 1. The default is [.25, .5, .75] , which returns the 25th, 50th, and 75th percentiles. include ('all', list-like of dtypes or None (default), optional) \u2014 A white list of data types to include in the result. Ignoredfor Series . Here are the options: 'all' : All columns of the input will be included in the output. A list-like of dtypes : Limits the results to the provided data types. To limit the result to numeric types submit numpy.number . To limit it instead to object columns submit the numpy.object data type. Strings can also be used in the style of select_dtypes (e.g. df.describe(include=['O']) ). To select pandas categorical columns, use 'category' None (default) : The result will include all numeric columns. exclude (list-like of dtypes or None (default), optional,) \u2014 A black list of data types to omit from the result. Ignoredfor Series . Here are the options: A list-like of dtypes : Excludes the provided data types from the result. To exclude numeric types submit numpy.number . To exclude object columns submit the data type numpy.object . Strings can also be used in the style of select_dtypes (e.g. df.describe(exclude=['O']) ). To exclude pandas categorical columns, use 'category' None (default) : The result will exclude nothing. Returns (Series or DataFrame) Summary statistics of the Series or Dataframe provided. See Also DataFrame.count: Count number of non-NA/null observations.DataFrame.max: Maximum of the values in the object. DataFrame.min: Minimum of the values in the object. DataFrame.mean: Mean of the values. DataFrame.std: Standard deviation of the observations. DataFrame.select_dtypes: Subset of a DataFrame including/excluding columns based on their dtype. Notes For numeric data, the result's index will include count , mean , std , min , max as well as lower, 50 and upper percentiles. By default the lower percentile is 25 and the upper percentile is 75 . The 50 percentile is the same as the median. For object data (e.g. strings or timestamps), the result's index will include count , unique , top , and freq . The top is the most common value. The freq is the most common value's frequency. Timestamps also include the first and last items. If multiple object values have the highest count, then the count and top results will be arbitrarily chosen from among those with the highest count. For mixed data types provided via a DataFrame , the default is to return only an analysis of numeric columns. If the dataframe consists only of object and categorical data without any numeric columns, the default is to return an analysis of both the object and categorical columns. If include='all' is provided as an option, the result will include a union of attributes of each type. The include and exclude parameters can be used to limit which columns in a DataFrame are analyzed for the output. The parameters are ignored when analyzing a Series . Examples Describing a numeric Series . >>> s = pd . Series ([ 1 , 2 , 3 ]) >>> s . describe () count 3.0 mean 2.0 std 1.0 min 1.0 25 % 1.5 50 % 2.0 75 % 2.5 max 3.0 dtype : float64 Describing a categorical Series . >>> s = pd . Series ([ 'a' , 'a' , 'b' , 'c' ]) >>> s . describe () count 4 unique 3 top a freq 2 dtype : object Describing a timestamp Series . >>> s = pd . Series ([ ... np . datetime64 ( \"2000-01-01\" ), ... np . datetime64 ( \"2010-01-01\" ), ... np . datetime64 ( \"2010-01-01\" ) ... ]) >>> s . describe () count 3 mean 2006 - 09 - 01 08 : 00 : 00 min 2000 - 01 - 01 00 : 00 : 00 25 % 2004 - 12 - 31 12 : 00 : 00 50 % 2010 - 01 - 01 00 : 00 : 00 75 % 2010 - 01 - 01 00 : 00 : 00 max 2010 - 01 - 01 00 : 00 : 00 dtype : object Describing a DataFrame . By default only numeric fields are returned. >>> df = pd . DataFrame ({ 'categorical' : pd . Categorical ([ 'd' , 'e' , 'f' ]), ... 'numeric' : [ 1 , 2 , 3 ], ... 'object' : [ 'a' , 'b' , 'c' ] ... }) >>> df . describe () numeric count 3.0 mean 2.0 std 1.0 min 1.0 25 % 1.5 50 % 2.0 75 % 2.5 max 3.0 Describing all columns of a DataFrame regardless of data type. >>> df . describe ( include = 'all' ) # doctest: +SKIP categorical numeric object count 3 3.0 3 unique 3 NaN 3 top f NaN a freq 1 NaN 1 mean NaN 2.0 NaN std NaN 1.0 NaN min NaN 1.0 NaN 25 % NaN 1.5 NaN 50 % NaN 2.0 NaN 75 % NaN 2.5 NaN max NaN 3.0 NaN Describing a column from a DataFrame by accessing it as an attribute. >>> df . numeric . describe () count 3.0 mean 2.0 std 1.0 min 1.0 25 % 1.5 50 % 2.0 75 % 2.5 max 3.0 Name : numeric , dtype : float64 Including only numeric columns in a DataFrame description. >>> df . describe ( include = [ np . number ]) numeric count 3.0 mean 2.0 std 1.0 min 1.0 25 % 1.5 50 % 2.0 75 % 2.5 max 3.0 Including only string columns in a DataFrame description. >>> df . describe ( include = [ object ]) # doctest: +SKIP object count 3 unique 3 top a freq 1 Including only categorical columns from a DataFrame description. >>> df . describe ( include = [ 'category' ]) categorical count 3 unique 3 top d freq 1 Excluding numeric columns from a DataFrame description. >>> df . describe ( exclude = [ np . number ]) # doctest: +SKIP categorical object count 3 3 unique 3 3 top f a freq 1 1 Excluding object columns from a DataFrame description. >>> df . describe ( exclude = [ object ]) # doctest: +SKIP categorical numeric count 3 3.0 unique 3 NaN top f NaN freq 1 NaN mean NaN 2.0 std NaN 1.0 min NaN 1.0 25 % NaN 1.5 50 % NaN 2.0 75 % NaN 2.5 max NaN 3.0 method","title":"pandas.core.generic.NDFrame.describe"},{"location":"api/pipen.channel/#pandascoregenericndframepct_change","text":"</> Fractional change between the current and a prior element. Computes the fractional change from the immediately previous row by default. This is useful in comparing the fraction of change in a time series of elements. .. note:: Despite the name of this method, it calculates fractional change (also known as per unit change or relative change) and not percentage change. If you need the percentage change, multiply these values by 100. Parameters periods (int, default 1) \u2014 Periods to shift for forming percent change. fill_method ({'backfill', 'bfill', 'pad', 'ffill', None}, default 'pad') \u2014 How to handle NAs before computing percent changes. .. deprecated:: 2.1 All options of fill_method are deprecated except fill_method=None . limit (int, default None) \u2014 The number of consecutive NAs to fill before stopping. .. deprecated:: 2.1 freq (DateOffset, timedelta, or str, optional) \u2014 Increment to use from time series API (e.g. 'ME' or BDay()). **kwargs \u2014 Additional keyword arguments are passed into DataFrame.shift or Series.shift . Returns (Series or DataFrame) The same type as the calling object. See Also Series.diff : Compute the difference of two elements in a Series.DataFrame.diff : Compute the difference of two elements in a DataFrame. Series.shift : Shift the index by some number of periods. DataFrame.shift : Shift the index by some number of periods. Examples Series >>> s = pd . Series ([ 90 , 91 , 85 ]) >>> s 0 90 1 91 2 85 dtype : int64 >>> s . pct_change () 0 NaN 1 0.011111 2 - 0.065934 dtype : float64 >>> s . pct_change ( periods = 2 ) 0 NaN 1 NaN 2 - 0.055556 dtype : float64 See the percentage change in a Series where filling NAs with last valid observation forward to next valid. >>> s = pd . Series ([ 90 , 91 , None , 85 ]) >>> s 0 90.0 1 91.0 2 NaN 3 85.0 dtype : float64 >>> s . ffill () . pct_change () 0 NaN 1 0.011111 2 0.000000 3 - 0.065934 dtype : float64 DataFrame Percentage change in French franc, Deutsche Mark, and Italian lira from 1980-01-01 to 1980-03-01. >>> df = pd . DataFrame ({ ... 'FR' : [ 4.0405 , 4.0963 , 4.3149 ], ... 'GR' : [ 1.7246 , 1.7482 , 1.8519 ], ... 'IT' : [ 804.74 , 810.01 , 860.13 ]}, ... index = [ '1980-01-01' , '1980-02-01' , '1980-03-01' ]) >>> df FR GR IT 1980 - 01 - 01 4.0405 1.7246 804.74 1980 - 02 - 01 4.0963 1.7482 810.01 1980 - 03 - 01 4.3149 1.8519 860.13 >>> df . pct_change () FR GR IT 1980 - 01 - 01 NaN NaN NaN 1980 - 02 - 01 0.013810 0.013684 0.006549 1980 - 03 - 01 0.053365 0.059318 0.061876 Percentage of change in GOOG and APPL stock volume. Shows computing the percentage change between columns. >>> df = pd . DataFrame ({ ... '2016' : [ 1769950 , 30586265 ], ... '2015' : [ 1500923 , 40912316 ], ... '2014' : [ 1371819 , 41403351 ]}, ... index = [ 'GOOG' , 'APPL' ]) >>> df 2016 2015 2014 GOOG 1769950 1500923 1371819 APPL 30586265 40912316 41403351 >>> df . pct_change ( axis = 'columns' , periods =- 1 ) 2016 2015 2014 GOOG 0.179241 0.094112 NaN APPL - 0.252395 - 0.011860 NaN method","title":"pandas.core.generic.NDFrame.pct_change"},{"location":"api/pipen.channel/#pandascoregenericndframerolling","text":"</> Provide rolling window calculations. Parameters window (int, timedelta, str, offset, or BaseIndexer subclass) \u2014 Size of the moving window. If an integer, the fixed number of observations used for each window. If a timedelta, str, or offset, the time period of each window. Each window will be a variable sized based on the observations included in the time-period. This is only valid for datetimelike indexes. To learn more about the offsets & frequency strings, please see this link <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases> __. If a BaseIndexer subclass, the window boundaries based on the defined get_window_bounds method. Additional rolling keyword arguments, namely min_periods , center , closed and step will be passed to get_window_bounds . min_periods (int, default None) \u2014 Minimum number of observations in window required to have a value;otherwise, result is np.nan . For a window that is specified by an offset, min_periods will default to 1. For a window that is specified by an integer, min_periods will default to the size of the window. center (bool, default False) \u2014 If False, set the window labels as the right edge of the window index. If True, set the window labels as the center of the window index. win_type (str, default None) \u2014 If None , all points are evenly weighted. If a string, it must be a valid scipy.signal window function <https://docs.scipy.org/doc/scipy/reference/signal.windows.html#module-scipy.signal.windows> __. Certain Scipy window types require additional parameters to be passed in the aggregation function. The additional parameters must match the keywords specified in the Scipy window type method signature. on (str, optional) \u2014 For a DataFrame, a column label or Index level on whichto calculate the rolling window, rather than the DataFrame's index. Provided integer column is ignored and excluded from result since an integer index is not used to calculate the rolling window. axis (int or str, default 0) \u2014 If 0 or 'index' , roll across the rows. If 1 or 'columns' , roll across the columns. For Series this parameter is unused and defaults to 0. .. deprecated:: 2.1.0 The axis keyword is deprecated. For ``axis=1``, transpose the DataFrame first instead. closed (str, default None) \u2014 If 'right' , the first point in the window is excluded from calculations. If 'left' , the last point in the window is excluded from calculations. If 'both' , the no points in the window are excluded from calculations. If 'neither' , the first and last points in the window are excluded from calculations. Default None ( 'right' ). step (int, default None) \u2014 0 s r . method (str {'single', 'table'}, default 'single') \u2014 0 ) . ` . Returns (pandas.api.typing.Window or pandas.api.typing.Rolling) An instance of Window is returned if win_type is passed. Otherwise,an instance of Rolling is returned. See Also expanding : Provides expanding transformations.ewm : Provides exponential weighted functions. Notes See :ref: Windowing Operations <window.generic> for further usage details and examples. Examples >>> df = pd . DataFrame ({ 'B' : [ 0 , 1 , 2 , np . nan , 4 ]}) >>> df B 0 0.0 1 1.0 2 2.0 3 NaN 4 4.0 window Rolling sum with a window length of 2 observations. >>> df . rolling ( 2 ) . sum () B 0 NaN 1 1.0 2 3.0 3 NaN 4 NaN Rolling sum with a window span of 2 seconds. >>> df_time = pd . DataFrame ({ 'B' : [ 0 , 1 , 2 , np . nan , 4 ]}, ... index = [ pd . Timestamp ( '20130101 09:00:00' ), ... pd . Timestamp ( '20130101 09:00:02' ), ... pd . Timestamp ( '20130101 09:00:03' ), ... pd . Timestamp ( '20130101 09:00:05' ), ... pd . Timestamp ( '20130101 09:00:06' )]) >>> df_time B 2013 - 01 - 01 09 : 00 : 00 0.0 2013 - 01 - 01 09 : 00 : 02 1.0 2013 - 01 - 01 09 : 00 : 03 2.0 2013 - 01 - 01 09 : 00 : 05 NaN 2013 - 01 - 01 09 : 00 : 06 4.0 >>> df_time . rolling ( '2s' ) . sum () B 2013 - 01 - 01 09 : 00 : 00 0.0 2013 - 01 - 01 09 : 00 : 02 1.0 2013 - 01 - 01 09 : 00 : 03 3.0 2013 - 01 - 01 09 : 00 : 05 NaN 2013 - 01 - 01 09 : 00 : 06 4.0 Rolling sum with forward looking windows with 2 observations. >>> indexer = pd . api . indexers . FixedForwardWindowIndexer ( window_size = 2 ) >>> df . rolling ( window = indexer , min_periods = 1 ) . sum () B 0 1.0 1 3.0 2 2.0 3 4.0 4 4.0 min_periods Rolling sum with a window length of 2 observations, but only needs a minimum of 1 observation to calculate a value. >>> df . rolling ( 2 , min_periods = 1 ) . sum () B 0 0.0 1 1.0 2 3.0 3 2.0 4 4.0 center Rolling sum with the result assigned to the center of the window index. >>> df . rolling ( 3 , min_periods = 1 , center = True ) . sum () B 0 1.0 1 3.0 2 3.0 3 6.0 4 4.0 >>> df . rolling ( 3 , min_periods = 1 , center = False ) . sum () B 0 0.0 1 1.0 2 3.0 3 3.0 4 6.0 step Rolling sum with a window length of 2 observations, minimum of 1 observation to calculate a value, and a step of 2. >>> df . rolling ( 2 , min_periods = 1 , step = 2 ) . sum () B 0 0.0 2 3.0 4 4.0 win_type Rolling sum with a window length of 2, using the Scipy 'gaussian' window type. std is required in the aggregation function. >>> df . rolling ( 2 , win_type = 'gaussian' ) . sum ( std = 3 ) B 0 NaN 1 0.986207 2 2.958621 3 NaN 4 NaN on Rolling sum with a window length of 2 days. >>> df = pd . DataFrame ({ ... 'A' : [ pd . to_datetime ( '2020-01-01' ), ... pd . to_datetime ( '2020-01-01' ), ... pd . to_datetime ( '2020-01-02' ),], ... 'B' : [ 1 , 2 , 3 ], }, ... index = pd . date_range ( '2020' , periods = 3 )) >>> df A B 2020 - 01 - 01 2020 - 01 - 01 1 2020 - 01 - 02 2020 - 01 - 01 2 2020 - 01 - 03 2020 - 01 - 02 3 >>> df . rolling ( '2D' , on = 'A' ) . sum () A B 2020 - 01 - 01 2020 - 01 - 01 1.0 2020 - 01 - 02 2020 - 01 - 01 3.0 2020 - 01 - 03 2020 - 01 - 02 6.0 method","title":"pandas.core.generic.NDFrame.rolling"},{"location":"api/pipen.channel/#pandascoregenericndframeexpanding","text":"</> Provide expanding window calculations. Parameters min_periods (int, default 1) \u2014 Minimum number of observations in window required to have a value;otherwise, result is np.nan . axis (int or str, default 0) \u2014 If 0 or 'index' , roll across the rows. If 1 or 'columns' , roll across the columns. For Series this parameter is unused and defaults to 0. method (str {'single', 'table'}, default 'single') \u2014 Execute the rolling operation per single column or row ( 'single' )or over the entire object ( 'table' ). This argument is only implemented when specifying engine='numba' in the method call. .. versionadded:: 1.3.0 See Also rolling : Provides rolling window calculations.ewm : Provides exponential weighted functions. Notes See :ref: Windowing Operations <window.expanding> for further usage details and examples. Examples >>> df = pd . DataFrame ({ \"B\" : [ 0 , 1 , 2 , np . nan , 4 ]}) >>> df B 0 0.0 1 1.0 2 2.0 3 NaN 4 4.0 min_periods Expanding sum with 1 vs 3 observations needed to calculate a value. >>> df . expanding ( 1 ) . sum () B 0 0.0 1 1.0 2 3.0 3 3.0 4 7.0 >>> df . expanding ( 3 ) . sum () B 0 NaN 1 NaN 2 3.0 3 3.0 4 7.0 method","title":"pandas.core.generic.NDFrame.expanding"},{"location":"api/pipen.channel/#pandascoregenericndframeewm","text":"</> Provide exponentially weighted (EW) calculations. Exactly one of com , span , halflife , or alpha must be provided if times is not provided. If times is provided, halflife and one of com , span or alpha may be provided. Parameters com (float, optional) \u2014 Specify decay in terms of center of mass :math: \\alpha = 1 / (1 + com) , for :math: com \\geq 0 . span (float, optional) \u2014 Specify decay in terms of span :math: \\alpha = 2 / (span + 1) , for :math: span \\geq 1 . halflife (float, str, timedelta, optional) \u2014 Specify decay in terms of half-life :math: \\alpha = 1 - \\exp\\left(-\\ln(2) / halflife\\right) , for :math: halflife > 0 . If times is specified, a timedelta convertible unit over which an observation decays to half its value. Only applicable to mean() , and halflife value will not apply to the other functions. alpha (float, optional) \u2014 Specify smoothing factor :math: \\alpha directly :math: 0 < \\alpha \\leq 1 . min_periods (int, default 0) \u2014 Minimum number of observations in window required to have a value;otherwise, result is np.nan . adjust (bool, default True) \u2014 Divide by decaying adjustment factor in beginning periods to accountfor imbalance in relative weightings (viewing EWMA as a moving average). When adjust=True (default), the EW function is calculated using weights :math: w_i = (1 - \\alpha)^i . For example, the EW moving average of the series [:math: x_0, x_1, ..., x_t ] would be: .. math:: y_t = \\frac{x_t + (1 - \\alpha)x_{t-1} + (1 - \\alpha)^2 x_{t-2} + ... + (1 - \\alpha)^t x_0}{1 + (1 - \\alpha) + (1 - \\alpha)^2 + ... + (1 - \\alpha)^t} When adjust=False , the exponentially weighted function is calculated recursively: .. math:: \\begin{split} y_0 &= x_0\\ y_t &= (1 - \\alpha) y_{t-1} + \\alpha x_t, \\end{split} ignore_na (bool, default False) \u2014 Ignore missing values when calculating weights. When ignore_na=False (default), weights are based on absolute positions. For example, the weights of :math: x_0 and :math: x_2 used in calculating the final weighted average of [:math: x_0 , None, :math: x_2 ] are :math: (1-\\alpha)^2 and :math: 1 if adjust=True , and :math: (1-\\alpha)^2 and :math: \\alpha if adjust=False . When ignore_na=True , weights are based on relative positions. For example, the weights of :math: x_0 and :math: x_2 used in calculating the final weighted average of [:math: x_0 , None, :math: x_2 ] are :math: 1-\\alpha and :math: 1 if adjust=True , and :math: 1-\\alpha and :math: \\alpha if adjust=False . axis ({0, 1}, default 0) \u2014 If 0 or 'index' , calculate across the rows. If 1 or 'columns' , calculate across the columns. For Series this parameter is unused and defaults to 0. times (np.ndarray, Series, default None) \u2014 . d . . method (str {'single', 'table'}, default 'single') \u2014 .. versionadded:: 1.4.0 Execute the rolling operation per single column or row ( 'single' ) or over the entire object ( 'table' ). This argument is only implemented when specifying engine='numba' in the method call. Only applicable to mean() See Also rolling : Provides rolling window calculations.expanding : Provides expanding transformations. Notes See :ref: Windowing Operations <window.exponentially_weighted> for further usage details and examples. Examples >>> df = pd . DataFrame ({ 'B' : [ 0 , 1 , 2 , np . nan , 4 ]}) >>> df B 0 0.0 1 1.0 2 2.0 3 NaN 4 4.0 >>> df . ewm ( com = 0.5 ) . mean () B 0 0.000000 1 0.750000 2 1.615385 3 1.615385 4 3.670213 >>> df . ewm ( alpha = 2 / 3 ) . mean () B 0 0.000000 1 0.750000 2 1.615385 3 1.615385 4 3.670213 adjust >>> df . ewm ( com = 0.5 , adjust = True ) . mean () B 0 0.000000 1 0.750000 2 1.615385 3 1.615385 4 3.670213 >>> df . ewm ( com = 0.5 , adjust = False ) . mean () B 0 0.000000 1 0.666667 2 1.555556 3 1.555556 4 3.650794 ignore_na >>> df . ewm ( com = 0.5 , ignore_na = True ) . mean () B 0 0.000000 1 0.750000 2 1.615385 3 1.615385 4 3.225000 >>> df . ewm ( com = 0.5 , ignore_na = False ) . mean () B 0 0.000000 1 0.750000 2 1.615385 3 1.615385 4 3.670213 times Exponentially weighted mean with weights calculated with a timedelta halflife relative to times . >>> times = [ '2020-01-01' , '2020-01-03' , '2020-01-10' , '2020-01-15' , '2020-01-17' ] >>> df . ewm ( halflife = '4 days' , times = pd . DatetimeIndex ( times )) . mean () B 0 0.000000 1 0.585786 2 1.523889 3 1.523889 4 3.233686 method","title":"pandas.core.generic.NDFrame.ewm"},{"location":"api/pipen.channel/#pandascoregenericndframefirst_valid_index","text":"</> Return index for first non-NA value or None, if no non-NA value is found. Examples For Series: >>> s = pd . Series ([ None , 3 , 4 ]) >>> s . first_valid_index () 1 >>> s . last_valid_index () 2 >>> s = pd . Series ([ None , None ]) >>> print ( s . first_valid_index ()) None >>> print ( s . last_valid_index ()) None If all elements in Series are NA/null, returns None. >>> s = pd . Series () >>> print ( s . first_valid_index ()) None >>> print ( s . last_valid_index ()) None If Series is empty, returns None. For DataFrame: >>> df = pd . DataFrame ({ 'A' : [ None , None , 2 ], 'B' : [ None , 3 , 4 ]}) >>> df A B 0 NaN NaN 1 NaN 3.0 2 2.0 4.0 >>> df . first_valid_index () 1 >>> df . last_valid_index () 2 >>> df = pd . DataFrame ({ 'A' : [ None , None , None ], 'B' : [ None , None , None ]}) >>> df A B 0 None None 1 None None 2 None None >>> print ( df . first_valid_index ()) None >>> print ( df . last_valid_index ()) None If all elements in DataFrame are NA/null, returns None. >>> df = pd . DataFrame () >>> df Empty DataFrame Columns : [] Index : [] >>> print ( df . first_valid_index ()) None >>> print ( df . last_valid_index ()) None If DataFrame is empty, returns None. method","title":"pandas.core.generic.NDFrame.first_valid_index"},{"location":"api/pipen.channel/#pandascoregenericndframelast_valid_index","text":"</> Return index for last non-NA value or None, if no non-NA value is found. Examples For Series: >>> s = pd . Series ([ None , 3 , 4 ]) >>> s . first_valid_index () 1 >>> s . last_valid_index () 2 >>> s = pd . Series ([ None , None ]) >>> print ( s . first_valid_index ()) None >>> print ( s . last_valid_index ()) None If all elements in Series are NA/null, returns None. >>> s = pd . Series () >>> print ( s . first_valid_index ()) None >>> print ( s . last_valid_index ()) None If Series is empty, returns None. For DataFrame: >>> df = pd . DataFrame ({ 'A' : [ None , None , 2 ], 'B' : [ None , 3 , 4 ]}) >>> df A B 0 NaN NaN 1 NaN 3.0 2 2.0 4.0 >>> df . first_valid_index () 1 >>> df . last_valid_index () 2 >>> df = pd . DataFrame ({ 'A' : [ None , None , None ], 'B' : [ None , None , None ]}) >>> df A B 0 None None 1 None None 2 None None >>> print ( df . first_valid_index ()) None >>> print ( df . last_valid_index ()) None If all elements in DataFrame are NA/null, returns None. >>> df = pd . DataFrame () >>> df Empty DataFrame Columns : [] Index : [] >>> print ( df . first_valid_index ()) None >>> print ( df . last_valid_index ()) None If DataFrame is empty, returns None. method","title":"pandas.core.generic.NDFrame.last_valid_index"},{"location":"api/pipen.channel/#pandascoreframedataframedataframe","text":"</> Return the dataframe interchange object implementing the interchange protocol. Parameters nan_as_null (bool, default False) \u2014 nan_as_null is DEPRECATED and has no effect. Please avoid usingit; it will be removed in a future release. allow_copy (bool, default True) \u2014 Whether to allow memory copying when exporting. If set to Falseit would cause non-zero-copy exports to fail. Returns (DataFrame interchange object) The object which consuming library can use to ingress the dataframe. Notes Details on the interchange protocol: https://data-apis.org/dataframe-protocol/latest/index.html Examples >>> df_not_necessarily_pandas = pd . DataFrame ({ 'A' : [ 1 , 2 ], 'B' : [ 3 , 4 ]}) >>> interchange_object = df_not_necessarily_pandas . __dataframe__ () >>> interchange_object . column_names () Index ([ 'A' , 'B' ], dtype = 'object' ) >>> df_pandas = ( pd . api . interchange . from_dataframe ... ( interchange_object . select_columns_by_name ([ 'A' ]))) >>> df_pandas A 0 1 1 2 These methods ( column_names , select_columns_by_name ) should work for any dataframe library which implements the interchange protocol. method","title":"pandas.core.frame.DataFrame.dataframe"},{"location":"api/pipen.channel/#pandascoreframedataframedataframe_consortium_standard","text":"</> Provide entry point to the Consortium DataFrame Standard API. This is developed and maintained outside of pandas. Please report any issues to https://github.com/data-apis/dataframe-api-compat. method","title":"pandas.core.frame.DataFrame.dataframe_consortium_standard"},{"location":"api/pipen.channel/#pandascoreframedataframearrow_c_stream","text":"</> Export the pandas DataFrame as an Arrow C stream PyCapsule. This relies on pyarrow to convert the pandas DataFrame to the Arrow format (and follows the default behaviour of pyarrow.Table.from_pandas in its handling of the index, i.e. store the index as a column except for RangeIndex). This conversion is not necessarily zero-copy. Parameters requested_schema (PyCapsule, default None) \u2014 The schema to which the dataframe should be casted, passed as aPyCapsule containing a C ArrowSchema representation of the requested schema. method","title":"pandas.core.frame.DataFrame.arrow_c_stream"},{"location":"api/pipen.channel/#pandascoreframedataframerepr","text":"</> Return a string representation for a particular DataFrame. method","title":"pandas.core.frame.DataFrame.repr"},{"location":"api/pipen.channel/#pandascoreframedataframeto_string","text":"</> Render a DataFrame to a console-friendly tabular output. Parameters buf (str, Path or StringIO-like, optional, default None) \u2014 Buffer to write to. If None, the output is returned as a string. columns (array-like, optional, default None) \u2014 The subset of columns to write. Writes all columns by default. col_space (int, list or dict of int, optional) \u2014 The minimum width of each column. If a list of ints is given every integers corresponds with one column. If a dict is given, the key references the column, while the value defines the space to use.. header (bool or list of str, optional) \u2014 Write out the column names. If a list of columns is given, it is assumed to be aliases for the column names. index (bool, optional, default True) \u2014 Whether to print index (row) labels. na_rep (str, optional, default 'NaN') \u2014 String representation of NaN to use. formatters (list, tuple or dict of one-param. functions, optional) \u2014 Formatter functions to apply to columns' elements by position orname. The result of each function must be a unicode string. List/tuple must be of length equal to the number of columns. float_format (one-parameter function, optional, default None) \u2014 Formatter function to apply to columns' elements if they arefloats. This function must return a unicode string and will be applied only to the non- NaN elements, with NaN being handled by na_rep . sparsify (bool, optional, default True) \u2014 Set to False for a DataFrame with a hierarchical index to printevery multiindex key at each row. index_names (bool, optional, default True) \u2014 Prints the names of the indexes. justify (str, default None) \u2014 How to justify the column labels. If None uses the option fromthe print configuration (controlled by set_option), 'right' out of the box. Valid values are left right center justify justify-all start end inherit match-parent initial unset. max_rows (int, optional) \u2014 Maximum number of rows to display in the console. max_cols (int, optional) \u2014 Maximum number of columns to display in the console. show_dimensions (bool, default False) \u2014 Display DataFrame dimensions (number of rows by number of columns). decimal (str, default '.') \u2014 Character recognized as decimal separator, e.g. ',' in Europe. line_width (int, optional) \u2014 Width to wrap a line in characters. min_rows (int, optional) \u2014 The number of rows to display in the console in a truncated repr(when number of rows is above max_rows ). max_colwidth (int, optional) \u2014 Max width to truncate each column in characters. By default, no limit. encoding (str, default \"utf-8\") \u2014 Set character encoding. Returns (str or None) If buf is None, returns the result as a string. Otherwise returnsNone. See Also to_html : Convert DataFrame to HTML. Examples >>> d = { 'col1' : [ 1 , 2 , 3 ], 'col2' : [ 4 , 5 , 6 ]} >>> df = pd . DataFrame ( d ) >>> print ( df . to_string ()) col1 col2 0 1 4 1 2 5 2 3 6 generator","title":"pandas.core.frame.DataFrame.to_string"},{"location":"api/pipen.channel/#pandascoreframedataframeitems","text":"</> Iterate over (column name, Series) pairs. Iterates over the DataFrame columns, returning a tuple with the column name and the content as a Series. Yields (label : object) The column names for the DataFrame being iterated over.ent : Series The column entries belonging to each label, as a Series. See Also DataFrame.iterrows : Iterate over DataFrame rows as (index, Series) pairs. DataFrame.itertuples : Iterate over DataFrame rows as namedtuples of the values. Examples >>> df = pd . DataFrame ({ 'species' : [ 'bear' , 'bear' , 'marsupial' ], ... 'population' : [ 1864 , 22000 , 80000 ]}, ... index = [ 'panda' , 'polar' , 'koala' ]) >>> df species population panda bear 1864 polar bear 22000 koala marsupial 80000 >>> for label , content in df . items (): ... print ( f 'label: { label } ' ) ... print ( f 'content: { content } ' , sep = ' \\n ' ) ... label : species content : panda bear polar bear koala marsupial Name : species , dtype : object label : population content : panda 1864 polar 22000 koala 80000 Name : population , dtype : int64 generator","title":"pandas.core.frame.DataFrame.items"},{"location":"api/pipen.channel/#pandascoreframedataframeiterrows","text":"</> Iterate over DataFrame rows as (index, Series) pairs. Yields (index : label or tuple of label) The index of the row. A tuple for a MultiIndex . : Series The data of the row as a Series. See Also DataFrame.itertuples : Iterate over DataFrame rows as namedtuples of the values.DataFrame.items : Iterate over (column name, Series) pairs. Notes Because iterrows returns a Series for each row, it does not preserve dtypes across the rows (dtypes are preserved across columns for DataFrames). To preserve dtypes while iterating over the rows, it is better to use :meth: itertuples which returns namedtuples of the values and which is generally faster than iterrows . You should never modify something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect. Examples )] w 0 5 4 ) 4 ) 4 method","title":"pandas.core.frame.DataFrame.iterrows"},{"location":"api/pipen.channel/#pandascoreframedataframeitertuples","text":"</> Iterate over DataFrame rows as namedtuples. Parameters index (bool, default True) \u2014 If True, return the index as the first element of the tuple. name (str or None, default \"Pandas\") \u2014 The name of the returned namedtuples or None to return regulartuples. Returns (iterator) An object to iterate over namedtuples for each row in theDataFrame with the first field possibly being the index and following fields being the column values. See Also DataFrame.iterrows : Iterate over DataFrame rows as (index, Series) pairs. DataFrame.items : Iterate over (column name, Series) pairs. Notes The column names will be renamed to positional names if they are invalid Python identifiers, repeated, or start with an underscore. Examples >>> df = pd . DataFrame ({ 'num_legs' : [ 4 , 2 ], 'num_wings' : [ 0 , 2 ]}, ... index = [ 'dog' , 'hawk' ]) >>> df num_legs num_wings dog 4 0 hawk 2 2 >>> for row in df . itertuples (): ... print ( row ) ... Pandas ( Index = 'dog' , num_legs = 4 , num_wings = 0 ) Pandas ( Index = 'hawk' , num_legs = 2 , num_wings = 2 ) By setting the index parameter to False we can remove the index as the first element of the tuple: >>> for row in df . itertuples ( index = False ): ... print ( row ) ... Pandas ( num_legs = 4 , num_wings = 0 ) Pandas ( num_legs = 2 , num_wings = 2 ) With the name parameter set we set a custom name for the yielded namedtuples: >>> for row in df . itertuples ( name = 'Animal' ): ... print ( row ) ... Animal ( Index = 'dog' , num_legs = 4 , num_wings = 0 ) Animal ( Index = 'hawk' , num_legs = 2 , num_wings = 2 ) method","title":"pandas.core.frame.DataFrame.itertuples"},{"location":"api/pipen.channel/#pandascoreframedataframelen","text":"</> Returns length of info axis, but here we use the index. method","title":"pandas.core.frame.DataFrame.len"},{"location":"api/pipen.channel/#pandascoreframedataframedot","text":"</> Compute the matrix multiplication between the DataFrame and other. This method computes the matrix product between the DataFrame and the values of an other Series, DataFrame or a numpy array. It can also be called using self @ other . Parameters other (Series, DataFrame or array-like) \u2014 The other object to compute the matrix product with. Returns (Series or DataFrame) If other is a Series, return the matrix product between self andother as a Series. If other is a DataFrame or a numpy.array, return the matrix product of self and other in a DataFrame of a np.array. See Also Series.dot: Similar method for Series. Notes The dimensions of DataFrame and other must be compatible in order to compute the matrix multiplication. In addition, the column names of DataFrame and the index of other must contain the same values, as they will be aligned prior to the multiplication. The dot method for Series computes the inner product, instead of the matrix product here. Examples Here we multiply a DataFrame with a Series. >>> df = pd . DataFrame ([[ 0 , 1 , - 2 , - 1 ], [ 1 , 1 , 1 , 1 ]]) >>> s = pd . Series ([ 1 , 1 , 2 , 1 ]) >>> df . dot ( s ) 0 - 4 1 5 dtype : int64 Here we multiply a DataFrame with another DataFrame. >>> other = pd . DataFrame ([[ 0 , 1 ], [ 1 , 2 ], [ - 1 , - 1 ], [ 2 , 0 ]]) >>> df . dot ( other ) 0 1 0 1 4 1 2 2 Note that the dot method give the same result as @ >>> df @ other 0 1 0 1 4 1 2 2 The dot method works also if other is an np.array. >>> arr = np . array ([[ 0 , 1 ], [ 1 , 2 ], [ - 1 , - 1 ], [ 2 , 0 ]]) >>> df . dot ( arr ) 0 1 0 1 4 1 2 2 Note how shuffling of the objects does not change the result. >>> s2 = s . reindex ([ 1 , 0 , 2 , 3 ]) >>> df . dot ( s2 ) 0 - 4 1 5 dtype : int64 method","title":"pandas.core.frame.DataFrame.dot"},{"location":"api/pipen.channel/#pandascoreframedataframematmul","text":"</> Matrix multiplication using binary @ operator. method","title":"pandas.core.frame.DataFrame.matmul"},{"location":"api/pipen.channel/#pandascoreframedataframermatmul","text":"</> Matrix multiplication using binary @ operator. classmethod","title":"pandas.core.frame.DataFrame.rmatmul"},{"location":"api/pipen.channel/#pandascoreframedataframefrom_dict","text":"</> Construct DataFrame from dict of array-like or dicts. Creates DataFrame object from dictionary by columns or by index allowing dtype specification. Parameters data (dict) \u2014 Of the form {field : array-like} or {field : dict}. orient ({'columns', 'index', 'tight'}, default 'columns') \u2014 The \"orientation\" of the data. If the keys of the passed dictshould be the columns of the resulting DataFrame, pass 'columns' (default). Otherwise if the keys should be rows, pass 'index'. If 'tight', assume a dict with keys ['index', 'columns', 'data', 'index_names', 'column_names']. .. versionadded:: 1.4.0 'tight' as an allowed value for the orient argument dtype (dtype, default None) \u2014 Data type to force after DataFrame construction, otherwise infer. columns (list, default None) \u2014 Column labels to use when orient='index' . Raises a ValueErrorif used with orient='columns' or orient='tight' . See Also DataFrame.from_records : DataFrame from structured ndarray, sequence of tuples or dicts, or DataFrame. DataFrame : DataFrame object creation using constructor. DataFrame.to_dict : Convert the DataFrame to a dictionary. Examples By default the keys of the dict become the DataFrame columns: >>> data = { 'col_1' : [ 3 , 2 , 1 , 0 ], 'col_2' : [ 'a' , 'b' , 'c' , 'd' ]} >>> pd . DataFrame . from_dict ( data ) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d Specify orient='index' to create the DataFrame using dictionary keys as rows: >>> data = { 'row_1' : [ 3 , 2 , 1 , 0 ], 'row_2' : [ 'a' , 'b' , 'c' , 'd' ]} >>> pd . DataFrame . from_dict ( data , orient = 'index' ) 0 1 2 3 row_1 3 2 1 0 row_2 a b c d When using the 'index' orientation, the column names can be specified manually: >>> pd . DataFrame . from_dict ( data , orient = 'index' , ... columns = [ 'A' , 'B' , 'C' , 'D' ]) A B C D row_1 3 2 1 0 row_2 a b c d Specify orient='tight' to create the DataFrame using a 'tight' format: >>> data = { 'index' : [( 'a' , 'b' ), ( 'a' , 'c' )], ... 'columns' : [( 'x' , 1 ), ( 'y' , 2 )], ... 'data' : [[ 1 , 3 ], [ 2 , 4 ]], ... 'index_names' : [ 'n1' , 'n2' ], ... 'column_names' : [ 'z1' , 'z2' ]} >>> pd . DataFrame . from_dict ( data , orient = 'tight' ) z1 x y z2 1 2 n1 n2 a b 1 3 c 2 4 method","title":"pandas.core.frame.DataFrame.from_dict"},{"location":"api/pipen.channel/#pandascoreframedataframeto_numpy","text":"</> Convert the DataFrame to a NumPy array. By default, the dtype of the returned array will be the common NumPy dtype of all types in the DataFrame. For example, if the dtypes are float16 and float32 , the results dtype will be float32 . This may require copying data and coercing values, which may be expensive. Parameters dtype (str or numpy.dtype, optional) \u2014 The dtype to pass to :meth: numpy.asarray . copy (bool, default False) \u2014 Whether to ensure that the returned value is not a view onanother array. Note that copy=False does not ensure that to_numpy() is no-copy. Rather, copy=True ensure that a copy is made, even if not strictly necessary. na_value (Any, optional) \u2014 The value to use for missing values. The default value dependson dtype and the dtypes of the DataFrame columns. See Also Series.to_numpy : Similar method for Series. Examples >>> pd . DataFrame ({ \"A\" : [ 1 , 2 ], \"B\" : [ 3 , 4 ]}) . to_numpy () array ([[ 1 , 3 ], [ 2 , 4 ]]) With heterogeneous data, the lowest common type will have to be used. >>> df = pd . DataFrame ({ \"A\" : [ 1 , 2 ], \"B\" : [ 3.0 , 4.5 ]}) >>> df . to_numpy () array ([[ 1. , 3. ], [ 2. , 4.5 ]]) For a mix of numeric and non-numeric types, the output array will have object dtype. >>> df [ 'C' ] = pd . date_range ( '2000' , periods = 2 ) >>> df . to_numpy () array ([[ 1 , 3.0 , Timestamp ( '2000-01-01 00:00:00' )], [ 2 , 4.5 , Timestamp ( '2000-01-02 00:00:00' )]], dtype = object ) method","title":"pandas.core.frame.DataFrame.to_numpy"},{"location":"api/pipen.channel/#pandascoreframedataframeto_dict","text":"</> Convert the DataFrame to a dictionary. The type of the key-value pairs can be customized with the parameters (see below). Parameters orient (str {'dict', 'list', 'series', 'split', 'tight', 'records', 'index'}) \u2014 Determines the type of the values of the dictionary. 'dict' (default) : dict like {column -> {index -> value}} 'list' : dict like {column -> [values]} 'series' : dict like {column -> Series(values)} 'split' : dict like 'tight' : dict like {'index' -> [index], 'columns' -> [columns], 'data' -> [values], 'index_names' -> [index.names], 'column_names' -> [column.names]} 'records' : list like [{column -> value}, ... , {column -> value}] 'index' : dict like {index -> {column -> value}} .. versionadded:: 1.4.0 'tight' as an allowed value for the orient argument into (class, default dict) \u2014 The collections.abc.MutableMapping subclass used for all Mappingsin the return value. Can be the actual class or an empty instance of the mapping type you want. If you want a collections.defaultdict, you must pass it initialized. index (bool, default True) \u2014 Whether to include the index item (and index_names item if orient is 'tight') in the returned dictionary. Can only be False when orient is 'split' or 'tight'. .. versionadded:: 2.0.0 Returns (dict, list or collections.abc.MutableMapping) Return a collections.abc.MutableMapping object representing theDataFrame. The resulting transformation depends on the orient parameter. See Also DataFrame.from_dict: Create a DataFrame from a dictionary.DataFrame.to_json: Convert a DataFrame to JSON format. Examples >>> df = pd . DataFrame ({ 'col1' : [ 1 , 2 ], ... 'col2' : [ 0.5 , 0.75 ]}, ... index = [ 'row1' , 'row2' ]) >>> df col1 col2 row1 1 0.50 row2 2 0.75 >>> df . to_dict () { 'col1' : { 'row1' : 1 , 'row2' : 2 }, 'col2' : { 'row1' : 0.5 , 'row2' : 0.75 }} You can specify the return orientation. >>> df . to_dict ( 'series' ) { 'col1' : row1 1 row2 2 Name : col1 , dtype : int64 , 'col2' : row1 0.50 row2 0.75 Name : col2 , dtype : float64 } >>> df . to_dict ( 'split' ) { 'index' : [ 'row1' , 'row2' ], 'columns' : [ 'col1' , 'col2' ], 'data' : [[ 1 , 0.5 ], [ 2 , 0.75 ]]} >>> df . to_dict ( 'records' ) [{ 'col1' : 1 , 'col2' : 0.5 }, { 'col1' : 2 , 'col2' : 0.75 }] >>> df . to_dict ( 'index' ) { 'row1' : { 'col1' : 1 , 'col2' : 0.5 }, 'row2' : { 'col1' : 2 , 'col2' : 0.75 }} >>> df . to_dict ( 'tight' ) { 'index' : [ 'row1' , 'row2' ], 'columns' : [ 'col1' , 'col2' ], 'data' : [[ 1 , 0.5 ], [ 2 , 0.75 ]], 'index_names' : [ None ], 'column_names' : [ None ]} You can also specify the mapping type. >>> from collections import OrderedDict , defaultdict >>> df . to_dict ( into = OrderedDict ) OrderedDict ([( 'col1' , OrderedDict ([( 'row1' , 1 ), ( 'row2' , 2 )])), ( 'col2' , OrderedDict ([( 'row1' , 0.5 ), ( 'row2' , 0.75 )]))]) If you want a defaultdict , you need to initialize it: >>> dd = defaultdict ( list ) >>> df . to_dict ( 'records' , into = dd ) [ defaultdict ( < class ' list '>, {' col1 ': 1, ' col2 ': 0.5}), defaultdict ( < class ' list '>, {' col1 ': 2, ' col2 ': 0.75})] method","title":"pandas.core.frame.DataFrame.to_dict"},{"location":"api/pipen.channel/#pandascoreframedataframeto_gbq","text":"</> Write a DataFrame to a Google BigQuery table. .. deprecated:: 2.2.0 Please use pandas_gbq.to_gbq instead. This function requires the pandas-gbq package <https://pandas-gbq.readthedocs.io> __. See the How to authenticate with Google BigQuery <https://pandas-gbq.readthedocs.io/en/latest/howto/authentication.html> __ guide for authentication instructions. Parameters destination_table (str) \u2014 Name of table to be written, in the form dataset.tablename . project_id (str, optional) \u2014 Google BigQuery Account project ID. Optional when available fromthe environment. chunksize (int, optional) \u2014 Number of rows to be inserted in each chunk from the dataframe.Set to None to load the whole dataframe at once. reauth (bool, default False) \u2014 Force Google BigQuery to re-authenticate the user. This is usefulif multiple accounts are used. if_exists (str, default 'fail') \u2014 Behavior when the destination table exists. Value can be one of: 'fail' If table exists raise pandasgbq.gbq.TableCreationError. 'replace' If table exists, drop it, recreate it, and insert data. 'append' If table exists, insert data. Create if does not exist. auth_local_webserver (bool, default True) \u2014 Use the local webserver flow instead of the console flow when getting user credentials. .. _local webserver flow: https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_local_server .. _console flow: https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_console New in version 0.2.0 of pandas-gbq . .. versionchanged:: 1.5.0 Default value is changed to True . Google has deprecated the auth_local_webserver = False \"out of band\" (copy-paste) flow <https://developers.googleblog.com/2022/02/making-oauth-flows-safer.html?m=1#disallowed-oob> _. table_schema (list of dicts, optional) \u2014 List of BigQuery table fields to which according DataFramecolumns conform to, e.g. [{'name': 'col1', 'type': 'STRING'},...] . If schema is not provided, it will be generated according to dtypes of DataFrame columns. See BigQuery API documentation on available names of a field. New in version 0.3.1 of pandas-gbq . location (str, optional) \u2014 Location where the load job should run. See the BigQuery locationsdocumentation <https://cloud.google.com/bigquery/docs/dataset-locations> __ for a list of available locations. The location must match that of the target dataset. New in version 0.5.0 of pandas-gbq . progress_bar (bool, default True) \u2014 Use the library tqdm to show the progress bar for the upload,chunk by chunk. New in version 0.5.0 of pandas-gbq . credentials (google.auth.credentials.Credentials, optional) \u2014 Credentials for accessing Google APIs. Use this parameter tooverride default credentials, such as to use Compute Engine :class: google.auth.compute_engine.Credentials or Service Account :class: google.oauth2.service_account.Credentials directly. New in version 0.8.0 of pandas-gbq . See Also pandas_gbq.to_gbq : This function in the pandas-gbq library.read_gbq : Read a DataFrame from Google BigQuery. Examples Example taken from Google BigQuery documentation<https://cloud.google.com/bigquery/docs/samples/bigquery-pandas-gbq-to-gbq-simple> _ >>> project_id = \"my-project\" >>> table_id = 'my_dataset.my_table' >>> df = pd . DataFrame ({ ... \"my_string\" : [ \"a\" , \"b\" , \"c\" ], ... \"my_int64\" : [ 1 , 2 , 3 ], ... \"my_float64\" : [ 4.0 , 5.0 , 6.0 ], ... \"my_bool1\" : [ True , False , True ], ... \"my_bool2\" : [ False , True , False ], ... \"my_dates\" : pd . date_range ( \"now\" , periods = 3 ), ... } ... ) >>> df . to_gbq ( table_id , project_id = project_id ) # doctest: +SKIP classmethod","title":"pandas.core.frame.DataFrame.to_gbq"},{"location":"api/pipen.channel/#pandascoreframedataframefrom_records","text":"</> Convert structured or record ndarray to DataFrame. Creates a DataFrame object from a structured ndarray, sequence of tuples or dicts, or DataFrame. Parameters data (structured ndarray, sequence of tuples or dicts, or DataFrame) \u2014 Structured input data. .. deprecated:: 2.1.0 Passing a DataFrame is deprecated. index (str, list of fields, array-like) \u2014 Field of array to use as the index, alternately a specific set ofinput labels to use. exclude (sequence, default None) \u2014 Columns or fields to exclude. columns (sequence, default None) \u2014 Column names to use. If the passed data do not have namesassociated with them, this argument provides names for the columns. Otherwise this argument indicates the order of the columns in the result (any names not found in the data will become all-NA columns). coerce_float (bool, default False) \u2014 Attempt to convert values of non-string, non-numeric objects (likedecimal.Decimal) to floating point, useful for SQL result sets. nrows (int, default None) \u2014 Number of rows to read if data is an iterator. See Also DataFrame.from_dict : DataFrame from dict of array-like or dicts.DataFrame : DataFrame object creation using constructor. Examples Data can be provided as a structured ndarray: >>> data = np . array ([( 3 , 'a' ), ( 2 , 'b' ), ( 1 , 'c' ), ( 0 , 'd' )], ... dtype = [( 'col_1' , 'i4' ), ( 'col_2' , 'U1' )]) >>> pd . DataFrame . from_records ( data ) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d Data can be provided as a list of dicts: >>> data = [{ 'col_1' : 3 , 'col_2' : 'a' }, ... { 'col_1' : 2 , 'col_2' : 'b' }, ... { 'col_1' : 1 , 'col_2' : 'c' }, ... { 'col_1' : 0 , 'col_2' : 'd' }] >>> pd . DataFrame . from_records ( data ) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d Data can be provided as a list of tuples with corresponding columns: >>> data = [( 3 , 'a' ), ( 2 , 'b' ), ( 1 , 'c' ), ( 0 , 'd' )] >>> pd . DataFrame . from_records ( data , columns = [ 'col_1' , 'col_2' ]) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d method","title":"pandas.core.frame.DataFrame.from_records"},{"location":"api/pipen.channel/#pandascoreframedataframeto_records","text":"</> Convert DataFrame to a NumPy record array. Index will be included as the first field of the record array if requested. Parameters index (bool, default True) \u2014 Include index in resulting record array, stored in 'index'field or using the index label, if set. column_dtypes (str, type, dict, default None) \u2014 If a string or type, the data type to store all columns. Ifa dictionary, a mapping of column names and indices (zero-indexed) to specific data types. index_dtypes (str, type, dict, default None) \u2014 If a string or type, the data type to store all index levels. Ifa dictionary, a mapping of index level names and indices (zero-indexed) to specific data types. This mapping is applied only if index=True . Returns (numpy.rec.recarray) NumPy ndarray with the DataFrame labels as fields and each rowof the DataFrame as entries. See Also DataFrame.from_records: Convert structured or record ndarray to DataFrame. numpy.rec.recarray: An ndarray that allows field access using attributes, analogous to typed columns in a spreadsheet. Examples >>> df = pd . DataFrame ({ 'A' : [ 1 , 2 ], 'B' : [ 0.5 , 0.75 ]}, ... index = [ 'a' , 'b' ]) >>> df A B a 1 0.50 b 2 0.75 >>> df . to_records () rec . array ([( 'a' , 1 , 0.5 ), ( 'b' , 2 , 0.75 )], dtype = [( 'index' , 'O' ), ( 'A' , '<i8' ), ( 'B' , '<f8' )]) If the DataFrame index has no label then the recarray field name is set to 'index'. If the index has a label then this is used as the field name: >>> df . index = df . index . rename ( \"I\" ) >>> df . to_records () rec . array ([( 'a' , 1 , 0.5 ), ( 'b' , 2 , 0.75 )], dtype = [( 'I' , 'O' ), ( 'A' , '<i8' ), ( 'B' , '<f8' )]) The index can be excluded from the record array: >>> df . to_records ( index = False ) rec . array ([( 1 , 0.5 ), ( 2 , 0.75 )], dtype = [( 'A' , '<i8' ), ( 'B' , '<f8' )]) Data types can be specified for the columns: >>> df . to_records ( column_dtypes = { \"A\" : \"int32\" }) rec . array ([( 'a' , 1 , 0.5 ), ( 'b' , 2 , 0.75 )], dtype = [( 'I' , 'O' ), ( 'A' , '<i4' ), ( 'B' , '<f8' )]) As well as for the index: >>> df . to_records ( index_dtypes = \"<S2\" ) rec . array ([( b 'a' , 1 , 0.5 ), ( b 'b' , 2 , 0.75 )], dtype = [( 'I' , 'S2' ), ( 'A' , '<i8' ), ( 'B' , '<f8' )]) >>> index_dtypes = f \"<S { df . index . str . len () . max () } \" >>> df . to_records ( index_dtypes = index_dtypes ) rec . array ([( b 'a' , 1 , 0.5 ), ( b 'b' , 2 , 0.75 )], dtype = [( 'I' , 'S1' ), ( 'A' , '<i8' ), ( 'B' , '<f8' )]) method","title":"pandas.core.frame.DataFrame.to_records"},{"location":"api/pipen.channel/#pandascoreframedataframeto_stata","text":"</> Export DataFrame object to Stata dta format. Writes the DataFrame to a Stata dataset file. \"dta\" files contain a Stata dataset. Parameters path (str, path object, or buffer) \u2014 String, path object (implementing os.PathLike[str] ), or file-likeobject implementing a binary write() function. convert_dates (dict) \u2014 Dictionary mapping columns containing datetime types to statainternal format to use when writing the dates. Options are 'tc', 'td', 'tm', 'tw', 'th', 'tq', 'ty'. Column can be either an integer or a name. Datetime columns that do not have a conversion type specified will be converted to 'tc'. Raises NotImplementedError if a datetime column has timezone information. write_index (bool) \u2014 Write the index to Stata dataset. byteorder (str) \u2014 Can be \">\", \"<\", \"little\", or \"big\". default is sys.byteorder . time_stamp (datetime) \u2014 A datetime to use as file creation date. Default is the currenttime. data_label (str, optional) \u2014 A label for the data set. Must be 80 characters or smaller. variable_labels (dict) \u2014 Dictionary containing columns as keys and variable labels asvalues. Each label must be 80 characters or smaller. version ({114, 117, 118, 119, None}, default 114) \u2014 Version to use in the output dta file. Set to None to let pandasdecide between 118 or 119 formats depending on the number of columns in the frame. Version 114 can be read by Stata 10 and later. Version 117 can be read by Stata 13 or later. Version 118 is supported in Stata 14 and later. Version 119 is supported in Stata 15 and later. Version 114 limits string variables to 244 characters or fewer while versions 117 and later allow strings with lengths up to 2,000,000 characters. Versions 118 and 119 support Unicode characters, and version 119 supports more than 32,767 variables. Version 119 should usually only be used when the number of variables exceeds the capacity of dta format 118. Exporting smaller datasets in format 119 may have unintended consequences, and, as of November 2020, Stata SE cannot read version 119 files. convert_strl (list, optional) \u2014 List of column names to convert to string columns to Stata StrLformat. Only available if version is 117. Storing strings in the StrL format can produce smaller dta files if strings have more than 8 characters and values are repeated. compression (str or dict, default 'infer') \u2014 For on-the-fly compression of the output data. If 'infer' and 'path' ispath-like, then detect compression from the following extensions: '.gz', '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2' (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of { 'zip' , 'gzip' , 'bz2' , 'zstd' , 'xz' , 'tar' } and other key-value pairs are forwarded to zipfile.ZipFile , gzip.GzipFile , bz2.BZ2File , zstandard.ZstdCompressor , lzma.LZMAFile or tarfile.TarFile , respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1} . .. versionadded:: 1.5.0 Added support for .tar files. .. versionchanged:: 1.4.0 Zstandard support. storage_options (dict, optional) \u2014 Extra options that make sense for a particular storage connection, e.g.host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to fsspec.open . Please see fsspec and urllib for more details, and for more examples on storage options refer here <https://pandas.pydata.org/docs/user_guide/io.html? highlight=storage_options#reading-writing-remote-files> _. value_labels (dict of dicts) \u2014 Dictionary containing columns as keys and dictionaries of column valueto labels as values. Labels for a single variable must be 32,000 characters or smaller. .. versionadded:: 1.4.0 Raises NotImplementedError \u2014 If datetimes contain timezone information Column dtype is not representable in Stata ValueError \u2014 Columns listed in convert_dates are neither datetime64[ns] or datetime.datetime Column listed in convert_dates is not in DataFrame Categorical label contains more than 32,000 characters See Also read_stata : Import Stata data files.io.stata.StataWriter : Low-level writer for Stata data files. io.stata.StataWriter117 : Low-level writer for version 117 files. Examples >>> df = pd . DataFrame ({ 'animal' : [ 'falcon' , 'parrot' , 'falcon' , ... 'parrot' ], ... 'speed' : [ 350 , 18 , 361 , 15 ]}) >>> df . to_stata ( 'animals.dta' ) # doctest: +SKIP method","title":"pandas.core.frame.DataFrame.to_stata"},{"location":"api/pipen.channel/#pandascoreframedataframeto_feather","text":"</> Write a DataFrame to the binary Feather format. Parameters path (str, path object, file-like object) \u2014 String, path object (implementing os.PathLike[str] ), or file-likeobject implementing a binary write() function. If a string or a path, it will be used as Root Directory path when writing a partitioned dataset. **kwargs \u2014 Additional keywords passed to :func: pyarrow.feather.write_feather .This includes the compression , compression_level , chunksize and version keywords. Notes This function writes the dataframe as a feather file <https://arrow.apache.org/docs/python/feather.html> _. Requires a default index. For saving the DataFrame with your custom index use a method that supports custom indices e.g. to_parquet . Examples >>> df = pd . DataFrame ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> df . to_feather ( \"file.feather\" ) # doctest: +SKIP method","title":"pandas.core.frame.DataFrame.to_feather"},{"location":"api/pipen.channel/#pandascoreframedataframeto_markdown","text":"</> Print DataFrame in Markdown-friendly format. Parameters buf (str, Path or StringIO-like, optional, default None) \u2014 Buffer to write to. If None, the output is returned as a string. mode (str, optional) \u2014 Mode in which file is opened, \"wt\" by default. index (bool, optional, default True) \u2014 Add index (row) labels. storage_options (dict, optional) \u2014 Extra options that make sense for a particular storage connection, e.g.host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to fsspec.open . Please see fsspec and urllib for more details, and for more examples on storage options refer here <https://pandas.pydata.org/docs/user_guide/io.html? highlight=storage_options#reading-writing-remote-files> _. **kwargs \u2014 These parameters will be passed to tabulate <https://pypi.org/project/tabulate> _. Returns (str) DataFrame in Markdown-friendly format. Notes Requires the tabulate <https://pypi.org/project/tabulate> _ package. Examples >>> df = pd . DataFrame ( ... data = { \"animal_1\" : [ \"elk\" , \"pig\" ], \"animal_2\" : [ \"dog\" , \"quetzal\" ]} ... ) >>> print ( df . to_markdown ()) | | animal_1 | animal_2 | |--- : | : -----------| : -----------| | 0 | elk | dog | | 1 | pig | quetzal | Output markdown with a tabulate option. >>> print ( df . to_markdown ( tablefmt = \"grid\" )) +----+------------+------------+ | | animal_1 | animal_2 | +====+============+============+ | 0 | elk | dog | +----+------------+------------+ | 1 | pig | quetzal | +----+------------+------------+ method","title":"pandas.core.frame.DataFrame.to_markdown"},{"location":"api/pipen.channel/#pandascoreframedataframeto_parquet","text":"</> Write a DataFrame to the binary parquet format. This function writes the dataframe as a parquet file <https://parquet.apache.org/> _. You can choose different parquet backends, and have the option of compression. See :ref: the user guide <io.parquet> for more details. Parameters path (str, path object, file-like object, or None, default None) \u2014 String, path object (implementing os.PathLike[str] ), or file-likeobject implementing a binary write() function. If None, the result is returned as bytes. If a string or path, it will be used as Root Directory path when writing a partitioned dataset. engine ({'auto', 'pyarrow', 'fastparquet'}, default 'auto') \u2014 Parquet library to use. If 'auto', then the option io.parquet.engine is used. The default io.parquet.engine behavior is to try 'pyarrow', falling back to 'fastparquet' if 'pyarrow' is unavailable. compression (str or None, default 'snappy') \u2014 Name of the compression to use. Use None for no compression.Supported options: 'snappy', 'gzip', 'brotli', 'lz4', 'zstd'. index (bool, default None) \u2014 If True , include the dataframe's index(es) in the file output.If False , they will not be written to the file. If None , similar to True the dataframe's index(es) will be saved. However, instead of being saved as values, the RangeIndex will be stored as a range in the metadata so it doesn't require much space and is faster. Other indexes will be included as columns in the file output. partition_cols (list, optional, default None) \u2014 Column names by which to partition the dataset.Columns are partitioned in the order they are given. Must be None if path is not a string. storage_options (dict, optional) \u2014 Extra options that make sense for a particular storage connection, e.g.host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to fsspec.open . Please see fsspec and urllib for more details, and for more examples on storage options refer here <https://pandas.pydata.org/docs/user_guide/io.html? highlight=storage_options#reading-writing-remote-files> _. **kwargs \u2014 Additional arguments passed to the parquet library. See:ref: pandas io <io.parquet> for more details. See Also read_parquet : Read a parquet file.DataFrame.to_orc : Write an orc file. DataFrame.to_csv : Write a csv file. DataFrame.to_sql : Write to a sql table. DataFrame.to_hdf : Write to hdf. Notes This function requires either the fastparquet <https://pypi.org/project/fastparquet> or pyarrow <https://arrow.apache.org/docs/python/> library. Examples >>> df = pd . DataFrame ( data = { 'col1' : [ 1 , 2 ], 'col2' : [ 3 , 4 ]}) >>> df . to_parquet ( 'df.parquet.gzip' , ... compression = 'gzip' ) # doctest: +SKIP >>> pd . read_parquet ( 'df.parquet.gzip' ) # doctest: +SKIP col1 col2 0 1 3 1 2 4 If you want to get a buffer to the parquet content you can use a io.BytesIO object, as long as you don't use partition_cols, which creates multiple files. >>> import io >>> f = io . BytesIO () >>> df . to_parquet ( f ) >>> f . seek ( 0 ) 0 >>> content = f . read () method","title":"pandas.core.frame.DataFrame.to_parquet"},{"location":"api/pipen.channel/#pandascoreframedataframeto_orc","text":"</> Write a DataFrame to the ORC format. .. versionadded:: 1.5.0 Parameters path (str, file-like object or None, default None) \u2014 If a string, it will be used as Root Directory pathwhen writing a partitioned dataset. By file-like object, we refer to objects with a write() method, such as a file handle (e.g. via builtin open function). If path is None, a bytes object is returned. engine ({'pyarrow'}, default 'pyarrow') \u2014 ORC library to use. index (bool, optional) \u2014 If True , include the dataframe's index(es) in the file output.If False , they will not be written to the file. If None , similar to infer the dataframe's index(es) will be saved. However, instead of being saved as values, the RangeIndex will be stored as a range in the metadata so it doesn't require much space and is faster. Other indexes will be included as columns in the file output. engine_kwargs (dict[str, Any] or None, default None) \u2014 Additional keyword arguments passed to :func: pyarrow.orc.write_table . Raises NotImplementedError \u2014 Dtype of one or more columns is category, unsigned integers, interval,period or sparse. ValueError \u2014 engine is not pyarrow. See Also read_orc : Read a ORC file.DataFrame.to_parquet : Write a parquet file. DataFrame.to_csv : Write a csv file. DataFrame.to_sql : Write to a sql table. DataFrame.to_hdf : Write to hdf. Notes Before using this function you should read the :ref: user guide about ORC <io.orc> and :ref: install optional dependencies <install.warn_orc> . This function requires pyarrow <https://arrow.apache.org/docs/python/> _ library. For supported dtypes please refer to supported ORC features in Arrow <https://arrow.apache.org/docs/cpp/orc.html#data-types> __. Currently timezones in datetime columns are not preserved when a dataframe is converted into ORC files. Examples >>> df = pd . DataFrame ( data = { 'col1' : [ 1 , 2 ], 'col2' : [ 4 , 3 ]}) >>> df . to_orc ( 'df.orc' ) # doctest: +SKIP >>> pd . read_orc ( 'df.orc' ) # doctest: +SKIP col1 col2 0 1 4 1 2 3 If you want to get a buffer to the orc content you can write it to io.BytesIO >>> import io >>> b = io . BytesIO ( df . to_orc ()) # doctest: +SKIP >>> b . seek ( 0 ) # doctest: +SKIP 0 >>> content = b . read () # doctest: +SKIP method","title":"pandas.core.frame.DataFrame.to_orc"},{"location":"api/pipen.channel/#pandascoreframedataframeto_html","text":"</> Render a DataFrame as an HTML table. Parameters buf (str, Path or StringIO-like, optional, default None) \u2014 Buffer to write to. If None, the output is returned as a string. columns (array-like, optional, default None) \u2014 The subset of columns to write. Writes all columns by default. col_space (str or int, list or dict of int or str, optional) \u2014 The minimum width of each column in CSS length units. An int is assumed to be px units.. header (bool, optional) \u2014 Whether to print column labels, default True. index (bool, optional, default True) \u2014 Whether to print index (row) labels. na_rep (str, optional, default 'NaN') \u2014 String representation of NaN to use. formatters (list, tuple or dict of one-param. functions, optional) \u2014 Formatter functions to apply to columns' elements by position orname. The result of each function must be a unicode string. List/tuple must be of length equal to the number of columns. float_format (one-parameter function, optional, default None) \u2014 Formatter function to apply to columns' elements if they arefloats. This function must return a unicode string and will be applied only to the non- NaN elements, with NaN being handled by na_rep . sparsify (bool, optional, default True) \u2014 Set to False for a DataFrame with a hierarchical index to printevery multiindex key at each row. index_names (bool, optional, default True) \u2014 Prints the names of the indexes. justify (str, default None) \u2014 How to justify the column labels. If None uses the option fromthe print configuration (controlled by set_option), 'right' out of the box. Valid values are left right center justify justify-all start end inherit match-parent initial unset. max_rows (int, optional) \u2014 Maximum number of rows to display in the console. max_cols (int, optional) \u2014 Maximum number of columns to display in the console. show_dimensions (bool, default False) \u2014 Display DataFrame dimensions (number of rows by number of columns). decimal (str, default '.') \u2014 Character recognized as decimal separator, e.g. ',' in Europe. bold_rows (bool, default True) \u2014 Make the row labels bold in the output. classes (str or list or tuple, default None) \u2014 CSS class(es) to apply to the resulting html table. escape (bool, default True) \u2014 Convert the characters <, >, and & to HTML-safe sequences. notebook ({True, False}, default False) \u2014 Whether the generated HTML is for IPython Notebook. border (int) \u2014 A border=border attribute is included in the opening <table> tag. Default pd.options.display.html.border . table_id (str, optional) \u2014 A css id is included in the opening <table> tag if specified. render_links (bool, default False) \u2014 Convert URLs to HTML links. encoding (str, default \"utf-8\") \u2014 Set character encoding. Returns (str or None) If buf is None, returns the result as a string. Otherwise returnsNone. See Also to_string : Convert DataFrame to a string. Examples >>> df = pd . DataFrame ( data = { 'col1' : [ 1 , 2 ], 'col2' : [ 4 , 3 ]}) >>> html_string = '''<table border=\"1\" class=\"dataframe\"> ... <thead> ... <tr style=\"text-align: right;\"> ... <th></th> ... <th>col1</th> ... <th>col2</th> ... </tr> ... </thead> ... <tbody> ... <tr> ... <th>0</th> ... <td>1</td> ... <td>4</td> ... </tr> ... <tr> ... <th>1</th> ... <td>2</td> ... <td>3</td> ... </tr> ... </tbody> ... </table>''' >>> assert html_string == df . to_html () method","title":"pandas.core.frame.DataFrame.to_html"},{"location":"api/pipen.channel/#pandascoreframedataframeto_xml","text":"</> Render a DataFrame to an XML document. .. versionadded:: 1.3.0 Parameters path_or_buffer (str, path object, file-like object, or None, default None) \u2014 String, path object (implementing os.PathLike[str] ), or file-likeobject implementing a write() function. If None, the result is returned as a string. index (bool, default True) \u2014 Whether to include index in XML document. root_name (str, default 'data') \u2014 The name of root element in XML document. row_name (str, default 'row') \u2014 The name of row element in XML document. na_rep (str, optional) \u2014 Missing data representation. attr_cols (list-like, optional) \u2014 List of columns to write as attributes in row element.Hierarchical columns will be flattened with underscore delimiting the different levels. elem_cols (list-like, optional) \u2014 List of columns to write as children in row element. By default,all columns output as children of row element. Hierarchical columns will be flattened with underscore delimiting the different levels. namespaces (dict, optional) \u2014 All namespaces to be defined in root element. Keys of dictshould be prefix names and values of dict corresponding URIs. Default namespaces should be given empty string key. For example, :: namespaces = {\"\": \"https://example.com\"} prefix (str, optional) \u2014 Namespace prefix to be used for every element and/or attributein document. This should be one of the keys in namespaces dict. encoding (str, default 'utf-8') \u2014 Encoding of the resulting document. xml_declaration (bool, default True) \u2014 Whether to include the XML declaration at start of document. pretty_print (bool, default True) \u2014 Whether output should be pretty printed with indentation andline breaks. parser ({'lxml','etree'}, default 'lxml') \u2014 Parser module to use for building of tree. Only 'lxml' and'etree' are supported. With 'lxml', the ability to use XSLT stylesheet is supported. stylesheet (str, path object or file-like object, optional) \u2014 A URL, file-like object, or a raw string containing an XSLTscript used to transform the raw XML output. Script should use layout of elements and attributes from original output. This argument requires lxml to be installed. Only XSLT 1.0 scripts and not later versions is currently supported. compression (str or dict, default 'infer') \u2014 For on-the-fly compression of the output data. If 'infer' and 'path_or_buffer' ispath-like, then detect compression from the following extensions: '.gz', '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2' (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of { 'zip' , 'gzip' , 'bz2' , 'zstd' , 'xz' , 'tar' } and other key-value pairs are forwarded to zipfile.ZipFile , gzip.GzipFile , bz2.BZ2File , zstandard.ZstdCompressor , lzma.LZMAFile or tarfile.TarFile , respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1} . .. versionadded:: 1.5.0 Added support for .tar files. .. versionchanged:: 1.4.0 Zstandard support. storage_options (dict, optional) \u2014 Extra options that make sense for a particular storage connection, e.g.host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to fsspec.open . Please see fsspec and urllib for more details, and for more examples on storage options refer here <https://pandas.pydata.org/docs/user_guide/io.html? highlight=storage_options#reading-writing-remote-files> _. Returns (None or str) If io is None, returns the resulting XML format as astring. Otherwise returns None. See Also to_json : Convert the pandas object to a JSON string.to_html : Convert DataFrame to a html. Examples >>> df = pd . DataFrame ({ 'shape' : [ 'square' , 'circle' , 'triangle' ], ... 'degrees' : [ 360 , 360 , 180 ], ... 'sides' : [ 4 , np . nan , 3 ]}) >>> df . to_xml () # doctest: +SKIP < ? xml version = '1.0' encoding = 'utf-8' ? > < data > < row > < index > 0 </ index > < shape > square </ shape > < degrees > 360 </ degrees > < sides > 4.0 </ sides > </ row > < row > < index > 1 </ index > < shape > circle </ shape > < degrees > 360 </ degrees > < sides /> </ row > < row > < index > 2 </ index > < shape > triangle </ shape > < degrees > 180 </ degrees > < sides > 3.0 </ sides > </ row > </ data > >>> df . to_xml ( attr_cols = [ ... 'index' , 'shape' , 'degrees' , 'sides' ... ]) # doctest: +SKIP < ? xml version = '1.0' encoding = 'utf-8' ? > < data > < row index = \"0\" shape = \"square\" degrees = \"360\" sides = \"4.0\" /> < row index = \"1\" shape = \"circle\" degrees = \"360\" /> < row index = \"2\" shape = \"triangle\" degrees = \"180\" sides = \"3.0\" /> </ data > >>> df . to_xml ( namespaces = { \"doc\" : \"https://example.com\" }, ... prefix = \"doc\" ) # doctest: +SKIP < ? xml version = '1.0' encoding = 'utf-8' ? > < doc : data xmlns : doc = \"https://example.com\" > < doc : row > < doc : index > 0 </ doc : index > < doc : shape > square </ doc : shape > < doc : degrees > 360 </ doc : degrees > < doc : sides > 4.0 </ doc : sides > </ doc : row > < doc : row > < doc : index > 1 </ doc : index > < doc : shape > circle </ doc : shape > < doc : degrees > 360 </ doc : degrees > < doc : sides /> </ doc : row > < doc : row > < doc : index > 2 </ doc : index > < doc : shape > triangle </ doc : shape > < doc : degrees > 180 </ doc : degrees > < doc : sides > 3.0 </ doc : sides > </ doc : row > </ doc : data > method","title":"pandas.core.frame.DataFrame.to_xml"},{"location":"api/pipen.channel/#pandascoreframedataframeinfo","text":"</> Print a concise summary of a DataFrame. This method prints information about a DataFrame including the index dtype and columns, non-null values and memory usage. Parameters verbose (bool, optional) \u2014 Whether to print the full summary. By default, the setting in pandas.options.display.max_info_columns is followed. buf (writable buffer, defaults to sys.stdout) \u2014 Where to send the output. By default, the output is printed tosys.stdout. Pass a writable buffer if you need to further process the output. max_cols (int, optional) \u2014 When to switch from the verbose to the truncated output. If theDataFrame has more than max_cols columns, the truncated output is used. By default, the setting in pandas.options.display.max_info_columns is used. memory_usage (bool, str, optional) \u2014 Specifies whether total memory usage of the DataFrameelements (including the index) should be displayed. By default, this follows the pandas.options.display.memory_usage setting. True always show memory usage. False never shows memory usage. A value of 'deep' is equivalent to \"True with deep introspection\". Memory usage is shown in human-readable units (base-2 representation). Without deep introspection a memory estimation is made based in column dtype and number of rows assuming values consume the same memory amount for corresponding dtypes. With deep memory introspection, a real memory usage calculation is performed at the cost of computational resources. See the :ref: Frequently Asked Questions <df-memory-usage> for more details. show_counts (bool, optional) \u2014 Whether to show the non-null counts. By default, this is shownonly if the DataFrame is smaller than pandas.options.display.max_info_rows and pandas.options.display.max_info_columns . A value of True always shows the counts, and False never shows the counts. Returns (None) This method prints a summary of a DataFrame and returns None. See Also DataFrame.describe: Generate descriptive statistics of DataFrame columns. DataFrame.memory_usage: Memory usage of DataFrame columns. Examples >>> int_values = [ 1 , 2 , 3 , 4 , 5 ] >>> text_values = [ 'alpha' , 'beta' , 'gamma' , 'delta' , 'epsilon' ] >>> float_values = [ 0.0 , 0.25 , 0.5 , 0.75 , 1.0 ] >>> df = pd . DataFrame ({ \"int_col\" : int_values , \"text_col\" : text_values , ... \"float_col\" : float_values }) >>> df int_col text_col float_col 0 1 alpha 0.00 1 2 beta 0.25 2 3 gamma 0.50 3 4 delta 0.75 4 5 epsilon 1.00 Prints information of all columns: >>> df . info ( verbose = True ) < class ' pandas . core . frame . DataFrame '> RangeIndex : 5 entries , 0 to 4 Data columns ( total 3 columns ): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 int_col 5 non - null int64 1 text_col 5 non - null object 2 float_col 5 non - null float64 dtypes : float64 ( 1 ), int64 ( 1 ), object ( 1 ) memory usage : 248.0 + bytes Prints a summary of columns count and its dtypes but not per column information: >>> df . info ( verbose = False ) < class ' pandas . core . frame . DataFrame '> RangeIndex : 5 entries , 0 to 4 Columns : 3 entries , int_col to float_col dtypes : float64 ( 1 ), int64 ( 1 ), object ( 1 ) memory usage : 248.0 + bytes Pipe output of DataFrame.info to buffer instead of sys.stdout, get buffer content and writes to a text file: >>> import io >>> buffer = io . StringIO () >>> df . info ( buf = buffer ) >>> s = buffer . getvalue () >>> with open ( \"df_info.txt\" , \"w\" , ... encoding = \"utf-8\" ) as f : # doctest: +SKIP ... f . write ( s ) 260 The memory_usage parameter allows deep introspection mode, specially useful for big DataFrames and fine-tune memory optimization: >>> random_strings_array = np . random . choice ([ 'a' , 'b' , 'c' ], 10 ** 6 ) >>> df = pd . DataFrame ({ ... 'column_1' : np . random . choice ([ 'a' , 'b' , 'c' ], 10 ** 6 ), ... 'column_2' : np . random . choice ([ 'a' , 'b' , 'c' ], 10 ** 6 ), ... 'column_3' : np . random . choice ([ 'a' , 'b' , 'c' ], 10 ** 6 ) ... }) >>> df . info () < class ' pandas . core . frame . DataFrame '> RangeIndex : 1000000 entries , 0 to 999999 Data columns ( total 3 columns ): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 column_1 1000000 non - null object 1 column_2 1000000 non - null object 2 column_3 1000000 non - null object dtypes : object ( 3 ) memory usage : 22.9 + MB >>> df . info ( memory_usage = 'deep' ) < class ' pandas . core . frame . DataFrame '> RangeIndex : 1000000 entries , 0 to 999999 Data columns ( total 3 columns ): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 column_1 1000000 non - null object 1 column_2 1000000 non - null object 2 column_3 1000000 non - null object dtypes : object ( 3 ) memory usage : 165.9 MB method","title":"pandas.core.frame.DataFrame.info"},{"location":"api/pipen.channel/#pandascoreframedataframememory_usage","text":"</> Return the memory usage of each column in bytes. The memory usage can optionally include the contribution of the index and elements of object dtype. This value is displayed in DataFrame.info by default. This can be suppressed by setting pandas.options.display.memory_usage to False. Parameters index (bool, default True) \u2014 Specifies whether to include the memory usage of the DataFrame'sindex in returned Series. If index=True , the memory usage of the index is the first item in the output. deep (bool, default False) \u2014 If True, introspect the data deeply by interrogating object dtypes for system-level memory consumption, and include it in the returned values. Returns (Series) A Series whose index is the original column names and whose valuesis the memory usage of each column in bytes. See Also numpy.ndarray.nbytes : Total bytes consumed by the elements of an ndarray. Series.memory_usage : Bytes consumed by a Series. Categorical : Memory-efficient array for string values with many repeated values. DataFrame.info : Concise summary of a DataFrame. Notes See the :ref: Frequently Asked Questions <df-memory-usage> for more details. Examples >>> dtypes = [ 'int64' , 'float64' , 'complex128' , 'object' , 'bool' ] >>> data = dict ([( t , np . ones ( shape = 5000 , dtype = int ) . astype ( t )) ... for t in dtypes ]) >>> df = pd . DataFrame ( data ) >>> df . head () int64 float64 complex128 object bool 0 1 1.0 1.0 + 0.0 j 1 True 1 1 1.0 1.0 + 0.0 j 1 True 2 1 1.0 1.0 + 0.0 j 1 True 3 1 1.0 1.0 + 0.0 j 1 True 4 1 1.0 1.0 + 0.0 j 1 True >>> df . memory_usage () Index 128 int64 40000 float64 40000 complex128 80000 object 40000 bool 5000 dtype : int64 >>> df . memory_usage ( index = False ) int64 40000 float64 40000 complex128 80000 object 40000 bool 5000 dtype : int64 The memory footprint of object dtype columns is ignored by default: >>> df . memory_usage ( deep = True ) Index 128 int64 40000 float64 40000 complex128 80000 object 180000 bool 5000 dtype : int64 Use a Categorical for efficient storage of an object-dtype column with many repeated values. >>> df [ 'object' ] . astype ( 'category' ) . memory_usage ( deep = True ) 5244 method","title":"pandas.core.frame.DataFrame.memory_usage"},{"location":"api/pipen.channel/#pandascoreframedataframetranspose","text":"</> Transpose index and columns. Reflect the DataFrame over its main diagonal by writing rows as columns and vice-versa. The property :attr: .T is an accessor to the method :meth: transpose . Parameters *args (tuple, optional) \u2014 Accepted for compatibility with NumPy. copy (bool, default False) \u2014 Whether to copy the data after transposing, even for DataFrameswith a single dtype. Note that a copy is always required for mixed dtype DataFrames, or for DataFrames with any extension types. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` Returns (DataFrame) The transposed DataFrame. See Also numpy.transpose : Permute the dimensions of a given array. Notes Transposing a DataFrame with mixed dtypes will result in a homogeneous DataFrame with the object dtype. In such a case, a copy of the data is always made. Examples Square DataFrame with homogeneous dtype >>> d1 = { 'col1' : [ 1 , 2 ], 'col2' : [ 3 , 4 ]} >>> df1 = pd . DataFrame ( data = d1 ) >>> df1 col1 col2 0 1 3 1 2 4 >>> df1_transposed = df1 . T # or df1.transpose() >>> df1_transposed 0 1 col1 1 2 col2 3 4 When the dtype is homogeneous in the original DataFrame, we get a transposed DataFrame with the same dtype: >>> df1 . dtypes col1 int64 col2 int64 dtype : object >>> df1_transposed . dtypes 0 int64 1 int64 dtype : object Non-square DataFrame with mixed dtypes >>> d2 = { 'name' : [ 'Alice' , 'Bob' ], ... 'score' : [ 9.5 , 8 ], ... 'employed' : [ False , True ], ... 'kids' : [ 0 , 0 ]} >>> df2 = pd . DataFrame ( data = d2 ) >>> df2 name score employed kids 0 Alice 9.5 False 0 1 Bob 8.0 True 0 >>> df2_transposed = df2 . T # or df2.transpose() >>> df2_transposed 0 1 name Alice Bob score 9.5 8.0 employed False True kids 0 0 When the DataFrame has mixed dtypes, we get a transposed DataFrame with the object dtype: >>> df2 . dtypes name object score float64 employed bool kids int64 dtype : object >>> df2_transposed . dtypes 0 object 1 object dtype : object method","title":"pandas.core.frame.DataFrame.transpose"},{"location":"api/pipen.channel/#pandascoreframedataframeisetitem","text":"</> Set the given value in the column with position loc . This is a positional analogue to __setitem__ . Parameters loc (int or sequence of ints) \u2014 Index position for the column. value (scalar or arraylike) \u2014 Value(s) for the column. Notes frame.isetitem(loc, value) is an in-place method as it will modify the DataFrame in place (not returning a new object). In contrast to frame.iloc[:, i] = value which will try to update the existing values in place, frame.isetitem(loc, value) will not update the values of the column itself in place, it will instead insert a new array. In cases where frame.columns is unique, this is equivalent to frame[frame.columns[i]] = value . method","title":"pandas.core.frame.DataFrame.isetitem"},{"location":"api/pipen.channel/#pandascoreframedataframequery","text":"</> Query the columns of a DataFrame with a boolean expression. Parameters expr (str) \u2014 The query string to evaluate. You can refer to variables in the environment by prefixing them with an '@' character like @a + b . You can refer to column names that are not valid Python variable names by surrounding them in backticks. Thus, column names containing spaces or punctuations (besides underscores) or starting with digits must be surrounded by backticks. (For example, a column named \"Area (cm^2)\" would be referenced as Area (cm^2) ). Column names which are Python keywords (like \"list\", \"for\", \"import\", etc) cannot be used. For example, if one of your columns is called a a and you want to sum it with b , your query should be `a a` + b . inplace (bool) \u2014 Whether to modify the DataFrame rather than creating a new one. **kwargs \u2014 See the documentation for :func: eval for complete detailson the keyword arguments accepted by :meth: DataFrame.query . Returns (DataFrame or None) DataFrame resulting from the provided query expression orNone if inplace=True . See Also eval : Evaluate a string describing operations on DataFrame columns. DataFrame.eval : Evaluate a string describing operations on DataFrame columns. Notes The result of the evaluation of this expression is first passed to :attr: DataFrame.loc and if that fails because of a multidimensional key (e.g., a DataFrame) then the result will be passed to :meth: DataFrame.__getitem__ . This method uses the top-level :func: eval function to evaluate the passed query. The :meth: ~pandas.DataFrame.query method uses a slightly modified Python syntax by default. For example, the & and | (bitwise) operators have the precedence of their boolean cousins, :keyword: and and :keyword: or . This is syntactically valid Python, however the semantics are different. You can change the semantics of the expression by passing the keyword argument parser='python' . This enforces the same semantics as evaluation in Python space. Likewise, you can pass engine='python' to evaluate an expression using Python itself as a backend. This is not recommended as it is inefficient compared to using numexpr as the engine. The :attr: DataFrame.index and :attr: DataFrame.columns attributes of the :class: ~pandas.DataFrame instance are placed in the query namespace by default, which allows you to treat both the index and columns of the frame as a column in the frame. The identifier index is used for the frame index; you can also use the name of the index to identify it in a query. Please note that Python keywords may not be used as identifiers. For further details and examples see the query documentation in :ref: indexing <indexing.query> . Backtick quoted variables Backtick quoted variables are parsed as literal Python code and are converted internally to a Python valid identifier. This can lead to the following problems. During parsing a number of disallowed characters inside the backtick quoted string are replaced by strings that are allowed as a Python identifier. These characters include all operators in Python, the space character, the question mark, the exclamation mark, the dollar sign, and the euro sign. For other characters that fall outside the ASCII range (U+0001..U+007F) and those that are not further specified in PEP 3131, the query parser will raise an error. This excludes whitespace different than the space character, but also the hashtag (as it is used for comments) and the backtick itself (backtick can also not be escaped). In a special case, quotes that make a pair around a backtick can confuse the parser. For example, it's` > `that's will raise an error, as it forms a quoted string ( 's > `that' ) with a backtick inside. See also the Python documentation about lexical analysis (https://docs.python.org/3/reference/lexical_analysis.html) in combination with the source code in :mod: pandas.core.computation.parsing . Examples >>> df = pd . DataFrame ({ 'A' : range ( 1 , 6 ), ... 'B' : range ( 10 , 0 , - 2 ), ... 'C C' : range ( 10 , 5 , - 1 )}) >>> df A B C C 0 1 10 10 1 2 8 9 2 3 6 8 3 4 4 7 4 5 2 6 >>> df . query ( 'A > B' ) A B C C 4 5 2 6 The previous expression is equivalent to >>> df [ df . A > df . B ] A B C C 4 5 2 6 For columns with spaces in their name, you can use backtick quoting. >>> df . query ( 'B == `C C`' ) A B C C 0 1 10 10 The previous expression is equivalent to >>> df [ df . B == df [ 'C C' ]] A B C C 0 1 10 10 method","title":"pandas.core.frame.DataFrame.query"},{"location":"api/pipen.channel/#pandascoreframedataframeeval","text":"</> Evaluate a string describing operations on DataFrame columns. Operates on columns only, not specific rows or elements. This allows eval to run arbitrary code, which can make you vulnerable to code injection if you pass user input to this function. Parameters expr (str) \u2014 The expression string to evaluate. inplace (bool, default False) \u2014 If the expression contains an assignment, whether to perform theoperation inplace and mutate the existing DataFrame. Otherwise, a new DataFrame is returned. **kwargs \u2014 See the documentation for :func: eval for complete detailson the keyword arguments accepted by :meth: ~pandas.DataFrame.query . Returns (ndarray, scalar, pandas object, or None) The result of the evaluation or None if inplace=True . See Also DataFrame.query : Evaluates a boolean expression to query the columns of a frame. DataFrame.assign : Can evaluate an expression or function to create new values for a column. eval : Evaluate a Python expression as a string using various backends. Notes For more details see the API documentation for :func: ~eval . For detailed examples see :ref: enhancing performance with eval <enhancingperf.eval> . Examples >>> df = pd . DataFrame ({ 'A' : range ( 1 , 6 ), 'B' : range ( 10 , 0 , - 2 )}) >>> df A B 0 1 10 1 2 8 2 3 6 3 4 4 4 5 2 >>> df . eval ( 'A + B' ) 0 11 1 10 2 9 3 8 4 7 dtype : int64 Assignment is allowed though by default the original DataFrame is not modified. >>> df . eval ( 'C = A + B' ) A B C 0 1 10 11 1 2 8 10 2 3 6 9 3 4 4 8 4 5 2 7 >>> df A B 0 1 10 1 2 8 2 3 6 3 4 4 4 5 2 Multiple columns can be assigned to using multi-line expressions: >>> df . eval ( ... ''' ... C = A + B ... D = A - B ... ''' ... ) A B C D 0 1 10 11 - 9 1 2 8 10 - 6 2 3 6 9 - 3 3 4 4 8 0 4 5 2 7 3 method","title":"pandas.core.frame.DataFrame.eval"},{"location":"api/pipen.channel/#pandascoreframedataframeselect_dtypes","text":"</> Return a subset of the DataFrame's columns based on the column dtypes. Returns (DataFrame) The subset of the frame including the dtypes in include andexcluding the dtypes in exclude . Raises ValueError \u2014 If both of include and exclude are empty If include and exclude have overlapping elements If any kind of string dtype is passed in. See Also DataFrame.dtypes: Return Series with the data type of each column. Notes To select all numeric types, use np.number or 'number' To select strings you must use the object dtype, but note that this will return all object dtype columns. With pd.options.future.infer_string enabled, using \"str\" will work to select all string columns. See the numpy dtype hierarchy <https://numpy.org/doc/stable/reference/arrays.scalars.html> __ To select datetimes, use np.datetime64 , 'datetime' or 'datetime64' To select timedeltas, use np.timedelta64 , 'timedelta' or 'timedelta64' To select Pandas categorical dtypes, use 'category' To select Pandas datetimetz dtypes, use 'datetimetz' or 'datetime64[ns, tz]' Examples >>> df = pd . DataFrame ({ 'a' : [ 1 , 2 ] * 3 , ... 'b' : [ True , False ] * 3 , ... 'c' : [ 1.0 , 2.0 ] * 3 }) >>> df a b c 0 1 True 1.0 1 2 False 2.0 2 1 True 1.0 3 2 False 2.0 4 1 True 1.0 5 2 False 2.0 >>> df . select_dtypes ( include = 'bool' ) b 0 True 1 False 2 True 3 False 4 True 5 False >>> df . select_dtypes ( include = [ 'float64' ]) c 0 1.0 1 2.0 2 1.0 3 2.0 4 1.0 5 2.0 >>> df . select_dtypes ( exclude = [ 'int64' ]) b c 0 True 1.0 1 False 2.0 2 True 1.0 3 False 2.0 4 True 1.0 5 False 2.0 method","title":"pandas.core.frame.DataFrame.select_dtypes"},{"location":"api/pipen.channel/#pandascoreframedataframeinsert","text":"</> Insert column into DataFrame at specified location. Raises a ValueError if column is already contained in the DataFrame, unless allow_duplicates is set to True. Parameters loc (int) \u2014 Insertion index. Must verify 0 <= loc <= len(columns). column (str, number, or hashable object) \u2014 Label of the inserted column. value (Scalar, Series, or array-like) \u2014 Content of the inserted column. allow_duplicates (bool, optional, default lib.no_default) \u2014 Allow duplicate column labels to be created. See Also Index.insert : Insert new item by index. Examples >>> df = pd . DataFrame ({ 'col1' : [ 1 , 2 ], 'col2' : [ 3 , 4 ]}) >>> df col1 col2 0 1 3 1 2 4 >>> df . insert ( 1 , \"newcol\" , [ 99 , 99 ]) >>> df col1 newcol col2 0 1 99 3 1 2 99 4 >>> df . insert ( 0 , \"col1\" , [ 100 , 100 ], allow_duplicates = True ) >>> df col1 col1 newcol col2 0 100 1 99 3 1 100 2 99 4 Notice that pandas uses index alignment in case of value from type Series : >>> df . insert ( 0 , \"col0\" , pd . Series ([ 5 , 6 ], index = [ 1 , 2 ])) >>> df col0 col1 col1 newcol col2 0 NaN 100 1 99 3 1 5.0 100 2 99 4 method","title":"pandas.core.frame.DataFrame.insert"},{"location":"api/pipen.channel/#pandascoreframedataframeassign","text":"</> Assign new columns to a DataFrame. Returns a new object with all original columns in addition to new ones. Existing columns that are re-assigned will be overwritten. Parameters **kwargs (dict of {str: callable or Series}) \u2014 The column names are keywords. If the values arecallable, they are computed on the DataFrame and assigned to the new columns. The callable must not change input DataFrame (though pandas doesn't check it). If the values are not callable, (e.g. a Series, scalar, or array), they are simply assigned. Returns (DataFrame) A new DataFrame with the new columns in addition toall the existing columns. Notes Assigning multiple columns within the same assign is possible. Later items in '**kwargs' may refer to newly created or modified columns in 'df'; items are computed and assigned into 'df' in order. Examples >>> df = pd . DataFrame ({ 'temp_c' : [ 17.0 , 25.0 ]}, ... index = [ 'Portland' , 'Berkeley' ]) >>> df temp_c Portland 17.0 Berkeley 25.0 Where the value is a callable, evaluated on df : >>> df . assign ( temp_f = lambda x : x . temp_c * 9 / 5 + 32 ) temp_c temp_f Portland 17.0 62.6 Berkeley 25.0 77.0 Alternatively, the same behavior can be achieved by directly referencing an existing Series or sequence: >>> df . assign ( temp_f = df [ 'temp_c' ] * 9 / 5 + 32 ) temp_c temp_f Portland 17.0 62.6 Berkeley 25.0 77.0 You can create multiple columns within the same assign where one of the columns depends on another one defined within the same assign: >>> df . assign ( temp_f = lambda x : x [ 'temp_c' ] * 9 / 5 + 32 , ... temp_k = lambda x : ( x [ 'temp_f' ] + 459.67 ) * 5 / 9 ) temp_c temp_f temp_k Portland 17.0 62.6 290.15 Berkeley 25.0 77.0 298.15 method","title":"pandas.core.frame.DataFrame.assign"},{"location":"api/pipen.channel/#pandascoreframedataframeset_axis","text":"</> Assign desired index to given axis. Indexes for column or row labels can be changed by assigning a list-like or Index. Parameters labels (list-like, Index) \u2014 The values for the new index. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to update. The value 0 identifies the rows. For Series this parameter is unused and defaults to 0. copy (bool, default True) \u2014 Whether to make a copy of the underlying data. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` Returns (DataFrame) An object of type DataFrame. See Also DataFrame.renameaxis : Alter the name of the index or columns. Examples -------- ~~~python df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]}) ~~~ Change the row labels. >>> df . set_axis ([ 'a' , 'b' , 'c' ], axis = 'index' ) A B a 1 4 b 2 5 c 3 6 Change the column labels. >>> df . set_axis ([ 'I' , 'II' ], axis = 'columns' ) I II 0 1 4 1 2 5 2 3 6 method","title":"pandas.core.frame.DataFrame.set_axis"},{"location":"api/pipen.channel/#pandascoreframedataframereindex","text":"</> Conform DataFrame to new index with optional filling logic. Places NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False . See Also DataFrame.set_index : Set row labels.DataFrame.reset_index : Remove row labels or move them to new columns. DataFrame.reindexlike : Change to same indices as other DataFrame. Examples DataFrame.reindex supports two calling conventions (index=index_labels, columns=column_labels, ...) (labels, axis={'index', 'columns'}, ...) We highly recommend using keyword arguments to clarify your intent. Create a dataframe with some fictional data. >>> index = [ 'Firefox' , 'Chrome' , 'Safari' , 'IE10' , 'Konqueror' ] >>> df = pd . DataFrame ({ 'http_status' : [ 200 , 200 , 404 , 404 , 301 ], ... 'response_time' : [ 0.04 , 0.02 , 0.07 , 0.08 , 1.0 ]}, ... index = index ) >>> df http_status response_time Firefox 200 0.04 Chrome 200 0.02 Safari 404 0.07 IE10 404 0.08 Konqueror 301 1.00 Create a new index and reindex the dataframe. By default values in the new index that do not have corresponding records in the dataframe are assigned NaN . >>> new_index = [ 'Safari' , 'Iceweasel' , 'Comodo Dragon' , 'IE10' , ... 'Chrome' ] >>> df . reindex ( new_index ) http_status response_time Safari 404.0 0.07 Iceweasel NaN NaN Comodo Dragon NaN NaN IE10 404.0 0.08 Chrome 200.0 0.02 We can fill in the missing values by passing a value to the keyword fill_value . Because the index is not monotonically increasing or decreasing, we cannot use arguments to the keyword method to fill the NaN values. >>> df . reindex ( new_index , fill_value = 0 ) http_status response_time Safari 404 0.07 Iceweasel 0 0.00 Comodo Dragon 0 0.00 IE10 404 0.08 Chrome 200 0.02 >>> df . reindex ( new_index , fill_value = 'missing' ) http_status response_time Safari 404 0.07 Iceweasel missing missing Comodo Dragon missing missing IE10 404 0.08 Chrome 200 0.02 We can also reindex the columns. >>> df . reindex ( columns = [ 'http_status' , 'user_agent' ]) http_status user_agent Firefox 200 NaN Chrome 200 NaN Safari 404 NaN IE10 404 NaN Konqueror 301 NaN Or we can use \"axis-style\" keyword arguments >>> df . reindex ([ 'http_status' , 'user_agent' ], axis = \"columns\" ) http_status user_agent Firefox 200 NaN Chrome 200 NaN Safari 404 NaN IE10 404 NaN Konqueror 301 NaN To further illustrate the filling functionality in reindex , we will create a dataframe with a monotonically increasing index (for example, a sequence of dates). >>> date_index = pd . date_range ( '1/1/2010' , periods = 6 , freq = 'D' ) >>> df2 = pd . DataFrame ({ \"prices\" : [ 100 , 101 , np . nan , 100 , 89 , 88 ]}, ... index = date_index ) >>> df2 prices 2010 - 01 - 01 100.0 2010 - 01 - 02 101.0 2010 - 01 - 03 NaN 2010 - 01 - 04 100.0 2010 - 01 - 05 89.0 2010 - 01 - 06 88.0 Suppose we decide to expand the dataframe to cover a wider date range. >>> date_index2 = pd . date_range ( '12/29/2009' , periods = 10 , freq = 'D' ) >>> df2 . reindex ( date_index2 ) prices 2009 - 12 - 29 NaN 2009 - 12 - 30 NaN 2009 - 12 - 31 NaN 2010 - 01 - 01 100.0 2010 - 01 - 02 101.0 2010 - 01 - 03 NaN 2010 - 01 - 04 100.0 2010 - 01 - 05 89.0 2010 - 01 - 06 88.0 2010 - 01 - 07 NaN The index entries that did not have a value in the original data frame (for example, '2009-12-29') are by default filled with NaN . If desired, we can fill in the missing values using one of several options. For example, to back-propagate the last valid value to fill the NaN values, pass bfill as an argument to the method keyword. >>> df2 . reindex ( date_index2 , method = 'bfill' ) prices 2009 - 12 - 29 100.0 2009 - 12 - 30 100.0 2009 - 12 - 31 100.0 2010 - 01 - 01 100.0 2010 - 01 - 02 101.0 2010 - 01 - 03 NaN 2010 - 01 - 04 100.0 2010 - 01 - 05 89.0 2010 - 01 - 06 88.0 2010 - 01 - 07 NaN Please note that the NaN value present in the original dataframe (at index value 2010-01-03) will not be filled by any of the value propagation schemes. This is because filling while reindexing does not look at dataframe values, but only compares the original and desired indexes. If you do want to fill in the NaN values present in the original dataframe, use the fillna() method. See the :ref: user guide <basics.reindexing> for more. method","title":"pandas.core.frame.DataFrame.reindex"},{"location":"api/pipen.channel/#pandascoreframedataframedrop","text":"</> Drop specified labels from rows or columns. Remove rows or columns by specifying label names and corresponding axis, or by directly specifying index or column names. When using a multi-index, labels on different levels can be removed by specifying the level. See the :ref: user guide <advanced.shown_levels> for more information about the now unused levels. Parameters labels (single label or list-like) \u2014 Index or column labels to drop. A tuple will be used as a singlelabel and not treated as a list-like. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Whether to drop labels from the index (0 or 'index') orcolumns (1 or 'columns'). index (single label or list-like) \u2014 Alternative to specifying axis ( labels, axis=0 is equivalent to index=labels ). columns (single label or list-like) \u2014 Alternative to specifying axis ( labels, axis=1 is equivalent to columns=labels ). level (int or level name, optional) \u2014 For MultiIndex, level from which the labels will be removed. inplace (bool, default False) \u2014 If False, return a copy. Otherwise, do operationin place and return None. errors ({'ignore', 'raise'}, default 'raise') \u2014 If 'ignore', suppress error and only existing labels aredropped. Returns (DataFrame or None) Returns DataFrame or None DataFrame with the specifiedindex or column labels removed or None if inplace=True. Raises KeyError \u2014 If any of the labels is not found in the selected axis. See Also DataFrame.loc : Label-location based indexer for selection by label.DataFrame.dropna : Return DataFrame with labels on given axis omitted where (all or any) data are missing. DataFrame.dropduplicates : Return DataFrame with duplicate rows removed, optionally only considering certain columns. Series.drop : Return Series with specified index labels removed. Examples >>> df = pd . DataFrame ( np . arange ( 12 ) . reshape ( 3 , 4 ), ... columns = [ 'A' , 'B' , 'C' , 'D' ]) >>> df A B C D 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 Drop columns >>> df . drop ([ 'B' , 'C' ], axis = 1 ) A D 0 0 3 1 4 7 2 8 11 >>> df . drop ( columns = [ 'B' , 'C' ]) A D 0 0 3 1 4 7 2 8 11 Drop a row by index >>> df . drop ([ 0 , 1 ]) A B C D 2 8 9 10 11 Drop columns and/or rows of MultiIndex DataFrame >>> midx = pd . MultiIndex ( levels = [[ 'llama' , 'cow' , 'falcon' ], ... [ 'speed' , 'weight' , 'length' ]], ... codes = [[ 0 , 0 , 0 , 1 , 1 , 1 , 2 , 2 , 2 ], ... [ 0 , 1 , 2 , 0 , 1 , 2 , 0 , 1 , 2 ]]) >>> df = pd . DataFrame ( index = midx , columns = [ 'big' , 'small' ], ... data = [[ 45 , 30 ], [ 200 , 100 ], [ 1.5 , 1 ], [ 30 , 20 ], ... [ 250 , 150 ], [ 1.5 , 0.8 ], [ 320 , 250 ], ... [ 1 , 0.8 ], [ 0.3 , 0.2 ]]) >>> df big small llama speed 45.0 30.0 weight 200.0 100.0 length 1.5 1.0 cow speed 30.0 20.0 weight 250.0 150.0 length 1.5 0.8 falcon speed 320.0 250.0 weight 1.0 0.8 length 0.3 0.2 Drop a specific index combination from the MultiIndex DataFrame, i.e., drop the combination 'falcon' and 'weight' , which deletes only the corresponding row >>> df . drop ( index = ( 'falcon' , 'weight' )) big small llama speed 45.0 30.0 weight 200.0 100.0 length 1.5 1.0 cow speed 30.0 20.0 weight 250.0 150.0 length 1.5 0.8 falcon speed 320.0 250.0 length 0.3 0.2 >>> df . drop ( index = 'cow' , columns = 'small' ) big llama speed 45.0 weight 200.0 length 1.5 falcon speed 320.0 weight 1.0 length 0.3 >>> df . drop ( index = 'length' , level = 1 ) big small llama speed 45.0 30.0 weight 200.0 100.0 cow speed 30.0 20.0 weight 250.0 150.0 falcon speed 320.0 250.0 weight 1.0 0.8 method","title":"pandas.core.frame.DataFrame.drop"},{"location":"api/pipen.channel/#pandascoreframedataframerename","text":"</> Rename columns or index labels. Function / dict values must be unique (1-to-1). Labels not contained in a dict / Series will be left as-is. Extra labels listed don't throw an error. See the :ref: user guide <basics.rename> for more. Parameters mapper (dict-like or function) \u2014 Dict-like or function transformations to apply tothat axis' values. Use either mapper and axis to specify the axis to target with mapper , or index and columns . index (dict-like or function) \u2014 Alternative to specifying axis ( mapper, axis=0 is equivalent to index=mapper ). columns (dict-like or function) \u2014 Alternative to specifying axis ( mapper, axis=1 is equivalent to columns=mapper ). axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Axis to target with mapper . Can be either the axis name('index', 'columns') or number (0, 1). The default is 'index'. copy (bool, default True) \u2014 Also copy underlying data. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` inplace (bool, default False) \u2014 Whether to modify the DataFrame rather than creating a new one.If True then value of copy is ignored. level (int or level name, default None) \u2014 In case of a MultiIndex, only rename labels in the specifiedlevel. errors ({'ignore', 'raise'}, default 'ignore') \u2014 If 'raise', raise a KeyError when a dict-like mapper , index ,or columns contains labels that are not present in the Index being transformed. If 'ignore', existing keys will be renamed and extra keys will be ignored. Returns (DataFrame or None) DataFrame with the renamed axis labels or None if inplace=True . Raises KeyError \u2014 If any of the labels is not found in the selected axis and\"errors='raise'\". See Also DataFrame.renameaxis : Set the name of the axis. Examples DataFrame.rename supports two calling conventions (index=index_mapper, columns=columns_mapper, ...) (mapper, axis={'index', 'columns'}, ...) We highly recommend using keyword arguments to clarify your intent. Rename columns using a mapping: >>> df = pd . DataFrame ({ \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ]}) >>> df . rename ( columns = { \"A\" : \"a\" , \"B\" : \"c\" }) a c 0 1 4 1 2 5 2 3 6 Rename index using a mapping: >>> df . rename ( index = { 0 : \"x\" , 1 : \"y\" , 2 : \"z\" }) A B x 1 4 y 2 5 z 3 6 Cast index labels to a different type: >>> df . index RangeIndex ( start = 0 , stop = 3 , step = 1 ) >>> df . rename ( index = str ) . index Index ([ '0' , '1' , '2' ], dtype = 'object' ) >>> df . rename ( columns = { \"A\" : \"a\" , \"B\" : \"b\" , \"C\" : \"c\" }, errors = \"raise\" ) Traceback ( most recent call last ): KeyError : [ 'C' ] not found in axis Using axis-style parameters: >>> df . rename ( str . lower , axis = 'columns' ) a b 0 1 4 1 2 5 2 3 6 >>> df . rename ({ 1 : 2 , 2 : 4 }, axis = 'index' ) A B 0 1 4 2 2 5 4 3 6 method","title":"pandas.core.frame.DataFrame.rename"},{"location":"api/pipen.channel/#pandascoreframedataframepop","text":"</> Return item and drop from frame. Raise KeyError if not found. Parameters item (label) \u2014 Label of column to be popped. Examples >>> df = pd . DataFrame ([( 'falcon' , 'bird' , 389.0 ), ... ( 'parrot' , 'bird' , 24.0 ), ... ( 'lion' , 'mammal' , 80.5 ), ... ( 'monkey' , 'mammal' , np . nan )], ... columns = ( 'name' , 'class' , 'max_speed' )) >>> df name class max_speed 0 falcon bird 389.0 1 parrot bird 24.0 2 lion mammal 80.5 3 monkey mammal NaN >>> df . pop ( 'class' ) 0 bird 1 bird 2 mammal 3 mammal Name : class , dtype : object >>> df name max_speed 0 falcon 389.0 1 parrot 24.0 2 lion 80.5 3 monkey NaN method","title":"pandas.core.frame.DataFrame.pop"},{"location":"api/pipen.channel/#pandascoreframedataframeshift","text":"</> Shift index by desired number of periods with an optional time freq . When freq is not passed, shift the index without realigning the data. If freq is passed (in this case, the index must be date or datetime, or it will raise a NotImplementedError ), the index will be increased using the periods and the freq . freq can be inferred when specified as \"infer\" as long as either freq or inferred_freq attribute is set in the index. Parameters periods (int or Sequence) \u2014 Number of periods to shift. Can be positive or negative.If an iterable of ints, the data will be shifted once by each int. This is equivalent to shifting by one value at a time and concatenating all resulting frames. The resulting columns will have the shift suffixed to their column names. For multiple periods, axis must not be 1. freq (DateOffset, tseries.offsets, timedelta, or str, optional) \u2014 Offset to use from the tseries module or time rule (e.g. 'EOM').If freq is specified then the index values are shifted but the data is not realigned. That is, use freq if you would like to extend the index when shifting and preserve the original data. If freq is specified as \"infer\" then it will be inferred from the freq or inferred_freq attributes of the index. If neither of those attributes exist, a ValueError is thrown. axis ({0 or 'index', 1 or 'columns', None}, default None) \u2014 Shift direction. For Series this parameter is unused and defaults to 0. fill_value (object, optional) \u2014 The scalar value to use for newly introduced missing values.the default depends on the dtype of self . For numeric data, np.nan is used. For datetime, timedelta, or period data, etc. :attr: NaT is used. For extension dtypes, self.dtype.na_value is used. suffix (str, optional) \u2014 If str and periods is an iterable, this is added after the columnname and before the shift value for each shifted column name. Returns (DataFrame) Copy of input object, shifted. See Also Index.shift : Shift values of Index.DatetimeIndex.shift : Shift values of DatetimeIndex. PeriodIndex.shift : Shift values of PeriodIndex. Examples >>> df = pd . DataFrame ({ \"Col1\" : [ 10 , 20 , 15 , 30 , 45 ], ... \"Col2\" : [ 13 , 23 , 18 , 33 , 48 ], ... \"Col3\" : [ 17 , 27 , 22 , 37 , 52 ]}, ... index = pd . date_range ( \"2020-01-01\" , \"2020-01-05\" )) >>> df Col1 Col2 Col3 2020 - 01 - 01 10 13 17 2020 - 01 - 02 20 23 27 2020 - 01 - 03 15 18 22 2020 - 01 - 04 30 33 37 2020 - 01 - 05 45 48 52 >>> df . shift ( periods = 3 ) Col1 Col2 Col3 2020 - 01 - 01 NaN NaN NaN 2020 - 01 - 02 NaN NaN NaN 2020 - 01 - 03 NaN NaN NaN 2020 - 01 - 04 10.0 13.0 17.0 2020 - 01 - 05 20.0 23.0 27.0 >>> df . shift ( periods = 1 , axis = \"columns\" ) Col1 Col2 Col3 2020 - 01 - 01 NaN 10 13 2020 - 01 - 02 NaN 20 23 2020 - 01 - 03 NaN 15 18 2020 - 01 - 04 NaN 30 33 2020 - 01 - 05 NaN 45 48 >>> df . shift ( periods = 3 , fill_value = 0 ) Col1 Col2 Col3 2020 - 01 - 01 0 0 0 2020 - 01 - 02 0 0 0 2020 - 01 - 03 0 0 0 2020 - 01 - 04 10 13 17 2020 - 01 - 05 20 23 27 >>> df . shift ( periods = 3 , freq = \"D\" ) Col1 Col2 Col3 2020 - 01 - 04 10 13 17 2020 - 01 - 05 20 23 27 2020 - 01 - 06 15 18 22 2020 - 01 - 07 30 33 37 2020 - 01 - 08 45 48 52 >>> df . shift ( periods = 3 , freq = \"infer\" ) Col1 Col2 Col3 2020 - 01 - 04 10 13 17 2020 - 01 - 05 20 23 27 2020 - 01 - 06 15 18 22 2020 - 01 - 07 30 33 37 2020 - 01 - 08 45 48 52 >>> df [ 'Col1' ] . shift ( periods = [ 0 , 1 , 2 ]) Col1_0 Col1_1 Col1_2 2020 - 01 - 01 10 NaN NaN 2020 - 01 - 02 20 10.0 NaN 2020 - 01 - 03 15 20.0 10.0 2020 - 01 - 04 30 15.0 20.0 2020 - 01 - 05 45 30.0 15.0 method","title":"pandas.core.frame.DataFrame.shift"},{"location":"api/pipen.channel/#pandascoreframedataframeset_index","text":"</> Set the DataFrame index using existing columns. Set the DataFrame index (row labels) using one or more existing columns or arrays (of the correct length). The index can replace the existing index or expand on it. Parameters keys (label or array-like or list of labels/arrays) \u2014 This parameter can be either a single column key, a single array ofthe same length as the calling DataFrame, or a list containing an arbitrary combination of column keys and arrays. Here, \"array\" encompasses :class: Series , :class: Index , np.ndarray , and instances of :class: ~collections.abc.Iterator . drop (bool, default True) \u2014 Delete columns to be used as the new index. append (bool, default False) \u2014 Whether to append columns to existing index. inplace (bool, default False) \u2014 Whether to modify the DataFrame rather than creating a new one. verify_integrity (bool, default False) \u2014 Check the new index for duplicates. Otherwise defer the check untilnecessary. Setting to False will improve the performance of this method. Returns (DataFrame or None) Changed row labels or None if inplace=True . See Also DataFrame.reset_index : Opposite of set_index.DataFrame.reindex : Change to new indices or expand indices. DataFrame.reindexlike : Change to same indices as other DataFrame. Examples >>> df = pd . DataFrame ({ 'month' : [ 1 , 4 , 7 , 10 ], ... 'year' : [ 2012 , 2014 , 2013 , 2014 ], ... 'sale' : [ 55 , 40 , 84 , 31 ]}) >>> df month year sale 0 1 2012 55 1 4 2014 40 2 7 2013 84 3 10 2014 31 Set the index to become the 'month' column: >>> df . set_index ( 'month' ) year sale month 1 2012 55 4 2014 40 7 2013 84 10 2014 31 Create a MultiIndex using columns 'year' and 'month': >>> df . set_index ([ 'year' , 'month' ]) sale year month 2012 1 55 2014 4 40 2013 7 84 2014 10 31 Create a MultiIndex using an Index and a column: >>> df . set_index ([ pd . Index ([ 1 , 2 , 3 , 4 ]), 'year' ]) month sale year 1 2012 1 55 2 2014 4 40 3 2013 7 84 4 2014 10 31 Create a MultiIndex using two Series: >>> s = pd . Series ([ 1 , 2 , 3 , 4 ]) >>> df . set_index ([ s , s ** 2 ]) month year sale 1 1 1 2012 55 2 4 4 2014 40 3 9 7 2013 84 4 16 10 2014 31 method","title":"pandas.core.frame.DataFrame.set_index"},{"location":"api/pipen.channel/#pandascoreframedataframereset_index","text":"</> Reset the index, or a level of it. Reset the index of the DataFrame, and use the default one instead. If the DataFrame has a MultiIndex, this method can remove one or more levels. Parameters level (int, str, tuple, or list, default None) \u2014 Only remove the given levels from the index. Removes all levels bydefault. drop (bool, default False) \u2014 Do not try to insert index into dataframe columns. This resetsthe index to the default integer index. inplace (bool, default False) \u2014 Whether to modify the DataFrame rather than creating a new one. col_level (int or str, default 0) \u2014 If the columns have multiple levels, determines which level thelabels are inserted into. By default it is inserted into the first level. col_fill (object, default '') \u2014 If the columns have multiple levels, determines how the otherlevels are named. If None then the index name is repeated. allow_duplicates (bool, optional, default lib.no_default) \u2014 Allow duplicate column labels to be created. .. versionadded:: 1.5.0 names (int, str or 1-dimensional list, default None) \u2014 Using the given string, rename the DataFrame column which contains theindex data. If the DataFrame has a MultiIndex, this has to be a list or tuple with length equal to the number of levels. .. versionadded:: 1.5.0 Returns (DataFrame or None) DataFrame with the new index or None if inplace=True . See Also DataFrame.set_index : Opposite of reset_index.DataFrame.reindex : Change to new indices or expand indices. DataFrame.reindexlike : Change to same indices as other DataFrame. Examples >>> df = pd . DataFrame ([( 'bird' , 389.0 ), ... ( 'bird' , 24.0 ), ... ( 'mammal' , 80.5 ), ... ( 'mammal' , np . nan )], ... index = [ 'falcon' , 'parrot' , 'lion' , 'monkey' ], ... columns = ( 'class' , 'max_speed' )) >>> df class max_speed falcon bird 389.0 parrot bird 24.0 lion mammal 80.5 monkey mammal NaN When we reset the index, the old index is added as a column, and a new sequential index is used: >>> df . reset_index () index class max_speed 0 falcon bird 389.0 1 parrot bird 24.0 2 lion mammal 80.5 3 monkey mammal NaN We can use the drop parameter to avoid the old index being added as a column: >>> df . reset_index ( drop = True ) class max_speed 0 bird 389.0 1 bird 24.0 2 mammal 80.5 3 mammal NaN You can also use reset_index with MultiIndex . >>> index = pd . MultiIndex . from_tuples ([( 'bird' , 'falcon' ), ... ( 'bird' , 'parrot' ), ... ( 'mammal' , 'lion' ), ... ( 'mammal' , 'monkey' )], ... names = [ 'class' , 'name' ]) >>> columns = pd . MultiIndex . from_tuples ([( 'speed' , 'max' ), ... ( 'species' , 'type' )]) >>> df = pd . DataFrame ([( 389.0 , 'fly' ), ... ( 24.0 , 'fly' ), ... ( 80.5 , 'run' ), ... ( np . nan , 'jump' )], ... index = index , ... columns = columns ) >>> df speed species max type class name bird falcon 389.0 fly parrot 24.0 fly mammal lion 80.5 run monkey NaN jump Using the names parameter, choose a name for the index column: >>> df . reset_index ( names = [ 'classes' , 'names' ]) classes names speed species max type 0 bird falcon 389.0 fly 1 bird parrot 24.0 fly 2 mammal lion 80.5 run 3 mammal monkey NaN jump If the index has multiple levels, we can reset a subset of them: >>> df . reset_index ( level = 'class' ) class speed species max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump If we are not dropping the index, by default, it is placed in the top level. We can place it in another level: >>> df . reset_index ( level = 'class' , col_level = 1 ) speed species class max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump When the index is inserted under another level, we can specify under which one with the parameter col_fill : >>> df . reset_index ( level = 'class' , col_level = 1 , col_fill = 'species' ) species speed species class max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump If we specify a nonexistent level for col_fill , it is created: >>> df . reset_index ( level = 'class' , col_level = 1 , col_fill = 'genus' ) genus speed species class max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump method","title":"pandas.core.frame.DataFrame.reset_index"},{"location":"api/pipen.channel/#pandascoreframedataframeisna","text":"</> Detect missing values. Return a boolean same-sized object indicating if the values are NA. NA values, such as None or :attr: numpy.NaN , gets mapped to True values. Everything else gets mapped to False values. Characters such as empty strings '' or :attr: numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True ). Returns (DataFrame) Mask of bool values for each element in DataFrame thatindicates whether an element is an NA value. See Also DataFrame.isnull : Alias of isna.DataFrame.notna : Boolean inverse of isna. DataFrame.dropna : Omit axes labels with missing values. isna : Top-level isna. Examples Show which entries in a DataFrame are NA. >>> df = pd . DataFrame ( dict ( age = [ 5 , 6 , np . nan ], ... born = [ pd . NaT , pd . Timestamp ( '1939-05-27' ), ... pd . Timestamp ( '1940-04-25' )], ... name = [ 'Alfred' , 'Batman' , '' ], ... toy = [ None , 'Batmobile' , 'Joker' ])) >>> df age born name toy 0 5.0 NaT Alfred None 1 6.0 1939 - 05 - 27 Batman Batmobile 2 NaN 1940 - 04 - 25 Joker >>> df . isna () age born name toy 0 False True False True 1 False False False False 2 True False False False Show which entries in a Series are NA. >>> ser = pd . Series ([ 5 , 6 , np . nan ]) >>> ser 0 5.0 1 6.0 2 NaN dtype : float64 >>> ser . isna () 0 False 1 False 2 True dtype : bool method","title":"pandas.core.frame.DataFrame.isna"},{"location":"api/pipen.channel/#pandascoreframedataframeisnull","text":"</> DataFrame.isnull is an alias for DataFrame.isna. Detect missing values. Return a boolean same-sized object indicating if the values are NA. NA values, such as None or :attr: numpy.NaN , gets mapped to True values. Everything else gets mapped to False values. Characters such as empty strings '' or :attr: numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True ). Returns (DataFrame) Mask of bool values for each element in DataFrame thatindicates whether an element is an NA value. See Also DataFrame.isnull : Alias of isna.DataFrame.notna : Boolean inverse of isna. DataFrame.dropna : Omit axes labels with missing values. isna : Top-level isna. Examples Show which entries in a DataFrame are NA. >>> df = pd . DataFrame ( dict ( age = [ 5 , 6 , np . nan ], ... born = [ pd . NaT , pd . Timestamp ( '1939-05-27' ), ... pd . Timestamp ( '1940-04-25' )], ... name = [ 'Alfred' , 'Batman' , '' ], ... toy = [ None , 'Batmobile' , 'Joker' ])) >>> df age born name toy 0 5.0 NaT Alfred None 1 6.0 1939 - 05 - 27 Batman Batmobile 2 NaN 1940 - 04 - 25 Joker >>> df . isna () age born name toy 0 False True False True 1 False False False False 2 True False False False Show which entries in a Series are NA. >>> ser = pd . Series ([ 5 , 6 , np . nan ]) >>> ser 0 5.0 1 6.0 2 NaN dtype : float64 >>> ser . isna () 0 False 1 False 2 True dtype : bool method","title":"pandas.core.frame.DataFrame.isnull"},{"location":"api/pipen.channel/#pandascoreframedataframenotna","text":"</> Detect existing (non-missing) values. Return a boolean same-sized object indicating if the values are not NA. Non-missing values get mapped to True. Characters such as empty strings '' or :attr: numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True ). NA values, such as None or :attr: numpy.NaN , get mapped to False values. Returns (DataFrame) Mask of bool values for each element in DataFrame thatindicates whether an element is not an NA value. See Also DataFrame.notnull : Alias of notna.DataFrame.isna : Boolean inverse of notna. DataFrame.dropna : Omit axes labels with missing values. notna : Top-level notna. Examples Show which entries in a DataFrame are not NA. >>> df = pd . DataFrame ( dict ( age = [ 5 , 6 , np . nan ], ... born = [ pd . NaT , pd . Timestamp ( '1939-05-27' ), ... pd . Timestamp ( '1940-04-25' )], ... name = [ 'Alfred' , 'Batman' , '' ], ... toy = [ None , 'Batmobile' , 'Joker' ])) >>> df age born name toy 0 5.0 NaT Alfred None 1 6.0 1939 - 05 - 27 Batman Batmobile 2 NaN 1940 - 04 - 25 Joker >>> df . notna () age born name toy 0 True False True False 1 True True True True 2 False True True True Show which entries in a Series are not NA. >>> ser = pd . Series ([ 5 , 6 , np . nan ]) >>> ser 0 5.0 1 6.0 2 NaN dtype : float64 >>> ser . notna () 0 True 1 True 2 False dtype : bool method","title":"pandas.core.frame.DataFrame.notna"},{"location":"api/pipen.channel/#pandascoreframedataframenotnull","text":"</> DataFrame.notnull is an alias for DataFrame.notna. Detect existing (non-missing) values. Return a boolean same-sized object indicating if the values are not NA. Non-missing values get mapped to True. Characters such as empty strings '' or :attr: numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True ). NA values, such as None or :attr: numpy.NaN , get mapped to False values. Returns (DataFrame) Mask of bool values for each element in DataFrame thatindicates whether an element is not an NA value. See Also DataFrame.notnull : Alias of notna.DataFrame.isna : Boolean inverse of notna. DataFrame.dropna : Omit axes labels with missing values. notna : Top-level notna. Examples Show which entries in a DataFrame are not NA. >>> df = pd . DataFrame ( dict ( age = [ 5 , 6 , np . nan ], ... born = [ pd . NaT , pd . Timestamp ( '1939-05-27' ), ... pd . Timestamp ( '1940-04-25' )], ... name = [ 'Alfred' , 'Batman' , '' ], ... toy = [ None , 'Batmobile' , 'Joker' ])) >>> df age born name toy 0 5.0 NaT Alfred None 1 6.0 1939 - 05 - 27 Batman Batmobile 2 NaN 1940 - 04 - 25 Joker >>> df . notna () age born name toy 0 True False True False 1 True True True True 2 False True True True Show which entries in a Series are not NA. >>> ser = pd . Series ([ 5 , 6 , np . nan ]) >>> ser 0 5.0 1 6.0 2 NaN dtype : float64 >>> ser . notna () 0 True 1 True 2 False dtype : bool method","title":"pandas.core.frame.DataFrame.notnull"},{"location":"api/pipen.channel/#pandascoreframedataframedropna","text":"</> Remove missing values. See the :ref: User Guide <missing_data> for more on which values are considered missing, and how to work with missing data. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Determine if rows or columns which contain missing values areremoved. 0, or 'index' : Drop rows which contain missing values. 1, or 'columns' : Drop columns which contain missing value. Only a single axis is allowed. how ({'any', 'all'}, default 'any') \u2014 Determine if row or column is removed from DataFrame, when we haveat least one NA or all NA. 'any' : If any NA values are present, drop that row or column. 'all' : If all values are NA, drop that row or column. thresh (int, optional) \u2014 Require that many non-NA values. Cannot be combined with how. subset (column label or sequence of labels, optional) \u2014 Labels along other axis to consider, e.g. if you are dropping rowsthese would be a list of columns to include. inplace (bool, default False) \u2014 Whether to modify the DataFrame rather than creating a new one. ignore_index (bool, default ``False``) \u2014 If True , the resulting axis will be labeled 0, 1, \u2026, n - 1. .. versionadded:: 2.0.0 Returns (DataFrame or None) DataFrame with NA entries dropped from it or None if inplace=True . See Also DataFrame.isna: Indicate missing values.DataFrame.notna : Indicate existing (non-missing) values. DataFrame.fillna : Replace missing values. Series.dropna : Drop missing values. Index.dropna : Drop missing indices. Examples >>> df = pd . DataFrame ({ \"name\" : [ 'Alfred' , 'Batman' , 'Catwoman' ], ... \"toy\" : [ np . nan , 'Batmobile' , 'Bullwhip' ], ... \"born\" : [ pd . NaT , pd . Timestamp ( \"1940-04-25\" ), ... pd . NaT ]}) >>> df name toy born 0 Alfred NaN NaT 1 Batman Batmobile 1940 - 04 - 25 2 Catwoman Bullwhip NaT Drop the rows where at least one element is missing. >>> df . dropna () name toy born 1 Batman Batmobile 1940 - 04 - 25 Drop the columns where at least one element is missing. >>> df . dropna ( axis = 'columns' ) name 0 Alfred 1 Batman 2 Catwoman Drop the rows where all elements are missing. >>> df . dropna ( how = 'all' ) name toy born 0 Alfred NaN NaT 1 Batman Batmobile 1940 - 04 - 25 2 Catwoman Bullwhip NaT Keep only the rows with at least 2 non-NA values. >>> df . dropna ( thresh = 2 ) name toy born 1 Batman Batmobile 1940 - 04 - 25 2 Catwoman Bullwhip NaT Define in which columns to look for missing values. >>> df . dropna ( subset = [ 'name' , 'toy' ]) name toy born 1 Batman Batmobile 1940 - 04 - 25 2 Catwoman Bullwhip NaT method","title":"pandas.core.frame.DataFrame.dropna"},{"location":"api/pipen.channel/#pandascoreframedataframedrop_duplicates","text":"</> Return DataFrame with duplicate rows removed. Considering certain columns is optional. Indexes, including time indexes are ignored. Parameters subset (column label or sequence of labels, optional) \u2014 Only consider certain columns for identifying duplicates, bydefault use all of the columns. keep ({'first', 'last', ``False``}, default 'first') \u2014 Determines which duplicates (if any) to keep. 'first' : Drop duplicates except for the first occurrence. 'last' : Drop duplicates except for the last occurrence. False : Drop all duplicates. inplace (bool, default ``False``) \u2014 Whether to modify the DataFrame rather than creating a new one. ignore_index (bool, default ``False``) \u2014 If True , the resulting axis will be labeled 0, 1, \u2026, n - 1. Returns (DataFrame or None) DataFrame with duplicates removed or None if inplace=True . See Also DataFrame.value_counts: Count unique combinations of columns. Examples Consider dataset containing ramen rating. >>> df = pd . DataFrame ({ ... 'brand' : [ 'Yum Yum' , 'Yum Yum' , 'Indomie' , 'Indomie' , 'Indomie' ], ... 'style' : [ 'cup' , 'cup' , 'cup' , 'pack' , 'pack' ], ... 'rating' : [ 4 , 4 , 3.5 , 15 , 5 ] ... }) >>> df brand style rating 0 Yum Yum cup 4.0 1 Yum Yum cup 4.0 2 Indomie cup 3.5 3 Indomie pack 15.0 4 Indomie pack 5.0 By default, it removes duplicate rows based on all columns. >>> df . drop_duplicates () brand style rating 0 Yum Yum cup 4.0 2 Indomie cup 3.5 3 Indomie pack 15.0 4 Indomie pack 5.0 To remove duplicates on specific column(s), use subset . >>> df . drop_duplicates ( subset = [ 'brand' ]) brand style rating 0 Yum Yum cup 4.0 2 Indomie cup 3.5 To remove duplicates and keep last occurrences, use keep . >>> df . drop_duplicates ( subset = [ 'brand' , 'style' ], keep = 'last' ) brand style rating 1 Yum Yum cup 4.0 2 Indomie cup 3.5 4 Indomie pack 5.0 method","title":"pandas.core.frame.DataFrame.drop_duplicates"},{"location":"api/pipen.channel/#pandascoreframedataframeduplicated","text":"</> Return boolean Series denoting duplicate rows. Considering certain columns is optional. Parameters subset (column label or sequence of labels, optional) \u2014 Only consider certain columns for identifying duplicates, bydefault use all of the columns. keep ({'first', 'last', False}, default 'first') \u2014 Determines which duplicates (if any) to mark. first : Mark duplicates as True except for the first occurrence. last : Mark duplicates as True except for the last occurrence. False : Mark all duplicates as True . Returns (Series) Boolean series for each duplicated rows. See Also Index.duplicated : Equivalent method on index.Series.duplicated : Equivalent method on Series. Series.dropduplicates : Remove duplicate values from Series. DataFrame.dropduplicates : Remove duplicate values from DataFrame. Examples Consider dataset containing ramen rating. >>> df = pd . DataFrame ({ ... 'brand' : [ 'Yum Yum' , 'Yum Yum' , 'Indomie' , 'Indomie' , 'Indomie' ], ... 'style' : [ 'cup' , 'cup' , 'cup' , 'pack' , 'pack' ], ... 'rating' : [ 4 , 4 , 3.5 , 15 , 5 ] ... }) >>> df brand style rating 0 Yum Yum cup 4.0 1 Yum Yum cup 4.0 2 Indomie cup 3.5 3 Indomie pack 15.0 4 Indomie pack 5.0 By default, for each set of duplicated values, the first occurrence is set on False and all others on True. >>> df . duplicated () 0 False 1 True 2 False 3 False 4 False dtype : bool By using 'last', the last occurrence of each set of duplicated values is set on False and all others on True. >>> df . duplicated ( keep = 'last' ) 0 True 1 False 2 False 3 False 4 False dtype : bool By setting keep on False, all duplicates are True. >>> df . duplicated ( keep = False ) 0 True 1 True 2 False 3 False 4 False dtype : bool To find duplicates on specific column(s), use subset . >>> df . duplicated ( subset = [ 'brand' ]) 0 False 1 True 2 False 3 True 4 True dtype : bool method","title":"pandas.core.frame.DataFrame.duplicated"},{"location":"api/pipen.channel/#pandascoreframedataframesort_values","text":"</> Sort by the values along either axis. Parameters by (str or list of str) \u2014 Name or list of names to sort by. if axis is 0 or 'index' then by may contain index levels and/or column labels. if axis is 1 or 'columns' then by may contain column levels and/or index labels. axis (\"{0 or 'index', 1 or 'columns'}\", default 0) \u2014 Axis to be sorted. ascending (bool or list of bool, default True) \u2014 Sort ascending vs. descending. Specify list for multiple sortorders. If this is a list of bools, must match the length of the by. inplace (bool, default False) \u2014 If True, perform operation in-place. kind ({'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort') \u2014 Choice of sorting algorithm. See also :func: numpy.sort for moreinformation. mergesort and stable are the only stable algorithms. For DataFrames, this option is only applied when sorting on a single column or label. na_position ({'first', 'last'}, default 'last') \u2014 Puts NaNs at the beginning if first ; last puts NaNs at theend. ignore_index (bool, default False) \u2014 If True, the resulting axis will be labeled 0, 1, \u2026, n - 1. key (callable, optional) \u2014 Apply the key function to the valuesbefore sorting. This is similar to the key argument in the builtin :meth: sorted function, with the notable difference that this key function should be vectorized . It should expect a Series and return a Series with the same shape as the input. It will be applied to each column in by independently. Returns (DataFrame or None) DataFrame with sorted values or None if inplace=True . See Also DataFrame.sort_index : Sort a DataFrame by the index.Series.sort_values : Similar method for a Series. Examples >>> df = pd . DataFrame ({ ... 'col1' : [ 'A' , 'A' , 'B' , np . nan , 'D' , 'C' ], ... 'col2' : [ 2 , 1 , 9 , 8 , 7 , 4 ], ... 'col3' : [ 0 , 1 , 9 , 4 , 2 , 3 ], ... 'col4' : [ 'a' , 'B' , 'c' , 'D' , 'e' , 'F' ] ... }) >>> df col1 col2 col3 col4 0 A 2 0 a 1 A 1 1 B 2 B 9 9 c 3 NaN 8 4 D 4 D 7 2 e 5 C 4 3 F Sort by col1 >>> df . sort_values ( by = [ 'col1' ]) col1 col2 col3 col4 0 A 2 0 a 1 A 1 1 B 2 B 9 9 c 5 C 4 3 F 4 D 7 2 e 3 NaN 8 4 D Sort by multiple columns >>> df . sort_values ( by = [ 'col1' , 'col2' ]) col1 col2 col3 col4 1 A 1 1 B 0 A 2 0 a 2 B 9 9 c 5 C 4 3 F 4 D 7 2 e 3 NaN 8 4 D Sort Descending >>> df . sort_values ( by = 'col1' , ascending = False ) col1 col2 col3 col4 4 D 7 2 e 5 C 4 3 F 2 B 9 9 c 0 A 2 0 a 1 A 1 1 B 3 NaN 8 4 D Putting NAs first >>> df . sort_values ( by = 'col1' , ascending = False , na_position = 'first' ) col1 col2 col3 col4 3 NaN 8 4 D 4 D 7 2 e 5 C 4 3 F 2 B 9 9 c 0 A 2 0 a 1 A 1 1 B Sorting with a key function >>> df . sort_values ( by = 'col4' , key = lambda col : col . str . lower ()) col1 col2 col3 col4 0 A 2 0 a 1 A 1 1 B 2 B 9 9 c 3 NaN 8 4 D 4 D 7 2 e 5 C 4 3 F Natural sort with the key argument, using the natsort <https://github.com/SethMMorton/natsort> package. >>> df = pd . DataFrame ({ ... \"time\" : [ '0hr' , '128hr' , '72hr' , '48hr' , '96hr' ], ... \"value\" : [ 10 , 20 , 30 , 40 , 50 ] ... }) >>> df time value 0 0 hr 10 1 128 hr 20 2 72 hr 30 3 48 hr 40 4 96 hr 50 >>> from natsort import index_natsorted >>> df . sort_values ( ... by = \"time\" , ... key = lambda x : np . argsort ( index_natsorted ( df [ \"time\" ])) ... ) time value 0 0 hr 10 3 48 hr 40 2 72 hr 30 4 96 hr 50 1 128 hr 20 method","title":"pandas.core.frame.DataFrame.sort_values"},{"location":"api/pipen.channel/#pandascoreframedataframesort_index","text":"</> Sort object by labels (along an axis). Returns a new DataFrame sorted by label if inplace argument is False , otherwise updates the original DataFrame and returns None. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis along which to sort. The value 0 identifies the rows,and 1 identifies the columns. level (int or level name or list of ints or list of level names) \u2014 If not None, sort on values in specified index level(s). ascending (bool or list-like of bools, default True) \u2014 Sort ascending vs. descending. When the index is a MultiIndex thesort direction can be controlled for each level individually. inplace (bool, default False) \u2014 Whether to modify the DataFrame rather than creating a new one. kind ({'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort') \u2014 Choice of sorting algorithm. See also :func: numpy.sort for moreinformation. mergesort and stable are the only stable algorithms. For DataFrames, this option is only applied when sorting on a single column or label. na_position ({'first', 'last'}, default 'last') \u2014 Puts NaNs at the beginning if first ; last puts NaNs at the end.Not implemented for MultiIndex. sort_remaining (bool, default True) \u2014 If True and sorting by level and index is multilevel, sort by otherlevels too (in order) after sorting by specified level. ignore_index (bool, default False) \u2014 If True, the resulting axis will be labeled 0, 1, \u2026, n - 1. key (callable, optional) \u2014 If not None, apply the key function to the index valuesbefore sorting. This is similar to the key argument in the builtin :meth: sorted function, with the notable difference that this key function should be vectorized . It should expect an Index and return an Index of the same shape. For MultiIndex inputs, the key is applied per level . Returns (DataFrame or None) The original DataFrame sorted by the labels or None if inplace=True . See Also Series.sort_index : Sort Series by the index.DataFrame.sort_values : Sort DataFrame by the value. Series.sort_values : Sort Series by the value. Examples >>> df = pd . DataFrame ([ 1 , 2 , 3 , 4 , 5 ], index = [ 100 , 29 , 234 , 1 , 150 ], ... columns = [ 'A' ]) >>> df . sort_index () A 1 4 29 2 100 1 150 5 234 3 By default, it sorts in ascending order, to sort in descending order, use ascending=False >>> df . sort_index ( ascending = False ) A 234 3 150 5 100 1 29 2 1 4 A key function can be specified which is applied to the index before sorting. For a MultiIndex this is applied to each level separately. >>> df = pd . DataFrame ({ \"a\" : [ 1 , 2 , 3 , 4 ]}, index = [ 'A' , 'b' , 'C' , 'd' ]) >>> df . sort_index ( key = lambda x : x . str . lower ()) a A 1 b 2 C 3 d 4 method","title":"pandas.core.frame.DataFrame.sort_index"},{"location":"api/pipen.channel/#pandascoreframedataframevalue_counts","text":"</> Return a Series containing the frequency of each distinct row in the Dataframe. Parameters subset (label or list of labels, optional) \u2014 Columns to use when counting unique combinations. normalize (bool, default False) \u2014 Return proportions rather than frequencies. sort (bool, default True) \u2014 Sort by frequencies when True. Sort by DataFrame column values when False. ascending (bool, default False) \u2014 Sort in ascending order. dropna (bool, default True) \u2014 Don't include counts of rows that contain NA values. .. versionadded:: 1.3.0 See Also Series.value_counts: Equivalent method on Series. Notes The returned Series will have a MultiIndex with one level per input column but an Index (non-multi) for a single label. By default, rows that contain any NA values are omitted from the result. By default, the resulting Series will be in descending order so that the first element is the most frequently-occurring row. Examples >>> df = pd . DataFrame ({ 'num_legs' : [ 2 , 4 , 4 , 6 ], ... 'num_wings' : [ 2 , 0 , 0 , 0 ]}, ... index = [ 'falcon' , 'dog' , 'cat' , 'ant' ]) >>> df num_legs num_wings falcon 2 2 dog 4 0 cat 4 0 ant 6 0 >>> df . value_counts () num_legs num_wings 4 0 2 2 2 1 6 0 1 Name : count , dtype : int64 >>> df . value_counts ( sort = False ) num_legs num_wings 2 2 1 4 0 2 6 0 1 Name : count , dtype : int64 >>> df . value_counts ( ascending = True ) num_legs num_wings 2 2 1 6 0 1 4 0 2 Name : count , dtype : int64 >>> df . value_counts ( normalize = True ) num_legs num_wings 4 0 0.50 2 2 0.25 6 0 0.25 Name : proportion , dtype : float64 With dropna set to False we can also count rows with NA values. >>> df = pd . DataFrame ({ 'first_name' : [ 'John' , 'Anne' , 'John' , 'Beth' ], ... 'middle_name' : [ 'Smith' , pd . NA , pd . NA , 'Louise' ]}) >>> df first_name middle_name 0 John Smith 1 Anne < NA > 2 John < NA > 3 Beth Louise >>> df . value_counts () first_name middle_name Beth Louise 1 John Smith 1 Name : count , dtype : int64 >>> df . value_counts ( dropna = False ) first_name middle_name Anne NaN 1 Beth Louise 1 John Smith 1 NaN 1 Name : count , dtype : int64 >>> df . value_counts ( \"first_name\" ) first_name John 2 Anne 1 Beth 1 Name : count , dtype : int64 method","title":"pandas.core.frame.DataFrame.value_counts"},{"location":"api/pipen.channel/#pandascoreframedataframenlargest","text":"</> Return the first n rows ordered by columns in descending order. Return the first n rows with the largest values in columns , in descending order. The columns that are not specified are returned as well, but not used for ordering. This method is equivalent to df.sort_values(columns, ascending=False).head(n) , but more performant. Parameters n (int) \u2014 Number of rows to return. columns (label or list of labels) \u2014 Column label(s) to order by. keep ({'first', 'last', 'all'}, default 'first') \u2014 Where there are duplicate values: first : prioritize the first occurrence(s) last : prioritize the last occurrence(s) all : keep all the ties of the smallest item even if it means selecting more than n items. Returns (DataFrame) The first n rows ordered by the given columns in descendingorder. See Also DataFrame.nsmallest : Return the first n rows ordered by columns in ascending order. DataFrame.sort_values : Sort DataFrame by the values. DataFrame.head : Return the first n rows without re-ordering. Notes This function cannot be used with all column types. For example, when specifying columns with object or category dtypes, TypeError is raised. Examples >>> df = pd . DataFrame ({ 'population' : [ 59000000 , 65000000 , 434000 , ... 434000 , 434000 , 337000 , 11300 , ... 11300 , 11300 ], ... 'GDP' : [ 1937894 , 2583560 , 12011 , 4520 , 12128 , ... 17036 , 182 , 38 , 311 ], ... 'alpha-2' : [ \"IT\" , \"FR\" , \"MT\" , \"MV\" , \"BN\" , ... \"IS\" , \"NR\" , \"TV\" , \"AI\" ]}, ... index = [ \"Italy\" , \"France\" , \"Malta\" , ... \"Maldives\" , \"Brunei\" , \"Iceland\" , ... \"Nauru\" , \"Tuvalu\" , \"Anguilla\" ]) >>> df population GDP alpha - 2 Italy 59000000 1937894 IT France 65000000 2583560 FR Malta 434000 12011 MT Maldives 434000 4520 MV Brunei 434000 12128 BN Iceland 337000 17036 IS Nauru 11300 182 NR Tuvalu 11300 38 TV Anguilla 11300 311 AI In the following example, we will use nlargest to select the three rows having the largest values in column \"population\". >>> df . nlargest ( 3 , 'population' ) population GDP alpha - 2 France 65000000 2583560 FR Italy 59000000 1937894 IT Malta 434000 12011 MT When using keep='last' , ties are resolved in reverse order: >>> df . nlargest ( 3 , 'population' , keep = 'last' ) population GDP alpha - 2 France 65000000 2583560 FR Italy 59000000 1937894 IT Brunei 434000 12128 BN When using keep='all' , the number of element kept can go beyond n if there are duplicate values for the smallest element, all the ties are kept: >>> df . nlargest ( 3 , 'population' , keep = 'all' ) population GDP alpha - 2 France 65000000 2583560 FR Italy 59000000 1937894 IT Malta 434000 12011 MT Maldives 434000 4520 MV Brunei 434000 12128 BN However, nlargest does not keep n distinct largest elements: >>> df . nlargest ( 5 , 'population' , keep = 'all' ) population GDP alpha - 2 France 65000000 2583560 FR Italy 59000000 1937894 IT Malta 434000 12011 MT Maldives 434000 4520 MV Brunei 434000 12128 BN To order by the largest values in column \"population\" and then \"GDP\", we can specify multiple columns like in the next example. >>> df . nlargest ( 3 , [ 'population' , 'GDP' ]) population GDP alpha - 2 France 65000000 2583560 FR Italy 59000000 1937894 IT Brunei 434000 12128 BN method","title":"pandas.core.frame.DataFrame.nlargest"},{"location":"api/pipen.channel/#pandascoreframedataframensmallest","text":"</> Return the first n rows ordered by columns in ascending order. Return the first n rows with the smallest values in columns , in ascending order. The columns that are not specified are returned as well, but not used for ordering. This method is equivalent to df.sort_values(columns, ascending=True).head(n) , but more performant. Parameters n (int) \u2014 Number of items to retrieve. columns (list or str) \u2014 Column name or names to order by. keep ({'first', 'last', 'all'}, default 'first') \u2014 Where there are duplicate values: first : take the first occurrence. last : take the last occurrence. all : keep all the ties of the largest item even if it means selecting more than n items. See Also DataFrame.nlargest : Return the first n rows ordered by columns in descending order. DataFrame.sort_values : Sort DataFrame by the values. DataFrame.head : Return the first n rows without re-ordering. Examples >>> df = pd . DataFrame ({ 'population' : [ 59000000 , 65000000 , 434000 , ... 434000 , 434000 , 337000 , 337000 , ... 11300 , 11300 ], ... 'GDP' : [ 1937894 , 2583560 , 12011 , 4520 , 12128 , ... 17036 , 182 , 38 , 311 ], ... 'alpha-2' : [ \"IT\" , \"FR\" , \"MT\" , \"MV\" , \"BN\" , ... \"IS\" , \"NR\" , \"TV\" , \"AI\" ]}, ... index = [ \"Italy\" , \"France\" , \"Malta\" , ... \"Maldives\" , \"Brunei\" , \"Iceland\" , ... \"Nauru\" , \"Tuvalu\" , \"Anguilla\" ]) >>> df population GDP alpha - 2 Italy 59000000 1937894 IT France 65000000 2583560 FR Malta 434000 12011 MT Maldives 434000 4520 MV Brunei 434000 12128 BN Iceland 337000 17036 IS Nauru 337000 182 NR Tuvalu 11300 38 TV Anguilla 11300 311 AI In the following example, we will use nsmallest to select the three rows having the smallest values in column \"population\". >>> df . nsmallest ( 3 , 'population' ) population GDP alpha - 2 Tuvalu 11300 38 TV Anguilla 11300 311 AI Iceland 337000 17036 IS When using keep='last' , ties are resolved in reverse order: >>> df . nsmallest ( 3 , 'population' , keep = 'last' ) population GDP alpha - 2 Anguilla 11300 311 AI Tuvalu 11300 38 TV Nauru 337000 182 NR When using keep='all' , the number of element kept can go beyond n if there are duplicate values for the largest element, all the ties are kept. >>> df . nsmallest ( 3 , 'population' , keep = 'all' ) population GDP alpha - 2 Tuvalu 11300 38 TV Anguilla 11300 311 AI Iceland 337000 17036 IS Nauru 337000 182 NR However, nsmallest does not keep n distinct smallest elements: >>> df . nsmallest ( 4 , 'population' , keep = 'all' ) population GDP alpha - 2 Tuvalu 11300 38 TV Anguilla 11300 311 AI Iceland 337000 17036 IS Nauru 337000 182 NR To order by the smallest values in column \"population\" and then \"GDP\", we can specify multiple columns like in the next example. >>> df . nsmallest ( 3 , [ 'population' , 'GDP' ]) population GDP alpha - 2 Tuvalu 11300 38 TV Anguilla 11300 311 AI Nauru 337000 182 NR method","title":"pandas.core.frame.DataFrame.nsmallest"},{"location":"api/pipen.channel/#pandascoreframedataframeswaplevel","text":"</> Swap levels i and j in a :class: MultiIndex . Default is to swap the two innermost levels of the index. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to swap levels on. 0 or 'index' for row-wise, 1 or'columns' for column-wise. Returns (DataFrame) DataFrame with levels swapped in MultiIndex. Examples >>> df = pd . DataFrame ( ... { \"Grade\" : [ \"A\" , \"B\" , \"A\" , \"C\" ]}, ... index = [ ... [ \"Final exam\" , \"Final exam\" , \"Coursework\" , \"Coursework\" ], ... [ \"History\" , \"Geography\" , \"History\" , \"Geography\" ], ... [ \"January\" , \"February\" , \"March\" , \"April\" ], ... ], ... ) >>> df Grade Final exam History January A Geography February B Coursework History March A Geography April C In the following example, we will swap the levels of the indices. Here, we will swap the levels column-wise, but levels can be swapped row-wise in a similar manner. Note that column-wise is the default behaviour. By not supplying any arguments for i and j, we swap the last and second to last indices. >>> df . swaplevel () Grade Final exam January History A February Geography B Coursework March History A April Geography C By supplying one argument, we can choose which index to swap the last index with. We can for example swap the first index with the last one as follows. >>> df . swaplevel ( 0 ) Grade January History Final exam A February Geography Final exam B March History Coursework A April Geography Coursework C We can also define explicitly which indices we want to swap by supplying values for both i and j. Here, we for example swap the first and second indices. >>> df . swaplevel ( 0 , 1 ) Grade History Final exam January A Geography Final exam February B History Coursework March A Geography Coursework April C method","title":"pandas.core.frame.DataFrame.swaplevel"},{"location":"api/pipen.channel/#pandascoreframedataframereorder_levels","text":"</> Rearrange index levels using input order. May not drop or duplicate levels. Parameters order (list of int or list of str) \u2014 List representing new level order. Reference level by number(position) or by key (label). axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Where to reorder levels. Examples >>> data = { ... \"class\" : [ \"Mammals\" , \"Mammals\" , \"Reptiles\" ], ... \"diet\" : [ \"Omnivore\" , \"Carnivore\" , \"Carnivore\" ], ... \"species\" : [ \"Humans\" , \"Dogs\" , \"Snakes\" ], ... } >>> df = pd . DataFrame ( data , columns = [ \"class\" , \"diet\" , \"species\" ]) >>> df = df . set_index ([ \"class\" , \"diet\" ]) >>> df species class diet Mammals Omnivore Humans Carnivore Dogs Reptiles Carnivore Snakes Let's reorder the levels of the index: >>> df . reorder_levels ([ \"diet\" , \"class\" ]) species diet class Omnivore Mammals Humans Carnivore Mammals Dogs Reptiles Snakes method","title":"pandas.core.frame.DataFrame.reorder_levels"},{"location":"api/pipen.channel/#pandascoreframedataframeeq","text":"</> Get Equal to of dataframe and other, element-wise (binary operator eq ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , != , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison. Parameters other (scalar, sequence, Series, or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}, default 'columns') \u2014 Whether to compare by the index (0 or 'index') or columns(1 or 'columns'). level (int or label) \u2014 Broadcast across a level, matching Index values on the passedMultiIndex level. Returns (DataFrame of bool) Result of the comparison. See Also DataFrame.eq : Compare DataFrames for equality elementwise.DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ). Examples >>> df = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 ], ... 'revenue' : [ 100 , 250 , 300 ]}, ... index = [ 'A' , 'B' , 'C' ]) >>> df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: >>> df == 100 cost revenue A False True B False False C True False >>> df . eq ( 100 ) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: >>> df != pd . Series ([ 100 , 250 ], index = [ \"cost\" , \"revenue\" ]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: >>> df . ne ( pd . Series ([ 100 , 300 ], index = [ \"A\" , \"D\" ]), axis = 'index' ) cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : >>> df == [ 250 , 100 ] cost revenue A True True B False False C False False Use the method to control the axis: >>> df . eq ([ 250 , 250 , 100 ], axis = 'index' ) cost revenue A True False B False True C True False Compare to a DataFrame of different shape. >>> other = pd . DataFrame ({ 'revenue' : [ 300 , 250 , 100 , 150 ]}, ... index = [ 'A' , 'B' , 'C' , 'D' ]) >>> other revenue A 300 B 250 C 100 D 150 >>> df . gt ( other ) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 , 150 , 300 , 220 ], ... 'revenue' : [ 100 , 250 , 300 , 200 , 175 , 225 ]}, ... index = [[ 'Q1' , 'Q1' , 'Q1' , 'Q2' , 'Q2' , 'Q2' ], ... [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ]]) >>> df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 >>> df . le ( df_multindex , level = 1 ) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False method","title":"pandas.core.frame.DataFrame.eq"},{"location":"api/pipen.channel/#pandascoreframedataframene","text":"</> Get Not equal to of dataframe and other, element-wise (binary operator ne ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , != , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison. Parameters other (scalar, sequence, Series, or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}, default 'columns') \u2014 Whether to compare by the index (0 or 'index') or columns(1 or 'columns'). level (int or label) \u2014 Broadcast across a level, matching Index values on the passedMultiIndex level. Returns (DataFrame of bool) Result of the comparison. See Also DataFrame.eq : Compare DataFrames for equality elementwise.DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ). Examples >>> df = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 ], ... 'revenue' : [ 100 , 250 , 300 ]}, ... index = [ 'A' , 'B' , 'C' ]) >>> df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: >>> df == 100 cost revenue A False True B False False C True False >>> df . eq ( 100 ) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: >>> df != pd . Series ([ 100 , 250 ], index = [ \"cost\" , \"revenue\" ]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: >>> df . ne ( pd . Series ([ 100 , 300 ], index = [ \"A\" , \"D\" ]), axis = 'index' ) cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : >>> df == [ 250 , 100 ] cost revenue A True True B False False C False False Use the method to control the axis: >>> df . eq ([ 250 , 250 , 100 ], axis = 'index' ) cost revenue A True False B False True C True False Compare to a DataFrame of different shape. >>> other = pd . DataFrame ({ 'revenue' : [ 300 , 250 , 100 , 150 ]}, ... index = [ 'A' , 'B' , 'C' , 'D' ]) >>> other revenue A 300 B 250 C 100 D 150 >>> df . gt ( other ) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 , 150 , 300 , 220 ], ... 'revenue' : [ 100 , 250 , 300 , 200 , 175 , 225 ]}, ... index = [[ 'Q1' , 'Q1' , 'Q1' , 'Q2' , 'Q2' , 'Q2' ], ... [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ]]) >>> df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 >>> df . le ( df_multindex , level = 1 ) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False method","title":"pandas.core.frame.DataFrame.ne"},{"location":"api/pipen.channel/#pandascoreframedataframele","text":"</> Get Less than or equal to of dataframe and other, element-wise (binary operator le ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , != , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison. Parameters other (scalar, sequence, Series, or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}, default 'columns') \u2014 Whether to compare by the index (0 or 'index') or columns(1 or 'columns'). level (int or label) \u2014 Broadcast across a level, matching Index values on the passedMultiIndex level. Returns (DataFrame of bool) Result of the comparison. See Also DataFrame.eq : Compare DataFrames for equality elementwise.DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ). Examples >>> df = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 ], ... 'revenue' : [ 100 , 250 , 300 ]}, ... index = [ 'A' , 'B' , 'C' ]) >>> df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: >>> df == 100 cost revenue A False True B False False C True False >>> df . eq ( 100 ) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: >>> df != pd . Series ([ 100 , 250 ], index = [ \"cost\" , \"revenue\" ]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: >>> df . ne ( pd . Series ([ 100 , 300 ], index = [ \"A\" , \"D\" ]), axis = 'index' ) cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : >>> df == [ 250 , 100 ] cost revenue A True True B False False C False False Use the method to control the axis: >>> df . eq ([ 250 , 250 , 100 ], axis = 'index' ) cost revenue A True False B False True C True False Compare to a DataFrame of different shape. >>> other = pd . DataFrame ({ 'revenue' : [ 300 , 250 , 100 , 150 ]}, ... index = [ 'A' , 'B' , 'C' , 'D' ]) >>> other revenue A 300 B 250 C 100 D 150 >>> df . gt ( other ) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 , 150 , 300 , 220 ], ... 'revenue' : [ 100 , 250 , 300 , 200 , 175 , 225 ]}, ... index = [[ 'Q1' , 'Q1' , 'Q1' , 'Q2' , 'Q2' , 'Q2' ], ... [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ]]) >>> df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 >>> df . le ( df_multindex , level = 1 ) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False method","title":"pandas.core.frame.DataFrame.le"},{"location":"api/pipen.channel/#pandascoreframedataframelt","text":"</> Get Less than of dataframe and other, element-wise (binary operator lt ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , != , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison. Parameters other (scalar, sequence, Series, or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}, default 'columns') \u2014 Whether to compare by the index (0 or 'index') or columns(1 or 'columns'). level (int or label) \u2014 Broadcast across a level, matching Index values on the passedMultiIndex level. Returns (DataFrame of bool) Result of the comparison. See Also DataFrame.eq : Compare DataFrames for equality elementwise.DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ). Examples >>> df = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 ], ... 'revenue' : [ 100 , 250 , 300 ]}, ... index = [ 'A' , 'B' , 'C' ]) >>> df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: >>> df == 100 cost revenue A False True B False False C True False >>> df . eq ( 100 ) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: >>> df != pd . Series ([ 100 , 250 ], index = [ \"cost\" , \"revenue\" ]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: >>> df . ne ( pd . Series ([ 100 , 300 ], index = [ \"A\" , \"D\" ]), axis = 'index' ) cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : >>> df == [ 250 , 100 ] cost revenue A True True B False False C False False Use the method to control the axis: >>> df . eq ([ 250 , 250 , 100 ], axis = 'index' ) cost revenue A True False B False True C True False Compare to a DataFrame of different shape. >>> other = pd . DataFrame ({ 'revenue' : [ 300 , 250 , 100 , 150 ]}, ... index = [ 'A' , 'B' , 'C' , 'D' ]) >>> other revenue A 300 B 250 C 100 D 150 >>> df . gt ( other ) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 , 150 , 300 , 220 ], ... 'revenue' : [ 100 , 250 , 300 , 200 , 175 , 225 ]}, ... index = [[ 'Q1' , 'Q1' , 'Q1' , 'Q2' , 'Q2' , 'Q2' ], ... [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ]]) >>> df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 >>> df . le ( df_multindex , level = 1 ) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False method","title":"pandas.core.frame.DataFrame.lt"},{"location":"api/pipen.channel/#pandascoreframedataframege","text":"</> Get Greater than or equal to of dataframe and other, element-wise (binary operator ge ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , != , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison. Parameters other (scalar, sequence, Series, or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}, default 'columns') \u2014 Whether to compare by the index (0 or 'index') or columns(1 or 'columns'). level (int or label) \u2014 Broadcast across a level, matching Index values on the passedMultiIndex level. Returns (DataFrame of bool) Result of the comparison. See Also DataFrame.eq : Compare DataFrames for equality elementwise.DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ). Examples >>> df = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 ], ... 'revenue' : [ 100 , 250 , 300 ]}, ... index = [ 'A' , 'B' , 'C' ]) >>> df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: >>> df == 100 cost revenue A False True B False False C True False >>> df . eq ( 100 ) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: >>> df != pd . Series ([ 100 , 250 ], index = [ \"cost\" , \"revenue\" ]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: >>> df . ne ( pd . Series ([ 100 , 300 ], index = [ \"A\" , \"D\" ]), axis = 'index' ) cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : >>> df == [ 250 , 100 ] cost revenue A True True B False False C False False Use the method to control the axis: >>> df . eq ([ 250 , 250 , 100 ], axis = 'index' ) cost revenue A True False B False True C True False Compare to a DataFrame of different shape. >>> other = pd . DataFrame ({ 'revenue' : [ 300 , 250 , 100 , 150 ]}, ... index = [ 'A' , 'B' , 'C' , 'D' ]) >>> other revenue A 300 B 250 C 100 D 150 >>> df . gt ( other ) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 , 150 , 300 , 220 ], ... 'revenue' : [ 100 , 250 , 300 , 200 , 175 , 225 ]}, ... index = [[ 'Q1' , 'Q1' , 'Q1' , 'Q2' , 'Q2' , 'Q2' ], ... [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ]]) >>> df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 >>> df . le ( df_multindex , level = 1 ) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False method","title":"pandas.core.frame.DataFrame.ge"},{"location":"api/pipen.channel/#pandascoreframedataframegt","text":"</> Get Greater than of dataframe and other, element-wise (binary operator gt ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , != , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison. Parameters other (scalar, sequence, Series, or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}, default 'columns') \u2014 Whether to compare by the index (0 or 'index') or columns(1 or 'columns'). level (int or label) \u2014 Broadcast across a level, matching Index values on the passedMultiIndex level. Returns (DataFrame of bool) Result of the comparison. See Also DataFrame.eq : Compare DataFrames for equality elementwise.DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ). Examples >>> df = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 ], ... 'revenue' : [ 100 , 250 , 300 ]}, ... index = [ 'A' , 'B' , 'C' ]) >>> df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: >>> df == 100 cost revenue A False True B False False C True False >>> df . eq ( 100 ) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: >>> df != pd . Series ([ 100 , 250 ], index = [ \"cost\" , \"revenue\" ]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: >>> df . ne ( pd . Series ([ 100 , 300 ], index = [ \"A\" , \"D\" ]), axis = 'index' ) cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : >>> df == [ 250 , 100 ] cost revenue A True True B False False C False False Use the method to control the axis: >>> df . eq ([ 250 , 250 , 100 ], axis = 'index' ) cost revenue A True False B False True C True False Compare to a DataFrame of different shape. >>> other = pd . DataFrame ({ 'revenue' : [ 300 , 250 , 100 , 150 ]}, ... index = [ 'A' , 'B' , 'C' , 'D' ]) >>> other revenue A 300 B 250 C 100 D 150 >>> df . gt ( other ) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'cost' : [ 250 , 150 , 100 , 150 , 300 , 220 ], ... 'revenue' : [ 100 , 250 , 300 , 200 , 175 , 225 ]}, ... index = [[ 'Q1' , 'Q1' , 'Q1' , 'Q2' , 'Q2' , 'Q2' ], ... [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ]]) >>> df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 >>> df . le ( df_multindex , level = 1 ) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False method","title":"pandas.core.frame.DataFrame.gt"},{"location":"api/pipen.channel/#pandascoreframedataframeadd","text":"</> Get Addition of dataframe and other, element-wise (binary operator add ). Equivalent to dataframe + other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, radd . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method","title":"pandas.core.frame.DataFrame.add"},{"location":"api/pipen.channel/#pandascoreframedataframeradd","text":"</> Get Addition of dataframe and other, element-wise (binary operator radd ). Equivalent to other + dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, add . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method","title":"pandas.core.frame.DataFrame.radd"},{"location":"api/pipen.channel/#pandascoreframedataframesub","text":"</> Get Subtraction of dataframe and other, element-wise (binary operator sub ). Equivalent to dataframe - other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rsub . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method","title":"pandas.core.frame.DataFrame.sub"},{"location":"api/pipen.channel/#pandascoreframedataframersub","text":"</> Get Subtraction of dataframe and other, element-wise (binary operator rsub ). Equivalent to other - dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, sub . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method","title":"pandas.core.frame.DataFrame.rsub"},{"location":"api/pipen.channel/#pandascoreframedataframemul","text":"</> Get Multiplication of dataframe and other, element-wise (binary operator mul ). Equivalent to dataframe * other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rmul . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method","title":"pandas.core.frame.DataFrame.mul"},{"location":"api/pipen.channel/#pandascoreframedataframermul","text":"</> Get Multiplication of dataframe and other, element-wise (binary operator rmul ). Equivalent to other * dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, mul . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method","title":"pandas.core.frame.DataFrame.rmul"},{"location":"api/pipen.channel/#pandascoreframedataframetruediv","text":"</> Get Floating division of dataframe and other, element-wise (binary operator truediv ). Equivalent to dataframe / other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rtruediv . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method","title":"pandas.core.frame.DataFrame.truediv"},{"location":"api/pipen.channel/#pandascoreframedataframertruediv","text":"</> Get Floating division of dataframe and other, element-wise (binary operator rtruediv ). Equivalent to other / dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, truediv . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method","title":"pandas.core.frame.DataFrame.rtruediv"},{"location":"api/pipen.channel/#pandascoreframedataframefloordiv","text":"</> Get Integer division of dataframe and other, element-wise (binary operator floordiv ). Equivalent to dataframe // other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rfloordiv . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method","title":"pandas.core.frame.DataFrame.floordiv"},{"location":"api/pipen.channel/#pandascoreframedataframerfloordiv","text":"</> Get Integer division of dataframe and other, element-wise (binary operator rfloordiv ). Equivalent to other // dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, floordiv . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method","title":"pandas.core.frame.DataFrame.rfloordiv"},{"location":"api/pipen.channel/#pandascoreframedataframemod","text":"</> Get Modulo of dataframe and other, element-wise (binary operator mod ). Equivalent to dataframe % other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rmod . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method","title":"pandas.core.frame.DataFrame.mod"},{"location":"api/pipen.channel/#pandascoreframedataframermod","text":"</> Get Modulo of dataframe and other, element-wise (binary operator rmod ). Equivalent to other % dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, mod . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method","title":"pandas.core.frame.DataFrame.rmod"},{"location":"api/pipen.channel/#pandascoreframedataframepow","text":"</> Get Exponential power of dataframe and other, element-wise (binary operator pow ). Equivalent to dataframe ** other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rpow . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method","title":"pandas.core.frame.DataFrame.pow"},{"location":"api/pipen.channel/#pandascoreframedataframerpow","text":"</> Get Exponential power of dataframe and other, element-wise (binary operator rpow ). Equivalent to other ** dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, pow . Among flexible wrappers ( add , sub , mul , div , floordiv , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other (scalar, sequence, Series, dict or DataFrame) \u2014 Any single or multiple element data structure, or list-like object. axis ({0 or 'index', 1 or 'columns'}) \u2014 Whether to compare by the index (0 or 'index') or columns.(1 or 'columns'). For Series input, axis to match Series index on. level (int or label) \u2014 Broadcast across a level, matching Index values on thepassed MultiIndex level. fill_value (float or None, default None) \u2014 Fill existing missing (NaN) values, and any new element needed forsuccessful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns (DataFrame) Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames.DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples >>> df = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ], ... 'degrees' : [ 360 , 180 , 360 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. >>> df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 >>> df . add ( 1 ) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. >>> df . div ( 10 ) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 >>> df . rdiv ( 10 ) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. >>> df - [ 1 , 2 ] angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ([ 1 , 2 ], axis = 'columns' ) angles degrees circle - 1 358 triangle 2 178 rectangle 3 358 >>> df . sub ( pd . Series ([ 1 , 1 , 1 ], index = [ 'circle' , 'triangle' , 'rectangle' ]), ... axis = 'index' ) angles degrees circle - 1 359 triangle 2 179 rectangle 3 359 Multiply a dictionary by axis. >>> df . mul ({ 'angles' : 0 , 'degrees' : 2 }) angles degrees circle 0 720 triangle 0 360 rectangle 0 720 >>> df . mul ({ 'circle' : 0 , 'triangle' : 2 , 'rectangle' : 3 }, axis = 'index' ) angles degrees circle 0 0 triangle 6 360 rectangle 12 1080 Multiply a DataFrame of different shape with operator version. >>> other = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 ]}, ... index = [ 'circle' , 'triangle' , 'rectangle' ]) >>> other angles circle 0 triangle 3 rectangle 4 >>> df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN >>> df . mul ( other , fill_value = 0 ) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. >>> df_multindex = pd . DataFrame ({ 'angles' : [ 0 , 3 , 4 , 4 , 5 , 6 ], ... 'degrees' : [ 360 , 180 , 360 , 360 , 540 , 720 ]}, ... index = [[ 'A' , 'A' , 'A' , 'B' , 'B' , 'B' ], ... [ 'circle' , 'triangle' , 'rectangle' , ... 'square' , 'pentagon' , 'hexagon' ]]) >>> df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 >>> df . div ( df_multindex , level = 1 , fill_value = 0 ) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 method","title":"pandas.core.frame.DataFrame.rpow"},{"location":"api/pipen.channel/#pandascoreframedataframecompare","text":"</> Compare to another DataFrame and show the differences. Parameters other (DataFrame) \u2014 Object to compare with. align_axis ({0 or 'index', 1 or 'columns'}, default 1) \u2014 Determine which axis to align the comparison on. 0, or 'index' : Resulting differences are stacked vertically with rows drawn alternately from self and other. 1, or 'columns' : Resulting differences are aligned horizontally with columns drawn alternately from self and other. keep_shape (bool, default False) \u2014 If true, all rows and columns are kept.Otherwise, only the ones with different values are kept. keep_equal (bool, default False) \u2014 If true, the result keeps values that are equal.Otherwise, equal values are shown as NaNs. result_names (tuple, default ('self', 'other')) \u2014 Set the dataframes names in the comparison. .. versionadded:: 1.5.0 Returns (DataFrame) DataFrame that shows the differences stacked side by side. The resulting index will be a MultiIndex with 'self' and 'other' stacked alternately at the inner level. Raises ValueError \u2014 When the two DataFrames don't have identical labels or shape. See Also Series.compare : Compare with another Series and show differences.DataFrame.equals : Test whether two objects contain the same elements. Notes Matching NaNs will not appear as a difference. Can only compare identically-labeled (i.e. same shape, identical row and column labels) DataFrames Examples >>> df = pd . DataFrame ( ... { ... \"col1\" : [ \"a\" , \"a\" , \"b\" , \"b\" , \"a\" ], ... \"col2\" : [ 1.0 , 2.0 , 3.0 , np . nan , 5.0 ], ... \"col3\" : [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ] ... }, ... columns = [ \"col1\" , \"col2\" , \"col3\" ], ... ) >>> df col1 col2 col3 0 a 1.0 1.0 1 a 2.0 2.0 2 b 3.0 3.0 3 b NaN 4.0 4 a 5.0 5.0 >>> df2 = df . copy () >>> df2 . loc [ 0 , 'col1' ] = 'c' >>> df2 . loc [ 2 , 'col3' ] = 4.0 >>> df2 col1 col2 col3 0 c 1.0 1.0 1 a 2.0 2.0 2 b 3.0 4.0 3 b NaN 4.0 4 a 5.0 5.0 Align the differences on columns >>> df . compare ( df2 ) col1 col3 self other self other 0 a c NaN NaN 2 NaN NaN 3.0 4.0 Assign result_names >>> df . compare ( df2 , result_names = ( \"left\" , \"right\" )) col1 col3 left right left right 0 a c NaN NaN 2 NaN NaN 3.0 4.0 Stack the differences on rows >>> df . compare ( df2 , align_axis = 0 ) col1 col3 0 self a NaN other c NaN 2 self NaN 3.0 other NaN 4.0 Keep the equal values >>> df . compare ( df2 , keep_equal = True ) col1 col3 self other self other 0 a c 1.0 1.0 2 b b 3.0 4.0 Keep all original rows and columns >>> df . compare ( df2 , keep_shape = True ) col1 col2 col3 self other self other self other 0 a c NaN NaN NaN NaN 1 NaN NaN NaN NaN NaN NaN 2 NaN NaN NaN NaN 3.0 4.0 3 NaN NaN NaN NaN NaN NaN 4 NaN NaN NaN NaN NaN NaN Keep all original rows and columns and also all original values >>> df . compare ( df2 , keep_shape = True , keep_equal = True ) col1 col2 col3 self other self other self other 0 a c 1.0 1.0 1.0 1.0 1 a a 2.0 2.0 2.0 2.0 2 b b 3.0 3.0 3.0 4.0 3 b b NaN NaN 4.0 4.0 4 a a 5.0 5.0 5.0 5.0 method","title":"pandas.core.frame.DataFrame.compare"},{"location":"api/pipen.channel/#pandascoreframedataframecombine","text":"</> Perform column-wise combine with another DataFrame. Combines a DataFrame with other DataFrame using func to element-wise combine columns. The row and column indexes of the resulting DataFrame will be the union of the two. Parameters other (DataFrame) \u2014 The DataFrame to merge column-wise. func (function) \u2014 Function that takes two series as inputs and return a Series or ascalar. Used to merge the two dataframes column by columns. fill_value (scalar value, default None) \u2014 The value to fill NaNs with prior to passing any column to themerge func. overwrite (bool, default True) \u2014 If True, columns in self that do not exist in other will beoverwritten with NaNs. Returns (DataFrame) Combination of the provided DataFrames. See Also DataFrame.combinefirst : Combine two DataFrame objects and default to non-null values in frame calling the method. Examples Combine using a simple function that chooses the smaller column. >>> df1 = pd . DataFrame ({ 'A' : [ 0 , 0 ], 'B' : [ 4 , 4 ]}) >>> df2 = pd . DataFrame ({ 'A' : [ 1 , 1 ], 'B' : [ 3 , 3 ]}) >>> take_smaller = lambda s1 , s2 : s1 if s1 . sum () < s2 . sum () else s2 >>> df1 . combine ( df2 , take_smaller ) A B 0 0 3 1 0 3 Example using a true element-wise combine function. >>> df1 = pd . DataFrame ({ 'A' : [ 5 , 0 ], 'B' : [ 2 , 4 ]}) >>> df2 = pd . DataFrame ({ 'A' : [ 1 , 1 ], 'B' : [ 3 , 3 ]}) >>> df1 . combine ( df2 , np . minimum ) A B 0 1 2 1 0 3 Using fill_value fills Nones prior to passing the column to the merge function. >>> df1 = pd . DataFrame ({ 'A' : [ 0 , 0 ], 'B' : [ None , 4 ]}) >>> df2 = pd . DataFrame ({ 'A' : [ 1 , 1 ], 'B' : [ 3 , 3 ]}) >>> df1 . combine ( df2 , take_smaller , fill_value =- 5 ) A B 0 0 - 5.0 1 0 4.0 However, if the same element in both dataframes is None, that None is preserved >>> df1 = pd . DataFrame ({ 'A' : [ 0 , 0 ], 'B' : [ None , 4 ]}) >>> df2 = pd . DataFrame ({ 'A' : [ 1 , 1 ], 'B' : [ None , 3 ]}) >>> df1 . combine ( df2 , take_smaller , fill_value =- 5 ) A B 0 0 - 5.0 1 0 3.0 Example that demonstrates the use of overwrite and behavior when the axis differ between the dataframes. >>> df1 = pd . DataFrame ({ 'A' : [ 0 , 0 ], 'B' : [ 4 , 4 ]}) >>> df2 = pd . DataFrame ({ 'B' : [ 3 , 3 ], 'C' : [ - 10 , 1 ], }, index = [ 1 , 2 ]) >>> df1 . combine ( df2 , take_smaller ) A B C 0 NaN NaN NaN 1 NaN 3.0 - 10.0 2 NaN 3.0 1.0 >>> df1 . combine ( df2 , take_smaller , overwrite = False ) A B C 0 0.0 NaN NaN 1 0.0 3.0 - 10.0 2 NaN 3.0 1.0 Demonstrating the preference of the passed in dataframe. >>> df2 = pd . DataFrame ({ 'B' : [ 3 , 3 ], 'C' : [ 1 , 1 ], }, index = [ 1 , 2 ]) >>> df2 . combine ( df1 , take_smaller ) A B C 0 0.0 NaN NaN 1 0.0 3.0 NaN 2 NaN 3.0 NaN >>> df2 . combine ( df1 , take_smaller , overwrite = False ) A B C 0 0.0 NaN NaN 1 0.0 3.0 1.0 2 NaN 3.0 1.0 method","title":"pandas.core.frame.DataFrame.combine"},{"location":"api/pipen.channel/#pandascoreframedataframecombine_first","text":"</> Update null elements with value in the same location in other . Combine two DataFrame objects by filling null values in one DataFrame with non-null values from other DataFrame. The row and column indexes of the resulting DataFrame will be the union of the two. The resulting dataframe contains the 'first' dataframe values and overrides the second one values where both first.loc[index, col] and second.loc[index, col] are not missing values, upon calling first.combine_first(second). Parameters other (DataFrame) \u2014 Provided DataFrame to use to fill null values. Returns (DataFrame) The result of combining the provided DataFrame with the other object. See Also DataFrame.combine : Perform series-wise operation on two DataFrames using a given function. Examples >>> df1 = pd . DataFrame ({ 'A' : [ None , 0 ], 'B' : [ None , 4 ]}) >>> df2 = pd . DataFrame ({ 'A' : [ 1 , 1 ], 'B' : [ 3 , 3 ]}) >>> df1 . combine_first ( df2 ) A B 0 1.0 3.0 1 0.0 4.0 Null values still persist if the location of that null value does not exist in other >>> df1 = pd . DataFrame ({ 'A' : [ None , 0 ], 'B' : [ 4 , None ]}) >>> df2 = pd . DataFrame ({ 'B' : [ 3 , 3 ], 'C' : [ 1 , 1 ]}, index = [ 1 , 2 ]) >>> df1 . combine_first ( df2 ) A B C 0 NaN 4.0 NaN 1 0.0 3.0 1.0 2 NaN 3.0 1.0 method","title":"pandas.core.frame.DataFrame.combine_first"},{"location":"api/pipen.channel/#pandascoreframedataframeupdate","text":"</> Modify in place using non-NA values from another DataFrame. Aligns on indices. There is no return value. Parameters other (DataFrame, or object coercible into a DataFrame) \u2014 Should have at least one matching index/column labelwith the original DataFrame. If a Series is passed, its name attribute must be set, and that will be used as the column name to align with the original DataFrame. join ({'left'}, default 'left') \u2014 Only left join is implemented, keeping the index and columns of theoriginal object. overwrite (bool, default True) \u2014 How to handle non-NA values for overlapping keys: True: overwrite original DataFrame's values with values from other . False: only update values that are NA in the original DataFrame. filter_func (callable(1d-array) -> bool 1d-array, optional) \u2014 Can choose to replace values other than NA. Return True for valuesthat should be updated. errors ({'raise', 'ignore'}, default 'ignore') \u2014 If 'raise', will raise a ValueError if the DataFrame and other both contain non-NA data in the same place. Returns (None) This method directly changes calling object. Raises NotImplementedError \u2014 If join != 'left' ValueError \u2014 When errors='raise' and there's overlapping non-NA data. When errors is not either 'ignore' or 'raise' See Also dict.update : Similar method for dictionaries.DataFrame.merge : For column(s)-on-column(s) operations. Examples >>> df = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 ], ... 'B' : [ 400 , 500 , 600 ]}) >>> new_df = pd . DataFrame ({ 'B' : [ 4 , 5 , 6 ], ... 'C' : [ 7 , 8 , 9 ]}) >>> df . update ( new_df ) >>> df A B 0 1 4 1 2 5 2 3 6 The DataFrame's length does not increase as a result of the update, only values at matching index/column labels are updated. >>> df = pd . DataFrame ({ 'A' : [ 'a' , 'b' , 'c' ], ... 'B' : [ 'x' , 'y' , 'z' ]}) >>> new_df = pd . DataFrame ({ 'B' : [ 'd' , 'e' , 'f' , 'g' , 'h' , 'i' ]}) >>> df . update ( new_df ) >>> df A B 0 a d 1 b e 2 c f >>> df = pd . DataFrame ({ 'A' : [ 'a' , 'b' , 'c' ], ... 'B' : [ 'x' , 'y' , 'z' ]}) >>> new_df = pd . DataFrame ({ 'B' : [ 'd' , 'f' ]}, index = [ 0 , 2 ]) >>> df . update ( new_df ) >>> df A B 0 a d 1 b y 2 c f For Series, its name attribute must be set. >>> df = pd . DataFrame ({ 'A' : [ 'a' , 'b' , 'c' ], ... 'B' : [ 'x' , 'y' , 'z' ]}) >>> new_column = pd . Series ([ 'd' , 'e' , 'f' ], name = 'B' ) >>> df . update ( new_column ) >>> df A B 0 a d 1 b e 2 c f If other contains NaNs the corresponding values are not updated in the original dataframe. >>> df = pd . DataFrame ({ 'A' : [ 1 , 2 , 3 ], ... 'B' : [ 400. , 500. , 600. ]}) >>> new_df = pd . DataFrame ({ 'B' : [ 4 , np . nan , 6 ]}) >>> df . update ( new_df ) >>> df A B 0 1 4.0 1 2 500.0 2 3 6.0 method","title":"pandas.core.frame.DataFrame.update"},{"location":"api/pipen.channel/#pandascoreframedataframegroupby","text":"</> Group DataFrame using a mapper or by a Series of columns. A groupby operation involves some combination of splitting the object, applying a function, and combining the results. This can be used to group large amounts of data and compute operations on these groups. Parameters by (mapping, function, label, pd.Grouper or list of such) \u2014 Used to determine the groups for the groupby.If by is a function, it's called on each value of the object's index. If a dict or Series is passed, the Series or dict VALUES will be used to determine the groups (the Series' values are first aligned; see .align() method). If a list or ndarray of length equal to the selected axis is passed (see the groupby user guide <https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#splitting-an-object-into-groups> _), the values are used as-is to determine the groups. A label or list of labels may be passed to group by the columns in self . Notice that a tuple is interpreted as a (single) key. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Split along rows (0) or columns (1). For Series this parameteris unused and defaults to 0. .. deprecated:: 2.1.0 Will be removed and behave like axis=0 in a future version. For ``axis=1``, do ``frame.T.groupby(...)`` instead. level (int, level name, or sequence of such, default None) \u2014 If the axis is a MultiIndex (hierarchical), group by a particularlevel or levels. Do not specify both by and level . as_index (bool, default True) \u2014 Return object with group labels as theindex. Only relevant for DataFrame input. as_index=False is effectively \"SQL-style\" grouped output. This argument has no effect on filtrations (see the filtrations in the user guide <https://pandas.pydata.org/docs/dev/user_guide/groupby.html#filtration> ), such as head() , tail() , nth() and in transformations (see the transformations in the user guide <https://pandas.pydata.org/docs/dev/user_guide/groupby.html#transformation> ). sort (bool, default True) \u2014 Sort group keys. Get better performance by turning this off.Note this does not influence the order of observations within each group. Groupby preserves the order of rows within each group. If False, the groups will appear in the same order as they did in the original DataFrame. This argument has no effect on filtrations (see the filtrations in the user guide <https://pandas.pydata.org/docs/dev/user_guide/groupby.html#filtration> ), such as head() , tail() , nth() and in transformations (see the transformations in the user guide <https://pandas.pydata.org/docs/dev/user_guide/groupby.html#transformation> ). .. versionchanged:: 2.0.0 Specifying ``sort=False`` with an ordered categorical grouper will no longer sort the values. group_keys (bool, default True) \u2014 When calling apply and the by argument produces a like-indexed(i.e. :ref: a transform <groupby.transform> ) result, add group keys to index to identify pieces. By default group keys are not included when the result's index (and column) labels match the inputs, and are included otherwise. .. versionchanged:: 1.5.0 Warns that group_keys will no longer be ignored when the result from apply is a like-indexed Series or DataFrame. Specify group_keys explicitly to include the group keys or not. .. versionchanged:: 2.0.0 group_keys now defaults to True . observed (bool, default False) \u2014 This only applies if any of the groupers are Categoricals.If True: only show observed values for categorical groupers. If False: show all values for categorical groupers. .. deprecated:: 2.1.0 The default value will change to True in a future version of pandas. dropna (bool, default True) \u2014 If True, and if group keys contain NA values, NA values togetherwith row/column will be dropped. If False, NA values will also be treated as the key in groups. Returns (pandas.api.typing.DataFrameGroupBy) Returns a groupby object that contains information about the groups. See Also resample : Convenience method for frequency conversion and resampling of time series. Notes See the user guide <https://pandas.pydata.org/pandas-docs/stable/groupby.html> __ for more detailed usage and examples, including splitting an object into groups, iterating through groups, selecting a group, aggregation, and more. Examples >>> df = pd . DataFrame ({ 'Animal' : [ 'Falcon' , 'Falcon' , ... 'Parrot' , 'Parrot' ], ... 'Max Speed' : [ 380. , 370. , 24. , 26. ]}) >>> df Animal Max Speed 0 Falcon 380.0 1 Falcon 370.0 2 Parrot 24.0 3 Parrot 26.0 >>> df . groupby ([ 'Animal' ]) . mean () Max Speed Animal Falcon 375.0 Parrot 25.0 Hierarchical Indexes We can groupby different levels of a hierarchical index using the level parameter: >>> arrays = [[ 'Falcon' , 'Falcon' , 'Parrot' , 'Parrot' ], ... [ 'Captive' , 'Wild' , 'Captive' , 'Wild' ]] >>> index = pd . MultiIndex . from_arrays ( arrays , names = ( 'Animal' , 'Type' )) >>> df = pd . DataFrame ({ 'Max Speed' : [ 390. , 350. , 30. , 20. ]}, ... index = index ) >>> df Max Speed Animal Type Falcon Captive 390.0 Wild 350.0 Parrot Captive 30.0 Wild 20.0 >>> df . groupby ( level = 0 ) . mean () Max Speed Animal Falcon 370.0 Parrot 25.0 >>> df . groupby ( level = \"Type\" ) . mean () Max Speed Type Captive 210.0 Wild 185.0 We can also choose to include NA in group keys or not by setting dropna parameter, the default setting is True . >>> l = [[ 1 , 2 , 3 ], [ 1 , None , 4 ], [ 2 , 1 , 3 ], [ 1 , 2 , 2 ]] >>> df = pd . DataFrame ( l , columns = [ \"a\" , \"b\" , \"c\" ]) >>> df . groupby ( by = [ \"b\" ]) . sum () a c b 1.0 2 3 2.0 2 5 >>> df . groupby ( by = [ \"b\" ], dropna = False ) . sum () a c b 1.0 2 3 2.0 2 5 NaN 1 4 >>> l = [[ \"a\" , 12 , 12 ], [ None , 12.3 , 33. ], [ \"b\" , 12.3 , 123 ], [ \"a\" , 1 , 1 ]] >>> df = pd . DataFrame ( l , columns = [ \"a\" , \"b\" , \"c\" ]) >>> df . groupby ( by = \"a\" ) . sum () b c a a 13.0 13.0 b 12.3 123.0 >>> df . groupby ( by = \"a\" , dropna = False ) . sum () b c a a 13.0 13.0 b 12.3 123.0 NaN 12.3 33.0 When using .apply() , use group_keys to include or exclude the group keys. The group_keys argument defaults to True (include). >>> df = pd . DataFrame ({ 'Animal' : [ 'Falcon' , 'Falcon' , ... 'Parrot' , 'Parrot' ], ... 'Max Speed' : [ 380. , 370. , 24. , 26. ]}) >>> df . groupby ( \"Animal\" , group_keys = True )[[ 'Max Speed' ]] . apply ( lambda x : x ) Max Speed Animal Falcon 0 380.0 1 370.0 Parrot 2 24.0 3 26.0 >>> df . groupby ( \"Animal\" , group_keys = False )[[ 'Max Speed' ]] . apply ( lambda x : x ) Max Speed 0 380.0 1 370.0 2 24.0 3 26.0 method","title":"pandas.core.frame.DataFrame.groupby"},{"location":"api/pipen.channel/#pandascoreframedataframepivot","text":"</> Return reshaped DataFrame organized by given index / column values. Reshape data (produce a \"pivot\" table) based on column values. Uses unique values from specified index / columns to form axes of the resulting DataFrame. This function does not support data aggregation, multiple values will result in a MultiIndex in the columns. See the :ref: User Guide <reshaping> for more on reshaping. Parameters columns (str or object or a list of str) \u2014 Column to use to make new frame's columns. index (str or object or a list of str, optional) \u2014 Column to use to make new frame's index. If not given, uses existing index. values (str, object or a list of the previous, optional) \u2014 Column(s) to use for populating new frame's values. If notspecified, all remaining columns will be used and the result will have hierarchically indexed columns. Returns (DataFrame) Returns reshaped DataFrame. Raises ValueError \u2014 When there are any index , columns combinations with multiplevalues. DataFrame.pivot_table when you need to aggregate. See Also DataFrame.pivottable : Generalization of pivot that can handle duplicate values for one index/column pair. DataFrame.unstack : Pivot based on the index values instead of a column. wide_to_long : Wide panel to long format. Less flexible but more user-friendly than melt. Notes For finer-tuned control, see hierarchical indexing documentation along with the related stack/unstack methods. Reference :ref: the user guide <reshaping.pivot> for more examples. Examples >>> df = pd . DataFrame ({ 'foo' : [ 'one' , 'one' , 'one' , 'two' , 'two' , ... 'two' ], ... 'bar' : [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ], ... 'baz' : [ 1 , 2 , 3 , 4 , 5 , 6 ], ... 'zoo' : [ 'x' , 'y' , 'z' , 'q' , 'w' , 't' ]}) >>> df foo bar baz zoo 0 one A 1 x 1 one B 2 y 2 one C 3 z 3 two A 4 q 4 two B 5 w 5 two C 6 t >>> df . pivot ( index = 'foo' , columns = 'bar' , values = 'baz' ) bar A B C foo one 1 2 3 two 4 5 6 >>> df . pivot ( index = 'foo' , columns = 'bar' )[ 'baz' ] bar A B C foo one 1 2 3 two 4 5 6 >>> df . pivot ( index = 'foo' , columns = 'bar' , values = [ 'baz' , 'zoo' ]) baz zoo bar A B C A B C foo one 1 2 3 x y z two 4 5 6 q w t You could also assign a list of column names or a list of index names. >>> df = pd . DataFrame ({ ... \"lev1\" : [ 1 , 1 , 1 , 2 , 2 , 2 ], ... \"lev2\" : [ 1 , 1 , 2 , 1 , 1 , 2 ], ... \"lev3\" : [ 1 , 2 , 1 , 2 , 1 , 2 ], ... \"lev4\" : [ 1 , 2 , 3 , 4 , 5 , 6 ], ... \"values\" : [ 0 , 1 , 2 , 3 , 4 , 5 ]}) >>> df lev1 lev2 lev3 lev4 values 0 1 1 1 1 0 1 1 1 2 2 1 2 1 2 1 3 2 3 2 1 2 4 3 4 2 1 1 5 4 5 2 2 2 6 5 >>> df . pivot ( index = \"lev1\" , columns = [ \"lev2\" , \"lev3\" ], values = \"values\" ) lev2 1 2 lev3 1 2 1 2 lev1 1 0.0 1.0 2.0 NaN 2 4.0 3.0 NaN 5.0 >>> df . pivot ( index = [ \"lev1\" , \"lev2\" ], columns = [ \"lev3\" ], values = \"values\" ) lev3 1 2 lev1 lev2 1 1 0.0 1.0 2 2.0 NaN 2 1 4.0 3.0 2 NaN 5.0 A ValueError is raised if there are any duplicates. >>> df = pd . DataFrame ({ \"foo\" : [ 'one' , 'one' , 'two' , 'two' ], ... \"bar\" : [ 'A' , 'A' , 'B' , 'C' ], ... \"baz\" : [ 1 , 2 , 3 , 4 ]}) >>> df foo bar baz 0 one A 1 1 one A 2 2 two B 3 3 two C 4 Notice that the first two rows are the same for our index and columns arguments. >>> df . pivot ( index = 'foo' , columns = 'bar' , values = 'baz' ) Traceback ( most recent call last ): ... ValueError : Index contains duplicate entries , cannot reshape method","title":"pandas.core.frame.DataFrame.pivot"},{"location":"api/pipen.channel/#pandascoreframedataframepivot_table","text":"</> Create a spreadsheet-style pivot table as a DataFrame. The levels in the pivot table will be stored in MultiIndex objects (hierarchical indexes) on the index and columns of the result DataFrame. Parameters values (list-like or scalar, optional) \u2014 Column or columns to aggregate. index (column, Grouper, array, or list of the previous) \u2014 Keys to group by on the pivot table index. If a list is passed,it can contain any of the other types (except list). If an array is passed, it must be the same length as the data and will be used in the same manner as column values. columns (column, Grouper, array, or list of the previous) \u2014 Keys to group by on the pivot table column. If a list is passed,it can contain any of the other types (except list). If an array is passed, it must be the same length as the data and will be used in the same manner as column values. aggfunc (function, list of functions, dict, default \"mean\") \u2014 If a list of functions is passed, the resulting pivot table will havehierarchical columns whose top level are the function names (inferred from the function objects themselves). If a dict is passed, the key is column to aggregate and the value is function or list of functions. If margin=True , aggfunc will be used to calculate the partial aggregates. fill_value (scalar, default None) \u2014 Value to replace missing values with (in the resulting pivot table,after aggregation). margins (bool, default False) \u2014 If margins=True , special All columns and rowswill be added with partial group aggregates across the categories on the rows and columns. dropna (bool, default True) \u2014 Do not include columns whose entries are all NaN. If True,rows with a NaN value in any column will be omitted before computing margins. margins_name (str, default 'All') \u2014 Name of the row / column that will contain the totalswhen margins is True. observed (bool, default False) \u2014 This only applies if any of the groupers are Categoricals.If True: only show observed values for categorical groupers. If False: show all values for categorical groupers. .. deprecated:: 2.2.0 The default value of ``False`` is deprecated and will change to ``True`` in a future version of pandas. sort (bool, default True) \u2014 Specifies if the result should be sorted. .. versionadded:: 1.3.0 Returns (DataFrame) An Excel style pivot table. See Also DataFrame.pivot : Pivot without aggregation that can handle non-numeric data. DataFrame.melt: Unpivot a DataFrame from wide to long format, optionally leaving identifiers set. wide_to_long : Wide panel to long format. Less flexible but more user-friendly than melt. Notes Reference :ref: the user guide <reshaping.pivot> for more examples. Examples >>> df = pd . DataFrame ({ \"A\" : [ \"foo\" , \"foo\" , \"foo\" , \"foo\" , \"foo\" , ... \"bar\" , \"bar\" , \"bar\" , \"bar\" ], ... \"B\" : [ \"one\" , \"one\" , \"one\" , \"two\" , \"two\" , ... \"one\" , \"one\" , \"two\" , \"two\" ], ... \"C\" : [ \"small\" , \"large\" , \"large\" , \"small\" , ... \"small\" , \"large\" , \"small\" , \"small\" , ... \"large\" ], ... \"D\" : [ 1 , 2 , 2 , 3 , 3 , 4 , 5 , 6 , 7 ], ... \"E\" : [ 2 , 4 , 5 , 5 , 6 , 6 , 8 , 9 , 9 ]}) >>> df A B C D E 0 foo one small 1 2 1 foo one large 2 4 2 foo one large 2 5 3 foo two small 3 5 4 foo two small 3 6 5 bar one large 4 6 6 bar one small 5 8 7 bar two small 6 9 8 bar two large 7 9 This first example aggregates values by taking the sum. >>> table = pd . pivot_table ( df , values = 'D' , index = [ 'A' , 'B' ], ... columns = [ 'C' ], aggfunc = \"sum\" ) >>> table C large small A B bar one 4.0 5.0 two 7.0 6.0 foo one 4.0 1.0 two NaN 6.0 We can also fill missing values using the fill_value parameter. >>> table = pd . pivot_table ( df , values = 'D' , index = [ 'A' , 'B' ], ... columns = [ 'C' ], aggfunc = \"sum\" , fill_value = 0 ) >>> table C large small A B bar one 4 5 two 7 6 foo one 4 1 two 0 6 The next example aggregates by taking the mean across multiple columns. >>> table = pd . pivot_table ( df , values = [ 'D' , 'E' ], index = [ 'A' , 'C' ], ... aggfunc = { 'D' : \"mean\" , 'E' : \"mean\" }) >>> table D E A C bar large 5.500000 7.500000 small 5.500000 8.500000 foo large 2.000000 4.500000 small 2.333333 4.333333 We can also calculate multiple types of aggregations for any given value column. >>> table = pd . pivot_table ( df , values = [ 'D' , 'E' ], index = [ 'A' , 'C' ], ... aggfunc = { 'D' : \"mean\" , ... 'E' : [ \"min\" , \"max\" , \"mean\" ]}) >>> table D E mean max mean min A C bar large 5.500000 9 7.500000 6 small 5.500000 9 8.500000 8 foo large 2.000000 5 4.500000 4 small 2.333333 6 4.333333 2 method","title":"pandas.core.frame.DataFrame.pivot_table"},{"location":"api/pipen.channel/#pandascoreframedataframestack","text":"</> Stack the prescribed level(s) from columns to index. Return a reshaped DataFrame or Series having a multi-level index with one or more new inner-most levels compared to the current DataFrame. The new inner-most levels are created by pivoting the columns of the current dataframe: if the columns have a single level, the output is a Series; if the columns have multiple levels, the new index level(s) is (are) taken from the prescribed level(s) and the output is a DataFrame. Parameters level (int, str, list, default -1) \u2014 Level(s) to stack from the column axis onto the indexaxis, defined as one index or label, or a list of indices or labels. dropna (bool, default True) \u2014 Whether to drop rows in the resulting Frame/Series withmissing values. Stacking a column level onto the index axis can create combinations of index and column values that are missing from the original dataframe. See Examples section. sort (bool, default True) \u2014 Whether to sort the levels of the resulting MultiIndex. future_stack (bool, default False) \u2014 Whether to use the new implementation that will replace the currentimplementation in pandas 3.0. When True, dropna and sort have no impact on the result and must remain unspecified. See :ref: pandas 2.1.0 Release notes <whatsnew_210.enhancements.new_stack> for more details. Returns (DataFrame or Series) Stacked dataframe or series. See Also DataFrame.unstack : Unstack prescribed level(s) from index axis onto column axis. DataFrame.pivot : Reshape dataframe from long format to wide format. DataFrame.pivottable : Create a spreadsheet-style pivot table as a DataFrame. Notes The function is named by analogy with a collection of books being reorganized from being side by side on a horizontal position (the columns of the dataframe) to being stacked vertically on top of each other (in the index of the dataframe). Reference :ref: the user guide <reshaping.stacking> for more examples. Examples Single level columns >>> df_single_level_cols = pd . DataFrame ([[ 0 , 1 ], [ 2 , 3 ]], ... index = [ 'cat' , 'dog' ], ... columns = [ 'weight' , 'height' ]) Stacking a dataframe with a single level column axis returns a Series: >>> df_single_level_cols weight height cat 0 1 dog 2 3 >>> df_single_level_cols . stack ( future_stack = True ) cat weight 0 height 1 dog weight 2 height 3 dtype : int64 Multi level columns: simple case >>> multicol1 = pd . MultiIndex . from_tuples ([( 'weight' , 'kg' ), ... ( 'weight' , 'pounds' )]) >>> df_multi_level_cols1 = pd . DataFrame ([[ 1 , 2 ], [ 2 , 4 ]], ... index = [ 'cat' , 'dog' ], ... columns = multicol1 ) Stacking a dataframe with a multi-level column axis: >>> df_multi_level_cols1 weight kg pounds cat 1 2 dog 2 4 >>> df_multi_level_cols1 . stack ( future_stack = True ) weight cat kg 1 pounds 2 dog kg 2 pounds 4 Missing values >>> multicol2 = pd . MultiIndex . from_tuples ([( 'weight' , 'kg' ), ... ( 'height' , 'm' )]) >>> df_multi_level_cols2 = pd . DataFrame ([[ 1.0 , 2.0 ], [ 3.0 , 4.0 ]], ... index = [ 'cat' , 'dog' ], ... columns = multicol2 ) It is common to have missing values when stacking a dataframe with multi-level columns, as the stacked dataframe typically has more values than the original dataframe. Missing values are filled with NaNs: >>> df_multi_level_cols2 weight height kg m cat 1.0 2.0 dog 3.0 4.0 >>> df_multi_level_cols2 . stack ( future_stack = True ) weight height cat kg 1.0 NaN m NaN 2.0 dog kg 3.0 NaN m NaN 4.0 Prescribing the level(s) to be stacked The first parameter controls which level or levels are stacked: >>> df_multi_level_cols2 . stack ( 0 , future_stack = True ) kg m cat weight 1.0 NaN height NaN 2.0 dog weight 3.0 NaN height NaN 4.0 >>> df_multi_level_cols2 . stack ([ 0 , 1 ], future_stack = True ) cat weight kg 1.0 height m 2.0 dog weight kg 3.0 height m 4.0 dtype : float64 method","title":"pandas.core.frame.DataFrame.stack"},{"location":"api/pipen.channel/#pandascoreframedataframeexplode","text":"</> Transform each element of a list-like to a row, replicating index values. Parameters column (IndexLabel) \u2014 Column(s) to explode.For multiple columns, specify a non-empty list with each element be str or tuple, and all specified columns their list-like data on same row of the frame must have matching length. .. versionadded:: 1.3.0 Multi-column explode ignore_index (bool, default False) \u2014 If True, the resulting index will be labeled 0, 1, \u2026, n - 1. Returns (DataFrame) Exploded lists to rows of the subset columns;index will be duplicated for these rows. Raises ValueError \u2014 If columns of the frame are not unique. If specified columns to explode is empty list. If specified columns to explode have not matching count of elements rowwise in the frame. See Also DataFrame.unstack : Pivot a level of the (necessarily hierarchical) index labels. DataFrame.melt : Unpivot a DataFrame from wide format to long format. Series.explode : Explode a DataFrame from list-like columns to long format. Notes This routine will explode list-likes including lists, tuples, sets, Series, and np.ndarray. The result dtype of the subset rows will be object. Scalars will be returned unchanged, and empty list-likes will result in a np.nan for that row. In addition, the ordering of rows in the output will be non-deterministic when exploding sets. Reference :ref: the user guide <reshaping.explode> for more examples. Examples >>> df = pd . DataFrame ({ 'A' : [[ 0 , 1 , 2 ], 'foo' , [], [ 3 , 4 ]], ... 'B' : 1 , ... 'C' : [[ 'a' , 'b' , 'c' ], np . nan , [], [ 'd' , 'e' ]]}) >>> df A B C 0 [ 0 , 1 , 2 ] 1 [ a , b , c ] 1 foo 1 NaN 2 [] 1 [] 3 [ 3 , 4 ] 1 [ d , e ] Single-column explode. >>> df . explode ( 'A' ) A B C 0 0 1 [ a , b , c ] 0 1 1 [ a , b , c ] 0 2 1 [ a , b , c ] 1 foo 1 NaN 2 NaN 1 [] 3 3 1 [ d , e ] 3 4 1 [ d , e ] Multi-column explode. >>> df . explode ( list ( 'AC' )) A B C 0 0 1 a 0 1 1 b 0 2 1 c 1 foo 1 NaN 2 NaN 1 NaN 3 3 1 d 3 4 1 e method","title":"pandas.core.frame.DataFrame.explode"},{"location":"api/pipen.channel/#pandascoreframedataframeunstack","text":"</> Pivot a level of the (necessarily hierarchical) index labels. Returns a DataFrame having a new level of column labels whose inner-most level consists of the pivoted index labels. If the index is not a MultiIndex, the output will be a Series (the analogue of stack when the columns are not a MultiIndex). Parameters level (int, str, or list of these, default -1 (last level)) \u2014 Level(s) of index to unstack, can pass level name. fill_value (int, str or dict) \u2014 Replace NaN with this value if the unstack produces missing values. sort (bool, default True) \u2014 Sort the level(s) in the resulting MultiIndex columns. See Also DataFrame.pivot : Pivot a table based on column values.DataFrame.stack : Pivot a level of the column labels (inverse operation from unstack ). Notes Reference :ref: the user guide <reshaping.stacking> for more examples. Examples >>> index = pd . MultiIndex . from_tuples ([( 'one' , 'a' ), ( 'one' , 'b' ), ... ( 'two' , 'a' ), ( 'two' , 'b' )]) >>> s = pd . Series ( np . arange ( 1.0 , 5.0 ), index = index ) >>> s one a 1.0 b 2.0 two a 3.0 b 4.0 dtype : float64 >>> s . unstack ( level =- 1 ) a b one 1.0 2.0 two 3.0 4.0 >>> s . unstack ( level = 0 ) one two a 1.0 3.0 b 2.0 4.0 >>> df = s . unstack ( level = 0 ) >>> df . unstack () one a 1.0 b 2.0 two a 3.0 b 4.0 dtype : float64 method","title":"pandas.core.frame.DataFrame.unstack"},{"location":"api/pipen.channel/#pandascoreframedataframemelt","text":"</> Unpivot a DataFrame from wide to long format, optionally leaving identifiers set. This function is useful to massage a DataFrame into a format where one or more columns are identifier variables ( id_vars ), while all other columns, considered measured variables ( value_vars ), are \"unpivoted\" to the row axis, leaving just two non-identifier columns, 'variable' and 'value'. Parameters id_vars (scalar, tuple, list, or ndarray, optional) \u2014 Column(s) to use as identifier variables. value_vars (scalar, tuple, list, or ndarray, optional) \u2014 Column(s) to unpivot. If not specified, uses all columns thatare not set as id_vars . var_name (scalar, default None) \u2014 Name to use for the 'variable' column. If None it uses frame.columns.name or 'variable'. value_name (scalar, default 'value') \u2014 Name to use for the 'value' column, can't be an existing column label. col_level (scalar, optional) \u2014 If columns are a MultiIndex then use this level to melt. ignore_index (bool, default True) \u2014 If True, original index is ignored. If False, the original index is retained.Index labels will be repeated as necessary. Returns (DataFrame) Unpivoted DataFrame. See Also melt : Identical method.pivot_table : Create a spreadsheet-style pivot table as a DataFrame. DataFrame.pivot : Return reshaped DataFrame organized by given index / column values. DataFrame.explode : Explode a DataFrame from list-like columns to long format. Notes Reference :ref: the user guide <reshaping.melt> for more examples. Examples >>> df = pd . DataFrame ({ 'A' : { 0 : 'a' , 1 : 'b' , 2 : 'c' }, ... 'B' : { 0 : 1 , 1 : 3 , 2 : 5 }, ... 'C' : { 0 : 2 , 1 : 4 , 2 : 6 }}) >>> df A B C 0 a 1 2 1 b 3 4 2 c 5 6 >>> df . melt ( id_vars = [ 'A' ], value_vars = [ 'B' ]) A variable value 0 a B 1 1 b B 3 2 c B 5 >>> df . melt ( id_vars = [ 'A' ], value_vars = [ 'B' , 'C' ]) A variable value 0 a B 1 1 b B 3 2 c B 5 3 a C 2 4 b C 4 5 c C 6 The names of 'variable' and 'value' columns can be customized: >>> df . melt ( id_vars = [ 'A' ], value_vars = [ 'B' ], ... var_name = 'myVarname' , value_name = 'myValname' ) A myVarname myValname 0 a B 1 1 b B 3 2 c B 5 Original index values can be kept around: >>> df . melt ( id_vars = [ 'A' ], value_vars = [ 'B' , 'C' ], ignore_index = False ) A variable value 0 a B 1 1 b B 3 2 c B 5 0 a C 2 1 b C 4 2 c C 6 If you have multi-index columns: >>> df . columns = [ list ( 'ABC' ), list ( 'DEF' )] >>> df A B C D E F 0 a 1 2 1 b 3 4 2 c 5 6 >>> df . melt ( col_level = 0 , id_vars = [ 'A' ], value_vars = [ 'B' ]) A variable value 0 a B 1 1 b B 3 2 c B 5 >>> df . melt ( id_vars = [( 'A' , 'D' )], value_vars = [( 'B' , 'E' )]) ( A , D ) variable_0 variable_1 value 0 a B E 1 1 b B E 3 2 c B E 5 method","title":"pandas.core.frame.DataFrame.melt"},{"location":"api/pipen.channel/#pandascoreframedataframediff","text":"</> First discrete difference of element. Calculates the difference of a DataFrame element compared with another element in the DataFrame (default is element in previous row). Parameters periods (int, default 1) \u2014 Periods to shift for calculating difference, accepts negativevalues. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Take difference over rows (0) or columns (1). Returns (DataFrame) First differences of the Series. See Also DataFrame.pct_change: Percent change over given number of periods.DataFrame.shift: Shift index by desired number of periods with an optional time freq. Series.diff: First discrete difference of object. Notes For boolean dtypes, this uses :meth: operator.xor rather than :meth: operator.sub . The result is calculated according to current dtype in DataFrame, however dtype of the result is always float64. Examples w , , ) f c 1 4 9 6 5 6 ) c N 0 0 0 0 0 n ) c 0 3 7 3 0 8 w ) c N N N 0 0 0 w ) c 0 0 0 0 0 N e ) ) a N 0 method","title":"pandas.core.frame.DataFrame.diff"},{"location":"api/pipen.channel/#pandascoreframedataframeaggregate","text":"</> Aggregate using one or more operations over the specified axis. Parameters func (function, str, list or dict) \u2014 Function to use for aggregating the data. If a function, must eitherwork when passed a DataFrame or when passed to DataFrame.apply. Accepted combinations are: function string function name list of functions and/or function names, e.g. [np.sum, 'mean'] dict of axis labels -> functions, function names or list of such. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 If 0 or 'index': apply function to each column.If 1 or 'columns': apply function to each row. *args \u2014 Positional arguments to pass to func . **kwargs \u2014 Keyword arguments to pass to func . Returns (scalar, Series or DataFrame) : n n s See Also DataFrame.apply : Perform any type of operations.DataFrame.transform : Perform transformation type operations. pandas.DataFrame.groupby : Perform operations over groups. pandas.DataFrame.resample : Perform operations over resampled bins. pandas.DataFrame.rolling : Perform operations over rolling window. pandas.DataFrame.expanding : Perform operations over expanding window. pandas.core.window.ewm.ExponentialMovingWindow : Perform operation over exponential weighted window. Notes The aggregation operations are always performed over an axis, either the index (default) or the column axis. This behavior is different from numpy aggregation functions ( mean , median , prod , sum , std , var ), where the default is to compute the aggregation of the flattened array, e.g., numpy.mean(arr_2d) as opposed to numpy.mean(arr_2d, axis=0) . agg is an alias for aggregate . Use the alias. Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See :ref: gotchas.udf-mutation for more details. A passed user-defined-function will be passed a Series for evaluation. Examples >>> df = pd . DataFrame ([[ 1 , 2 , 3 ], ... [ 4 , 5 , 6 ], ... [ 7 , 8 , 9 ], ... [ np . nan , np . nan , np . nan ]], ... columns = [ 'A' , 'B' , 'C' ]) Aggregate these functions over the rows. >>> df . agg ([ 'sum' , 'min' ]) A B C sum 12.0 15.0 18.0 min 1.0 2.0 3.0 Different aggregations per column. >>> df . agg ({ 'A' : [ 'sum' , 'min' ], 'B' : [ 'min' , 'max' ]}) A B sum 12.0 NaN min 1.0 2.0 max NaN 8.0 Aggregate different functions over the columns and rename the index of the resulting DataFrame. >>> df . agg ( x = ( 'A' , 'max' ), y = ( 'B' , 'min' ), z = ( 'C' , 'mean' )) A B C x 7.0 NaN NaN y NaN 2.0 NaN z NaN NaN 6.0 Aggregate over the columns. >>> df . agg ( \"mean\" , axis = \"columns\" ) 0 2.0 1 5.0 2 8.0 3 NaN dtype : float64 method","title":"pandas.core.frame.DataFrame.aggregate"},{"location":"api/pipen.channel/#pandascoreframedataframetransform","text":"</> Call func on self producing a DataFrame with the same axis shape as self. Parameters func (function, str, list-like or dict-like) \u2014 Function to use for transforming the data. If a function, must eitherwork when passed a DataFrame or when passed to DataFrame.apply. If func is both list-like and dict-like, dict-like behavior takes precedence. Accepted combinations are: function string function name list-like of functions and/or function names, e.g. [np.exp, 'sqrt'] dict-like of axis labels -> functions, function names or list-like of such. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 If 0 or 'index': apply function to each column.If 1 or 'columns': apply function to each row. *args \u2014 Positional arguments to pass to func . **kwargs \u2014 Keyword arguments to pass to func . Returns (DataFrame) A DataFrame that must have the same length as self. See Also DataFrame.agg : Only perform aggregating type operations.DataFrame.apply : Invoke function on a DataFrame. Notes Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See :ref: gotchas.udf-mutation for more details. Examples >>> df = pd . DataFrame ({ 'A' : range ( 3 ), 'B' : range ( 1 , 4 )}) >>> df A B 0 0 1 1 1 2 2 2 3 >>> df . transform ( lambda x : x + 1 ) A B 0 1 2 1 2 3 2 3 4 Even though the resulting DataFrame must have the same length as the input DataFrame, it is possible to provide several input functions: >>> s = pd . Series ( range ( 3 )) >>> s 0 0 1 1 2 2 dtype : int64 >>> s . transform ([ np . sqrt , np . exp ]) sqrt exp 0 0.000000 1.000000 1 1.000000 2.718282 2 1.414214 7.389056 You can call transform on a GroupBy object: >>> df = pd . DataFrame ({ ... \"Date\" : [ ... \"2015-05-08\" , \"2015-05-07\" , \"2015-05-06\" , \"2015-05-05\" , ... \"2015-05-08\" , \"2015-05-07\" , \"2015-05-06\" , \"2015-05-05\" ], ... \"Data\" : [ 5 , 8 , 6 , 1 , 50 , 100 , 60 , 120 ], ... }) >>> df Date Data 0 2015 - 05 - 08 5 1 2015 - 05 - 07 8 2 2015 - 05 - 06 6 3 2015 - 05 - 05 1 4 2015 - 05 - 08 50 5 2015 - 05 - 07 100 6 2015 - 05 - 06 60 7 2015 - 05 - 05 120 >>> df . groupby ( 'Date' )[ 'Data' ] . transform ( 'sum' ) 0 55 1 108 2 66 3 121 4 55 5 108 6 66 7 121 Name : Data , dtype : int64 >>> df = pd . DataFrame ({ ... \"c\" : [ 1 , 1 , 1 , 2 , 2 , 2 , 2 ], ... \"type\" : [ \"m\" , \"n\" , \"o\" , \"m\" , \"m\" , \"n\" , \"n\" ] ... }) >>> df c type 0 1 m 1 1 n 2 1 o 3 2 m 4 2 m 5 2 n 6 2 n >>> df [ 'size' ] = df . groupby ( 'c' )[ 'type' ] . transform ( len ) >>> df c type size 0 1 m 3 1 1 n 3 2 1 o 3 3 2 m 4 4 2 m 4 5 2 n 4 6 2 n 4 method","title":"pandas.core.frame.DataFrame.transform"},{"location":"api/pipen.channel/#pandascoreframedataframeapply","text":"</> Apply a function along an axis of the DataFrame. Objects passed to the function are Series objects whose index is either the DataFrame's index ( axis=0 ) or the DataFrame's columns ( axis=1 ). By default ( result_type=None ), the final return type is inferred from the return type of the applied function. Otherwise, it depends on the result_type argument. Parameters func (function) \u2014 Function to apply to each column or row. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Axis along which the function is applied: 0 or 'index': apply function to each column. 1 or 'columns': apply function to each row. raw (bool, default False) \u2014 Determines if row or column is passed as a Series or ndarray object: False : passes each row or column as a Series to the function. True : the passed function will receive ndarray objects instead. If you are just applying a NumPy reduction function this will achieve much better performance. result_type ({'expand', 'reduce', 'broadcast', None}, default None) \u2014 These only act when axis=1 (columns): 'expand' : list-like results will be turned into columns. 'reduce' : returns a Series if possible rather than expanding list-like results. This is the opposite of 'expand'. 'broadcast' : results will be broadcast to the original shape of the DataFrame, the original index and columns will be retained. The default behaviour (None) depends on the return value of the applied function: list-like results will be returned as a Series of those. However if the apply function returns a Series these are expanded to columns. args (tuple) \u2014 Positional arguments to pass to func in addition to thearray/series. by_row (False or \"compat\", default \"compat\") \u2014 Only has an effect when func is a listlike or dictlike of funcsand the func isn't a string. If \"compat\", will if possible first translate the func into pandas methods (e.g. Series().apply(np.sum) will be translated to Series().sum() ). If that doesn't work, will try call to apply again with by_row=True and if that fails, will call apply again with by_row=False (backward compatible). If False, the funcs will be passed the whole Series at once. .. versionadded:: 2.1.0 engine ({'python', 'numba'}, default 'python') \u2014 Choose between the python (default) engine or the numba engine in apply. The numba engine will attempt to JIT compile the passed function, which may result in speedups for large DataFrames. It also supports the following engine_kwargs : nopython (compile the function in nopython mode) nogil (release the GIL inside the JIT compiled function) parallel (try to apply the function in parallel over the DataFrame) Note: Due to limitations within numba/how pandas interfaces with numba, you should only use this if raw=True Note: The numba compiler only supports a subset of valid Python/numpy operations. Please read more about the supported python features <https://numba.pydata.org/numba-doc/dev/reference/pysupported.html> and supported numpy features <https://numba.pydata.org/numba-doc/dev/reference/numpysupported.html> in numba to learn what you can or cannot use in the passed function. .. versionadded:: 2.2.0 engine_kwargs (dict) \u2014 Pass keyword arguments to the engine.This is currently only used by the numba engine, see the documentation for the engine argument for more information. **kwargs \u2014 Additional keyword arguments to pass as keywords arguments to func . Returns (Series or DataFrame) Result of applying func along the given axis of theDataFrame. See Also DataFrame.map: For elementwise operations.DataFrame.aggregate: Only perform aggregating type operations. DataFrame.transform: Only perform transforming type operations. Notes Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See :ref: gotchas.udf-mutation for more details. Examples >>> df = pd . DataFrame ([[ 4 , 9 ]] * 3 , columns = [ 'A' , 'B' ]) >>> df A B 0 4 9 1 4 9 2 4 9 Using a numpy universal function (in this case the same as np.sqrt(df) ): >>> df . apply ( np . sqrt ) A B 0 2.0 3.0 1 2.0 3.0 2 2.0 3.0 Using a reducing function on either axis >>> df . apply ( np . sum , axis = 0 ) A 12 B 27 dtype : int64 >>> df . apply ( np . sum , axis = 1 ) 0 13 1 13 2 13 dtype : int64 Returning a list-like will result in a Series >>> df . apply ( lambda x : [ 1 , 2 ], axis = 1 ) 0 [ 1 , 2 ] 1 [ 1 , 2 ] 2 [ 1 , 2 ] dtype : object Passing result_type='expand' will expand list-like results to columns of a Dataframe >>> df . apply ( lambda x : [ 1 , 2 ], axis = 1 , result_type = 'expand' ) 0 1 0 1 2 1 1 2 2 1 2 Returning a Series inside the function is similar to passing result_type='expand' . The resulting column names will be the Series index. >>> df . apply ( lambda x : pd . Series ([ 1 , 2 ], index = [ 'foo' , 'bar' ]), axis = 1 ) foo bar 0 1 2 1 1 2 2 1 2 Passing result_type='broadcast' will ensure the same shape result, whether list-like or scalar is returned by the function, and broadcast it along the axis. The resulting column names will be the originals. >>> df . apply ( lambda x : [ 1 , 2 ], axis = 1 , result_type = 'broadcast' ) A B 0 1 2 1 1 2 2 1 2 method","title":"pandas.core.frame.DataFrame.apply"},{"location":"api/pipen.channel/#pandascoreframedataframemap","text":"</> Apply a function to a Dataframe elementwise. .. versionadded:: 2.1.0 DataFrame.applymap was deprecated and renamed to DataFrame.map. This method applies a function that accepts and returns a scalar to every element of a DataFrame. Parameters func (callable) \u2014 Python function, returns a single value from a single value. na_action ({None, 'ignore'}, default None) \u2014 If 'ignore', propagate NaN values, without passing them to func. **kwargs \u2014 Additional keyword arguments to pass as keywords arguments to func . Returns (DataFrame) Transformed DataFrame. See Also DataFrame.apply : Apply a function along input axis of DataFrame.DataFrame.replace: Replace values given in to_replace with value . Series.map : Apply a function elementwise on a Series. Examples >>> df = pd . DataFrame ([[ 1 , 2.12 ], [ 3.356 , 4.567 ]]) >>> df 0 1 0 1.000 2.120 1 3.356 4.567 >>> df . map ( lambda x : len ( str ( x ))) 0 1 0 3 4 1 5 5 Like Series.map, NA values can be ignored: >>> df_copy = df . copy () >>> df_copy . iloc [ 0 , 0 ] = pd . NA >>> df_copy . map ( lambda x : len ( str ( x )), na_action = 'ignore' ) 0 1 0 NaN 4 1 5.0 5 It is also possible to use map with functions that are not lambda functions: >>> df . map ( round , ndigits = 1 ) 0 1 0 1.0 2.1 1 3.4 4.6 Note that a vectorized version of func often exists, which will be much faster. You could square each number elementwise. >>> df . map ( lambda x : x ** 2 ) 0 1 0 1.000000 4.494400 1 11.262736 20.857489 But it's better to avoid map in that case. >>> df ** 2 0 1 0 1.000000 4.494400 1 11.262736 20.857489 method","title":"pandas.core.frame.DataFrame.map"},{"location":"api/pipen.channel/#pandascoreframedataframeapplymap","text":"</> Apply a function to a Dataframe elementwise. .. deprecated:: 2.1.0 DataFrame.applymap has been deprecated. Use DataFrame.map instead. This method applies a function that accepts and returns a scalar to every element of a DataFrame. Parameters func (callable) \u2014 Python function, returns a single value from a single value. na_action ({None, 'ignore'}, default None) \u2014 If 'ignore', propagate NaN values, without passing them to func. **kwargs \u2014 Additional keyword arguments to pass as keywords arguments to func . Returns (DataFrame) Transformed DataFrame. See Also DataFrame.apply : Apply a function along input axis of DataFrame.DataFrame.map : Apply a function along input axis of DataFrame. DataFrame.replace: Replace values given in to_replace with value . Examples >>> df = pd . DataFrame ([[ 1 , 2.12 ], [ 3.356 , 4.567 ]]) >>> df 0 1 0 1.000 2.120 1 3.356 4.567 >>> df . map ( lambda x : len ( str ( x ))) 0 1 0 3 4 1 5 5 method","title":"pandas.core.frame.DataFrame.applymap"},{"location":"api/pipen.channel/#pandascoreframedataframejoin","text":"</> Join columns of another DataFrame. Join columns with other DataFrame either on index or on a key column. Efficiently join multiple DataFrame objects by index at once by passing a list. Parameters other (DataFrame, Series, or a list containing any combination of them) \u2014 Index should be similar to one of the columns in this one. If aSeries is passed, its name attribute must be set, and that will be used as the column name in the resulting joined DataFrame. on (str, list of str, or array-like, optional) \u2014 Column or index level name(s) in the caller to join on the indexin other , otherwise joins index-on-index. If multiple values given, the other DataFrame must have a MultiIndex. Can pass an array as the join key if it is not already contained in the calling DataFrame. Like an Excel VLOOKUP operation. how ({'left', 'right', 'outer', 'inner', 'cross'}, default 'left') \u2014 How to handle the operation of the two objects. left: use calling frame's index (or column if on is specified) right: use other 's index. outer: form union of calling frame's index (or column if on is specified) with other 's index, and sort it lexicographically. inner: form intersection of calling frame's index (or column if on is specified) with other 's index, preserving the order of the calling's one. cross: creates the cartesian product from both frames, preserves the order of the left keys. lsuffix (str, default '') \u2014 Suffix to use from left frame's overlapping columns. rsuffix (str, default '') \u2014 Suffix to use from right frame's overlapping columns. sort (bool, default False) \u2014 Order result DataFrame lexicographically by the join key. If False,the order of the join key depends on the join type (how keyword). validate (str, optional) \u2014 If specified, checks if join is of specified type. \"one_to_one\" or \"1:1\": check if join keys are unique in both left and right datasets. \"one_to_many\" or \"1:m\": check if join keys are unique in left dataset. \"many_to_one\" or \"m:1\": check if join keys are unique in right dataset. \"many_to_many\" or \"m:m\": allowed, but does not result in checks. .. versionadded:: 1.5.0 Returns (DataFrame) A dataframe containing columns from both the caller and other . See Also DataFrame.merge : For column(s)-on-column(s) operations. Notes Parameters on , lsuffix , and rsuffix are not supported when passing a list of DataFrame objects. Examples >>> df = pd . DataFrame ({ 'key' : [ 'K0' , 'K1' , 'K2' , 'K3' , 'K4' , 'K5' ], ... 'A' : [ 'A0' , 'A1' , 'A2' , 'A3' , 'A4' , 'A5' ]}) >>> df key A 0 K0 A0 1 K1 A1 2 K2 A2 3 K3 A3 4 K4 A4 5 K5 A5 >>> other = pd . DataFrame ({ 'key' : [ 'K0' , 'K1' , 'K2' ], ... 'B' : [ 'B0' , 'B1' , 'B2' ]}) >>> other key B 0 K0 B0 1 K1 B1 2 K2 B2 Join DataFrames using their indexes. >>> df . join ( other , lsuffix = '_caller' , rsuffix = '_other' ) key_caller A key_other B 0 K0 A0 K0 B0 1 K1 A1 K1 B1 2 K2 A2 K2 B2 3 K3 A3 NaN NaN 4 K4 A4 NaN NaN 5 K5 A5 NaN NaN If we want to join using the key columns, we need to set key to be the index in both df and other . The joined DataFrame will have key as its index. >>> df . set_index ( 'key' ) . join ( other . set_index ( 'key' )) A B key K0 A0 B0 K1 A1 B1 K2 A2 B2 K3 A3 NaN K4 A4 NaN K5 A5 NaN Another option to join using the key columns is to use the on parameter. DataFrame.join always uses other 's index but we can use any column in df . This method preserves the original DataFrame's index in the result. >>> df . join ( other . set_index ( 'key' ), on = 'key' ) key A B 0 K0 A0 B0 1 K1 A1 B1 2 K2 A2 B2 3 K3 A3 NaN 4 K4 A4 NaN 5 K5 A5 NaN Using non-unique key values shows how they are matched. >>> df = pd . DataFrame ({ 'key' : [ 'K0' , 'K1' , 'K1' , 'K3' , 'K0' , 'K1' ], ... 'A' : [ 'A0' , 'A1' , 'A2' , 'A3' , 'A4' , 'A5' ]}) >>> df key A 0 K0 A0 1 K1 A1 2 K1 A2 3 K3 A3 4 K0 A4 5 K1 A5 >>> df . join ( other . set_index ( 'key' ), on = 'key' , validate = 'm:1' ) key A B 0 K0 A0 B0 1 K1 A1 B1 2 K1 A2 B1 3 K3 A3 NaN 4 K0 A4 B0 5 K1 A5 B1 method","title":"pandas.core.frame.DataFrame.join"},{"location":"api/pipen.channel/#pandascoreframedataframemerge","text":"</> Merge DataFrame or named Series objects with a database-style join. A named Series object is treated as a DataFrame with a single named column. The join is done on columns or indexes. If joining columns on columns, the DataFrame indexes will be ignored . Otherwise if joining indexes on indexes or indexes on a column or columns, the index will be passed on. When performing a cross merge, no column specifications to merge on are allowed. .. warning:: If both key columns contain rows where the key is a null value, those rows will be matched against each other. This is different from usual SQL join behaviour and can lead to unexpected results. Parameters right (DataFrame or named Series) \u2014 Object to merge with. how ({'left', 'right', 'outer', 'inner', 'cross'}, default 'inner') \u2014 Type of merge to be performed. left: use only keys from left frame, similar to a SQL left outer join; preserve key order. right: use only keys from right frame, similar to a SQL right outer join; preserve key order. outer: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically. inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys. cross: creates the cartesian product from both frames, preserves the order of the left keys. on (label or list) \u2014 Column or index level names to join on. These must be found in bothDataFrames. If on is None and not merging on indexes then this defaults to the intersection of the columns in both DataFrames. left_on (label or list, or array-like) \u2014 Column or index level names to join on in the left DataFrame. Can alsobe an array or list of arrays of the length of the left DataFrame. These arrays are treated as if they are columns. right_on (label or list, or array-like) \u2014 Column or index level names to join on in the right DataFrame. Can alsobe an array or list of arrays of the length of the right DataFrame. These arrays are treated as if they are columns. left_index (bool, default False) \u2014 Use the index from the left DataFrame as the join key(s). If it is aMultiIndex, the number of keys in the other DataFrame (either the index or a number of columns) must match the number of levels. right_index (bool, default False) \u2014 Use the index from the right DataFrame as the join key. Same caveats asleft_index. sort (bool, default False) \u2014 Sort the join keys lexicographically in the result DataFrame. If False,the order of the join keys depends on the join type (how keyword). suffixes (list-like, default is (\"_x\", \"_y\")) \u2014 A length-2 sequence where each element is optionally a stringindicating the suffix to add to overlapping column names in left and right respectively. Pass a value of None instead of a string to indicate that the column name from left or right should be left as-is, with no suffix. At least one of the values must not be None. copy (bool, default True) \u2014 If False, avoid copy if possible. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` indicator (bool or str, default False) \u2014 If True, adds a column to the output DataFrame called \"_merge\" withinformation on the source of each row. The column can be given a different name by providing a string argument. The column will have a Categorical type with the value of \"left_only\" for observations whose merge key only appears in the left DataFrame, \"right_only\" for observations whose merge key only appears in the right DataFrame, and \"both\" if the observation's merge key is found in both DataFrames. validate (str, optional) \u2014 If specified, checks if merge is of specified type. \"one_to_one\" or \"1:1\": check if merge keys are unique in both left and right datasets. \"one_to_many\" or \"1:m\": check if merge keys are unique in left dataset. \"many_to_one\" or \"m:1\": check if merge keys are unique in right dataset. \"many_to_many\" or \"m:m\": allowed, but does not result in checks. Returns (DataFrame) A DataFrame of the two merged objects. See Also merge_ordered : Merge with optional filling/interpolation.merge_asof : Merge on nearest keys. DataFrame.join : Similar method using indices. Examples >>> df1 = pd . DataFrame ({ 'lkey' : [ 'foo' , 'bar' , 'baz' , 'foo' ], ... 'value' : [ 1 , 2 , 3 , 5 ]}) >>> df2 = pd . DataFrame ({ 'rkey' : [ 'foo' , 'bar' , 'baz' , 'foo' ], ... 'value' : [ 5 , 6 , 7 , 8 ]}) >>> df1 lkey value 0 foo 1 1 bar 2 2 baz 3 3 foo 5 >>> df2 rkey value 0 foo 5 1 bar 6 2 baz 7 3 foo 8 Merge df1 and df2 on the lkey and rkey columns. The value columns have the default suffixes, _x and _y, appended. >>> df1 . merge ( df2 , left_on = 'lkey' , right_on = 'rkey' ) lkey value_x rkey value_y 0 foo 1 foo 5 1 foo 1 foo 8 2 bar 2 bar 6 3 baz 3 baz 7 4 foo 5 foo 5 5 foo 5 foo 8 Merge DataFrames df1 and df2 with specified left and right suffixes appended to any overlapping columns. >>> df1 . merge ( df2 , left_on = 'lkey' , right_on = 'rkey' , ... suffixes = ( '_left' , '_right' )) lkey value_left rkey value_right 0 foo 1 foo 5 1 foo 1 foo 8 2 bar 2 bar 6 3 baz 3 baz 7 4 foo 5 foo 5 5 foo 5 foo 8 Merge DataFrames df1 and df2, but raise an exception if the DataFrames have any overlapping columns. >>> df1 . merge ( df2 , left_on = 'lkey' , right_on = 'rkey' , suffixes = ( False , False )) Traceback ( most recent call last ): ... ValueError : columns overlap but no suffix specified : Index ([ 'value' ], dtype = 'object' ) >>> df1 = pd . DataFrame ({ 'a' : [ 'foo' , 'bar' ], 'b' : [ 1 , 2 ]}) >>> df2 = pd . DataFrame ({ 'a' : [ 'foo' , 'baz' ], 'c' : [ 3 , 4 ]}) >>> df1 a b 0 foo 1 1 bar 2 >>> df2 a c 0 foo 3 1 baz 4 >>> df1 . merge ( df2 , how = 'inner' , on = 'a' ) a b c 0 foo 1 3 >>> df1 . merge ( df2 , how = 'left' , on = 'a' ) a b c 0 foo 1 3.0 1 bar 2 NaN >>> df1 = pd . DataFrame ({ 'left' : [ 'foo' , 'bar' ]}) >>> df2 = pd . DataFrame ({ 'right' : [ 7 , 8 ]}) >>> df1 left 0 foo 1 bar >>> df2 right 0 7 1 8 >>> df1 . merge ( df2 , how = 'cross' ) left right 0 foo 7 1 foo 8 2 bar 7 3 bar 8 method","title":"pandas.core.frame.DataFrame.merge"},{"location":"api/pipen.channel/#pandascoreframedataframeround","text":"</> Round a DataFrame to a variable number of decimal places. Parameters decimals (int, dict, Series) \u2014 Number of decimal places to round each column to. If an int isgiven, round each column to the same number of places. Otherwise dict and Series round to variable numbers of places. Column names should be in the keys if decimals is a dict-like, or in the index if decimals is a Series. Any columns not included in decimals will be left as is. Elements of decimals which are not columns of the input will be ignored. *args \u2014 Additional keywords have no effect but might be accepted forcompatibility with numpy. **kwargs \u2014 Additional keywords have no effect but might be accepted forcompatibility with numpy. Returns (DataFrame) A DataFrame with the affected columns rounded to the specifiednumber of decimal places. See Also numpy.around : Round a numpy array to the given number of decimals.Series.round : Round a Series to the given number of decimals. Examples >>> df = pd . DataFrame ([( .21 , .32 ), ( .01 , .67 ), ( .66 , .03 ), ( .21 , .18 )], ... columns = [ 'dogs' , 'cats' ]) >>> df dogs cats 0 0.21 0.32 1 0.01 0.67 2 0.66 0.03 3 0.21 0.18 By providing an integer each column is rounded to the same number of decimal places >>> df . round ( 1 ) dogs cats 0 0.2 0.3 1 0.0 0.7 2 0.7 0.0 3 0.2 0.2 With a dict, the number of places for specific columns can be specified with the column names as key and the number of decimal places as value >>> df . round ({ 'dogs' : 1 , 'cats' : 0 }) dogs cats 0 0.2 0.0 1 0.0 1.0 2 0.7 0.0 3 0.2 0.0 Using a Series, the number of places for specific columns can be specified with the column names as index and the number of decimal places as value >>> decimals = pd . Series ([ 0 , 1 ], index = [ 'cats' , 'dogs' ]) >>> df . round ( decimals ) dogs cats 0 0.2 0.0 1 0.0 1.0 2 0.7 0.0 3 0.2 0.0 method","title":"pandas.core.frame.DataFrame.round"},{"location":"api/pipen.channel/#pandascoreframedataframecorr","text":"</> Compute pairwise correlation of columns, excluding NA/null values. Parameters method ({'pearson', 'kendall', 'spearman'} or callable) \u2014 Method of correlation: pearson : standard correlation coefficient kendall : Kendall Tau correlation coefficient spearman : Spearman rank correlation callable: callable with input two 1d ndarrays and returning a float. Note that the returned matrix from corr will have 1 along the diagonals and will be symmetric regardless of the callable's behavior. min_periods (int, optional) \u2014 Minimum number of observations required per pair of columnsto have a valid result. Currently only available for Pearson and Spearman correlation. numeric_only (bool, default False) \u2014 Include only float , int or boolean data. .. versionadded:: 1.5.0 .. versionchanged:: 2.0.0 The default value of numeric_only is now False . Returns (DataFrame) Correlation matrix. See Also DataFrame.corrwith : Compute pairwise correlation with another DataFrame or Series. Series.corr : Compute the correlation between two Series. Notes Pearson, Kendall and Spearman correlation are currently computed using pairwise complete observations. Pearson correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient> _ Kendall rank correlation coefficient <https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient> _ Spearman's rank correlation coefficient <https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient> _ Examples >>> def histogram_intersection ( a , b ): ... v = np . minimum ( a , b ) . sum () . round ( decimals = 1 ) ... return v >>> df = pd . DataFrame ([( .2 , .3 ), ( .0 , .6 ), ( .6 , .0 ), ( .2 , .1 )], ... columns = [ 'dogs' , 'cats' ]) >>> df . corr ( method = histogram_intersection ) dogs cats dogs 1.0 0.3 cats 0.3 1.0 >>> df = pd . DataFrame ([( 1 , 1 ), ( 2 , np . nan ), ( np . nan , 3 ), ( 4 , 4 )], ... columns = [ 'dogs' , 'cats' ]) >>> df . corr ( min_periods = 3 ) dogs cats dogs 1.0 NaN cats NaN 1.0 method","title":"pandas.core.frame.DataFrame.corr"},{"location":"api/pipen.channel/#pandascoreframedataframecov","text":"</> Compute pairwise covariance of columns, excluding NA/null values. Compute the pairwise covariance among the series of a DataFrame. The returned data frame is the covariance matrix <https://en.wikipedia.org/wiki/Covariance_matrix> __ of the columns of the DataFrame. Both NA and null values are automatically excluded from the calculation. (See the note below about bias from missing values.) A threshold can be set for the minimum number of observations for each value created. Comparisons with observations below this threshold will be returned as NaN . This method is generally used for the analysis of time series data to understand the relationship between different measures across time. Parameters min_periods (int, optional) \u2014 Minimum number of observations required per pair of columnsto have a valid result. ddof (int, default 1) \u2014 Delta degrees of freedom. The divisor used in calculationsis N - ddof , where N represents the number of elements. This argument is applicable only when no nan is in the dataframe. numeric_only (bool, default False) \u2014 Include only float , int or boolean data. .. versionadded:: 1.5.0 .. versionchanged:: 2.0.0 The default value of numeric_only is now False . Returns (DataFrame) The covariance matrix of the series of the DataFrame. See Also Series.cov : Compute covariance with another Series.core.window.ewm.ExponentialMovingWindow.cov : Exponential weighted sample covariance. core.window.expanding.Expanding.cov : Expanding sample covariance. core.window.rolling.Rolling.cov : Rolling sample covariance. Notes Returns the covariance matrix of the DataFrame's time series. The covariance is normalized by N-ddof. For DataFrames that have Series that are missing data (assuming that data is missing at random <https://en.wikipedia.org/wiki/Missing_data#Missing_at_random> __) the returned covariance matrix will be an unbiased estimate of the variance and covariance between the member Series. However, for many applications this estimate may not be acceptable because the estimate covariance matrix is not guaranteed to be positive semi-definite. This could lead to estimate correlations having absolute values which are greater than one, and/or a non-invertible covariance matrix. See Estimation of covariance matrices <https://en.wikipedia.org/w/index.php?title=Estimation_of_covariance_ matrices> __ for more details. Examples >>> df = pd . DataFrame ([( 1 , 2 ), ( 0 , 3 ), ( 2 , 0 ), ( 1 , 1 )], ... columns = [ 'dogs' , 'cats' ]) >>> df . cov () dogs cats dogs 0.666667 - 1.000000 cats - 1.000000 1.666667 >>> np . random . seed ( 42 ) >>> df = pd . DataFrame ( np . random . randn ( 1000 , 5 ), ... columns = [ 'a' , 'b' , 'c' , 'd' , 'e' ]) >>> df . cov () a b c d e a 0.998438 - 0.020161 0.059277 - 0.008943 0.014144 b - 0.020161 1.059352 - 0.008543 - 0.024738 0.009826 c 0.059277 - 0.008543 1.010670 - 0.001486 - 0.000271 d - 0.008943 - 0.024738 - 0.001486 0.921297 - 0.013692 e 0.014144 0.009826 - 0.000271 - 0.013692 0.977795 Minimum number of periods This method also supports an optional min_periods keyword that specifies the required minimum number of non-NA observations for each column pair in order to have a valid result: >>> np . random . seed ( 42 ) >>> df = pd . DataFrame ( np . random . randn ( 20 , 3 ), ... columns = [ 'a' , 'b' , 'c' ]) >>> df . loc [ df . index [: 5 ], 'a' ] = np . nan >>> df . loc [ df . index [ 5 : 10 ], 'b' ] = np . nan >>> df . cov ( min_periods = 12 ) a b c a 0.316741 NaN - 0.150812 b NaN 1.248003 0.191417 c - 0.150812 0.191417 0.895202 method","title":"pandas.core.frame.DataFrame.cov"},{"location":"api/pipen.channel/#pandascoreframedataframecorrwith","text":"</> Compute pairwise correlation. Pairwise correlation is computed between rows or columns of DataFrame with rows or columns of Series or DataFrame. DataFrames are first aligned along both axes before computing the correlations. Parameters other (DataFrame, Series) \u2014 Object with which to compute correlations. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to use. 0 or 'index' to compute row-wise, 1 or 'columns' forcolumn-wise. drop (bool, default False) \u2014 Drop missing indices from result. method ({'pearson', 'kendall', 'spearman'} or callable) \u2014 Method of correlation: pearson : standard correlation coefficient kendall : Kendall Tau correlation coefficient spearman : Spearman rank correlation callable: callable with input two 1d ndarrays and returning a float. numeric_only (bool, default False) \u2014 Include only float , int or boolean data. .. versionadded:: 1.5.0 .. versionchanged:: 2.0.0 The default value of numeric_only is now False . Returns (Series) Pairwise correlations. See Also DataFrame.corr : Compute pairwise correlation of columns. Examples >>> index = [ \"a\" , \"b\" , \"c\" , \"d\" , \"e\" ] >>> columns = [ \"one\" , \"two\" , \"three\" , \"four\" ] >>> df1 = pd . DataFrame ( np . arange ( 20 ) . reshape ( 5 , 4 ), index = index , columns = columns ) >>> df2 = pd . DataFrame ( np . arange ( 16 ) . reshape ( 4 , 4 ), index = index [: 4 ], columns = columns ) >>> df1 . corrwith ( df2 ) one 1.0 two 1.0 three 1.0 four 1.0 dtype : float64 >>> df2 . corrwith ( df1 , axis = 1 ) a 1.0 b 1.0 c 1.0 d 1.0 e NaN dtype : float64 method","title":"pandas.core.frame.DataFrame.corrwith"},{"location":"api/pipen.channel/#pandascoreframedataframecount","text":"</> Count non-NA cells for each column or row. The values None , NaN , NaT , pandas.NA are considered NA. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 If 0 or 'index' counts are generated for each column.If 1 or 'columns' counts are generated for each row. numeric_only (bool, default False) \u2014 Include only float , int or boolean data. Returns (Series) For each column/row the number of non-NA/null entries. See Also Series.count: Number of non-NA elements in a Series.DataFrame.value_counts: Count unique combinations of columns. DataFrame.shape: Number of DataFrame rows and columns (including NA elements). DataFrame.isna: Boolean same-sized DataFrame showing places of NA elements. Examples Constructing DataFrame from a dictionary: >>> df = pd . DataFrame ({ \"Person\" : ... [ \"John\" , \"Myla\" , \"Lewis\" , \"John\" , \"Myla\" ], ... \"Age\" : [ 24. , np . nan , 21. , 33 , 26 ], ... \"Single\" : [ False , True , True , True , False ]}) >>> df Person Age Single 0 John 24.0 False 1 Myla NaN True 2 Lewis 21.0 True 3 John 33.0 True 4 Myla 26.0 False Notice the uncounted NA values: >>> df . count () Person 5 Age 4 Single 5 dtype : int64 Counts for each row : >>> df . count ( axis = 'columns' ) 0 3 1 2 2 3 3 3 4 3 dtype : int64 method","title":"pandas.core.frame.DataFrame.count"},{"location":"api/pipen.channel/#pandascoreframedataframeany","text":"</> Return whether any element is True, potentially over an axis. Returns False unless there is at least one element within a series or along a Dataframe axis that is True or equivalent (e.g. non-zero or non-empty). Parameters axis ({0 or 'index', 1 or 'columns', None}, default 0) \u2014 Indicate which axis or axes should be reduced. For Series this parameteris unused and defaults to 0. 0 / 'index' : reduce the index, return a Series whose index is the original column labels. 1 / 'columns' : reduce the columns, return a Series whose index is the original index. None : reduce all axes, return a scalar. bool_only (bool, default False) \u2014 Include only boolean columns. Not implemented for Series. skipna (bool, default True) \u2014 Exclude NA/null values. If the entire row/column is NA and skipna isTrue, then the result will be False, as for an empty row/column. If skipna is False, then NA are treated as True, because these are not equal to zero. **kwargs (any, default None) \u2014 Additional keywords have no effect but might be accepted forcompatibility with NumPy. Returns (Series or DataFrame) If level is specified, then, DataFrame is returned; otherwise, Seriesis returned. See Also numpy.any : Numpy version of this method.Series.any : Return whether any element is True. Series.all : Return whether all elements are True. DataFrame.any : Return whether any element is True over requested axis. DataFrame.all : Return whether all elements are True over requested axis. Examples Series For Series input, the output is a scalar indicating whether any element is True. >>> pd . Series ([ False , False ]) . any () False >>> pd . Series ([ True , False ]) . any () True >>> pd . Series ([], dtype = \"float64\" ) . any () False >>> pd . Series ([ np . nan ]) . any () False >>> pd . Series ([ np . nan ]) . any ( skipna = False ) True DataFrame Whether each column contains at least one True element (the default). >>> df = pd . DataFrame ({ \"A\" : [ 1 , 2 ], \"B\" : [ 0 , 2 ], \"C\" : [ 0 , 0 ]}) >>> df A B C 0 1 0 0 1 2 2 0 >>> df . any () A True B True C False dtype : bool Aggregating over the columns. >>> df = pd . DataFrame ({ \"A\" : [ True , False ], \"B\" : [ 1 , 2 ]}) >>> df A B 0 True 1 1 False 2 >>> df . any ( axis = 'columns' ) 0 True 1 True dtype : bool >>> df = pd . DataFrame ({ \"A\" : [ True , False ], \"B\" : [ 1 , 0 ]}) >>> df A B 0 True 1 1 False 0 >>> df . any ( axis = 'columns' ) 0 True 1 False dtype : bool Aggregating over the entire DataFrame with axis=None . >>> df . any ( axis = None ) True any for an empty DataFrame is an empty Series. >>> pd . DataFrame ([]) . any () Series ([], dtype : bool ) method","title":"pandas.core.frame.DataFrame.any"},{"location":"api/pipen.channel/#pandascoreframedataframeall","text":"</> Return whether all elements are True, potentially over an axis. Returns True unless there at least one element within a series or along a Dataframe axis that is False or equivalent (e.g. zero or empty). Parameters axis ({0 or 'index', 1 or 'columns', None}, default 0) \u2014 Indicate which axis or axes should be reduced. For Series this parameteris unused and defaults to 0. 0 / 'index' : reduce the index, return a Series whose index is the original column labels. 1 / 'columns' : reduce the columns, return a Series whose index is the original index. None : reduce all axes, return a scalar. bool_only (bool, default False) \u2014 Include only boolean columns. Not implemented for Series. skipna (bool, default True) \u2014 Exclude NA/null values. If the entire row/column is NA and skipna isTrue, then the result will be True, as for an empty row/column. If skipna is False, then NA are treated as True, because these are not equal to zero. **kwargs (any, default None) \u2014 Additional keywords have no effect but might be accepted forcompatibility with NumPy. Returns (Series or DataFrame) If level is specified, then, DataFrame is returned; otherwise, Seriesis returned. See Also Series.all : Return True if all elements are True.DataFrame.any : Return True if one (or more) elements are True. Examples Series >>> pd . Series ([ True , True ]) . all () True >>> pd . Series ([ True , False ]) . all () False >>> pd . Series ([], dtype = \"float64\" ) . all () True >>> pd . Series ([ np . nan ]) . all () True >>> pd . Series ([ np . nan ]) . all ( skipna = False ) True DataFrames Create a dataframe from a dictionary. >>> df = pd . DataFrame ({ 'col1' : [ True , True ], 'col2' : [ True , False ]}) >>> df col1 col2 0 True True 1 True False Default behaviour checks if values in each column all return True. >>> df . all () col1 True col2 False dtype : bool Specify axis='columns' to check if values in each row all return True. >>> df . all ( axis = 'columns' ) 0 True 1 False dtype : bool Or axis=None for whether every value is True. >>> df . all ( axis = None ) False method","title":"pandas.core.frame.DataFrame.all"},{"location":"api/pipen.channel/#pandascoreframedataframemin","text":"</> Return the minimum of the values over the requested axis. If you want the index of the minimum, use idxmin . This is the equivalent of the numpy.ndarray method argmin . Parameters axis ({index (0), columns (1)}) \u2014 Axis for the function to be applied on.For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. .. versionadded:: 2.0.0 skipna (bool, default True) \u2014 Exclude NA/null values when computing the result. numeric_only (bool, default False) \u2014 Include only float, int, boolean columns. Not implemented for Series. **kwargs \u2014 Additional keyword arguments to be passed to the function. See Also Series.sum : Return the sum.Series.min : Return the minimum. Series.max : Return the maximum. Series.idxmin : Return the index of the minimum. Series.idxmax : Return the index of the maximum. DataFrame.sum : Return the sum over the requested axis. DataFrame.min : Return the minimum over the requested axis. DataFrame.max : Return the maximum over the requested axis. DataFrame.idxmin : Return the index of the minimum over the requested axis. DataFrame.idxmax : Return the index of the maximum over the requested axis. Examples >>> idx = pd . MultiIndex . from_arrays ([ ... [ 'warm' , 'warm' , 'cold' , 'cold' ], ... [ 'dog' , 'falcon' , 'fish' , 'spider' ]], ... names = [ 'blooded' , 'animal' ]) >>> s = pd . Series ([ 4 , 2 , 0 , 8 ], name = 'legs' , index = idx ) >>> s blooded animal warm dog 4 falcon 2 cold fish 0 spider 8 Name : legs , dtype : int64 >>> s . min () 0 method","title":"pandas.core.frame.DataFrame.min"},{"location":"api/pipen.channel/#pandascoreframedataframemax","text":"</> Return the maximum of the values over the requested axis. If you want the index of the maximum, use idxmax . This is the equivalent of the numpy.ndarray method argmax . Parameters axis ({index (0), columns (1)}) \u2014 Axis for the function to be applied on.For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. .. versionadded:: 2.0.0 skipna (bool, default True) \u2014 Exclude NA/null values when computing the result. numeric_only (bool, default False) \u2014 Include only float, int, boolean columns. Not implemented for Series. **kwargs \u2014 Additional keyword arguments to be passed to the function. See Also Series.sum : Return the sum.Series.min : Return the minimum. Series.max : Return the maximum. Series.idxmin : Return the index of the minimum. Series.idxmax : Return the index of the maximum. DataFrame.sum : Return the sum over the requested axis. DataFrame.min : Return the minimum over the requested axis. DataFrame.max : Return the maximum over the requested axis. DataFrame.idxmin : Return the index of the minimum over the requested axis. DataFrame.idxmax : Return the index of the maximum over the requested axis. Examples >>> idx = pd . MultiIndex . from_arrays ([ ... [ 'warm' , 'warm' , 'cold' , 'cold' ], ... [ 'dog' , 'falcon' , 'fish' , 'spider' ]], ... names = [ 'blooded' , 'animal' ]) >>> s = pd . Series ([ 4 , 2 , 0 , 8 ], name = 'legs' , index = idx ) >>> s blooded animal warm dog 4 falcon 2 cold fish 0 spider 8 Name : legs , dtype : int64 >>> s . max () 8 method","title":"pandas.core.frame.DataFrame.max"},{"location":"api/pipen.channel/#pandascoreframedataframesum","text":"</> Return the sum of the values over the requested axis. This is equivalent to the method numpy.sum . Parameters axis ({index (0), columns (1)}) \u2014 Axis for the function to be applied on.For Series this parameter is unused and defaults to 0. .. warning:: The behavior of DataFrame.sum with ``axis=None`` is deprecated, in a future version this will reduce over both axes and return a scalar To retain the old behavior, pass axis=0 (or do not pass axis). .. versionadded:: 2.0.0 skipna (bool, default True) \u2014 Exclude NA/null values when computing the result. numeric_only (bool, default False) \u2014 Include only float, int, boolean columns. Not implemented for Series. min_count (int, default 0) \u2014 The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. **kwargs \u2014 Additional keyword arguments to be passed to the function. See Also Series.sum : Return the sum.Series.min : Return the minimum. Series.max : Return the maximum. Series.idxmin : Return the index of the minimum. Series.idxmax : Return the index of the maximum. DataFrame.sum : Return the sum over the requested axis. DataFrame.min : Return the minimum over the requested axis. DataFrame.max : Return the maximum over the requested axis. DataFrame.idxmin : Return the index of the minimum over the requested axis. DataFrame.idxmax : Return the index of the maximum over the requested axis. Examples >>> idx = pd . MultiIndex . from_arrays ([ ... [ 'warm' , 'warm' , 'cold' , 'cold' ], ... [ 'dog' , 'falcon' , 'fish' , 'spider' ]], ... names = [ 'blooded' , 'animal' ]) >>> s = pd . Series ([ 4 , 2 , 0 , 8 ], name = 'legs' , index = idx ) >>> s blooded animal warm dog 4 falcon 2 cold fish 0 spider 8 Name : legs , dtype : int64 >>> s . sum () 14 By default, the sum of an empty or all-NA Series is 0 . >>> pd . Series ([], dtype = \"float64\" ) . sum () # min_count=0 is the default 0.0 This can be controlled with the min_count parameter. For example, if you'd like the sum of an empty series to be NaN, pass min_count=1 . >>> pd . Series ([], dtype = \"float64\" ) . sum ( min_count = 1 ) nan Thanks to the skipna parameter, min_count handles all-NA and empty series identically. >>> pd . Series ([ np . nan ]) . sum () 0.0 >>> pd . Series ([ np . nan ]) . sum ( min_count = 1 ) nan method","title":"pandas.core.frame.DataFrame.sum"},{"location":"api/pipen.channel/#pandascoreframedataframeprod","text":"</> Return the product of the values over the requested axis. Parameters axis ({index (0), columns (1)}) \u2014 Axis for the function to be applied on.For Series this parameter is unused and defaults to 0. .. warning:: The behavior of DataFrame.prod with ``axis=None`` is deprecated, in a future version this will reduce over both axes and return a scalar To retain the old behavior, pass axis=0 (or do not pass axis). .. versionadded:: 2.0.0 skipna (bool, default True) \u2014 Exclude NA/null values when computing the result. numeric_only (bool, default False) \u2014 Include only float, int, boolean columns. Not implemented for Series. min_count (int, default 0) \u2014 The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. **kwargs \u2014 Additional keyword arguments to be passed to the function. See Also Series.sum : Return the sum.Series.min : Return the minimum. Series.max : Return the maximum. Series.idxmin : Return the index of the minimum. Series.idxmax : Return the index of the maximum. DataFrame.sum : Return the sum over the requested axis. DataFrame.min : Return the minimum over the requested axis. DataFrame.max : Return the maximum over the requested axis. DataFrame.idxmin : Return the index of the minimum over the requested axis. DataFrame.idxmax : Return the index of the maximum over the requested axis. Examples By default, the product of an empty or all-NA Series is 1 >>> pd . Series ([], dtype = \"float64\" ) . prod () 1.0 This can be controlled with the min_count parameter >>> pd . Series ([], dtype = \"float64\" ) . prod ( min_count = 1 ) nan Thanks to the skipna parameter, min_count handles all-NA and empty series identically. >>> pd . Series ([ np . nan ]) . prod () 1.0 >>> pd . Series ([ np . nan ]) . prod ( min_count = 1 ) nan method","title":"pandas.core.frame.DataFrame.prod"},{"location":"api/pipen.channel/#pandascoreframedataframemean","text":"</> Return the mean of the values over the requested axis. Parameters axis ({index (0), columns (1)}) \u2014 Axis for the function to be applied on.For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. .. versionadded:: 2.0.0 skipna (bool, default True) \u2014 Exclude NA/null values when computing the result. numeric_only (bool, default False) \u2014 Include only float, int, boolean columns. Not implemented for Series. **kwargs \u2014 Additional keyword arguments to be passed to the function. Returns (Series or scalar)","title":"pandas.core.frame.DataFrame.mean"},{"location":"api/pipen.channel/#s","text":") ) 0 e ) f b 2 3 ) 5 5 4 1 ) 5 5 4 d . , ) ) 5 4 method","title":"s"},{"location":"api/pipen.channel/#pandascoreframedataframemedian","text":"</> Return the median of the values over the requested axis. Parameters axis ({index (0), columns (1)}) \u2014 Axis for the function to be applied on.For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. .. versionadded:: 2.0.0 skipna (bool, default True) \u2014 Exclude NA/null values when computing the result. numeric_only (bool, default False) \u2014 Include only float, int, boolean columns. Not implemented for Series. **kwargs \u2014 Additional keyword arguments to be passed to the function. Returns (Series or scalar)","title":"pandas.core.frame.DataFrame.median"},{"location":"api/pipen.channel/#s_1","text":") ) 0 e ) f b 2 3 ) 5 5 4 1 ) 5 5 4 ` . , ) ) 5 4 method","title":"s"},{"location":"api/pipen.channel/#pandascoreframedataframesem","text":"</> Return unbiased standard error of the mean over requested axis. Normalized by N-1 by default. This can be changed using the ddof argument Parameters axis ({index (0), columns (1)}) \u2014 For Series this parameter is unused and defaults to 0. .. warning:: The behavior of DataFrame.sem with ``axis=None`` is deprecated, in a future version this will reduce over both axes and return a scalar To retain the old behavior, pass axis=0 (or do not pass axis). skipna (bool, default True) \u2014 Exclude NA/null values. If an entire row/column is NA, the resultwill be NA. ddof (int, default 1) \u2014 Delta Degrees of Freedom. The divisor used in calculations is N - ddof,where N represents the number of elements. numeric_only (bool, default False) \u2014 Include only float, int, boolean columns. Not implemented for Series. Returns (Series or DataFrame (if level specified))","title":"pandas.core.frame.DataFrame.sem"},{"location":"api/pipen.channel/#s_2","text":") ) 5 e ) f b 2 3 ) 5 5 4 1 ) 5 5 4 ` . , ) ) 5 4 method","title":"s"},{"location":"api/pipen.channel/#pandascoreframedataframevar","text":"</> Return unbiased variance over requested axis. Normalized by N-1 by default. This can be changed using the ddof argument. Parameters axis ({index (0), columns (1)}) \u2014 For Series this parameter is unused and defaults to 0. .. warning:: The behavior of DataFrame.var with ``axis=None`` is deprecated, in a future version this will reduce over both axes and return a scalar To retain the old behavior, pass axis=0 (or do not pass axis). skipna (bool, default True) \u2014 Exclude NA/null values. If an entire row/column is NA, the resultwill be NA. ddof (int, default 1) \u2014 Delta Degrees of Freedom. The divisor used in calculations is N - ddof,where N represents the number of elements. numeric_only (bool, default False) \u2014 Include only float, int, boolean columns. Not implemented for Series. Examples >>> df = pd . DataFrame ({ 'person_id' : [ 0 , 1 , 2 , 3 ], ... 'age' : [ 21 , 25 , 62 , 43 ], ... 'height' : [ 1.61 , 1.87 , 1.49 , 2.01 ]} ... ) . set_index ( 'person_id' ) >>> df age height person_id 0 21 1.61 1 25 1.87 2 62 1.49 3 43 2.01 >>> df . var () age 352.916667 height 0.056367 dtype : float64 Alternatively, ddof=0 can be set to normalize by N instead of N-1: >>> df . var ( ddof = 0 ) age 264.687500 height 0.042275 dtype : float64 method","title":"pandas.core.frame.DataFrame.var"},{"location":"api/pipen.channel/#pandascoreframedataframestd","text":"</> Return sample standard deviation over requested axis. Normalized by N-1 by default. This can be changed using the ddof argument. Parameters axis ({index (0), columns (1)}) \u2014 For Series this parameter is unused and defaults to 0. .. warning:: The behavior of DataFrame.std with ``axis=None`` is deprecated, in a future version this will reduce over both axes and return a scalar To retain the old behavior, pass axis=0 (or do not pass axis). skipna (bool, default True) \u2014 Exclude NA/null values. If an entire row/column is NA, the resultwill be NA. ddof (int, default 1) \u2014 Delta Degrees of Freedom. The divisor used in calculations is N - ddof,where N represents the number of elements. numeric_only (bool, default False) \u2014 Include only float, int, boolean columns. Not implemented for Series. Notes To have the same behaviour as numpy.std , use ddof=0 (instead of the default ddof=1 ) Examples >>> df = pd . DataFrame ({ 'person_id' : [ 0 , 1 , 2 , 3 ], ... 'age' : [ 21 , 25 , 62 , 43 ], ... 'height' : [ 1.61 , 1.87 , 1.49 , 2.01 ]} ... ) . set_index ( 'person_id' ) >>> df age height person_id 0 21 1.61 1 25 1.87 2 62 1.49 3 43 2.01 The standard deviation of the columns can be found as follows: >>> df . std () age 18.786076 height 0.237417 dtype : float64 Alternatively, ddof=0 can be set to normalize by N instead of N-1: >>> df . std ( ddof = 0 ) age 16.269219 height 0.205609 dtype : float64 method","title":"pandas.core.frame.DataFrame.std"},{"location":"api/pipen.channel/#pandascoreframedataframeskew","text":"</> Return unbiased skew over requested axis. Normalized by N-1. Parameters axis ({index (0), columns (1)}) \u2014 Axis for the function to be applied on.For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. .. versionadded:: 2.0.0 skipna (bool, default True) \u2014 Exclude NA/null values when computing the result. numeric_only (bool, default False) \u2014 Include only float, int, boolean columns. Not implemented for Series. **kwargs \u2014 Additional keyword arguments to be passed to the function. Returns (Series or scalar)","title":"pandas.core.frame.DataFrame.skew"},{"location":"api/pipen.channel/#s_3","text":") ) 0 e , ) f c 1 3 5 ) 0 0 0 4 1 ) 1 1 0 4 d . , ) ) 0 4 method","title":"s"},{"location":"api/pipen.channel/#pandascoreframedataframekurt","text":"</> Return unbiased kurtosis over requested axis. Kurtosis obtained using Fisher's definition of kurtosis (kurtosis of normal == 0.0). Normalized by N-1. Parameters axis ({index (0), columns (1)}) \u2014 Axis for the function to be applied on.For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. .. versionadded:: 2.0.0 skipna (bool, default True) \u2014 Exclude NA/null values when computing the result. numeric_only (bool, default False) \u2014 Include only float, int, boolean columns. Not implemented for Series. **kwargs \u2014 Additional keyword arguments to be passed to the function. Returns (Series or scalar)","title":"pandas.core.frame.DataFrame.kurt"},{"location":"api/pipen.channel/#s_4","text":") s 1 2 2 3 4 ) 5 e , ) f b 3 4 4 4 ) 5 0 4 e ) 3 1 , ) ) 0 0 4 method","title":"s"},{"location":"api/pipen.channel/#pandascoreframedataframecummin","text":"</> Return cumulative minimum over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative minimum. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The index or the name of the axis. 0 is equivalent to None or 'index'.For Series this parameter is unused and defaults to 0. skipna (bool, default True) \u2014 Exclude NA/null values. If an entire row/column is NA, the resultwill be NA. Returns (Series or DataFrame) Return cumulative minimum of Series or DataFrame. See Also core.window.expanding.Expanding.min : Similar functionality but ignores NaN values. DataFrame.min : Return the minimum over DataFrame axis. DataFrame.cummax : Return cumulative maximum over DataFrame axis. DataFrame.cummin : Return cumulative minimum over DataFrame axis. DataFrame.cumsum : Return cumulative sum over DataFrame axis. DataFrame.cumprod : Return cumulative product over DataFrame axis. Examples Series >>> s = pd . Series ([ 2 , np . nan , 5 , - 1 , 0 ]) >>> s 0 2.0 1 NaN 2 5.0 3 - 1.0 4 0.0 dtype : float64 By default, NA values are ignored. >>> s . cummin () 0 2.0 1 NaN 2 2.0 3 - 1.0 4 - 1.0 dtype : float64 To include NA values in the operation, use skipna=False >>> s . cummin ( skipna = False ) 0 2.0 1 NaN 2 NaN 3 NaN 4 NaN dtype : float64 DataFrame >>> df = pd . DataFrame ([[ 2.0 , 1.0 ], ... [ 3.0 , np . nan ], ... [ 1.0 , 0.0 ]], ... columns = list ( 'AB' )) >>> df A B 0 2.0 1.0 1 3.0 NaN 2 1.0 0.0 By default, iterates over rows and finds the minimum in each column. This is equivalent to axis=None or axis='index' . >>> df . cummin () A B 0 2.0 1.0 1 2.0 NaN 2 1.0 0.0 To iterate over columns and find the minimum in each row, use axis=1 >>> df . cummin ( axis = 1 ) A B 0 2.0 1.0 1 3.0 NaN 2 1.0 0.0 method","title":"pandas.core.frame.DataFrame.cummin"},{"location":"api/pipen.channel/#pandascoreframedataframecummax","text":"</> Return cumulative maximum over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative maximum. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The index or the name of the axis. 0 is equivalent to None or 'index'.For Series this parameter is unused and defaults to 0. skipna (bool, default True) \u2014 Exclude NA/null values. If an entire row/column is NA, the resultwill be NA. Returns (Series or DataFrame) Return cumulative maximum of Series or DataFrame. See Also core.window.expanding.Expanding.max : Similar functionality but ignores NaN values. DataFrame.max : Return the maximum over DataFrame axis. DataFrame.cummax : Return cumulative maximum over DataFrame axis. DataFrame.cummin : Return cumulative minimum over DataFrame axis. DataFrame.cumsum : Return cumulative sum over DataFrame axis. DataFrame.cumprod : Return cumulative product over DataFrame axis. Examples Series >>> s = pd . Series ([ 2 , np . nan , 5 , - 1 , 0 ]) >>> s 0 2.0 1 NaN 2 5.0 3 - 1.0 4 0.0 dtype : float64 By default, NA values are ignored. >>> s . cummax () 0 2.0 1 NaN 2 5.0 3 5.0 4 5.0 dtype : float64 To include NA values in the operation, use skipna=False >>> s . cummax ( skipna = False ) 0 2.0 1 NaN 2 NaN 3 NaN 4 NaN dtype : float64 DataFrame >>> df = pd . DataFrame ([[ 2.0 , 1.0 ], ... [ 3.0 , np . nan ], ... [ 1.0 , 0.0 ]], ... columns = list ( 'AB' )) >>> df A B 0 2.0 1.0 1 3.0 NaN 2 1.0 0.0 By default, iterates over rows and finds the maximum in each column. This is equivalent to axis=None or axis='index' . >>> df . cummax () A B 0 2.0 1.0 1 3.0 NaN 2 3.0 1.0 To iterate over columns and find the maximum in each row, use axis=1 >>> df . cummax ( axis = 1 ) A B 0 2.0 2.0 1 3.0 NaN 2 1.0 1.0 method","title":"pandas.core.frame.DataFrame.cummax"},{"location":"api/pipen.channel/#pandascoreframedataframecumsum","text":"</> Return cumulative sum over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative sum. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The index or the name of the axis. 0 is equivalent to None or 'index'.For Series this parameter is unused and defaults to 0. skipna (bool, default True) \u2014 Exclude NA/null values. If an entire row/column is NA, the resultwill be NA. Returns (Series or DataFrame) Return cumulative sum of Series or DataFrame. See Also core.window.expanding.Expanding.sum : Similar functionality but ignores NaN values. DataFrame.sum : Return the sum over DataFrame axis. DataFrame.cummax : Return cumulative maximum over DataFrame axis. DataFrame.cummin : Return cumulative minimum over DataFrame axis. DataFrame.cumsum : Return cumulative sum over DataFrame axis. DataFrame.cumprod : Return cumulative product over DataFrame axis. Examples Series >>> s = pd . Series ([ 2 , np . nan , 5 , - 1 , 0 ]) >>> s 0 2.0 1 NaN 2 5.0 3 - 1.0 4 0.0 dtype : float64 By default, NA values are ignored. >>> s . cumsum () 0 2.0 1 NaN 2 7.0 3 6.0 4 6.0 dtype : float64 To include NA values in the operation, use skipna=False >>> s . cumsum ( skipna = False ) 0 2.0 1 NaN 2 NaN 3 NaN 4 NaN dtype : float64 DataFrame >>> df = pd . DataFrame ([[ 2.0 , 1.0 ], ... [ 3.0 , np . nan ], ... [ 1.0 , 0.0 ]], ... columns = list ( 'AB' )) >>> df A B 0 2.0 1.0 1 3.0 NaN 2 1.0 0.0 By default, iterates over rows and finds the sum in each column. This is equivalent to axis=None or axis='index' . >>> df . cumsum () A B 0 2.0 1.0 1 5.0 NaN 2 6.0 1.0 To iterate over columns and find the sum in each row, use axis=1 >>> df . cumsum ( axis = 1 ) A B 0 2.0 3.0 1 3.0 NaN 2 1.0 1.0 method","title":"pandas.core.frame.DataFrame.cumsum"},{"location":"api/pipen.channel/#pandascoreframedataframecumprod","text":"</> Return cumulative product over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative product. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The index or the name of the axis. 0 is equivalent to None or 'index'.For Series this parameter is unused and defaults to 0. skipna (bool, default True) \u2014 Exclude NA/null values. If an entire row/column is NA, the resultwill be NA. Returns (Series or DataFrame) Return cumulative product of Series or DataFrame. See Also core.window.expanding.Expanding.prod : Similar functionality but ignores NaN values. DataFrame.prod : Return the product over DataFrame axis. DataFrame.cummax : Return cumulative maximum over DataFrame axis. DataFrame.cummin : Return cumulative minimum over DataFrame axis. DataFrame.cumsum : Return cumulative sum over DataFrame axis. DataFrame.cumprod : Return cumulative product over DataFrame axis. Examples Series >>> s = pd . Series ([ 2 , np . nan , 5 , - 1 , 0 ]) >>> s 0 2.0 1 NaN 2 5.0 3 - 1.0 4 0.0 dtype : float64 By default, NA values are ignored. >>> s . cumprod () 0 2.0 1 NaN 2 10.0 3 - 10.0 4 - 0.0 dtype : float64 To include NA values in the operation, use skipna=False >>> s . cumprod ( skipna = False ) 0 2.0 1 NaN 2 NaN 3 NaN 4 NaN dtype : float64 DataFrame >>> df = pd . DataFrame ([[ 2.0 , 1.0 ], ... [ 3.0 , np . nan ], ... [ 1.0 , 0.0 ]], ... columns = list ( 'AB' )) >>> df A B 0 2.0 1.0 1 3.0 NaN 2 1.0 0.0 By default, iterates over rows and finds the product in each column. This is equivalent to axis=None or axis='index' . >>> df . cumprod () A B 0 2.0 1.0 1 6.0 NaN 2 6.0 0.0 To iterate over columns and find the product in each row, use axis=1 >>> df . cumprod ( axis = 1 ) A B 0 2.0 2.0 1 3.0 NaN 2 1.0 0.0 method","title":"pandas.core.frame.DataFrame.cumprod"},{"location":"api/pipen.channel/#pandascoreframedataframenunique","text":"</> Count number of distinct elements in specified axis. Return Series with number of distinct elements. Can ignore NaN values. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to use. 0 or 'index' for row-wise, 1 or 'columns' forcolumn-wise. dropna (bool, default True) \u2014 Don't include NaN in the counts. See Also Series.nunique: Method nunique for Series.DataFrame.count: Count non-NA cells for each column or row. Examples >>> df = pd . DataFrame ({ 'A' : [ 4 , 5 , 6 ], 'B' : [ 4 , 1 , 1 ]}) >>> df . nunique () A 3 B 2 dtype : int64 >>> df . nunique ( axis = 1 ) 0 1 1 2 2 2 dtype : int64 method","title":"pandas.core.frame.DataFrame.nunique"},{"location":"api/pipen.channel/#pandascoreframedataframeidxmin","text":"</> Return index of first occurrence of minimum over requested axis. NA/null values are excluded. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise. skipna (bool, default True) \u2014 Exclude NA/null values. If an entire row/column is NA, the resultwill be NA. numeric_only (bool, default False) \u2014 Include only float , int or boolean data. .. versionadded:: 1.5.0 Returns (Series) Indexes of minima along the specified axis. Raises ValueError \u2014 If the row/column is empty See Also Series.idxmin : Return index of the minimum element. Notes This method is the DataFrame version of ndarray.argmin . Examples Consider a dataset containing food consumption in Argentina. >>> df = pd . DataFrame ({ 'consumption' : [ 10.51 , 103.11 , 55.48 ], ... 'co2_emissions' : [ 37.2 , 19.66 , 1712 ]}, ... index = [ 'Pork' , 'Wheat Products' , 'Beef' ]) >>> df consumption co2_emissions Pork 10.51 37.20 Wheat Products 103.11 19.66 Beef 55.48 1712.00 By default, it returns the index for the minimum value in each column. >>> df . idxmin () consumption Pork co2_emissions Wheat Products dtype : object To return the index for the minimum value in each row, use axis=\"columns\" . >>> df . idxmin ( axis = \"columns\" ) Pork consumption Wheat Products co2_emissions Beef consumption dtype : object method","title":"pandas.core.frame.DataFrame.idxmin"},{"location":"api/pipen.channel/#pandascoreframedataframeidxmax","text":"</> Return index of first occurrence of maximum over requested axis. NA/null values are excluded. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise. skipna (bool, default True) \u2014 Exclude NA/null values. If an entire row/column is NA, the resultwill be NA. numeric_only (bool, default False) \u2014 Include only float , int or boolean data. .. versionadded:: 1.5.0 Returns (Series) Indexes of maxima along the specified axis. Raises ValueError \u2014 If the row/column is empty See Also Series.idxmax : Return index of the maximum element. Notes This method is the DataFrame version of ndarray.argmax . Examples Consider a dataset containing food consumption in Argentina. >>> df = pd . DataFrame ({ 'consumption' : [ 10.51 , 103.11 , 55.48 ], ... 'co2_emissions' : [ 37.2 , 19.66 , 1712 ]}, ... index = [ 'Pork' , 'Wheat Products' , 'Beef' ]) >>> df consumption co2_emissions Pork 10.51 37.20 Wheat Products 103.11 19.66 Beef 55.48 1712.00 By default, it returns the index for the maximum value in each column. >>> df . idxmax () consumption Wheat Products co2_emissions Beef dtype : object To return the index for the maximum value in each row, use axis=\"columns\" . >>> df . idxmax ( axis = \"columns\" ) Pork co2_emissions Wheat Products consumption Beef co2_emissions dtype : object method","title":"pandas.core.frame.DataFrame.idxmax"},{"location":"api/pipen.channel/#pandascoreframedataframemode","text":"</> Get the mode(s) of each element along the selected axis. The mode of a set of values is the value that appears most often. It can be multiple values. Parameters axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to iterate over while searching for the mode: 0 or 'index' : get mode of each column 1 or 'columns' : get mode of each row. numeric_only (bool, default False) \u2014 If True, only apply to numeric columns. dropna (bool, default True) \u2014 Don't consider counts of NaN/NaT. Returns (DataFrame) The modes of each column or row. See Also Series.mode : Return the highest frequency value in a Series.Series.value_counts : Return the counts of values in a Series. Examples >>> df = pd . DataFrame ([( 'bird' , 2 , 2 ), ... ( 'mammal' , 4 , np . nan ), ... ( 'arthropod' , 8 , 0 ), ... ( 'bird' , 2 , np . nan )], ... index = ( 'falcon' , 'horse' , 'spider' , 'ostrich' ), ... columns = ( 'species' , 'legs' , 'wings' )) >>> df species legs wings falcon bird 2 2.0 horse mammal 4 NaN spider arthropod 8 0.0 ostrich bird 2 NaN By default, missing values are not considered, and the mode of wings are both 0 and 2. Because the resulting DataFrame has two rows, the second row of species and legs contains NaN . >>> df . mode () species legs wings 0 bird 2.0 0.0 1 NaN NaN 2.0 Setting dropna=False NaN values are considered and they can be the mode (like for wings). >>> df . mode ( dropna = False ) species legs wings 0 bird 2 NaN Setting numeric_only=True , only the mode of numeric columns is computed, and columns of other types are ignored. >>> df . mode ( numeric_only = True ) legs wings 0 2.0 0.0 1 NaN 2.0 To compute the mode over columns and not rows, use the axis parameter: >>> df . mode ( axis = 'columns' , numeric_only = True ) 0 1 falcon 2.0 NaN horse 4.0 NaN spider 0.0 8.0 ostrich 2.0 NaN method","title":"pandas.core.frame.DataFrame.mode"},{"location":"api/pipen.channel/#pandascoreframedataframequantile","text":"</> Return values at the given quantile over requested axis. Parameters q (float or array-like, default 0.5 (50% quantile)) \u2014 Value between 0 <= q <= 1, the quantile(s) to compute. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 Equals 0 or 'index' for row-wise, 1 or 'columns' for column-wise. numeric_only (bool, default False) \u2014 Include only float , int or boolean data. .. versionchanged:: 2.0.0 The default value of numeric_only is now False . interpolation ({'linear', 'lower', 'higher', 'midpoint', 'nearest'}) \u2014 This optional parameter specifies the interpolation method to use,when the desired quantile lies between two data points i and j : linear: i + (j - i) * fraction , where fraction is the fractional part of the index surrounded by i and j . lower: i . higher: j . nearest: i or j whichever is nearest. midpoint: ( i + j ) / 2. method ({'single', 'table'}, default 'single') \u2014 Whether to compute quantiles per-column ('single') or over all columns('table'). When 'table', the only allowed interpolation methods are 'nearest', 'lower', and 'higher'. Returns (Series or DataFrame) ee . e . See Also core.window.rolling.Rolling.quantile: Rolling quantile.numpy.percentile: Numpy function to compute the percentile. Examples >>> df = pd . DataFrame ( np . array ([[ 1 , 1 ], [ 2 , 10 ], [ 3 , 100 ], [ 4 , 100 ]]), ... columns = [ 'a' , 'b' ]) >>> df . quantile ( .1 ) a 1.3 b 3.7 Name : 0.1 , dtype : float64 >>> df . quantile ([ .1 , .5 ]) a b 0.1 1.3 3.7 0.5 2.5 55.0 Specifying method='table' will compute the quantile over all columns. >>> df . quantile ( .1 , method = \"table\" , interpolation = \"nearest\" ) a 1 b 1 Name : 0.1 , dtype : int64 >>> df . quantile ([ .1 , .5 ], method = \"table\" , interpolation = \"nearest\" ) a b 0.1 1 1 0.5 3 100 Specifying numeric_only=False will also compute the quantile of datetime and timedelta data. >>> df = pd . DataFrame ({ 'A' : [ 1 , 2 ], ... 'B' : [ pd . Timestamp ( '2010' ), ... pd . Timestamp ( '2011' )], ... 'C' : [ pd . Timedelta ( '1 days' ), ... pd . Timedelta ( '2 days' )]}) >>> df . quantile ( 0.5 , numeric_only = False ) A 1.5 B 2010 - 07 - 02 12 : 00 : 00 C 1 days 12 : 00 : 00 Name : 0.5 , dtype : object method","title":"pandas.core.frame.DataFrame.quantile"},{"location":"api/pipen.channel/#pandascoreframedataframeto_timestamp","text":"</> Cast to DatetimeIndex of timestamps, at beginning of period. Parameters freq (str, default frequency of PeriodIndex) \u2014 Desired frequency. how ({'s', 'e', 'start', 'end'}) \u2014 Convention for converting period to timestamp; start of periodvs. end. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to convert (the index by default). copy (bool, default True) \u2014 If False then underlying input data is not copied. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` Returns (DataFrame) The DataFrame has a DatetimeIndex. Examples >>> idx = pd . PeriodIndex ([ '2023' , '2024' ], freq = 'Y' ) >>> d = { 'col1' : [ 1 , 2 ], 'col2' : [ 3 , 4 ]} >>> df1 = pd . DataFrame ( data = d , index = idx ) >>> df1 col1 col2 2023 1 3 2024 2 4 The resulting timestamps will be at the beginning of the year in this case >>> df1 = df1 . to_timestamp () >>> df1 col1 col2 2023 - 01 - 01 1 3 2024 - 01 - 01 2 4 >>> df1 . index DatetimeIndex ([ '2023-01-01' , '2024-01-01' ], dtype = 'datetime64[ns]' , freq = None ) Using freq which is the offset that the Timestamps will have >>> df2 = pd . DataFrame ( data = d , index = idx ) >>> df2 = df2 . to_timestamp ( freq = 'M' ) >>> df2 col1 col2 2023 - 01 - 31 1 3 2024 - 01 - 31 2 4 >>> df2 . index DatetimeIndex ([ '2023-01-31' , '2024-01-31' ], dtype = 'datetime64[ns]' , freq = None ) method","title":"pandas.core.frame.DataFrame.to_timestamp"},{"location":"api/pipen.channel/#pandascoreframedataframeto_period","text":"</> Convert DataFrame from DatetimeIndex to PeriodIndex. Convert DataFrame from DatetimeIndex to PeriodIndex with desired frequency (inferred from index if not passed). Parameters freq (str, default) \u2014 Frequency of the PeriodIndex. axis ({0 or 'index', 1 or 'columns'}, default 0) \u2014 The axis to convert (the index by default). copy (bool, default True) \u2014 If False then underlying input data is not copied. .. note:: The copy keyword will change behavior in pandas 3.0. Copy-on-Write <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html> __ will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write ``pd.options.mode.copy_on_write = True`` Returns (DataFrame) The DataFrame has a PeriodIndex. Examples >>> idx = pd . to_datetime ( ... [ ... \"2001-03-31 00:00:00\" , ... \"2002-05-31 00:00:00\" , ... \"2003-08-31 00:00:00\" , ... ] ... ) >>> idx DatetimeIndex ([ '2001-03-31' , '2002-05-31' , '2003-08-31' ], dtype = 'datetime64[ns]' , freq = None ) >>> idx . to_period ( \"M\" ) PeriodIndex ([ '2001-03' , '2002-05' , '2003-08' ], dtype = 'period[M]' ) For the yearly frequency >>> idx . to_period ( \"Y\" ) PeriodIndex ([ '2001' , '2002' , '2003' ], dtype = 'period[Y-DEC]' ) method","title":"pandas.core.frame.DataFrame.to_period"},{"location":"api/pipen.channel/#pandascoreframedataframeisin","text":"</> Whether each element in the DataFrame is contained in values. Parameters values (iterable, Series, DataFrame or dict) \u2014 The result will only be true at a location if all thelabels match. If values is a Series, that's the index. If values is a dict, the keys must be the column names, which must match. If values is a DataFrame, then both the index and column labels must match. Returns (DataFrame) DataFrame of booleans showing whether each element in the DataFrameis contained in values. See Also DataFrame.eq: Equality test for DataFrame.Series.isin: Equivalent method on Series. Series.str.contains: Test if pattern or regex is contained within a string of a Series or Index. Examples >>> df = pd . DataFrame ({ 'num_legs' : [ 2 , 4 ], 'num_wings' : [ 2 , 0 ]}, ... index = [ 'falcon' , 'dog' ]) >>> df num_legs num_wings falcon 2 2 dog 4 0 When values is a list check whether every value in the DataFrame is present in the list (which animals have 0 or 2 legs or wings) >>> df . isin ([ 0 , 2 ]) num_legs num_wings falcon True True dog False True To check if values is not in the DataFrame, use the ~ operator: >>> ~ df . isin ([ 0 , 2 ]) num_legs num_wings falcon False False dog True False When values is a dict, we can pass values to check for each column separately: >>> df . isin ({ 'num_wings' : [ 0 , 3 ]}) num_legs num_wings falcon False False dog False True When values is a Series or DataFrame the index and column must match. Note that 'falcon' does not match based on the number of legs in other. >>> other = pd . DataFrame ({ 'num_legs' : [ 8 , 3 ], 'num_wings' : [ 0 , 2 ]}, ... index = [ 'spider' , 'falcon' ]) >>> df . isin ( other ) num_legs num_wings falcon False True dog False False classmethod","title":"pandas.core.frame.DataFrame.isin"},{"location":"api/pipen.channel/#pipenchannelchannelcreate","text":"</> Create a channel from a list. The second dimension is identified by tuple. if all elements are tuple, then a channel is created directly. Otherwise, elements are converted to tuples first and channels are created then. Examples >>> Channel . create ([ 1 , 2 , 3 ]) # 3 rows, 1 column >>> Channel . create ([( 1 , 2 , 3 )]) # 1 row, 3 columns Parameters value (Union) \u2014 The value to create a channel Returns (DataFrame) A channel (dataframe) classmethod","title":"pipen.channel.Channel.create"},{"location":"api/pipen.channel/#pipenchannelchannelfrom_glob","text":"</> Create a channel with a glob pattern Parameters ftype (str, optional) \u2014 The file type, one of any, link, dir and file sortby (str, optional) \u2014 How the files should be sorted. One of name, mtime and size reverse (bool, optional) \u2014 Whether sort them in a reversed way. Returns (DataFrame) The channel classmethod","title":"pipen.channel.Channel.from_glob"},{"location":"api/pipen.channel/#pipenchannelchannela_from_glob","text":"</> Create a channel with a glob pattern asynchronously Parameters pattern (str) \u2014 The glob pattern, supported: \"dir1/dir2/*.txt\" ftype (str, optional) \u2014 The file type, one of any, link, dir and file sortby (str, optional) \u2014 How the files should be sorted. One of name, mtime and size reverse (bool, optional) \u2014 Whether sort them in a reversed way. Returns (DataFrame) The channel classmethod","title":"pipen.channel.Channel.a_from_glob"},{"location":"api/pipen.channel/#pipenchannelchannelfrom_pairs","text":"</> Create a width=2 channel with a glob pattern Parameters ftype (str, optional) \u2014 The file type, one of any, link, dir and file sortby (str, optional) \u2014 How the files should be sorted. One of name, mtime and size reverse (bool, optional) \u2014 Whether sort them in a reversed way. Returns (DataFrame) The channel classmethod","title":"pipen.channel.Channel.from_pairs"},{"location":"api/pipen.channel/#pipenchannelchannela_from_pairs","text":"</> Create a width=2 channel with a glob pattern Parameters ftype (str, optional) \u2014 The file type, one of any, link, dir and file sortby (str, optional) \u2014 How the files should be sorted. One of name, mtime and size reverse (bool, optional) \u2014 Whether sort them in a reversed way. Returns (DataFrame) The channel classmethod","title":"pipen.channel.Channel.a_from_pairs"},{"location":"api/pipen.channel/#pipenchannelchannelfrom_csv","text":"</> Create a channel from a csv file Uses pandas.read_csv() to create a channel Parameters *args \u2014 and **kwargs \u2014 Arguments passing to pandas.read_csv() classmethod","title":"pipen.channel.Channel.from_csv"},{"location":"api/pipen.channel/#pipenchannelchannelfrom_excel","text":"</> Create a channel from an excel file. Uses pandas.read_excel() to create a channel Parameters *args \u2014 and **kwargs \u2014 Arguments passing to pandas.read_excel() classmethod","title":"pipen.channel.Channel.from_excel"},{"location":"api/pipen.channel/#pipenchannelchannelfrom_table","text":"</> Create a channel from a table file. Uses pandas.read_table() to create a channel Parameters *args \u2014 and **kwargs \u2014 Arguments passing to pandas.read_table() function","title":"pipen.channel.Channel.from_table"},{"location":"api/pipen.channel/#pipenchannelexpand_dir","text":"</> Expand a Channel according to the files in ,other cols will keep the same. This is only applicable to a 1-row channel. Examples >>> ch = channel . create ([( './' , 1 )]) >>> ch >> expand () >>> [[ './a' , 1 ], [ './b' , 1 ], [ './c' , 1 ]] Parameters col (str | int, optional) \u2014 the index or name of the column used to expand pattern (str, optional) \u2014 use a pattern to filter the files/dirs, default: * ftype (str, optional) \u2014 the type of the files/dirs to include - 'dir', 'file', 'link' or 'any' (default) sortby (str, optional) \u2014 how the list is sorted - 'name' (default), 'mtime', 'size' reverse (bool, optional) \u2014 reverse sort. Returns (DataFrame) The expanded channel function","title":"pipen.channel.expand_dir"},{"location":"api/pipen.channel/#pipenchannelcollapse_files","text":"</> Collapse a Channel according to the files in ,other cols will use the values in row 0. Note that other values in other rows will be discarded. Examples >>> ch = channel . create ([[ './a' , 1 ], [ './b' , 1 ], [ './c' , 1 ]]) >>> ch >> collapse () >>> [[ '.' , 1 ]] Parameters data (DataFrame) \u2014 The original channel col (str | int, optional) \u2014 the index or name of the column used to collapse on Returns (DataFrame) The collapsed channel","title":"pipen.channel.collapse_files"},{"location":"api/pipen.cli.help/","text":"module pipen.cli. help </> Print help for commands Classes CLIHelpPlugin \u2014 Print help for commands </> class pipen.cli.help. CLIHelpPlugin ( parser , subparser ) </> Bases pipen.cli._hooks.CLIPlugin Print help for commands Methods exec_command ( args ) \u2014 Run the command </> parse_args ( known_parsed , unparsed_argv ) (Namespace) \u2014 Define arguments for the command </> method parse_args ( known_parsed , unparsed_argv ) \u2192 Namespace </> Define arguments for the command method exec_command ( args ) </> Run the command","title":"pipen.cli.help"},{"location":"api/pipen.cli.help/#pipenclihelp","text":"</> Print help for commands Classes CLIHelpPlugin \u2014 Print help for commands </> class","title":"pipen.cli.help"},{"location":"api/pipen.cli.help/#pipenclihelpclihelpplugin","text":"</> Bases pipen.cli._hooks.CLIPlugin Print help for commands Methods exec_command ( args ) \u2014 Run the command </> parse_args ( known_parsed , unparsed_argv ) (Namespace) \u2014 Define arguments for the command </> method","title":"pipen.cli.help.CLIHelpPlugin"},{"location":"api/pipen.cli.help/#pipencli_hooksclipluginparse_args","text":"</> Define arguments for the command method","title":"pipen.cli._hooks.CLIPlugin.parse_args"},{"location":"api/pipen.cli.help/#pipenclihelpclihelppluginexec_command","text":"</> Run the command","title":"pipen.cli.help.CLIHelpPlugin.exec_command"},{"location":"api/pipen.cli/","text":"package pipen. cli </> Provide CLI for pipen module pipen.cli. profile </> List available profiles. Classes CLIProfilePlugin \u2014 List available profiles. </> module pipen.cli. version </> Print help for commands Classes CLIVersionPlugin \u2014 Print versions of pipen and its dependencies </> module pipen.cli. help </> Print help for commands Classes CLIHelpPlugin \u2014 Print help for commands </> module pipen.cli. plugins </> List plugins Classes CliPluginsPlugin \u2014 List installed plugins </>","title":"pipen.cli"},{"location":"api/pipen.cli/#pipencli","text":"</> Provide CLI for pipen module","title":"pipen.cli"},{"location":"api/pipen.cli/#pipencliprofile","text":"</> List available profiles. Classes CLIProfilePlugin \u2014 List available profiles. </> module","title":"pipen.cli.profile"},{"location":"api/pipen.cli/#pipencliversion","text":"</> Print help for commands Classes CLIVersionPlugin \u2014 Print versions of pipen and its dependencies </> module","title":"pipen.cli.version"},{"location":"api/pipen.cli/#pipenclihelp","text":"</> Print help for commands Classes CLIHelpPlugin \u2014 Print help for commands </> module","title":"pipen.cli.help"},{"location":"api/pipen.cli/#pipencliplugins","text":"</> List plugins Classes CliPluginsPlugin \u2014 List installed plugins </>","title":"pipen.cli.plugins"},{"location":"api/pipen.cli.plugins/","text":"module pipen.cli. plugins </> List plugins Classes CliPluginsPlugin \u2014 List installed plugins </> class pipen.cli.plugins. CliPluginsPlugin ( parser , subparser ) </> Bases pipen.cli._hooks.CLIPlugin List installed plugins Methods exec_command ( args ) \u2014 Execute the command </> parse_args ( known_parsed , unparsed_argv ) (Namespace) \u2014 Define arguments for the command </> method parse_args ( known_parsed , unparsed_argv ) \u2192 Namespace </> Define arguments for the command method exec_command ( args ) </> Execute the command","title":"pipen.cli.plugins"},{"location":"api/pipen.cli.plugins/#pipencliplugins","text":"</> List plugins Classes CliPluginsPlugin \u2014 List installed plugins </> class","title":"pipen.cli.plugins"},{"location":"api/pipen.cli.plugins/#pipenclipluginsclipluginsplugin","text":"</> Bases pipen.cli._hooks.CLIPlugin List installed plugins Methods exec_command ( args ) \u2014 Execute the command </> parse_args ( known_parsed , unparsed_argv ) (Namespace) \u2014 Define arguments for the command </> method","title":"pipen.cli.plugins.CliPluginsPlugin"},{"location":"api/pipen.cli.plugins/#pipencli_hooksclipluginparse_args","text":"</> Define arguments for the command method","title":"pipen.cli._hooks.CLIPlugin.parse_args"},{"location":"api/pipen.cli.plugins/#pipenclipluginsclipluginspluginexec_command","text":"</> Execute the command","title":"pipen.cli.plugins.CliPluginsPlugin.exec_command"},{"location":"api/pipen.cli.profile/","text":"module pipen.cli. profile </> List available profiles. Classes CLIProfilePlugin \u2014 List available profiles. </> class pipen.cli.profile. CLIProfilePlugin ( parser , subparser ) </> Bases pipen.cli._hooks.AsyncCLIPlugin List available profiles. Methods exec_command ( args ) \u2014 Run the command </> parse_args ( known_parsed , unparsed_argv ) (Namespace) \u2014 Define arguments for the command </> post_init ( ) \u2014 Async post init function called after all plugins are loaded </> method post_init ( ) </> Async post init function called after all plugins are loaded method parse_args ( known_parsed , unparsed_argv ) \u2192 Namespace </> Define arguments for the command method exec_command ( args ) </> Run the command","title":"pipen.cli.profile"},{"location":"api/pipen.cli.profile/#pipencliprofile","text":"</> List available profiles. Classes CLIProfilePlugin \u2014 List available profiles. </> class","title":"pipen.cli.profile"},{"location":"api/pipen.cli.profile/#pipencliprofilecliprofileplugin","text":"</> Bases pipen.cli._hooks.AsyncCLIPlugin List available profiles. Methods exec_command ( args ) \u2014 Run the command </> parse_args ( known_parsed , unparsed_argv ) (Namespace) \u2014 Define arguments for the command </> post_init ( ) \u2014 Async post init function called after all plugins are loaded </> method","title":"pipen.cli.profile.CLIProfilePlugin"},{"location":"api/pipen.cli.profile/#pipencli_hooksasyncclipluginpost_init","text":"</> Async post init function called after all plugins are loaded method","title":"pipen.cli._hooks.AsyncCLIPlugin.post_init"},{"location":"api/pipen.cli.profile/#pipencli_hooksasyncclipluginparse_args","text":"</> Define arguments for the command method","title":"pipen.cli._hooks.AsyncCLIPlugin.parse_args"},{"location":"api/pipen.cli.profile/#pipencliprofilecliprofilepluginexec_command","text":"</> Run the command","title":"pipen.cli.profile.CLIProfilePlugin.exec_command"},{"location":"api/pipen.cli.version/","text":"module pipen.cli. version </> Print help for commands Classes CLIVersionPlugin \u2014 Print versions of pipen and its dependencies </> class pipen.cli.version. CLIVersionPlugin ( parser , subparser ) </> Bases pipen.cli._hooks.CLIPlugin Print versions of pipen and its dependencies Methods exec_command ( args ) \u2014 Run the command </> parse_args ( known_parsed , unparsed_argv ) (Namespace) \u2014 Define arguments for the command </> method parse_args ( known_parsed , unparsed_argv ) \u2192 Namespace </> Define arguments for the command method exec_command ( args ) </> Run the command","title":"pipen.cli.version"},{"location":"api/pipen.cli.version/#pipencliversion","text":"</> Print help for commands Classes CLIVersionPlugin \u2014 Print versions of pipen and its dependencies </> class","title":"pipen.cli.version"},{"location":"api/pipen.cli.version/#pipencliversioncliversionplugin","text":"</> Bases pipen.cli._hooks.CLIPlugin Print versions of pipen and its dependencies Methods exec_command ( args ) \u2014 Run the command </> parse_args ( known_parsed , unparsed_argv ) (Namespace) \u2014 Define arguments for the command </> method","title":"pipen.cli.version.CLIVersionPlugin"},{"location":"api/pipen.cli.version/#pipencli_hooksclipluginparse_args","text":"</> Define arguments for the command method","title":"pipen.cli._hooks.CLIPlugin.parse_args"},{"location":"api/pipen.cli.version/#pipencliversioncliversionpluginexec_command","text":"</> Run the command","title":"pipen.cli.version.CLIVersionPlugin.exec_command"},{"location":"api/pipen.defaults/","text":"module pipen. defaults </> Provide some default values/objects Classes ProcInputType \u2014 Types for process inputs </> ProcOutputType \u2014 Types for process outputs </> class pipen.defaults. ProcInputType ( ) </> Types for process inputs class pipen.defaults. ProcOutputType ( ) </> Types for process outputs","title":"pipen.defaults"},{"location":"api/pipen.defaults/#pipendefaults","text":"</> Provide some default values/objects Classes ProcInputType \u2014 Types for process inputs </> ProcOutputType \u2014 Types for process outputs </> class","title":"pipen.defaults"},{"location":"api/pipen.defaults/#pipendefaultsprocinputtype","text":"</> Types for process inputs class","title":"pipen.defaults.ProcInputType"},{"location":"api/pipen.defaults/#pipendefaultsprocoutputtype","text":"</> Types for process outputs","title":"pipen.defaults.ProcOutputType"},{"location":"api/pipen.exceptions/","text":"module pipen. exceptions </> Provide exception classes Classes PipenException \u2014 Base exception class for pipen </> PipenSetDataError \u2014 When trying to set input data to processes with input_data already setusing Pipen.set_data(). </> ProcInputTypeError \u2014 When an unsupported input type is provided </> ProcInputKeyError \u2014 When an unsupported input key is provided </> ProcInputValueError \u2014 When an unsupported input value is provided </> ProcScriptFileNotFound \u2014 When script file specified as 'file://' cannot be found </> ProcOutputNameError \u2014 When no name or malformatted output is provided </> ProcOutputTypeError \u2014 When an unsupported output type is provided </> ProcOutputValueError \u2014 When a malformatted output value is provided </> ProcDependencyError \u2014 When there is something wrong the process dependencies </> NoSuchSchedulerError \u2014 When specified scheduler cannot be found </> WrongSchedulerTypeError \u2014 When specified scheduler is not a subclass of Scheduler </> NoSuchTemplateEngineError \u2014 When specified template engine cannot be found </> WrongTemplateEngineTypeError \u2014 When specified tempalte engine is not a subclass of Scheduler </> TemplateRenderingError \u2014 Failed to render a template </> ConfigurationError \u2014 When something wrong set as configuration </> PipenOrProcNameError \u2014 \"When more than one processes are sharing the same workdir </> class pipen.exceptions. PipenException ( ) </> Bases Exception BaseException Base exception class for pipen class pipen.exceptions. PipenSetDataError ( ) </> Bases pipen.exceptions.PipenException ValueError Exception BaseException When trying to set input data to processes with input_data already setusing Pipen.set_data(). class pipen.exceptions. ProcInputTypeError ( ) </> Bases pipen.exceptions.PipenException TypeError Exception BaseException When an unsupported input type is provided class pipen.exceptions. ProcInputKeyError ( ) </> Bases pipen.exceptions.PipenException KeyError LookupError Exception BaseException When an unsupported input key is provided class pipen.exceptions. ProcInputValueError ( ) </> Bases pipen.exceptions.PipenException ValueError Exception BaseException When an unsupported input value is provided class pipen.exceptions. ProcScriptFileNotFound ( ) </> Bases pipen.exceptions.PipenException FileNotFoundError OSError Exception BaseException When script file specified as 'file://' cannot be found class pipen.exceptions. ProcOutputNameError ( ) </> Bases pipen.exceptions.PipenException NameError Exception BaseException When no name or malformatted output is provided class pipen.exceptions. ProcOutputTypeError ( ) </> Bases pipen.exceptions.PipenException TypeError Exception BaseException When an unsupported output type is provided class pipen.exceptions. ProcOutputValueError ( ) </> Bases pipen.exceptions.PipenException ValueError Exception BaseException When a malformatted output value is provided class pipen.exceptions. ProcDependencyError ( ) </> Bases pipen.exceptions.PipenException Exception BaseException When there is something wrong the process dependencies class pipen.exceptions. NoSuchSchedulerError ( ) </> Bases pipen.exceptions.PipenException Exception BaseException When specified scheduler cannot be found class pipen.exceptions. WrongSchedulerTypeError ( ) </> Bases pipen.exceptions.PipenException TypeError Exception BaseException When specified scheduler is not a subclass of Scheduler class pipen.exceptions. NoSuchTemplateEngineError ( ) </> Bases pipen.exceptions.PipenException Exception BaseException When specified template engine cannot be found class pipen.exceptions. WrongTemplateEngineTypeError ( ) </> Bases pipen.exceptions.PipenException TypeError Exception BaseException When specified tempalte engine is not a subclass of Scheduler class pipen.exceptions. TemplateRenderingError ( ) </> Bases pipen.exceptions.PipenException Exception BaseException Failed to render a template class pipen.exceptions. ConfigurationError ( ) </> Bases pipen.exceptions.PipenException Exception BaseException When something wrong set as configuration class pipen.exceptions. PipenOrProcNameError ( ) </> Bases pipen.exceptions.PipenException Exception BaseException \"When more than one processes are sharing the same workdir","title":"pipen.exceptions"},{"location":"api/pipen.exceptions/#pipenexceptions","text":"</> Provide exception classes Classes PipenException \u2014 Base exception class for pipen </> PipenSetDataError \u2014 When trying to set input data to processes with input_data already setusing Pipen.set_data(). </> ProcInputTypeError \u2014 When an unsupported input type is provided </> ProcInputKeyError \u2014 When an unsupported input key is provided </> ProcInputValueError \u2014 When an unsupported input value is provided </> ProcScriptFileNotFound \u2014 When script file specified as 'file://' cannot be found </> ProcOutputNameError \u2014 When no name or malformatted output is provided </> ProcOutputTypeError \u2014 When an unsupported output type is provided </> ProcOutputValueError \u2014 When a malformatted output value is provided </> ProcDependencyError \u2014 When there is something wrong the process dependencies </> NoSuchSchedulerError \u2014 When specified scheduler cannot be found </> WrongSchedulerTypeError \u2014 When specified scheduler is not a subclass of Scheduler </> NoSuchTemplateEngineError \u2014 When specified template engine cannot be found </> WrongTemplateEngineTypeError \u2014 When specified tempalte engine is not a subclass of Scheduler </> TemplateRenderingError \u2014 Failed to render a template </> ConfigurationError \u2014 When something wrong set as configuration </> PipenOrProcNameError \u2014 \"When more than one processes are sharing the same workdir </> class","title":"pipen.exceptions"},{"location":"api/pipen.exceptions/#pipenexceptionspipenexception","text":"</> Bases Exception BaseException Base exception class for pipen class","title":"pipen.exceptions.PipenException"},{"location":"api/pipen.exceptions/#pipenexceptionspipensetdataerror","text":"</> Bases pipen.exceptions.PipenException ValueError Exception BaseException When trying to set input data to processes with input_data already setusing Pipen.set_data(). class","title":"pipen.exceptions.PipenSetDataError"},{"location":"api/pipen.exceptions/#pipenexceptionsprocinputtypeerror","text":"</> Bases pipen.exceptions.PipenException TypeError Exception BaseException When an unsupported input type is provided class","title":"pipen.exceptions.ProcInputTypeError"},{"location":"api/pipen.exceptions/#pipenexceptionsprocinputkeyerror","text":"</> Bases pipen.exceptions.PipenException KeyError LookupError Exception BaseException When an unsupported input key is provided class","title":"pipen.exceptions.ProcInputKeyError"},{"location":"api/pipen.exceptions/#pipenexceptionsprocinputvalueerror","text":"</> Bases pipen.exceptions.PipenException ValueError Exception BaseException When an unsupported input value is provided class","title":"pipen.exceptions.ProcInputValueError"},{"location":"api/pipen.exceptions/#pipenexceptionsprocscriptfilenotfound","text":"</> Bases pipen.exceptions.PipenException FileNotFoundError OSError Exception BaseException When script file specified as 'file://' cannot be found class","title":"pipen.exceptions.ProcScriptFileNotFound"},{"location":"api/pipen.exceptions/#pipenexceptionsprocoutputnameerror","text":"</> Bases pipen.exceptions.PipenException NameError Exception BaseException When no name or malformatted output is provided class","title":"pipen.exceptions.ProcOutputNameError"},{"location":"api/pipen.exceptions/#pipenexceptionsprocoutputtypeerror","text":"</> Bases pipen.exceptions.PipenException TypeError Exception BaseException When an unsupported output type is provided class","title":"pipen.exceptions.ProcOutputTypeError"},{"location":"api/pipen.exceptions/#pipenexceptionsprocoutputvalueerror","text":"</> Bases pipen.exceptions.PipenException ValueError Exception BaseException When a malformatted output value is provided class","title":"pipen.exceptions.ProcOutputValueError"},{"location":"api/pipen.exceptions/#pipenexceptionsprocdependencyerror","text":"</> Bases pipen.exceptions.PipenException Exception BaseException When there is something wrong the process dependencies class","title":"pipen.exceptions.ProcDependencyError"},{"location":"api/pipen.exceptions/#pipenexceptionsnosuchschedulererror","text":"</> Bases pipen.exceptions.PipenException Exception BaseException When specified scheduler cannot be found class","title":"pipen.exceptions.NoSuchSchedulerError"},{"location":"api/pipen.exceptions/#pipenexceptionswrongschedulertypeerror","text":"</> Bases pipen.exceptions.PipenException TypeError Exception BaseException When specified scheduler is not a subclass of Scheduler class","title":"pipen.exceptions.WrongSchedulerTypeError"},{"location":"api/pipen.exceptions/#pipenexceptionsnosuchtemplateengineerror","text":"</> Bases pipen.exceptions.PipenException Exception BaseException When specified template engine cannot be found class","title":"pipen.exceptions.NoSuchTemplateEngineError"},{"location":"api/pipen.exceptions/#pipenexceptionswrongtemplateenginetypeerror","text":"</> Bases pipen.exceptions.PipenException TypeError Exception BaseException When specified tempalte engine is not a subclass of Scheduler class","title":"pipen.exceptions.WrongTemplateEngineTypeError"},{"location":"api/pipen.exceptions/#pipenexceptionstemplaterenderingerror","text":"</> Bases pipen.exceptions.PipenException Exception BaseException Failed to render a template class","title":"pipen.exceptions.TemplateRenderingError"},{"location":"api/pipen.exceptions/#pipenexceptionsconfigurationerror","text":"</> Bases pipen.exceptions.PipenException Exception BaseException When something wrong set as configuration class","title":"pipen.exceptions.ConfigurationError"},{"location":"api/pipen.exceptions/#pipenexceptionspipenorprocnameerror","text":"</> Bases pipen.exceptions.PipenException Exception BaseException \"When more than one processes are sharing the same workdir","title":"pipen.exceptions.PipenOrProcNameError"},{"location":"api/pipen.job/","text":"module pipen. job </> Provide the Job class Classes Job \u2014 The job for pipen </> class pipen.job. Job ( *args , **kwargs ) </> Bases xqute.job.Job pipen._job_caching.JobCaching The job for pipen Attributes cached \u2014 Check if a job is cached </> jid_file (SpecPath) \u2014 The jid file of the job </> rc_file (SpecPath) \u2014 The rc file of the job </> retry_dir (SpecPath) \u2014 The retry directory of the job </> script_file \u2014 Get the path to script file </> signature_file \u2014 Get the path to the signature file </> status_file (SpecPath) \u2014 The status file of the job </> stderr_file (SpecPath) \u2014 The stderr file of the job </> stdout_file (SpecPath) \u2014 The stdout file of the job </> Methods __repr__ ( ) (str) \u2014 repr of the job </> cache ( ) \u2014 write signature to signature file </> clean ( retry ) \u2014 Clean up the meta files </> get_jid ( ) (int | str | none) \u2014 Get the jid of the job in scheduler system </> get_rc ( ) (int) \u2014 The return code of the job </> get_status ( refresh ) (int) \u2014 Query the status of the job </> log ( level , msg , *args , limit , limit_indicator , logger ) \u2014 Log message for the jobs </> prepare ( proc ) \u2014 Prepare the job by given process </> prepare_outdir ( ) (SpecPath) \u2014 Get the path to the output directory. </> prepare_output ( ) \u2014 Get the output data of the job </> set_rc ( rc ) \u2014 Set the return code of the job </> set_status ( stat , flush ) \u2014 Set the status manually </> method cache ( ) </> write signature to signature file method __repr__ ( ) \u2192 str </> repr of the job method get_jid ( ) \u2192 int | str | none </> Get the jid of the job in scheduler system method get_status ( refresh=False ) \u2192 int </> Query the status of the job If the job is submitted, try to query it from the status file Make sure the status is updated by trap in wrapped script Uses caching to avoid excessive file I/O. Cache is invalidated when status is explicitly set. Parameters refresh (bool, optional) \u2014 Whether to refresh the status from file method set_status ( stat , flush=True ) </> Set the status manually Parameters stat (int) \u2014 The status to set flush (bool, optional) \u2014 Whether to flush the status to file method get_rc ( ) \u2192 int </> The return code of the job method set_rc ( rc ) </> Set the return code of the job Parameters rc (int | str) \u2014 The return code method clean ( retry=False ) </> Clean up the meta files Parameters retry (bool, optional) \u2014 Whether clean it for retrying method prepare ( proc ) </> Prepare the job by given process Primarily prepare the script, and provide cmd to the job for xqute to wrap and run Parameters proc (Proc) \u2014 the process object method prepare_outdir ( ) </> Get the path to the output directory. When proc.export is True, the output directory is based on the pipeline.outdir and the process name. Otherwise, it is based on the metadir. When the job is running in a detached system (a VM, typically), this will return the mounted path to the output directory. To access the real path, use self.outdir Returns (SpecPath) The path to the job output directory method prepare_output ( ) </> Get the output data of the job Returns The key-value map where the keys are the output keys method log ( level , msg , *args , limit=3 , limit_indicator=True , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the jobs Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message limit (int, optional) \u2014 limitation of the log (don't log for all jobs) limit_indicator (bool, optional) \u2014 Whether to show an indicator saying the loghas been limited (the level of the indicator will be DEBUG) logger (LoggerAdapter, optional) \u2014 The logger used to log","title":"pipen.job"},{"location":"api/pipen.job/#pipenjob","text":"</> Provide the Job class Classes Job \u2014 The job for pipen </> class","title":"pipen.job"},{"location":"api/pipen.job/#pipenjobjob","text":"</> Bases xqute.job.Job pipen._job_caching.JobCaching The job for pipen Attributes cached \u2014 Check if a job is cached </> jid_file (SpecPath) \u2014 The jid file of the job </> rc_file (SpecPath) \u2014 The rc file of the job </> retry_dir (SpecPath) \u2014 The retry directory of the job </> script_file \u2014 Get the path to script file </> signature_file \u2014 Get the path to the signature file </> status_file (SpecPath) \u2014 The status file of the job </> stderr_file (SpecPath) \u2014 The stderr file of the job </> stdout_file (SpecPath) \u2014 The stdout file of the job </> Methods __repr__ ( ) (str) \u2014 repr of the job </> cache ( ) \u2014 write signature to signature file </> clean ( retry ) \u2014 Clean up the meta files </> get_jid ( ) (int | str | none) \u2014 Get the jid of the job in scheduler system </> get_rc ( ) (int) \u2014 The return code of the job </> get_status ( refresh ) (int) \u2014 Query the status of the job </> log ( level , msg , *args , limit , limit_indicator , logger ) \u2014 Log message for the jobs </> prepare ( proc ) \u2014 Prepare the job by given process </> prepare_outdir ( ) (SpecPath) \u2014 Get the path to the output directory. </> prepare_output ( ) \u2014 Get the output data of the job </> set_rc ( rc ) \u2014 Set the return code of the job </> set_status ( stat , flush ) \u2014 Set the status manually </> method","title":"pipen.job.Job"},{"location":"api/pipen.job/#pipen_job_cachingjobcachingcache","text":"</> write signature to signature file method","title":"pipen._job_caching.JobCaching.cache"},{"location":"api/pipen.job/#xqutejobjobrepr","text":"</> repr of the job method","title":"xqute.job.Job.repr"},{"location":"api/pipen.job/#xqutejobjobget_jid","text":"</> Get the jid of the job in scheduler system method","title":"xqute.job.Job.get_jid"},{"location":"api/pipen.job/#xqutejobjobget_status","text":"</> Query the status of the job If the job is submitted, try to query it from the status file Make sure the status is updated by trap in wrapped script Uses caching to avoid excessive file I/O. Cache is invalidated when status is explicitly set. Parameters refresh (bool, optional) \u2014 Whether to refresh the status from file method","title":"xqute.job.Job.get_status"},{"location":"api/pipen.job/#xqutejobjobset_status","text":"</> Set the status manually Parameters stat (int) \u2014 The status to set flush (bool, optional) \u2014 Whether to flush the status to file method","title":"xqute.job.Job.set_status"},{"location":"api/pipen.job/#xqutejobjobget_rc","text":"</> The return code of the job method","title":"xqute.job.Job.get_rc"},{"location":"api/pipen.job/#xqutejobjobset_rc","text":"</> Set the return code of the job Parameters rc (int | str) \u2014 The return code method","title":"xqute.job.Job.set_rc"},{"location":"api/pipen.job/#xqutejobjobclean","text":"</> Clean up the meta files Parameters retry (bool, optional) \u2014 Whether clean it for retrying method","title":"xqute.job.Job.clean"},{"location":"api/pipen.job/#pipenjobjobprepare","text":"</> Prepare the job by given process Primarily prepare the script, and provide cmd to the job for xqute to wrap and run Parameters proc (Proc) \u2014 the process object method","title":"pipen.job.Job.prepare"},{"location":"api/pipen.job/#pipenjobjobprepare_outdir","text":"</> Get the path to the output directory. When proc.export is True, the output directory is based on the pipeline.outdir and the process name. Otherwise, it is based on the metadir. When the job is running in a detached system (a VM, typically), this will return the mounted path to the output directory. To access the real path, use self.outdir Returns (SpecPath) The path to the job output directory method","title":"pipen.job.Job.prepare_outdir"},{"location":"api/pipen.job/#pipenjobjobprepare_output","text":"</> Get the output data of the job Returns The key-value map where the keys are the output keys method","title":"pipen.job.Job.prepare_output"},{"location":"api/pipen.job/#pipenjobjoblog","text":"</> Log message for the jobs Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message limit (int, optional) \u2014 limitation of the log (don't log for all jobs) limit_indicator (bool, optional) \u2014 Whether to show an indicator saying the loghas been limited (the level of the indicator will be DEBUG) logger (LoggerAdapter, optional) \u2014 The logger used to log","title":"pipen.job.Job.log"},{"location":"api/pipen/","text":"package pipen </> A pipeline framework for python module pipen. pluginmgr </> Define hooks specifications and provide plugin manager Classes PipenMainPlugin \u2014 The builtin core plugin, used to update the progress bar andcache the job </> XqutePipenPlugin \u2014 The plugin for xqute working as proxy for pipen plugin hooks </> Functions on_complete ( pipen , succeeded ) \u2014 The the pipeline is completed. </> on_init ( pipen ) \u2014 When the pipeline is initialized, and default configs are loaded </> on_job_cached ( job ) \u2014 When a job is cached. </> on_job_failed ( job ) \u2014 When a job is done but failed. </> on_job_init ( job ) \u2014 When a job is initialized </> on_job_killed ( job ) \u2014 When a job is killed </> on_job_killing ( job ) (bool) \u2014 When a job is being killed. </> on_job_polling ( job , counter ) \u2014 When status of a job is being polled. </> on_job_queued ( job ) \u2014 When a job is queued in xqute. Note it might not be queued yet inthe scheduler system. </> on_job_started ( job ) \u2014 When a job starts to run in then scheduler system. </> on_job_submitted ( job ) \u2014 When a job is submitted in the scheduler system. </> on_job_submitting ( job ) (bool) \u2014 When a job is submitting. </> on_job_succeeded ( job ) \u2014 When a job completes successfully. </> on_jobcmd_end ( job ) (str) \u2014 When the job command finishes and after the postscript is run </> on_jobcmd_init ( job ) (str) \u2014 When the job command wrapper script is initialized before the prescript is run </> on_jobcmd_prep ( job ) (str) \u2014 When the job command right about to be run </> on_proc_create ( proc ) \u2014 Called Proc constructor when a process is created. </> on_proc_done ( proc , succeeded ) \u2014 When a process is done </> on_proc_input_computed ( proc ) \u2014 Called after process input data is computed. </> on_proc_script_computed ( proc ) \u2014 Called after process script is computed. </> on_proc_shutdown ( proc , sig ) \u2014 When pipeline is shutting down, by Ctrl-c for example. </> on_proc_start ( proc ) \u2014 When a process is starting </> on_setup ( pipen ) \u2014 Setup for plugins, primarily used for the plugins tosetup some default configurations. </> on_start ( pipen ) \u2014 Right before the pipeline starts running. </> module pipen. utils </> Provide some utilities Classes RichHandler \u2014 Subclass of rich.logging.RichHandler, showing log levels as a singlecharacter </> RichConsole \u2014 A high level console interface. </> Functions brief_list ( blist ) (str) \u2014 Briefly show an integer list, combine the continuous numbers. </> copy_dict ( dic , depth ) (Mapping) \u2014 Deep copy a dict </> desc_from_docstring ( obj , base ) (str) \u2014 Get the description from docstring </> get_base ( klass , abc_base , value , value_getter ) (Type) \u2014 Get the base class where the value was first defined </> get_logger ( name , level ) (LoggerAdapter) \u2014 Get the logger by given plugin name </> get_logpanel_width ( ) (int) \u2014 Get the width of the log content </> get_marked ( cls , mark_name , default ) (Any) \u2014 Get the marked value from a proc </> get_mtime ( path , dir_depth ) (float) \u2014 Get the modification time of a path.If path is a directory, try to get the last modification time of the contents in the directory at given dir_depth </> get_shebang ( script ) (str) \u2014 Get the shebang of the script </> ignore_firstline_dedent ( text ) (str) \u2014 Like textwrap.dedent(), but ignore first empty lines </> is_loading_pipeline ( *flags , argv ) (bool) \u2014 Check if we are loading the pipeline. Works only when argv0 is \"@pipen\" while loading the pipeline. </> is_subclass ( obj , cls ) (bool) \u2014 Tell if obj is a subclass of clsDifferences with issubclass is that we don't raise Type error if obj is not a class </> is_valid_name ( name ) (bool) \u2014 Check if a name is valid for a proc or pipen </> load_entrypoints ( group ) (Iterable) \u2014 Load objects from setuptools entrypoints by given group name </> load_pipeline ( obj , argv0 , argv1p , **kwargs ) (Pipen) \u2014 Load a pipeline from a Pipen, Proc or ProcGroup object </> log_rich_renderable ( renderable , color , logfunc , *args , **kwargs ) \u2014 Log a rich renderable to logger </> make_df_colnames_unique_inplace ( thedf ) \u2014 Make the columns of a data frame unique </> mark ( **kwargs ) (Callable) \u2014 Mark a class (e.g. Proc) with given kwargs as metadata </> path_is_symlink ( path ) (bool) \u2014 Check if a path is a symlink. </> path_is_symlink_sync ( path ) (bool) \u2014 Check if a path is a symlink synchronously. </> path_symlink_to ( src , dst , target_is_directory ) \u2014 Create a symbolic link pointing to src named dst. </> pipen_banner ( ) (RenderableType) \u2014 The banner for pipen </> strsplit ( string , sep , maxsplit , trim ) (List) \u2014 Split the string, with the ability to trim each part. </> truncate_text ( text , width , end ) (str) \u2014 Truncate a text not based on words/whitespacesOtherwise, we could use textwrap.shorten. </> update_dict ( parent , new , depth , try_list ) (Mapping) \u2014 Update the new dict to the parent, but make sure parent does not change </> module pipen. proc </> Provides the process class: Proc Classes ProcMeta \u2014 Meta class for Proc </> Proc ( Proc ) \u2014 The abstract class for processes. </> module pipen. version </> Provide version of pipen module pipen. job </> Provide the Job class Classes Job \u2014 The job for pipen </> module pipen. scheduler </> Provide builting schedulers Classes SchedulerPostInit \u2014 Provides post init function for all schedulers </> LocalScheduler \u2014 Local scheduler </> SgeScheduler \u2014 SGE scheduler </> SlurmScheduler \u2014 Slurm scheduler </> SshScheduler \u2014 SSH scheduler </> GbatchScheduler \u2014 Google Cloud Batch scheduler </> ContainerScheduler \u2014 Scheduler to run jobs via containers (Docker/Podman/Apptainer) </> Functions get_scheduler ( scheduler ) (Type) \u2014 Get the scheduler by name of the scheduler class itself </> module pipen. exceptions </> Provide exception classes Classes PipenException \u2014 Base exception class for pipen </> PipenSetDataError \u2014 When trying to set input data to processes with input_data already setusing Pipen.set_data(). </> ProcInputTypeError \u2014 When an unsupported input type is provided </> ProcInputKeyError \u2014 When an unsupported input key is provided </> ProcInputValueError \u2014 When an unsupported input value is provided </> ProcScriptFileNotFound \u2014 When script file specified as 'file://' cannot be found </> ProcOutputNameError \u2014 When no name or malformatted output is provided </> ProcOutputTypeError \u2014 When an unsupported output type is provided </> ProcOutputValueError \u2014 When a malformatted output value is provided </> ProcDependencyError \u2014 When there is something wrong the process dependencies </> NoSuchSchedulerError \u2014 When specified scheduler cannot be found </> WrongSchedulerTypeError \u2014 When specified scheduler is not a subclass of Scheduler </> NoSuchTemplateEngineError \u2014 When specified template engine cannot be found </> WrongTemplateEngineTypeError \u2014 When specified tempalte engine is not a subclass of Scheduler </> TemplateRenderingError \u2014 Failed to render a template </> ConfigurationError \u2014 When something wrong set as configuration </> PipenOrProcNameError \u2014 \"When more than one processes are sharing the same workdir </> module pipen. pipen </> Main entry module, provide the Pipen class Classes Pipen \u2014 The Pipen class provides interface to assemble and run the pipeline </> Functions async_run ( name , starts , data , desc , outdir , profile , **kwargs ) (bool) \u2014 Shortcut to run a pipeline </> run ( name , starts , data , desc , outdir , profile , **kwargs ) (bool) \u2014 Shortcut to run a pipeline </> module pipen. channel </> Provide some function for creating and modifying channels (dataframes) Classes Channel \u2014 A DataFrame wrapper with creators </> Functions collapse_files ( data , col ) (DataFrame) \u2014 Collapse a Channel according to the files in ,other cols will use the values in row 0. </> expand_dir ( data , col , pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Expand a Channel according to the files in ,other cols will keep the same. </> module pipen. progressbar </> Provide the PipelinePBar and ProcPBar classes Classes ProcPBar \u2014 The progress bar for processes </> PipelinePBar \u2014 Progress bar for the pipeline </> module pipen. template </> Template adaptor for pipen Classes Template ( source , **kwargs ) \u2014 Base class wrapper to wrap template for pipen </> TemplateLiquid \u2014 Liquidpy template wrapper. </> TemplateJinja2 \u2014 Jinja2 template wrapper </> Functions get_template_engine ( template ) (Type) \u2014 Get the template engine by name or the template engine itself </> module pipen. procgroup </> Process group that contains a set of processes. It can be easily used to create a pipeline that runs independently or integrated into a larger pipeline. Runs directly: >>> proc_group = ProcGroup ( < options > ) >>> proc_group . as_pipen ( < pipeline options > ) . set_data ( < data > ) . run () Integrated into a larger pipeline >>> proc_group = ProcGroup ( < options > ) >>> # proc could be a process within the larger pipeline >>> proc . requires = prog_group .< proc > To add a process to the proc group, use the add_proc method: >>> class MyProcGroup ( ProcGroup ): >>> ... >>> >>> proc_group = MyProcGroup ( ... ) >>> @proc_group . add_proc >>> class MyProc ( Proc ): >>> ... Or add a process at runtime: >>> class MyProcGroup ( ProcGroup ): >>> ... >>> >>> @ProcGroup . add_proc >>> def my_proc ( self ): >>> class MyProc ( Proc ): >>> # You may use self.options here >>> ... >>> return MyProc >>> proc_group = MyProcGroup ( ... ) Classes ProcGropuMeta \u2014 Meta class for ProcGroup </> ProcGroup \u2014 A group of processes that can be run independently orintegrated into a larger pipeline. </> module pipen. defaults </> Provide some default values/objects Classes ProcInputType \u2014 Types for process inputs </> ProcOutputType \u2014 Types for process outputs </> package pipen. cli </> Provide CLI for pipen","title":"pipen"},{"location":"api/pipen/#pipen","text":"</> A pipeline framework for python module","title":"pipen"},{"location":"api/pipen/#pipenpluginmgr","text":"</> Define hooks specifications and provide plugin manager Classes PipenMainPlugin \u2014 The builtin core plugin, used to update the progress bar andcache the job </> XqutePipenPlugin \u2014 The plugin for xqute working as proxy for pipen plugin hooks </> Functions on_complete ( pipen , succeeded ) \u2014 The the pipeline is completed. </> on_init ( pipen ) \u2014 When the pipeline is initialized, and default configs are loaded </> on_job_cached ( job ) \u2014 When a job is cached. </> on_job_failed ( job ) \u2014 When a job is done but failed. </> on_job_init ( job ) \u2014 When a job is initialized </> on_job_killed ( job ) \u2014 When a job is killed </> on_job_killing ( job ) (bool) \u2014 When a job is being killed. </> on_job_polling ( job , counter ) \u2014 When status of a job is being polled. </> on_job_queued ( job ) \u2014 When a job is queued in xqute. Note it might not be queued yet inthe scheduler system. </> on_job_started ( job ) \u2014 When a job starts to run in then scheduler system. </> on_job_submitted ( job ) \u2014 When a job is submitted in the scheduler system. </> on_job_submitting ( job ) (bool) \u2014 When a job is submitting. </> on_job_succeeded ( job ) \u2014 When a job completes successfully. </> on_jobcmd_end ( job ) (str) \u2014 When the job command finishes and after the postscript is run </> on_jobcmd_init ( job ) (str) \u2014 When the job command wrapper script is initialized before the prescript is run </> on_jobcmd_prep ( job ) (str) \u2014 When the job command right about to be run </> on_proc_create ( proc ) \u2014 Called Proc constructor when a process is created. </> on_proc_done ( proc , succeeded ) \u2014 When a process is done </> on_proc_input_computed ( proc ) \u2014 Called after process input data is computed. </> on_proc_script_computed ( proc ) \u2014 Called after process script is computed. </> on_proc_shutdown ( proc , sig ) \u2014 When pipeline is shutting down, by Ctrl-c for example. </> on_proc_start ( proc ) \u2014 When a process is starting </> on_setup ( pipen ) \u2014 Setup for plugins, primarily used for the plugins tosetup some default configurations. </> on_start ( pipen ) \u2014 Right before the pipeline starts running. </> module","title":"pipen.pluginmgr"},{"location":"api/pipen/#pipenutils","text":"</> Provide some utilities Classes RichHandler \u2014 Subclass of rich.logging.RichHandler, showing log levels as a singlecharacter </> RichConsole \u2014 A high level console interface. </> Functions brief_list ( blist ) (str) \u2014 Briefly show an integer list, combine the continuous numbers. </> copy_dict ( dic , depth ) (Mapping) \u2014 Deep copy a dict </> desc_from_docstring ( obj , base ) (str) \u2014 Get the description from docstring </> get_base ( klass , abc_base , value , value_getter ) (Type) \u2014 Get the base class where the value was first defined </> get_logger ( name , level ) (LoggerAdapter) \u2014 Get the logger by given plugin name </> get_logpanel_width ( ) (int) \u2014 Get the width of the log content </> get_marked ( cls , mark_name , default ) (Any) \u2014 Get the marked value from a proc </> get_mtime ( path , dir_depth ) (float) \u2014 Get the modification time of a path.If path is a directory, try to get the last modification time of the contents in the directory at given dir_depth </> get_shebang ( script ) (str) \u2014 Get the shebang of the script </> ignore_firstline_dedent ( text ) (str) \u2014 Like textwrap.dedent(), but ignore first empty lines </> is_loading_pipeline ( *flags , argv ) (bool) \u2014 Check if we are loading the pipeline. Works only when argv0 is \"@pipen\" while loading the pipeline. </> is_subclass ( obj , cls ) (bool) \u2014 Tell if obj is a subclass of clsDifferences with issubclass is that we don't raise Type error if obj is not a class </> is_valid_name ( name ) (bool) \u2014 Check if a name is valid for a proc or pipen </> load_entrypoints ( group ) (Iterable) \u2014 Load objects from setuptools entrypoints by given group name </> load_pipeline ( obj , argv0 , argv1p , **kwargs ) (Pipen) \u2014 Load a pipeline from a Pipen, Proc or ProcGroup object </> log_rich_renderable ( renderable , color , logfunc , *args , **kwargs ) \u2014 Log a rich renderable to logger </> make_df_colnames_unique_inplace ( thedf ) \u2014 Make the columns of a data frame unique </> mark ( **kwargs ) (Callable) \u2014 Mark a class (e.g. Proc) with given kwargs as metadata </> path_is_symlink ( path ) (bool) \u2014 Check if a path is a symlink. </> path_is_symlink_sync ( path ) (bool) \u2014 Check if a path is a symlink synchronously. </> path_symlink_to ( src , dst , target_is_directory ) \u2014 Create a symbolic link pointing to src named dst. </> pipen_banner ( ) (RenderableType) \u2014 The banner for pipen </> strsplit ( string , sep , maxsplit , trim ) (List) \u2014 Split the string, with the ability to trim each part. </> truncate_text ( text , width , end ) (str) \u2014 Truncate a text not based on words/whitespacesOtherwise, we could use textwrap.shorten. </> update_dict ( parent , new , depth , try_list ) (Mapping) \u2014 Update the new dict to the parent, but make sure parent does not change </> module","title":"pipen.utils"},{"location":"api/pipen/#pipenproc","text":"</> Provides the process class: Proc Classes ProcMeta \u2014 Meta class for Proc </> Proc ( Proc ) \u2014 The abstract class for processes. </> module","title":"pipen.proc"},{"location":"api/pipen/#pipenversion","text":"</> Provide version of pipen module","title":"pipen.version"},{"location":"api/pipen/#pipenjob","text":"</> Provide the Job class Classes Job \u2014 The job for pipen </> module","title":"pipen.job"},{"location":"api/pipen/#pipenscheduler","text":"</> Provide builting schedulers Classes SchedulerPostInit \u2014 Provides post init function for all schedulers </> LocalScheduler \u2014 Local scheduler </> SgeScheduler \u2014 SGE scheduler </> SlurmScheduler \u2014 Slurm scheduler </> SshScheduler \u2014 SSH scheduler </> GbatchScheduler \u2014 Google Cloud Batch scheduler </> ContainerScheduler \u2014 Scheduler to run jobs via containers (Docker/Podman/Apptainer) </> Functions get_scheduler ( scheduler ) (Type) \u2014 Get the scheduler by name of the scheduler class itself </> module","title":"pipen.scheduler"},{"location":"api/pipen/#pipenexceptions","text":"</> Provide exception classes Classes PipenException \u2014 Base exception class for pipen </> PipenSetDataError \u2014 When trying to set input data to processes with input_data already setusing Pipen.set_data(). </> ProcInputTypeError \u2014 When an unsupported input type is provided </> ProcInputKeyError \u2014 When an unsupported input key is provided </> ProcInputValueError \u2014 When an unsupported input value is provided </> ProcScriptFileNotFound \u2014 When script file specified as 'file://' cannot be found </> ProcOutputNameError \u2014 When no name or malformatted output is provided </> ProcOutputTypeError \u2014 When an unsupported output type is provided </> ProcOutputValueError \u2014 When a malformatted output value is provided </> ProcDependencyError \u2014 When there is something wrong the process dependencies </> NoSuchSchedulerError \u2014 When specified scheduler cannot be found </> WrongSchedulerTypeError \u2014 When specified scheduler is not a subclass of Scheduler </> NoSuchTemplateEngineError \u2014 When specified template engine cannot be found </> WrongTemplateEngineTypeError \u2014 When specified tempalte engine is not a subclass of Scheduler </> TemplateRenderingError \u2014 Failed to render a template </> ConfigurationError \u2014 When something wrong set as configuration </> PipenOrProcNameError \u2014 \"When more than one processes are sharing the same workdir </> module","title":"pipen.exceptions"},{"location":"api/pipen/#pipenpipen","text":"</> Main entry module, provide the Pipen class Classes Pipen \u2014 The Pipen class provides interface to assemble and run the pipeline </> Functions async_run ( name , starts , data , desc , outdir , profile , **kwargs ) (bool) \u2014 Shortcut to run a pipeline </> run ( name , starts , data , desc , outdir , profile , **kwargs ) (bool) \u2014 Shortcut to run a pipeline </> module","title":"pipen.pipen"},{"location":"api/pipen/#pipenchannel","text":"</> Provide some function for creating and modifying channels (dataframes) Classes Channel \u2014 A DataFrame wrapper with creators </> Functions collapse_files ( data , col ) (DataFrame) \u2014 Collapse a Channel according to the files in ,other cols will use the values in row 0. </> expand_dir ( data , col , pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Expand a Channel according to the files in ,other cols will keep the same. </> module","title":"pipen.channel"},{"location":"api/pipen/#pipenprogressbar","text":"</> Provide the PipelinePBar and ProcPBar classes Classes ProcPBar \u2014 The progress bar for processes </> PipelinePBar \u2014 Progress bar for the pipeline </> module","title":"pipen.progressbar"},{"location":"api/pipen/#pipentemplate","text":"</> Template adaptor for pipen Classes Template ( source , **kwargs ) \u2014 Base class wrapper to wrap template for pipen </> TemplateLiquid \u2014 Liquidpy template wrapper. </> TemplateJinja2 \u2014 Jinja2 template wrapper </> Functions get_template_engine ( template ) (Type) \u2014 Get the template engine by name or the template engine itself </> module","title":"pipen.template"},{"location":"api/pipen/#pipenprocgroup","text":"</> Process group that contains a set of processes. It can be easily used to create a pipeline that runs independently or integrated into a larger pipeline. Runs directly: >>> proc_group = ProcGroup ( < options > ) >>> proc_group . as_pipen ( < pipeline options > ) . set_data ( < data > ) . run () Integrated into a larger pipeline >>> proc_group = ProcGroup ( < options > ) >>> # proc could be a process within the larger pipeline >>> proc . requires = prog_group .< proc > To add a process to the proc group, use the add_proc method: >>> class MyProcGroup ( ProcGroup ): >>> ... >>> >>> proc_group = MyProcGroup ( ... ) >>> @proc_group . add_proc >>> class MyProc ( Proc ): >>> ... Or add a process at runtime: >>> class MyProcGroup ( ProcGroup ): >>> ... >>> >>> @ProcGroup . add_proc >>> def my_proc ( self ): >>> class MyProc ( Proc ): >>> # You may use self.options here >>> ... >>> return MyProc >>> proc_group = MyProcGroup ( ... ) Classes ProcGropuMeta \u2014 Meta class for ProcGroup </> ProcGroup \u2014 A group of processes that can be run independently orintegrated into a larger pipeline. </> module","title":"pipen.procgroup"},{"location":"api/pipen/#pipendefaults","text":"</> Provide some default values/objects Classes ProcInputType \u2014 Types for process inputs </> ProcOutputType \u2014 Types for process outputs </> package","title":"pipen.defaults"},{"location":"api/pipen/#pipencli","text":"</> Provide CLI for pipen","title":"pipen.cli"},{"location":"api/pipen.pipen/","text":"module pipen. pipen </> Main entry module, provide the Pipen class Classes Pipen \u2014 The Pipen class provides interface to assemble and run the pipeline </> Functions async_run ( name , starts , data , desc , outdir , profile , **kwargs ) (bool) \u2014 Shortcut to run a pipeline </> run ( name , starts , data , desc , outdir , profile , **kwargs ) (bool) \u2014 Shortcut to run a pipeline </> class pipen.pipen. Pipen ( name=None , desc=None , outdir=None , **kwargs ) </> The Pipen class provides interface to assemble and run the pipeline Attributes PIPELINE_COUNT \u2014 How many pipelines are loaded SETUP \u2014 Whether the one-time setup hook is called _kwargs \u2014 The extra configrations passed to overwrite the default ones config \u2014 The configurations desc \u2014 The description of the pipeline name \u2014 The name of the pipeline outdir \u2014 The output directory of the results pbar \u2014 The progress bar procs \u2014 The processes profile \u2014 The profile of the configurations to run the pipeline starts \u2014 The start processes workdir \u2014 The workdir for the pipeline Parameters name (str | none, optional) \u2014 The name of the pipeline desc (str | none, optional) \u2014 The description of the pipeline outdir (str | pathlib.path, optional) \u2014 The output directory of the results **kwargs \u2014 Other configurations Methods __init_subclass__ ( ) \u2014 This method is called when a class is subclassed. </> async_run ( profile ) (bool) \u2014 Run the processes one by one </> build_proc_relationships ( ) \u2014 Build the proc relationships for the pipeline </> run ( profile ) (bool) \u2014 Run the pipeline with the given profileThis is just a sync wrapper for the async async_run function using asyncio.run() </> set_data ( *indata ) ( Pipen ) \u2014 Set the input_data for start processes </> set_starts ( *procs , clear ) \u2014 Set the starts </> classmethod __init_subclass__ ( ) </> This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. method async_run ( profile='default' ) </> Run the processes one by one Parameters profile (str, optional) \u2014 The default profile to use for the run Returns (bool) True if the pipeline ends successfully else False method run ( profile='default' ) </> Run the pipeline with the given profileThis is just a sync wrapper for the async async_run function using asyncio.run() Parameters profile (str, optional) \u2014 The default profile to use for the run Returns (bool) True if the pipeline ends successfully else False method set_data ( *indata ) </> Set the input_data for start processes Parameters *indata (Any) \u2014 The input data for the start processesThe data will set for the processes in the order determined by set_starts() . If a process has input_data set, an error will be raised. To use that input_data, set None here in the corresponding position for the process Raises ProcInputDataError \u2014 When trying to set input data toprocesses with input_data already set Returns ( Pipen ) self to chain the operations method set_starts ( *procs , clear=True ) </> Set the starts Parameters *procs (Union) \u2014 The processes to set as starts of the pipeline. clear (bool, optional) \u2014 Wether to clear previous set starts Raises ProcDependencyError \u2014 When processes set as starts repeatedly Returns self to chain the operations method build_proc_relationships ( ) </> Build the proc relationships for the pipeline function pipen.pipen. async_run ( name , starts , data=None , desc=None , outdir=None , profile='default' , **kwargs ) </> Shortcut to run a pipeline Parameters name (str) \u2014 The name of the pipeline starts (Union) \u2014 The start processes data (Iterable, optional) \u2014 The input data for the start processes desc (str, optional) \u2014 The description of the pipeline outdir (str | pathlib.path | none, optional) \u2014 The output directory of the results profile (str, optional) \u2014 The profile to use **kwargs \u2014 Other options pass to Pipen to create the pipeline Returns (bool) True if the pipeline ends successfully else False function pipen.pipen. run ( name , starts , data=None , desc=None , outdir=None , profile='default' , **kwargs ) </> Shortcut to run a pipeline Parameters name (str) \u2014 The name of the pipeline starts (Union) \u2014 The start processes data (Iterable, optional) \u2014 The input data for the start processes desc (str, optional) \u2014 The description of the pipeline outdir (str | pathlib.path | none, optional) \u2014 The output directory of the results profile (str, optional) \u2014 The profile to use **kwargs \u2014 Other options pass to Pipen to create the pipeline Returns (bool) True if the pipeline ends successfully else False","title":"pipen.pipen"},{"location":"api/pipen.pipen/#pipenpipen","text":"</> Main entry module, provide the Pipen class Classes Pipen \u2014 The Pipen class provides interface to assemble and run the pipeline </> Functions async_run ( name , starts , data , desc , outdir , profile , **kwargs ) (bool) \u2014 Shortcut to run a pipeline </> run ( name , starts , data , desc , outdir , profile , **kwargs ) (bool) \u2014 Shortcut to run a pipeline </> class","title":"pipen.pipen"},{"location":"api/pipen.pipen/#pipenpipenpipen","text":"</> The Pipen class provides interface to assemble and run the pipeline Attributes PIPELINE_COUNT \u2014 How many pipelines are loaded SETUP \u2014 Whether the one-time setup hook is called _kwargs \u2014 The extra configrations passed to overwrite the default ones config \u2014 The configurations desc \u2014 The description of the pipeline name \u2014 The name of the pipeline outdir \u2014 The output directory of the results pbar \u2014 The progress bar procs \u2014 The processes profile \u2014 The profile of the configurations to run the pipeline starts \u2014 The start processes workdir \u2014 The workdir for the pipeline Parameters name (str | none, optional) \u2014 The name of the pipeline desc (str | none, optional) \u2014 The description of the pipeline outdir (str | pathlib.path, optional) \u2014 The output directory of the results **kwargs \u2014 Other configurations Methods __init_subclass__ ( ) \u2014 This method is called when a class is subclassed. </> async_run ( profile ) (bool) \u2014 Run the processes one by one </> build_proc_relationships ( ) \u2014 Build the proc relationships for the pipeline </> run ( profile ) (bool) \u2014 Run the pipeline with the given profileThis is just a sync wrapper for the async async_run function using asyncio.run() </> set_data ( *indata ) ( Pipen ) \u2014 Set the input_data for start processes </> set_starts ( *procs , clear ) \u2014 Set the starts </> classmethod","title":"pipen.pipen.Pipen"},{"location":"api/pipen.pipen/#pipenpipenpipeninit_subclass","text":"</> This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. method","title":"pipen.pipen.Pipen.init_subclass"},{"location":"api/pipen.pipen/#pipenpipenpipenasync_run","text":"</> Run the processes one by one Parameters profile (str, optional) \u2014 The default profile to use for the run Returns (bool) True if the pipeline ends successfully else False method","title":"pipen.pipen.Pipen.async_run"},{"location":"api/pipen.pipen/#pipenpipenpipenrun","text":"</> Run the pipeline with the given profileThis is just a sync wrapper for the async async_run function using asyncio.run() Parameters profile (str, optional) \u2014 The default profile to use for the run Returns (bool) True if the pipeline ends successfully else False method","title":"pipen.pipen.Pipen.run"},{"location":"api/pipen.pipen/#pipenpipenpipenset_data","text":"</> Set the input_data for start processes Parameters *indata (Any) \u2014 The input data for the start processesThe data will set for the processes in the order determined by set_starts() . If a process has input_data set, an error will be raised. To use that input_data, set None here in the corresponding position for the process Raises ProcInputDataError \u2014 When trying to set input data toprocesses with input_data already set Returns ( Pipen ) self to chain the operations method","title":"pipen.pipen.Pipen.set_data"},{"location":"api/pipen.pipen/#pipenpipenpipenset_starts","text":"</> Set the starts Parameters *procs (Union) \u2014 The processes to set as starts of the pipeline. clear (bool, optional) \u2014 Wether to clear previous set starts Raises ProcDependencyError \u2014 When processes set as starts repeatedly Returns self to chain the operations method","title":"pipen.pipen.Pipen.set_starts"},{"location":"api/pipen.pipen/#pipenpipenpipenbuild_proc_relationships","text":"</> Build the proc relationships for the pipeline function","title":"pipen.pipen.Pipen.build_proc_relationships"},{"location":"api/pipen.pipen/#pipenpipenasync_run","text":"</> Shortcut to run a pipeline Parameters name (str) \u2014 The name of the pipeline starts (Union) \u2014 The start processes data (Iterable, optional) \u2014 The input data for the start processes desc (str, optional) \u2014 The description of the pipeline outdir (str | pathlib.path | none, optional) \u2014 The output directory of the results profile (str, optional) \u2014 The profile to use **kwargs \u2014 Other options pass to Pipen to create the pipeline Returns (bool) True if the pipeline ends successfully else False function","title":"pipen.pipen.async_run"},{"location":"api/pipen.pipen/#pipenpipenrun","text":"</> Shortcut to run a pipeline Parameters name (str) \u2014 The name of the pipeline starts (Union) \u2014 The start processes data (Iterable, optional) \u2014 The input data for the start processes desc (str, optional) \u2014 The description of the pipeline outdir (str | pathlib.path | none, optional) \u2014 The output directory of the results profile (str, optional) \u2014 The profile to use **kwargs \u2014 Other options pass to Pipen to create the pipeline Returns (bool) True if the pipeline ends successfully else False","title":"pipen.pipen.run"},{"location":"api/pipen.pluginmgr/","text":"module pipen. pluginmgr </> Define hooks specifications and provide plugin manager Classes PipenMainPlugin \u2014 The builtin core plugin, used to update the progress bar andcache the job </> XqutePipenPlugin \u2014 The plugin for xqute working as proxy for pipen plugin hooks </> Functions on_complete ( pipen , succeeded ) \u2014 The the pipeline is completed. </> on_init ( pipen ) \u2014 When the pipeline is initialized, and default configs are loaded </> on_job_cached ( job ) \u2014 When a job is cached. </> on_job_failed ( job ) \u2014 When a job is done but failed. </> on_job_init ( job ) \u2014 When a job is initialized </> on_job_killed ( job ) \u2014 When a job is killed </> on_job_killing ( job ) (bool) \u2014 When a job is being killed. </> on_job_polling ( job , counter ) \u2014 When status of a job is being polled. </> on_job_queued ( job ) \u2014 When a job is queued in xqute. Note it might not be queued yet inthe scheduler system. </> on_job_started ( job ) \u2014 When a job starts to run in then scheduler system. </> on_job_submitted ( job ) \u2014 When a job is submitted in the scheduler system. </> on_job_submitting ( job ) (bool) \u2014 When a job is submitting. </> on_job_succeeded ( job ) \u2014 When a job completes successfully. </> on_jobcmd_end ( job ) (str) \u2014 When the job command finishes and after the postscript is run </> on_jobcmd_init ( job ) (str) \u2014 When the job command wrapper script is initialized before the prescript is run </> on_jobcmd_prep ( job ) (str) \u2014 When the job command right about to be run </> on_proc_create ( proc ) \u2014 Called Proc constructor when a process is created. </> on_proc_done ( proc , succeeded ) \u2014 When a process is done </> on_proc_input_computed ( proc ) \u2014 Called after process input data is computed. </> on_proc_script_computed ( proc ) \u2014 Called after process script is computed. </> on_proc_shutdown ( proc , sig ) \u2014 When pipeline is shutting down, by Ctrl-c for example. </> on_proc_start ( proc ) \u2014 When a process is starting </> on_setup ( pipen ) \u2014 Setup for plugins, primarily used for the plugins tosetup some default configurations. </> on_start ( pipen ) \u2014 Right before the pipeline starts running. </> function pipen.pluginmgr. on_setup ( pipen ) </> Setup for plugins, primarily used for the plugins tosetup some default configurations. This is only called once for all pipelines. Parameters pipen (Pipen) \u2014 The Pipen object function pipen.pluginmgr. on_init ( pipen ) </> When the pipeline is initialized, and default configs are loaded Parameters pipen (Pipen) \u2014 The Pipen object function pipen.pluginmgr. on_start ( pipen ) </> Right before the pipeline starts running. Process relationships are inferred. Parameters pipen (Pipen) \u2014 The Pipen object function pipen.pluginmgr. on_complete ( pipen , succeeded ) </> The the pipeline is completed. Parameters pipen (Pipen) \u2014 The Pipen object succeeded (bool) \u2014 Whether the pipeline has successfully completed. function pipen.pluginmgr. on_proc_create ( proc ) </> Called Proc constructor when a process is created. Enables plugins to modify the default attributes of processes Parameters proc (Proc) \u2014 The Proc object function pipen.pluginmgr. on_proc_input_computed ( proc ) </> Called after process input data is computed. Parameters proc (Proc) \u2014 The Proc object function pipen.pluginmgr. on_proc_script_computed ( proc ) </> Called after process script is computed. The script is computed as a string that is about to compiled into a template. Parameters proc (Proc) \u2014 The Proc object function pipen.pluginmgr. on_proc_start ( proc ) </> When a process is starting Parameters proc (Proc) \u2014 The process function pipen.pluginmgr. on_proc_shutdown ( proc , sig ) </> When pipeline is shutting down, by Ctrl-c for example. Return False to stop shutting down, but you have to shut it down by yourself, for example, proc.xqute.task.cancel() Only the first return value will be used. Parameters sig (signal.Signals) \u2014 The signal. None means a natural shutdown pipen \u2014 The xqute object function pipen.pluginmgr. on_proc_done ( proc , succeeded ) </> When a process is done Parameters proc (Proc) \u2014 The process succeeded (bool | str) \u2014 Whether the process succeeded or not. 'cached' if all jobsare cached. function pipen.pluginmgr. on_job_init ( job ) </> When a job is initialized Parameters job (Job) \u2014 The job function pipen.pluginmgr. on_job_queued ( job ) </> When a job is queued in xqute. Note it might not be queued yet inthe scheduler system. Parameters job (Job) \u2014 The job function pipen.pluginmgr. on_job_submitting ( job ) </> When a job is submitting. The first plugin (based on priority) have this hook return False will cancel the submission Parameters job (Job) \u2014 The job Returns (bool) False to cancel submission function pipen.pluginmgr. on_job_submitted ( job ) </> When a job is submitted in the scheduler system. Parameters job (Job) \u2014 The job function pipen.pluginmgr. on_job_started ( job ) </> When a job starts to run in then scheduler system. Note that the job might not be running yet in the scheduler system. Parameters job (Job) \u2014 The job function pipen.pluginmgr. on_job_polling ( job , counter ) </> When status of a job is being polled. Parameters job (Job) \u2014 The job function pipen.pluginmgr. on_job_killing ( job ) </> When a job is being killed. The first plugin (based on priority) have this hook return False will cancel the killing Parameters job (Job) \u2014 The job Returns (bool) False to cancel killing function pipen.pluginmgr. on_job_killed ( job ) </> When a job is killed Parameters job (Job) \u2014 The job function pipen.pluginmgr. on_job_succeeded ( job ) </> When a job completes successfully. Parameters job (Job) \u2014 The job function pipen.pluginmgr. on_job_cached ( job ) </> When a job is cached. Parameters job (Job) \u2014 The job function pipen.pluginmgr. on_job_failed ( job ) </> When a job is done but failed. Parameters job (Job) \u2014 The job function pipen.pluginmgr. on_jobcmd_init ( job ) </> When the job command wrapper script is initialized before the prescript is run This should return a piece of bash code to be inserted in the wrapped job script (template), which is a python template string, with the following variables available: status and job . status is the class JobStatus from xqute.defaults.py and job is the Job instance. For multiple plugins, the code will be inserted in the order of the plugin priority. The code will replace the #![jobcmd_init] placeholder in the wrapped job script. See also https://github.com/pwwang/xqute/blob/master/xqute/defaults.py#L95 Parameters job (Job) \u2014 The job object Returns (str) The bash code to be inserted function pipen.pluginmgr. on_jobcmd_prep ( job ) </> When the job command right about to be run This should return a piece of bash code to be inserted in the wrapped job script (template), which is a python template string, with the following variables available: status and job . status is the class JobStatus from xqute.defaults.py and job is the Job instance. The bash variable $cmd is accessible in the context. It is also possible to modify the cmd variable. Just remember to assign the modified value to cmd . For multiple plugins, the code will be inserted in the order of the plugin priority. Keep in mind that the $cmd may be modified by other plugins. The code will replace the #![jobcmd_prep] placeholder in the wrapped job script. See also https://github.com/pwwang/xqute/blob/master/xqute/defaults.py#L95 Parameters job (Job) \u2014 The job object Returns (str) The bash code to be inserted function pipen.pluginmgr. on_jobcmd_end ( job ) </> When the job command finishes and after the postscript is run This should return a piece of bash code to be inserted in the wrapped job script (template), which is a python template string, with the following variables available: status and job . status is the class JobStatus from xqute.defaults.py and job is the Job instance. The bash variable $rc is accessible in the context, which is the return code of the job command. For multiple plugins, the code will be inserted in the order of the plugin priority. The code will replace the #![jobcmd_end] placeholder in the wrapped job script. See also https://github.com/pwwang/xqute/blob/master/xqute/defaults.py#L95 Parameters job (Job) \u2014 The job object Returns (str) The bash code to be inserted class pipen.pluginmgr. PipenMainPlugin ( ) </> The builtin core plugin, used to update the progress bar andcache the job class pipen.pluginmgr. XqutePipenPlugin ( ) </> The plugin for xqute working as proxy for pipen plugin hooks","title":"pipen.pluginmgr"},{"location":"api/pipen.pluginmgr/#pipenpluginmgr","text":"</> Define hooks specifications and provide plugin manager Classes PipenMainPlugin \u2014 The builtin core plugin, used to update the progress bar andcache the job </> XqutePipenPlugin \u2014 The plugin for xqute working as proxy for pipen plugin hooks </> Functions on_complete ( pipen , succeeded ) \u2014 The the pipeline is completed. </> on_init ( pipen ) \u2014 When the pipeline is initialized, and default configs are loaded </> on_job_cached ( job ) \u2014 When a job is cached. </> on_job_failed ( job ) \u2014 When a job is done but failed. </> on_job_init ( job ) \u2014 When a job is initialized </> on_job_killed ( job ) \u2014 When a job is killed </> on_job_killing ( job ) (bool) \u2014 When a job is being killed. </> on_job_polling ( job , counter ) \u2014 When status of a job is being polled. </> on_job_queued ( job ) \u2014 When a job is queued in xqute. Note it might not be queued yet inthe scheduler system. </> on_job_started ( job ) \u2014 When a job starts to run in then scheduler system. </> on_job_submitted ( job ) \u2014 When a job is submitted in the scheduler system. </> on_job_submitting ( job ) (bool) \u2014 When a job is submitting. </> on_job_succeeded ( job ) \u2014 When a job completes successfully. </> on_jobcmd_end ( job ) (str) \u2014 When the job command finishes and after the postscript is run </> on_jobcmd_init ( job ) (str) \u2014 When the job command wrapper script is initialized before the prescript is run </> on_jobcmd_prep ( job ) (str) \u2014 When the job command right about to be run </> on_proc_create ( proc ) \u2014 Called Proc constructor when a process is created. </> on_proc_done ( proc , succeeded ) \u2014 When a process is done </> on_proc_input_computed ( proc ) \u2014 Called after process input data is computed. </> on_proc_script_computed ( proc ) \u2014 Called after process script is computed. </> on_proc_shutdown ( proc , sig ) \u2014 When pipeline is shutting down, by Ctrl-c for example. </> on_proc_start ( proc ) \u2014 When a process is starting </> on_setup ( pipen ) \u2014 Setup for plugins, primarily used for the plugins tosetup some default configurations. </> on_start ( pipen ) \u2014 Right before the pipeline starts running. </> function","title":"pipen.pluginmgr"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_setup","text":"</> Setup for plugins, primarily used for the plugins tosetup some default configurations. This is only called once for all pipelines. Parameters pipen (Pipen) \u2014 The Pipen object function","title":"pipen.pluginmgr.on_setup"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_init","text":"</> When the pipeline is initialized, and default configs are loaded Parameters pipen (Pipen) \u2014 The Pipen object function","title":"pipen.pluginmgr.on_init"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_start","text":"</> Right before the pipeline starts running. Process relationships are inferred. Parameters pipen (Pipen) \u2014 The Pipen object function","title":"pipen.pluginmgr.on_start"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_complete","text":"</> The the pipeline is completed. Parameters pipen (Pipen) \u2014 The Pipen object succeeded (bool) \u2014 Whether the pipeline has successfully completed. function","title":"pipen.pluginmgr.on_complete"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_proc_create","text":"</> Called Proc constructor when a process is created. Enables plugins to modify the default attributes of processes Parameters proc (Proc) \u2014 The Proc object function","title":"pipen.pluginmgr.on_proc_create"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_proc_input_computed","text":"</> Called after process input data is computed. Parameters proc (Proc) \u2014 The Proc object function","title":"pipen.pluginmgr.on_proc_input_computed"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_proc_script_computed","text":"</> Called after process script is computed. The script is computed as a string that is about to compiled into a template. Parameters proc (Proc) \u2014 The Proc object function","title":"pipen.pluginmgr.on_proc_script_computed"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_proc_start","text":"</> When a process is starting Parameters proc (Proc) \u2014 The process function","title":"pipen.pluginmgr.on_proc_start"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_proc_shutdown","text":"</> When pipeline is shutting down, by Ctrl-c for example. Return False to stop shutting down, but you have to shut it down by yourself, for example, proc.xqute.task.cancel() Only the first return value will be used. Parameters sig (signal.Signals) \u2014 The signal. None means a natural shutdown pipen \u2014 The xqute object function","title":"pipen.pluginmgr.on_proc_shutdown"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_proc_done","text":"</> When a process is done Parameters proc (Proc) \u2014 The process succeeded (bool | str) \u2014 Whether the process succeeded or not. 'cached' if all jobsare cached. function","title":"pipen.pluginmgr.on_proc_done"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_job_init","text":"</> When a job is initialized Parameters job (Job) \u2014 The job function","title":"pipen.pluginmgr.on_job_init"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_job_queued","text":"</> When a job is queued in xqute. Note it might not be queued yet inthe scheduler system. Parameters job (Job) \u2014 The job function","title":"pipen.pluginmgr.on_job_queued"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_job_submitting","text":"</> When a job is submitting. The first plugin (based on priority) have this hook return False will cancel the submission Parameters job (Job) \u2014 The job Returns (bool) False to cancel submission function","title":"pipen.pluginmgr.on_job_submitting"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_job_submitted","text":"</> When a job is submitted in the scheduler system. Parameters job (Job) \u2014 The job function","title":"pipen.pluginmgr.on_job_submitted"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_job_started","text":"</> When a job starts to run in then scheduler system. Note that the job might not be running yet in the scheduler system. Parameters job (Job) \u2014 The job function","title":"pipen.pluginmgr.on_job_started"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_job_polling","text":"</> When status of a job is being polled. Parameters job (Job) \u2014 The job function","title":"pipen.pluginmgr.on_job_polling"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_job_killing","text":"</> When a job is being killed. The first plugin (based on priority) have this hook return False will cancel the killing Parameters job (Job) \u2014 The job Returns (bool) False to cancel killing function","title":"pipen.pluginmgr.on_job_killing"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_job_killed","text":"</> When a job is killed Parameters job (Job) \u2014 The job function","title":"pipen.pluginmgr.on_job_killed"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_job_succeeded","text":"</> When a job completes successfully. Parameters job (Job) \u2014 The job function","title":"pipen.pluginmgr.on_job_succeeded"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_job_cached","text":"</> When a job is cached. Parameters job (Job) \u2014 The job function","title":"pipen.pluginmgr.on_job_cached"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_job_failed","text":"</> When a job is done but failed. Parameters job (Job) \u2014 The job function","title":"pipen.pluginmgr.on_job_failed"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_jobcmd_init","text":"</> When the job command wrapper script is initialized before the prescript is run This should return a piece of bash code to be inserted in the wrapped job script (template), which is a python template string, with the following variables available: status and job . status is the class JobStatus from xqute.defaults.py and job is the Job instance. For multiple plugins, the code will be inserted in the order of the plugin priority. The code will replace the #![jobcmd_init] placeholder in the wrapped job script. See also https://github.com/pwwang/xqute/blob/master/xqute/defaults.py#L95 Parameters job (Job) \u2014 The job object Returns (str) The bash code to be inserted function","title":"pipen.pluginmgr.on_jobcmd_init"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_jobcmd_prep","text":"</> When the job command right about to be run This should return a piece of bash code to be inserted in the wrapped job script (template), which is a python template string, with the following variables available: status and job . status is the class JobStatus from xqute.defaults.py and job is the Job instance. The bash variable $cmd is accessible in the context. It is also possible to modify the cmd variable. Just remember to assign the modified value to cmd . For multiple plugins, the code will be inserted in the order of the plugin priority. Keep in mind that the $cmd may be modified by other plugins. The code will replace the #![jobcmd_prep] placeholder in the wrapped job script. See also https://github.com/pwwang/xqute/blob/master/xqute/defaults.py#L95 Parameters job (Job) \u2014 The job object Returns (str) The bash code to be inserted function","title":"pipen.pluginmgr.on_jobcmd_prep"},{"location":"api/pipen.pluginmgr/#pipenpluginmgron_jobcmd_end","text":"</> When the job command finishes and after the postscript is run This should return a piece of bash code to be inserted in the wrapped job script (template), which is a python template string, with the following variables available: status and job . status is the class JobStatus from xqute.defaults.py and job is the Job instance. The bash variable $rc is accessible in the context, which is the return code of the job command. For multiple plugins, the code will be inserted in the order of the plugin priority. The code will replace the #![jobcmd_end] placeholder in the wrapped job script. See also https://github.com/pwwang/xqute/blob/master/xqute/defaults.py#L95 Parameters job (Job) \u2014 The job object Returns (str) The bash code to be inserted class","title":"pipen.pluginmgr.on_jobcmd_end"},{"location":"api/pipen.pluginmgr/#pipenpluginmgrpipenmainplugin","text":"</> The builtin core plugin, used to update the progress bar andcache the job class","title":"pipen.pluginmgr.PipenMainPlugin"},{"location":"api/pipen.pluginmgr/#pipenpluginmgrxqutepipenplugin","text":"</> The plugin for xqute working as proxy for pipen plugin hooks","title":"pipen.pluginmgr.XqutePipenPlugin"},{"location":"api/pipen.proc/","text":"module pipen. proc </> Provides the process class: Proc Classes ProcMeta \u2014 Meta class for Proc </> Proc ( Proc ) \u2014 The abstract class for processes. </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) ( Proc ) \u2014 Make sure Proc subclasses are singletons </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns ( Proc ) The Proc instance class pipen.proc. Proc ( *args , **kwds ) \u2192 Proc </> The abstract class for processes. It's an abstract class. You can't instantise a process using it directly. You have to subclass it. The subclass itself can be used as a process directly. Each subclass is a singleton, so to intantise a new process, each subclass an existing Proc subclass, or use Proc.from_proc() . Never use the constructor directly. The Proc is designed as a singleton class, and is instansiated internally. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously.The program entrance for some schedulers may take too much resources when submitting a job or checking the job status. So we may use a smaller number here to limit the simultaneous submissions. template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Init all other properties and jobs </> class pipen.proc. ProcMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) ( Proc ) \u2014 Make sure Proc subclasses are singletons </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns ( Proc ) The Proc instance classmethod from_proc ( proc , name=None , desc=None , envs=None , envs_depth=None , cache=None , export=None , error_strategy=None , num_retries=None , forks=None , input_data=None , order=None , plugin_opts=None , requires=None , scheduler=None , scheduler_opts=None , submission_batch=None ) </> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously. Returns (Type) The new process class classmethod __init_subclass__ ( ) </> Do the requirements inferring since we need them to build up theprocess relationship method run ( ) </> Init all other properties and jobs method gc ( ) </> GC process for the process to save memory after it's done method log ( level , msg , *args , logger=<LoggerAdapter pipen.core (WARNING)> ) </> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger","title":"pipen.proc"},{"location":"api/pipen.proc/#pipenproc","text":"</> Provides the process class: Proc Classes ProcMeta \u2014 Meta class for Proc </> Proc ( Proc ) \u2014 The abstract class for processes. </> class","title":"pipen.proc"},{"location":"api/pipen.proc/#pipenprocprocmeta","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) ( Proc ) \u2014 Make sure Proc subclasses are singletons </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> staticmethod","title":"pipen.proc.ProcMeta"},{"location":"api/pipen.proc/#pipenprocprocmetarepr","text":"</> Representation for the Proc subclasses staticmethod","title":"pipen.proc.ProcMeta.repr"},{"location":"api/pipen.proc/#pipenprocprocmetacall","text":"</> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns ( Proc ) The Proc instance class","title":"pipen.proc.ProcMeta.call"},{"location":"api/pipen.proc/#pipenprocproc","text":"</> The abstract class for processes. It's an abstract class. You can't instantise a process using it directly. You have to subclass it. The subclass itself can be used as a process directly. Each subclass is a singleton, so to intantise a new process, each subclass an existing Proc subclass, or use Proc.from_proc() . Never use the constructor directly. The Proc is designed as a singleton class, and is instansiated internally. Attributes cache \u2014 Should we detect whether the jobs are cached? desc \u2014 The description of the process. Will use the summary fromthe docstring by default. dirsig \u2014 When checking the signature for caching, whether should we walkthrough the content of the directory? This is sometimes time-consuming if the directory is big. envs \u2014 The arguments that are job-independent, useful for common optionsacross jobs. envs_depth \u2014 How deep to update the envs when subclassed. error_strategy \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself export \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes forks \u2014 How many jobs to run simultaneously? input \u2014 The keys for the input channel input_data \u2014 The input data (will be computed for dependent processes) lang \u2014 The language for the script to run. Should be the path to theinterpreter if lang is not in $PATH . name \u2014 The name of the process. Will use the class name by default. nexts \u2014 Computed from requires to build the process relationships num_retries \u2014 How many times to retry to jobs once error occurs order \u2014 The execution order for this process. The bigger the numberis, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by Pipen.set_starts() output \u2014 The output keys for the output channel(the data will be computed) output_data \u2014 The output data (to pass to the next processes) plugin_opts \u2014 Options for process-level plugins requires \u2014 The dependency processes scheduler \u2014 The scheduler to run the jobs scheduler_opts \u2014 The options for the scheduler script \u2014 The script template for the process submission_batch \u2014 How many jobs to be submited simultaneously.The program entrance for some schedulers may take too much resources when submitting a job or checking the job status. So we may use a smaller number here to limit the simultaneous submissions. template \u2014 Define the template engine to use.This could be either a template engine or a dict with key engine indicating the template engine and the rest the arguments passed to the constructor of the pipen.template.Template object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of pipen.template.Template . You can subclass pipen.template.Template to use your own template engine. Classes ProcMeta \u2014 Meta class for Proc </> Methods __init_subclass__ ( ) \u2014 Do the requirements inferring since we need them to build up theprocess relationship </> from_proc ( proc , name , desc , envs , envs_depth , cache , export , error_strategy , num_retries , forks , input_data , order , plugin_opts , requires , scheduler , scheduler_opts , submission_batch ) (Type) \u2014 Create a subclass of Proc using another Proc subclass or Proc itself </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> run ( ) \u2014 Init all other properties and jobs </> class","title":"pipen.proc.Proc"},{"location":"api/pipen.proc/#pipenprocprocmeta_1","text":"</> Bases abc.ABCMeta Meta class for Proc Methods __call__ ( cls , *args , **kwds ) ( Proc ) \u2014 Make sure Proc subclasses are singletons </> __repr__ ( cls ) (str) \u2014 Representation for the Proc subclasses </> staticmethod __repr__ ( cls ) \u2192 str </> Representation for the Proc subclasses staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args (Any) \u2014 and **kwds (Any) \u2014 Arguments for the constructor Returns ( Proc ) The Proc instance classmethod","title":"pipen.proc.ProcMeta"},{"location":"api/pipen.proc/#pipenprocprocfrom_proc","text":"</> Create a subclass of Proc using another Proc subclass or Proc itself Parameters proc (Type) \u2014 The Proc subclass name (str, optional) \u2014 The new name of the process desc (str, optional) \u2014 The new description of the process envs (Mapping, optional) \u2014 The arguments of the process, will overwrite parent oneThe items that are specified will be inherited envs_depth (int, optional) \u2014 How deep to update the envs when subclassed. cache (bool, optional) \u2014 Whether we should check the cache for the jobs export (bool, optional) \u2014 When True, the results will be exported to <pipeline.outdir> Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy (str, optional) \u2014 How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries (int, optional) \u2014 How many times to retry to jobs once error occurs forks (int, optional) \u2014 New forks for the new process input_data (Any, optional) \u2014 The input data for the process. Only when this processis a start process order (int, optional) \u2014 The order to execute the new process plugin_opts (Mapping, optional) \u2014 The new plugin options, unspecified items will beinherited. requires (Sequence, optional) \u2014 The required processes for the new process scheduler (str, optional) \u2014 The new shedular to run the new process scheduler_opts (Mapping, optional) \u2014 The new scheduler options, unspecified items willbe inherited. submission_batch (int, optional) \u2014 How many jobs to be submited simultaneously. Returns (Type) The new process class classmethod","title":"pipen.proc.Proc.from_proc"},{"location":"api/pipen.proc/#pipenprocprocinit_subclass","text":"</> Do the requirements inferring since we need them to build up theprocess relationship method","title":"pipen.proc.Proc.init_subclass"},{"location":"api/pipen.proc/#pipenprocprocrun","text":"</> Init all other properties and jobs method","title":"pipen.proc.Proc.run"},{"location":"api/pipen.proc/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/pipen.proc/#pipenprocproclog","text":"</> Log message for the process Parameters level (int | str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (LoggerAdapter, optional) \u2014 The logging logger","title":"pipen.proc.Proc.log"},{"location":"api/pipen.procgroup/","text":"module pipen. procgroup </> Process group that contains a set of processes. It can be easily used to create a pipeline that runs independently or integrated into a larger pipeline. Runs directly: >>> proc_group = ProcGroup ( < options > ) >>> proc_group . as_pipen ( < pipeline options > ) . set_data ( < data > ) . run () Integrated into a larger pipeline >>> proc_group = ProcGroup ( < options > ) >>> # proc could be a process within the larger pipeline >>> proc . requires = prog_group .< proc > To add a process to the proc group, use the add_proc method: >>> class MyProcGroup ( ProcGroup ): >>> ... >>> >>> proc_group = MyProcGroup ( ... ) >>> @proc_group . add_proc >>> class MyProc ( Proc ): >>> ... Or add a process at runtime: >>> class MyProcGroup ( ProcGroup ): >>> ... >>> >>> @ProcGroup . add_proc >>> def my_proc ( self ): >>> class MyProc ( Proc ): >>> # You may use self.options here >>> ... >>> return MyProc >>> proc_group = MyProcGroup ( ... ) Classes ProcGropuMeta \u2014 Meta class for ProcGroup </> ProcGroup \u2014 A group of processes that can be run independently orintegrated into a larger pipeline. </> class pipen.procgroup. ProcGropuMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for ProcGroup Methods __call__ ( cls , *args , **kwds ) \u2014 Make sure Proc subclasses are singletons </> staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args \u2014 and **kwds \u2014 Arguments for the constructor Returns The Proc instance class pipen.procgroup. ProcGroup ( *args , **kwds ) </> A group of processes that can be run independently orintegrated into a larger pipeline. Classes ProcGropuMeta \u2014 Meta class for ProcGroup </> Methods __init_subclass__ ( ) \u2014 This method is called when a class is subclassed. </> add_proc ( self_or_method , proc ) (Union) \u2014 Add a process to the proc group </> as_pipen ( name , desc , outdir , **kwargs ) ( Pipen ) \u2014 Convert the pipeline to a Pipen instance </> class pipen.procgroup. ProcGropuMeta ( name , bases , namespace , **kwargs ) </> Bases abc.ABCMeta Meta class for ProcGroup Methods __call__ ( cls , *args , **kwds ) \u2014 Make sure Proc subclasses are singletons </> staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args \u2014 and **kwds \u2014 Arguments for the constructor Returns The Proc instance classmethod __init_subclass__ ( ) </> This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. staticmethod add_proc ( self_or_method , proc=None ) </> Add a process to the proc group It works either as a decorator to the process directly or as a decorator to a method that returns the process. Parameters self_or_method (Union) \u2014 The proc group instance or a method thatreturns the process proc (Optional, optional) \u2014 The process class if self_or_method is the proc group Returns (Union) The process class if self_or_method is the proc group, ora cached property that returns the process class method as_pipen ( name=None , desc=None , outdir=None , **kwargs ) </> Convert the pipeline to a Pipen instance Parameters name (str | none, optional) \u2014 The name of the pipeline desc (str | none, optional) \u2014 The description of the pipeline outdir (str | pathlib.path | none, optional) \u2014 The output directory of the pipeline **kwargs \u2014 The keyword arguments to pass to Pipen Returns ( Pipen ) The Pipen instance","title":"pipen.procgroup"},{"location":"api/pipen.procgroup/#pipenprocgroup","text":"</> Process group that contains a set of processes. It can be easily used to create a pipeline that runs independently or integrated into a larger pipeline. Runs directly: >>> proc_group = ProcGroup ( < options > ) >>> proc_group . as_pipen ( < pipeline options > ) . set_data ( < data > ) . run () Integrated into a larger pipeline >>> proc_group = ProcGroup ( < options > ) >>> # proc could be a process within the larger pipeline >>> proc . requires = prog_group .< proc > To add a process to the proc group, use the add_proc method: >>> class MyProcGroup ( ProcGroup ): >>> ... >>> >>> proc_group = MyProcGroup ( ... ) >>> @proc_group . add_proc >>> class MyProc ( Proc ): >>> ... Or add a process at runtime: >>> class MyProcGroup ( ProcGroup ): >>> ... >>> >>> @ProcGroup . add_proc >>> def my_proc ( self ): >>> class MyProc ( Proc ): >>> # You may use self.options here >>> ... >>> return MyProc >>> proc_group = MyProcGroup ( ... ) Classes ProcGropuMeta \u2014 Meta class for ProcGroup </> ProcGroup \u2014 A group of processes that can be run independently orintegrated into a larger pipeline. </> class","title":"pipen.procgroup"},{"location":"api/pipen.procgroup/#pipenprocgroupprocgropumeta","text":"</> Bases abc.ABCMeta Meta class for ProcGroup Methods __call__ ( cls , *args , **kwds ) \u2014 Make sure Proc subclasses are singletons </> staticmethod","title":"pipen.procgroup.ProcGropuMeta"},{"location":"api/pipen.procgroup/#pipenprocgroupprocgropumetacall","text":"</> Make sure Proc subclasses are singletons Parameters *args \u2014 and **kwds \u2014 Arguments for the constructor Returns The Proc instance class","title":"pipen.procgroup.ProcGropuMeta.call"},{"location":"api/pipen.procgroup/#pipenprocgroupprocgroup","text":"</> A group of processes that can be run independently orintegrated into a larger pipeline. Classes ProcGropuMeta \u2014 Meta class for ProcGroup </> Methods __init_subclass__ ( ) \u2014 This method is called when a class is subclassed. </> add_proc ( self_or_method , proc ) (Union) \u2014 Add a process to the proc group </> as_pipen ( name , desc , outdir , **kwargs ) ( Pipen ) \u2014 Convert the pipeline to a Pipen instance </> class","title":"pipen.procgroup.ProcGroup"},{"location":"api/pipen.procgroup/#pipenprocgroupprocgropumeta_1","text":"</> Bases abc.ABCMeta Meta class for ProcGroup Methods __call__ ( cls , *args , **kwds ) \u2014 Make sure Proc subclasses are singletons </> staticmethod __call__ ( cls , *args , **kwds ) </> Make sure Proc subclasses are singletons Parameters *args \u2014 and **kwds \u2014 Arguments for the constructor Returns The Proc instance classmethod","title":"pipen.procgroup.ProcGropuMeta"},{"location":"api/pipen.procgroup/#pipenprocgroupprocgroupinit_subclass","text":"</> This method is called when a class is subclassed. The default implementation does nothing. It may be overridden to extend subclasses. staticmethod","title":"pipen.procgroup.ProcGroup.init_subclass"},{"location":"api/pipen.procgroup/#pipenprocgroupprocgroupadd_proc","text":"</> Add a process to the proc group It works either as a decorator to the process directly or as a decorator to a method that returns the process. Parameters self_or_method (Union) \u2014 The proc group instance or a method thatreturns the process proc (Optional, optional) \u2014 The process class if self_or_method is the proc group Returns (Union) The process class if self_or_method is the proc group, ora cached property that returns the process class method","title":"pipen.procgroup.ProcGroup.add_proc"},{"location":"api/pipen.procgroup/#pipenprocgroupprocgroupas_pipen","text":"</> Convert the pipeline to a Pipen instance Parameters name (str | none, optional) \u2014 The name of the pipeline desc (str | none, optional) \u2014 The description of the pipeline outdir (str | pathlib.path | none, optional) \u2014 The output directory of the pipeline **kwargs \u2014 The keyword arguments to pass to Pipen Returns ( Pipen ) The Pipen instance","title":"pipen.procgroup.ProcGroup.as_pipen"},{"location":"api/pipen.progressbar/","text":"module pipen. progressbar </> Provide the PipelinePBar and ProcPBar classes Classes ProcPBar \u2014 The progress bar for processes </> PipelinePBar \u2014 Progress bar for the pipeline </> class pipen.progressbar. ProcPBar ( manager , proc_size , proc_name ) </> The progress bar for processes Methods done ( ) \u2014 The process is done </> update_job_failed ( ) \u2014 Update the progress bar when a job is failed </> update_job_inited ( ) \u2014 Update the progress bar when a job is init'ed </> update_job_queued ( ) \u2014 Update the progress bar when a job is queued </> update_job_retrying ( ) \u2014 Update the progress bar when a job is retrying </> update_job_running ( ) \u2014 Update the progress bar when a job is running </> update_job_submitted ( ) \u2014 Update the progress bar when a job is init'ed </> update_job_succeeded ( cached ) \u2014 Update the progress bar when a job is succeeded </> method update_job_inited ( ) </> Update the progress bar when a job is init'ed method update_job_queued ( ) </> Update the progress bar when a job is queued method update_job_submitted ( ) </> Update the progress bar when a job is init'ed method update_job_retrying ( ) </> Update the progress bar when a job is retrying method update_job_running ( ) </> Update the progress bar when a job is running method update_job_succeeded ( cached=False ) </> Update the progress bar when a job is succeeded method update_job_failed ( ) </> Update the progress bar when a job is failed method done ( ) </> The process is done class pipen.progressbar. PipelinePBar ( n_procs , ppln_name ) </> Progress bar for the pipeline Methods done ( ) \u2014 When the pipeline is done </> proc_bar ( proc_size , proc_name ) ( ProcPBar ) \u2014 Get the progress bar for a process </> update_proc_done ( ) \u2014 Update the progress bar when a process is done </> update_proc_error ( ) \u2014 Update the progress bar when a process is errored </> update_proc_running ( ) \u2014 Update the progress bar when a process is running </> method proc_bar ( proc_size , proc_name ) </> Get the progress bar for a process Parameters proc_size (int) \u2014 The size of the process proc_name (str) \u2014 The name of the process Returns ( ProcPBar ) The progress bar for the given process method update_proc_running ( ) </> Update the progress bar when a process is running method update_proc_done ( ) </> Update the progress bar when a process is done method update_proc_error ( ) </> Update the progress bar when a process is errored method done ( ) </> When the pipeline is done","title":"pipen.progressbar"},{"location":"api/pipen.progressbar/#pipenprogressbar","text":"</> Provide the PipelinePBar and ProcPBar classes Classes ProcPBar \u2014 The progress bar for processes </> PipelinePBar \u2014 Progress bar for the pipeline </> class","title":"pipen.progressbar"},{"location":"api/pipen.progressbar/#pipenprogressbarprocpbar","text":"</> The progress bar for processes Methods done ( ) \u2014 The process is done </> update_job_failed ( ) \u2014 Update the progress bar when a job is failed </> update_job_inited ( ) \u2014 Update the progress bar when a job is init'ed </> update_job_queued ( ) \u2014 Update the progress bar when a job is queued </> update_job_retrying ( ) \u2014 Update the progress bar when a job is retrying </> update_job_running ( ) \u2014 Update the progress bar when a job is running </> update_job_submitted ( ) \u2014 Update the progress bar when a job is init'ed </> update_job_succeeded ( cached ) \u2014 Update the progress bar when a job is succeeded </> method","title":"pipen.progressbar.ProcPBar"},{"location":"api/pipen.progressbar/#pipenprogressbarprocpbarupdate_job_inited","text":"</> Update the progress bar when a job is init'ed method","title":"pipen.progressbar.ProcPBar.update_job_inited"},{"location":"api/pipen.progressbar/#pipenprogressbarprocpbarupdate_job_queued","text":"</> Update the progress bar when a job is queued method","title":"pipen.progressbar.ProcPBar.update_job_queued"},{"location":"api/pipen.progressbar/#pipenprogressbarprocpbarupdate_job_submitted","text":"</> Update the progress bar when a job is init'ed method","title":"pipen.progressbar.ProcPBar.update_job_submitted"},{"location":"api/pipen.progressbar/#pipenprogressbarprocpbarupdate_job_retrying","text":"</> Update the progress bar when a job is retrying method","title":"pipen.progressbar.ProcPBar.update_job_retrying"},{"location":"api/pipen.progressbar/#pipenprogressbarprocpbarupdate_job_running","text":"</> Update the progress bar when a job is running method","title":"pipen.progressbar.ProcPBar.update_job_running"},{"location":"api/pipen.progressbar/#pipenprogressbarprocpbarupdate_job_succeeded","text":"</> Update the progress bar when a job is succeeded method","title":"pipen.progressbar.ProcPBar.update_job_succeeded"},{"location":"api/pipen.progressbar/#pipenprogressbarprocpbarupdate_job_failed","text":"</> Update the progress bar when a job is failed method","title":"pipen.progressbar.ProcPBar.update_job_failed"},{"location":"api/pipen.progressbar/#pipenprogressbarprocpbardone","text":"</> The process is done class","title":"pipen.progressbar.ProcPBar.done"},{"location":"api/pipen.progressbar/#pipenprogressbarpipelinepbar","text":"</> Progress bar for the pipeline Methods done ( ) \u2014 When the pipeline is done </> proc_bar ( proc_size , proc_name ) ( ProcPBar ) \u2014 Get the progress bar for a process </> update_proc_done ( ) \u2014 Update the progress bar when a process is done </> update_proc_error ( ) \u2014 Update the progress bar when a process is errored </> update_proc_running ( ) \u2014 Update the progress bar when a process is running </> method","title":"pipen.progressbar.PipelinePBar"},{"location":"api/pipen.progressbar/#pipenprogressbarpipelinepbarproc_bar","text":"</> Get the progress bar for a process Parameters proc_size (int) \u2014 The size of the process proc_name (str) \u2014 The name of the process Returns ( ProcPBar ) The progress bar for the given process method","title":"pipen.progressbar.PipelinePBar.proc_bar"},{"location":"api/pipen.progressbar/#pipenprogressbarpipelinepbarupdate_proc_running","text":"</> Update the progress bar when a process is running method","title":"pipen.progressbar.PipelinePBar.update_proc_running"},{"location":"api/pipen.progressbar/#pipenprogressbarpipelinepbarupdate_proc_done","text":"</> Update the progress bar when a process is done method","title":"pipen.progressbar.PipelinePBar.update_proc_done"},{"location":"api/pipen.progressbar/#pipenprogressbarpipelinepbarupdate_proc_error","text":"</> Update the progress bar when a process is errored method","title":"pipen.progressbar.PipelinePBar.update_proc_error"},{"location":"api/pipen.progressbar/#pipenprogressbarpipelinepbardone","text":"</> When the pipeline is done","title":"pipen.progressbar.PipelinePBar.done"},{"location":"api/pipen.scheduler/","text":"module pipen. scheduler </> Provide builting schedulers Classes SchedulerPostInit \u2014 Provides post init function for all schedulers </> LocalScheduler \u2014 Local scheduler </> SgeScheduler \u2014 SGE scheduler </> SlurmScheduler \u2014 Slurm scheduler </> SshScheduler \u2014 SSH scheduler </> GbatchScheduler \u2014 Google Cloud Batch scheduler </> ContainerScheduler \u2014 Scheduler to run jobs via containers (Docker/Podman/Apptainer) </> Functions get_scheduler ( scheduler ) (Type) \u2014 Get the scheduler by name of the scheduler class itself </> class pipen.scheduler. SchedulerPostInit ( ) </> Provides post init function for all schedulers class pipen.scheduler. LocalScheduler ( workdir , forks=1 , error_strategy='ignore' , num_retries=3 , prescript='' , postscript='' , jobname_prefix=None , submission_batch=None , recheck_interval=60 , cwd=None , **kwargs ) </> Bases pipen.scheduler.SchedulerPostInit xqute.schedulers.local_scheduler.LocalScheduler xqute.scheduler.Scheduler Local scheduler Parameters workdir (str | Path) \u2014 The working directory forks (int, optional) \u2014 Max number of job forks error_strategy (str, optional) \u2014 The strategy when there is error happened num_retries (int, optional) \u2014 Max number of retries when error_strategy is retry prescript (str, optional) \u2014 The prescript to run before the job commandIt is a piece of script that inserted into the wrapper script, running on the scheduler system. postscript (str, optional) \u2014 The postscript to run when job finishedIt is a piece of script that inserted into the wrapper script, running on the scheduler system. jobname_prefix (str | none, optional) \u2014 The prefix for the job name submission_batch (int | none, optional) \u2014 The number of consumers to submit jobs. This allowsmultiple jobs to be submitted in parallel. This is useful when there are many jobs to be submitted and the scheduler has a high latency for each submission. Set this to a smaller number if the scheduler cannot handle too many simultaneous submissions. recheck_interval (int, optional) \u2014 The number of polling iterations between rechecks ofwhether a job is still running on the scheduler. Helps detect jobs that fail before the wrapped script updates status (e.g., resource allocation failures). Each iteration takes xqute.defaults.SLEEP_INTERVAL_POLLING_JOBS cwd (str | Path, optional) \u2014 The working directory for the job command wrapper **kwargs \u2014 Other arguments for the scheduler Attributes job_class \u2014 The job class jobcmd_wrapper_init \u2014 The init script for the job command wrapper jobcmd_wrapper_init (str) \u2014 The init script for the job command wrapper </> name \u2014 The name of the scheduler Methods check_all_done ( jobs , polling_counter ) (bool) \u2014 Check if all jobs are done (full polling with hooks) </> count_running_jobs ( jobs ) (int) \u2014 Count currently running/active jobs (lightweight check) </> create_job ( index , cmd , envs ) (Job) \u2014 Create a job </> job_fails_before_running ( job ) (bool) \u2014 Check if a job fails before running. </> job_is_running ( job ) (bool) \u2014 Tell if a job is really running, not only the job.jid_file </> job_is_submitted_or_running ( job ) (bool) \u2014 Check if a job is already submitted or running </> jobcmd_end ( job ) (str) \u2014 The job command end </> jobcmd_init ( job ) (str) \u2014 The job command init </> jobcmd_prep ( job ) (str) \u2014 The job command preparation </> jobcmd_shebang ( job ) (str) \u2014 The shebang of the wrapper script </> kill_job ( job ) \u2014 Kill a job asynchronously </> kill_job_and_update_status ( job ) \u2014 Kill a job and update its status </> kill_running_jobs ( jobs ) \u2014 Try to kill all running jobs </> retry_job ( job ) \u2014 Retry a job </> submit_job ( job ) (int) \u2014 Submit a job locally </> submit_job_and_update_status ( job ) \u2014 Submit and update the status </> transition_job_status ( job , new_status , old_status , flush , rc , error_msg , is_killed ) \u2014 Centralized status transition handler </> wrap_job_script ( job ) (str) \u2014 Wrap the job script </> wrapped_job_script ( job ) (SpecPath) \u2014 Get the wrapped job script </> method create_job ( index , cmd , envs=None ) </> Create a job Parameters index (int) \u2014 The index of the job cmd (Union) \u2014 The command of the job Returns (Job) The job method submit_job_and_update_status ( job ) </> Submit and update the status Check if the job is already submitted or running If not, run the hook If the hook is not cancelled, clean the job Submit the job, raising an exception if it fails If the job is submitted successfully, update the status If the job fails to submit, update the status and write stderr to the job file Parameters job (Job) \u2014 The job method transition_job_status ( job , new_status , old_status=None , flush=True , rc=None , error_msg=None , is_killed=False ) </> Centralized status transition handler Handles all aspects of job status transitions: - Status change logging - Hook lifecycle management (ensuring on_job_started is called) - Appropriate hook calls based on new status - RC file updates - Error message appending to stderr - JID file cleanup for terminal states - Pipeline halt on errors if configured Note that this method will not flush status changes to disk (job.status_file).You need to call job.set_status() separately if needed. Parameters job (Job) \u2014 The job to transition new_status (int) \u2014 The new status to transition to old_status (int | none, optional) \u2014 The previous status (if known).If None, will use job._status flush (bool, optional) \u2014 Whether to flush the status to disk rc (str | none, optional) \u2014 Optional return code to write to rc_file error_msg (str | none, optional) \u2014 Optional error message to append to stderr_file is_killed (bool, optional) \u2014 Whether this is a killed job (uses on_job_killed hook) method kill_job_and_update_status ( job ) </> Kill a job and update its status Parameters job (Job) \u2014 The job method retry_job ( job ) </> Retry a job Parameters job (Job) \u2014 The job method count_running_jobs ( jobs ) </> Count currently running/active jobs (lightweight check) This is optimized for the producer to check if new jobs can be submitted. It only counts jobs without refreshing status or calling hooks. Parameters jobs (List) \u2014 The list of jobs Returns (int) Number of jobs currently in active states method check_all_done ( jobs , polling_counter ) </> Check if all jobs are done (full polling with hooks) This does complete status refresh and calls all lifecycle hooks. Used by the main polling loop to track job completion. Parameters jobs (List) \u2014 The list of jobs polling_counter (int) \u2014 The polling counter for hook calls Returns (bool) True if all jobs are done, False otherwise method kill_running_jobs ( jobs ) </> Try to kill all running jobs Parameters jobs (List) \u2014 The list of jobs method job_is_submitted_or_running ( job ) </> Check if a job is already submitted or running Parameters job (Job) \u2014 The job Returns (bool) True if yes otherwise False. method job_fails_before_running ( job ) </> Check if a job fails before running. For some schedulers, the job might fail before running (after submission). For example, the job might fail to allocate resources. In such a case, the wrapped script might not be executed, and the job status will not be updated (stays in SUBMITTED). We need to check such jobs and mark them as FAILED. For the instant scheduler, for example, the local scheduler, the failure will be immediately reported when submitting the job, so we don't need to check such jobs. Parameters job (Job) \u2014 The job to check Returns (bool) True if the job fails before running, otherwise False. method jobcmd_shebang ( job ) \u2192 str </> The shebang of the wrapper script method jobcmd_init ( job ) \u2192 str </> The job command init method jobcmd_end ( job ) \u2192 str </> The job command end method wrap_job_script ( job ) </> Wrap the job script Parameters job (Job) \u2014 The job Returns (str) The wrapped script method wrapped_job_script ( job ) </> Get the wrapped job script Parameters job (Job) \u2014 The job Returns (SpecPath) The path of the wrapped job script method submit_job ( job ) </> Submit a job locally Parameters job (Job) \u2014 The job Returns (int) The process id method kill_job ( job ) </> Kill a job asynchronously Parameters job (Job) \u2014 The job method job_is_running ( job ) </> Tell if a job is really running, not only the job.jid_file In case where the jid file is not cleaned when job is done. Parameters job (Job) \u2014 The job Returns (bool) True if it is, otherwise False method jobcmd_prep ( job ) \u2192 str </> The job command preparation class pipen.scheduler. SgeScheduler ( *args , **kwargs ) </> Bases pipen.scheduler.SchedulerPostInit xqute.schedulers.sge_scheduler.SgeScheduler xqute.scheduler.Scheduler SGE scheduler Parameters **kwargs \u2014 Other arguments for the scheduler Attributes job_class \u2014 The job class jobcmd_wrapper_init \u2014 The init script for the job command wrapper jobcmd_wrapper_init (str) \u2014 The init script for the job command wrapper </> name \u2014 The name of the scheduler Methods check_all_done ( jobs , polling_counter ) (bool) \u2014 Check if all jobs are done (full polling with hooks) </> count_running_jobs ( jobs ) (int) \u2014 Count currently running/active jobs (lightweight check) </> create_job ( index , cmd , envs ) (Job) \u2014 Create a job </> job_fails_before_running ( job ) (bool) \u2014 Check if a job fails before running. </> job_is_running ( job ) (bool) \u2014 Tell if a job is really running, not only the job.jid_file </> job_is_submitted_or_running ( job ) (bool) \u2014 Check if a job is already submitted or running </> jobcmd_end ( job ) (str) \u2014 The job command end </> jobcmd_init ( job ) (str) \u2014 The job command init </> jobcmd_prep ( job ) (str) \u2014 The job command preparation </> jobcmd_shebang ( job ) (str) \u2014 The shebang of the wrapper script </> kill_job ( job ) \u2014 Kill a job on SGE </> kill_job_and_update_status ( job ) \u2014 Kill a job and update its status </> kill_running_jobs ( jobs ) \u2014 Try to kill all running jobs </> retry_job ( job ) \u2014 Retry a job </> submit_job ( job ) (str) \u2014 Submit a job to SGE </> submit_job_and_update_status ( job ) \u2014 Submit and update the status </> transition_job_status ( job , new_status , old_status , flush , rc , error_msg , is_killed ) \u2014 Centralized status transition handler </> wrap_job_script ( job ) (str) \u2014 Wrap the job script </> wrapped_job_script ( job ) (SpecPath) \u2014 Get the wrapped job script </> method create_job ( index , cmd , envs=None ) </> Create a job Parameters index (int) \u2014 The index of the job cmd (Union) \u2014 The command of the job Returns (Job) The job method submit_job_and_update_status ( job ) </> Submit and update the status Check if the job is already submitted or running If not, run the hook If the hook is not cancelled, clean the job Submit the job, raising an exception if it fails If the job is submitted successfully, update the status If the job fails to submit, update the status and write stderr to the job file Parameters job (Job) \u2014 The job method transition_job_status ( job , new_status , old_status=None , flush=True , rc=None , error_msg=None , is_killed=False ) </> Centralized status transition handler Handles all aspects of job status transitions: - Status change logging - Hook lifecycle management (ensuring on_job_started is called) - Appropriate hook calls based on new status - RC file updates - Error message appending to stderr - JID file cleanup for terminal states - Pipeline halt on errors if configured Note that this method will not flush status changes to disk (job.status_file).You need to call job.set_status() separately if needed. Parameters job (Job) \u2014 The job to transition new_status (int) \u2014 The new status to transition to old_status (int | none, optional) \u2014 The previous status (if known).If None, will use job._status flush (bool, optional) \u2014 Whether to flush the status to disk rc (str | none, optional) \u2014 Optional return code to write to rc_file error_msg (str | none, optional) \u2014 Optional error message to append to stderr_file is_killed (bool, optional) \u2014 Whether this is a killed job (uses on_job_killed hook) method kill_job_and_update_status ( job ) </> Kill a job and update its status Parameters job (Job) \u2014 The job method retry_job ( job ) </> Retry a job Parameters job (Job) \u2014 The job method count_running_jobs ( jobs ) </> Count currently running/active jobs (lightweight check) This is optimized for the producer to check if new jobs can be submitted. It only counts jobs without refreshing status or calling hooks. Parameters jobs (List) \u2014 The list of jobs Returns (int) Number of jobs currently in active states method check_all_done ( jobs , polling_counter ) </> Check if all jobs are done (full polling with hooks) This does complete status refresh and calls all lifecycle hooks. Used by the main polling loop to track job completion. Parameters jobs (List) \u2014 The list of jobs polling_counter (int) \u2014 The polling counter for hook calls Returns (bool) True if all jobs are done, False otherwise method kill_running_jobs ( jobs ) </> Try to kill all running jobs Parameters jobs (List) \u2014 The list of jobs method job_is_submitted_or_running ( job ) </> Check if a job is already submitted or running Parameters job (Job) \u2014 The job Returns (bool) True if yes otherwise False. method job_fails_before_running ( job ) </> Check if a job fails before running. For some schedulers, the job might fail before running (after submission). For example, the job might fail to allocate resources. In such a case, the wrapped script might not be executed, and the job status will not be updated (stays in SUBMITTED). We need to check such jobs and mark them as FAILED. For the instant scheduler, for example, the local scheduler, the failure will be immediately reported when submitting the job, so we don't need to check such jobs. Parameters job (Job) \u2014 The job to check Returns (bool) True if the job fails before running, otherwise False. method jobcmd_init ( job ) \u2192 str </> The job command init method jobcmd_prep ( job ) \u2192 str </> The job command preparation method jobcmd_end ( job ) \u2192 str </> The job command end method wrap_job_script ( job ) </> Wrap the job script Parameters job (Job) \u2014 The job Returns (str) The wrapped script method wrapped_job_script ( job ) </> Get the wrapped job script Parameters job (Job) \u2014 The job Returns (SpecPath) The path of the wrapped job script method jobcmd_shebang ( job ) \u2192 str </> The shebang of the wrapper script method submit_job ( job ) </> Submit a job to SGE Parameters job (Job) \u2014 The job Returns (str) The job id method kill_job ( job ) </> Kill a job on SGE Parameters job (Job) \u2014 The job method job_is_running ( job ) </> Tell if a job is really running, not only the job.jid_file In case where the jid file is not cleaned when job is done. Parameters job (Job) \u2014 The job Returns (bool) True if it is, otherwise False class pipen.scheduler. SlurmScheduler ( *args , **kwargs ) </> Bases pipen.scheduler.SchedulerPostInit xqute.schedulers.slurm_scheduler.SlurmScheduler xqute.scheduler.Scheduler Slurm scheduler Parameters **kwargs \u2014 Other arguments for the scheduler Attributes job_class \u2014 The job class jobcmd_wrapper_init \u2014 The init script for the job command wrapper jobcmd_wrapper_init (str) \u2014 The init script for the job command wrapper </> name \u2014 The name of the scheduler Methods check_all_done ( jobs , polling_counter ) (bool) \u2014 Check if all jobs are done (full polling with hooks) </> count_running_jobs ( jobs ) (int) \u2014 Count currently running/active jobs (lightweight check) </> create_job ( index , cmd , envs ) (Job) \u2014 Create a job </> job_fails_before_running ( job ) (bool) \u2014 Check if a job fails before running. </> job_is_running ( job ) (bool) \u2014 Tell if a job is really running, not only the job.jid_file </> job_is_submitted_or_running ( job ) (bool) \u2014 Check if a job is already submitted or running </> jobcmd_end ( job ) (str) \u2014 The job command end </> jobcmd_init ( job ) (str) \u2014 The job command init </> jobcmd_prep ( job ) (str) \u2014 The job command preparation </> jobcmd_shebang ( job ) (str) \u2014 The shebang of the wrapper script </> kill_job ( job ) \u2014 Kill a job on Slurm </> kill_job_and_update_status ( job ) \u2014 Kill a job and update its status </> kill_running_jobs ( jobs ) \u2014 Try to kill all running jobs </> retry_job ( job ) \u2014 Retry a job </> submit_job ( job ) (str) \u2014 Submit a job to Slurm </> submit_job_and_update_status ( job ) \u2014 Submit and update the status </> transition_job_status ( job , new_status , old_status , flush , rc , error_msg , is_killed ) \u2014 Centralized status transition handler </> wrap_job_script ( job ) (str) \u2014 Wrap the job script </> wrapped_job_script ( job ) (SpecPath) \u2014 Get the wrapped job script </> method create_job ( index , cmd , envs=None ) </> Create a job Parameters index (int) \u2014 The index of the job cmd (Union) \u2014 The command of the job Returns (Job) The job method submit_job_and_update_status ( job ) </> Submit and update the status Check if the job is already submitted or running If not, run the hook If the hook is not cancelled, clean the job Submit the job, raising an exception if it fails If the job is submitted successfully, update the status If the job fails to submit, update the status and write stderr to the job file Parameters job (Job) \u2014 The job method transition_job_status ( job , new_status , old_status=None , flush=True , rc=None , error_msg=None , is_killed=False ) </> Centralized status transition handler Handles all aspects of job status transitions: - Status change logging - Hook lifecycle management (ensuring on_job_started is called) - Appropriate hook calls based on new status - RC file updates - Error message appending to stderr - JID file cleanup for terminal states - Pipeline halt on errors if configured Note that this method will not flush status changes to disk (job.status_file).You need to call job.set_status() separately if needed. Parameters job (Job) \u2014 The job to transition new_status (int) \u2014 The new status to transition to old_status (int | none, optional) \u2014 The previous status (if known).If None, will use job._status flush (bool, optional) \u2014 Whether to flush the status to disk rc (str | none, optional) \u2014 Optional return code to write to rc_file error_msg (str | none, optional) \u2014 Optional error message to append to stderr_file is_killed (bool, optional) \u2014 Whether this is a killed job (uses on_job_killed hook) method kill_job_and_update_status ( job ) </> Kill a job and update its status Parameters job (Job) \u2014 The job method retry_job ( job ) </> Retry a job Parameters job (Job) \u2014 The job method count_running_jobs ( jobs ) </> Count currently running/active jobs (lightweight check) This is optimized for the producer to check if new jobs can be submitted. It only counts jobs without refreshing status or calling hooks. Parameters jobs (List) \u2014 The list of jobs Returns (int) Number of jobs currently in active states method check_all_done ( jobs , polling_counter ) </> Check if all jobs are done (full polling with hooks) This does complete status refresh and calls all lifecycle hooks. Used by the main polling loop to track job completion. Parameters jobs (List) \u2014 The list of jobs polling_counter (int) \u2014 The polling counter for hook calls Returns (bool) True if all jobs are done, False otherwise method kill_running_jobs ( jobs ) </> Try to kill all running jobs Parameters jobs (List) \u2014 The list of jobs method job_is_submitted_or_running ( job ) </> Check if a job is already submitted or running Parameters job (Job) \u2014 The job Returns (bool) True if yes otherwise False. method jobcmd_init ( job ) \u2192 str </> The job command init method jobcmd_prep ( job ) \u2192 str </> The job command preparation method jobcmd_end ( job ) \u2192 str </> The job command end method wrap_job_script ( job ) </> Wrap the job script Parameters job (Job) \u2014 The job Returns (str) The wrapped script method wrapped_job_script ( job ) </> Get the wrapped job script Parameters job (Job) \u2014 The job Returns (SpecPath) The path of the wrapped job script method jobcmd_shebang ( job ) \u2192 str </> The shebang of the wrapper script method submit_job ( job ) </> Submit a job to Slurm Parameters job (Job) \u2014 The job Returns (str) The job id method kill_job ( job ) </> Kill a job on Slurm Parameters job (Job) \u2014 The job method job_fails_before_running ( job ) </> Check if a job fails before running. For some schedulers, the job might fail before running (after submission). For example, the job might fail to allocate resources. In such a case, the wrapped script might not be executed, and the job status will not be updated (stays in SUBMITTED). We need to check such jobs and mark them as FAILED. For the instant scheduler, for example, the local scheduler, the failure will be immediately reported when submitting the job, so we don't need to check such jobs. Parameters job (Job) \u2014 The job to check Returns (bool) True if the job fails before running, otherwise False. method job_is_running ( job ) </> Tell if a job is really running, not only the job.jid_file In case where the jid file is not cleaned when job is done. Parameters job (Job) \u2014 The job Returns (bool) True if it is, otherwise False class pipen.scheduler. SshScheduler ( *args , **kwargs ) </> Bases pipen.scheduler.SchedulerPostInit xqute.schedulers.ssh_scheduler.scheduler.SshScheduler xqute.schedulers.local_scheduler.LocalScheduler xqute.scheduler.Scheduler SSH scheduler Parameters **kwargs \u2014 Other arguments for the scheduler Attributes job_class \u2014 The job class jobcmd_wrapper_init \u2014 The init script for the job command wrapper jobcmd_wrapper_init (str) \u2014 The init script for the job command wrapper </> name \u2014 The name of the scheduler Methods check_all_done ( jobs , polling_counter ) (bool) \u2014 Check if all jobs are done (full polling with hooks) </> count_running_jobs ( jobs ) (int) \u2014 Count currently running/active jobs (lightweight check) </> create_job ( index , cmd , envs ) (Job) \u2014 Create a job </> job_fails_before_running ( job ) (bool) \u2014 Check if a job fails before running. </> job_is_running ( job ) (bool) \u2014 Tell if a job is really running, not only the job.jid_file </> job_is_submitted_or_running ( job ) (bool) \u2014 Check if a job is already submitted or running </> jobcmd_end ( job ) (str) \u2014 The job command end </> jobcmd_init ( job ) (str) \u2014 The job command init </> jobcmd_prep ( job ) (str) \u2014 The job command preparation </> jobcmd_shebang ( job ) (str) \u2014 The shebang of the wrapper script </> kill_job ( job ) \u2014 Kill a job on SSH </> kill_job_and_update_status ( job ) \u2014 Kill a job and update its status </> kill_running_jobs ( jobs ) \u2014 Try to kill all running jobs </> retry_job ( job ) \u2014 Retry a job </> submit_job ( job ) (str) \u2014 Submit a job to SSH </> submit_job_and_update_status ( job ) \u2014 Submit and update the status </> transition_job_status ( job , new_status , old_status , flush , rc , error_msg , is_killed ) \u2014 Centralized status transition handler </> wrap_job_script ( job ) (str) \u2014 Wrap the job script </> wrapped_job_script ( job ) (SpecPath) \u2014 Get the wrapped job script </> method create_job ( index , cmd , envs=None ) </> Create a job Parameters index (int) \u2014 The index of the job cmd (Union) \u2014 The command of the job Returns (Job) The job method submit_job_and_update_status ( job ) </> Submit and update the status Check if the job is already submitted or running If not, run the hook If the hook is not cancelled, clean the job Submit the job, raising an exception if it fails If the job is submitted successfully, update the status If the job fails to submit, update the status and write stderr to the job file Parameters job (Job) \u2014 The job method transition_job_status ( job , new_status , old_status=None , flush=True , rc=None , error_msg=None , is_killed=False ) </> Centralized status transition handler Handles all aspects of job status transitions: - Status change logging - Hook lifecycle management (ensuring on_job_started is called) - Appropriate hook calls based on new status - RC file updates - Error message appending to stderr - JID file cleanup for terminal states - Pipeline halt on errors if configured Note that this method will not flush status changes to disk (job.status_file).You need to call job.set_status() separately if needed. Parameters job (Job) \u2014 The job to transition new_status (int) \u2014 The new status to transition to old_status (int | none, optional) \u2014 The previous status (if known).If None, will use job._status flush (bool, optional) \u2014 Whether to flush the status to disk rc (str | none, optional) \u2014 Optional return code to write to rc_file error_msg (str | none, optional) \u2014 Optional error message to append to stderr_file is_killed (bool, optional) \u2014 Whether this is a killed job (uses on_job_killed hook) method kill_job_and_update_status ( job ) </> Kill a job and update its status Parameters job (Job) \u2014 The job method retry_job ( job ) </> Retry a job Parameters job (Job) \u2014 The job method count_running_jobs ( jobs ) </> Count currently running/active jobs (lightweight check) This is optimized for the producer to check if new jobs can be submitted. It only counts jobs without refreshing status or calling hooks. Parameters jobs (List) \u2014 The list of jobs Returns (int) Number of jobs currently in active states method check_all_done ( jobs , polling_counter ) </> Check if all jobs are done (full polling with hooks) This does complete status refresh and calls all lifecycle hooks. Used by the main polling loop to track job completion. Parameters jobs (List) \u2014 The list of jobs polling_counter (int) \u2014 The polling counter for hook calls Returns (bool) True if all jobs are done, False otherwise method kill_running_jobs ( jobs ) </> Try to kill all running jobs Parameters jobs (List) \u2014 The list of jobs method job_is_submitted_or_running ( job ) </> Check if a job is already submitted or running Parameters job (Job) \u2014 The job Returns (bool) True if yes otherwise False. method job_fails_before_running ( job ) </> Check if a job fails before running. For some schedulers, the job might fail before running (after submission). For example, the job might fail to allocate resources. In such a case, the wrapped script might not be executed, and the job status will not be updated (stays in SUBMITTED). We need to check such jobs and mark them as FAILED. For the instant scheduler, for example, the local scheduler, the failure will be immediately reported when submitting the job, so we don't need to check such jobs. Parameters job (Job) \u2014 The job to check Returns (bool) True if the job fails before running, otherwise False. method jobcmd_shebang ( job ) \u2192 str </> The shebang of the wrapper script method jobcmd_init ( job ) \u2192 str </> The job command init method jobcmd_end ( job ) \u2192 str </> The job command end method wrap_job_script ( job ) </> Wrap the job script Parameters job (Job) \u2014 The job Returns (str) The wrapped script method wrapped_job_script ( job ) </> Get the wrapped job script Parameters job (Job) \u2014 The job Returns (SpecPath) The path of the wrapped job script method jobcmd_prep ( job ) \u2192 str </> The job command preparation method submit_job ( job ) </> Submit a job to SSH Parameters job (Job) \u2014 The job Returns (str) The job id method kill_job ( job ) </> Kill a job on SSH Parameters job (Job) \u2014 The job method job_is_running ( job ) </> Tell if a job is really running, not only the job.jid_file In case where the jid file is not cleaned when job is done. Parameters job (Job) \u2014 The job Returns (bool) True if it is, otherwise False class pipen.scheduler. GbatchScheduler ( *args , project , location , mount=None , service_account=None , network=None , subnetwork=None , no_external_ip_address=None , machine_type=None , provisioning_model=None , image_uri=None , entrypoint=None , commands=None , runnables=None , **kwargs ) </> Bases pipen.scheduler.SchedulerPostInit xqute.schedulers.gbatch_scheduler.GbatchScheduler xqute.scheduler.Scheduler Google Cloud Batch scheduler Parameters **kwargs \u2014 Keyword arguments for the configuration of a job (e.g. taskGroups).See more details at https://cloud.google.com/batch/docs/get-started . *args \u2014 Positional arguments for the base class project (str) \u2014 Google Cloud project ID location (str) \u2014 Google Cloud region or zone mount (str | Sequence[str] | None, optional) \u2014 GCS path to mount (e.g. gs://my-bucket:/mnt/my-bucket)You can pass a list of mounts. service_account (str | none, optional) \u2014 GCP service account email (e.g. test-account@example.com) network (str | none, optional) \u2014 GCP network (e.g. default-network) subnetwork (str | none, optional) \u2014 GCP subnetwork (e.g. regions/us-central1/subnetworks/default) no_external_ip_address (bool | none, optional) \u2014 Whether to disable external IP address machine_type (str | none, optional) \u2014 GCP machine type (e.g. e2-standard-4) provisioning_model (str | none, optional) \u2014 GCP provisioning model (e.g. SPOT) image_uri (str | none, optional) \u2014 Container image URI (e.g. ubuntu-2004-lts) entrypoint (str, optional) \u2014 Container entrypoint (e.g. /bin/bash) commands (str | Sequence[str] | None, optional) \u2014 The command list to run in the container.There are three ways to specify the commands: 1. If no entrypoint is specified, the final command will be [commands, wrapped_script], where the entrypoint is the wrapper script interpreter that is determined by JOBCMD_WRAPPER_LANG (e.g. /bin/bash), commands is the list you provided, and wrapped_script is the path to the wrapped job script. 2. You can specify something like \"-c\", then the final command will be [\"-c\", \"wrapper_script_interpreter, wrapper_script\"] 3. You can use the placeholders {lang} and {script} in the commands list, where {lang} will be replaced with the interpreter (e.g. /bin/bash) and {script} will be replaced with the path to the wrapped job script. For example, you can specify [\"{lang} {script}\"] and the final command will be [\"wrapper_interpreter, wrapper_script\"] runnables (Sequence[dict] | None, optional) \u2014 Additional runnables to run before or after the main job.Each runnable should be a dictionary that follows the GCP Batch API specification . You can also specify an \"order\" key in the dictionary to control the execution order of the runnables. Runnables with negative order will be executed before the main job, and those with non-negative order will be executed after the main job. The main job runnable will always be executed in the order it is defined in the list. Attributes jobcmd_wrapper_init \u2014 The init script for the job command wrapper jobcmd_wrapper_init (str) \u2014 The init script for the job command wrapper </> name \u2014 The name of the scheduler Methods check_all_done ( jobs , polling_counter ) (bool) \u2014 Check if all jobs are done (full polling with hooks) </> count_running_jobs ( jobs ) (int) \u2014 Count currently running/active jobs (lightweight check) </> create_job ( index , cmd , envs ) (Job) \u2014 Create a job </> job_fails_before_running ( job ) (bool) \u2014 Check if a job fails before running. </> job_is_running ( job ) (bool) \u2014 Check if a job is really running </> job_is_submitted_or_running ( job ) (bool) \u2014 Check if a job is already submitted or running </> jobcmd_end ( job ) (str) \u2014 The job command end </> jobcmd_init ( job ) (str) \u2014 The job command init </> jobcmd_prep ( job ) (str) \u2014 The job command preparation </> jobcmd_shebang ( job ) (str) \u2014 The shebang of the wrapper script </> kill_job ( job ) \u2014 Kill a job </> kill_job_and_update_status ( job ) \u2014 Kill a job and update its status </> kill_running_jobs ( jobs ) \u2014 Try to kill all running jobs </> retry_job ( job ) \u2014 Retry a job </> submit_job ( job ) (str) \u2014 Submit a job </> submit_job_and_update_status ( job ) \u2014 Submit and update the status </> transition_job_status ( job , new_status , old_status , flush , rc , error_msg , is_killed ) \u2014 Centralized status transition handler </> wrap_job_script ( job ) (str) \u2014 Wrap the job script </> wrapped_job_script ( job ) (SpecPath) \u2014 Get the wrapped job script </> method create_job ( index , cmd , envs=None ) </> Create a job Parameters index (int) \u2014 The index of the job cmd (Union) \u2014 The command of the job Returns (Job) The job method submit_job_and_update_status ( job ) </> Submit and update the status Check if the job is already submitted or running If not, run the hook If the hook is not cancelled, clean the job Submit the job, raising an exception if it fails If the job is submitted successfully, update the status If the job fails to submit, update the status and write stderr to the job file Parameters job (Job) \u2014 The job method transition_job_status ( job , new_status , old_status=None , flush=True , rc=None , error_msg=None , is_killed=False ) </> Centralized status transition handler Handles all aspects of job status transitions: - Status change logging - Hook lifecycle management (ensuring on_job_started is called) - Appropriate hook calls based on new status - RC file updates - Error message appending to stderr - JID file cleanup for terminal states - Pipeline halt on errors if configured Note that this method will not flush status changes to disk (job.status_file).You need to call job.set_status() separately if needed. Parameters job (Job) \u2014 The job to transition new_status (int) \u2014 The new status to transition to old_status (int | none, optional) \u2014 The previous status (if known).If None, will use job._status flush (bool, optional) \u2014 Whether to flush the status to disk rc (str | none, optional) \u2014 Optional return code to write to rc_file error_msg (str | none, optional) \u2014 Optional error message to append to stderr_file is_killed (bool, optional) \u2014 Whether this is a killed job (uses on_job_killed hook) method kill_job_and_update_status ( job ) </> Kill a job and update its status Parameters job (Job) \u2014 The job method retry_job ( job ) </> Retry a job Parameters job (Job) \u2014 The job method count_running_jobs ( jobs ) </> Count currently running/active jobs (lightweight check) This is optimized for the producer to check if new jobs can be submitted. It only counts jobs without refreshing status or calling hooks. Parameters jobs (List) \u2014 The list of jobs Returns (int) Number of jobs currently in active states method check_all_done ( jobs , polling_counter ) </> Check if all jobs are done (full polling with hooks) This does complete status refresh and calls all lifecycle hooks. Used by the main polling loop to track job completion. Parameters jobs (List) \u2014 The list of jobs polling_counter (int) \u2014 The polling counter for hook calls Returns (bool) True if all jobs are done, False otherwise method kill_running_jobs ( jobs ) </> Try to kill all running jobs Parameters jobs (List) \u2014 The list of jobs method job_is_submitted_or_running ( job ) </> Check if a job is already submitted or running Parameters job (Job) \u2014 The job Returns (bool) True if yes otherwise False. method jobcmd_shebang ( job ) \u2192 str </> The shebang of the wrapper script method jobcmd_prep ( job ) \u2192 str </> The job command preparation method jobcmd_end ( job ) \u2192 str </> The job command end method wrap_job_script ( job ) </> Wrap the job script Parameters job (Job) \u2014 The job Returns (str) The wrapped script method wrapped_job_script ( job ) </> Get the wrapped job script Parameters job (Job) \u2014 The job Returns (SpecPath) The path of the wrapped job script method submit_job ( job ) </> Submit a job Parameters job (Job) \u2014 The job Returns (str) The unique id in the scheduler system method kill_job ( job ) </> Kill a job Parameters job (Job) \u2014 The job method job_fails_before_running ( job ) </> Check if a job fails before running. For some schedulers, the job might fail before running (after submission). For example, the job might fail to allocate resources. In such a case, the wrapped script might not be executed, and the job status will not be updated (stays in SUBMITTED). We need to check such jobs and mark them as FAILED. For the instant scheduler, for example, the local scheduler, the failure will be immediately reported when submitting the job, so we don't need to check such jobs. Parameters job (Job) \u2014 The job to check Returns (bool) True if the job fails before running, otherwise False. method job_is_running ( job ) </> Check if a job is really running Parameters job (Job) \u2014 The job Returns (bool) True if yes otherwise False. method jobcmd_init ( job ) \u2192 str </> The job command init class pipen.scheduler. ContainerScheduler ( image , entrypoint='/bin/bash' , bin='docker' , volumes=None , remove=True , user=None , bin_args=None , **kwargs ) </> Bases pipen.scheduler.SchedulerPostInit xqute.schedulers.container_scheduler.ContainerScheduler xqute.schedulers.local_scheduler.LocalScheduler xqute.scheduler.Scheduler Scheduler to run jobs via containers (Docker/Podman/Apptainer) Parameters **kwargs \u2014 Additional arguments passed to parent Scheduler image (str) \u2014 Container image to use for running jobs entrypoint (str | List[str], optional) \u2014 Entrypoint command for the container bin (str, optional) \u2014 Path to container runtime binary (e.g. /path/to/docker) volumes (str | Sequence[str] | None, optional) \u2014 host:container volume mapping string or stringsor named volume mapping like MOUNTED=/path/on/host then it will be mounted to /mnt/disks/MOUNTED in the container. You can use environment variable MOUNTED in your job scripts to refer to the mounted path. remove (bool, optional) \u2014 Whether to remove the container after execution.Only applies to Docker/Podman. user (str | none, optional) \u2014 User to run the container as (only for Docker/Podman)By default, it runs as the current user (os.getuid() and os.getgid()) bin_args (List[str] | None, optional) \u2014 Additional arguments to pass to the container runtime Attributes job_class \u2014 The job class jobcmd_wrapper_init \u2014 The init script for the job command wrapper jobcmd_wrapper_init (str) \u2014 The init script for the job command wrapper </> name \u2014 The name of the scheduler Methods check_all_done ( jobs , polling_counter ) (bool) \u2014 Check if all jobs are done (full polling with hooks) </> count_running_jobs ( jobs ) (int) \u2014 Count currently running/active jobs (lightweight check) </> create_job ( index , cmd , envs ) (Job) \u2014 Create a job </> job_fails_before_running ( job ) (bool) \u2014 Check if a job fails before running. </> job_is_running ( job ) (bool) \u2014 Tell if a job is really running, not only the job.jid_file </> job_is_submitted_or_running ( job ) (bool) \u2014 Check if a job is already submitted or running </> jobcmd_end ( job ) (str) \u2014 The job command end </> jobcmd_init ( job ) (str) \u2014 The job command init </> jobcmd_prep ( job ) (str) \u2014 The job command preparation </> jobcmd_shebang ( job ) (str) \u2014 The shebang of the wrapper script </> kill_job ( job ) \u2014 Kill a job asynchronously </> kill_job_and_update_status ( job ) \u2014 Kill a job and update its status </> kill_running_jobs ( jobs ) \u2014 Try to kill all running jobs </> retry_job ( job ) \u2014 Retry a job </> submit_job ( job ) (int) \u2014 Submit a job locally </> submit_job_and_update_status ( job ) \u2014 Submit and update the status </> transition_job_status ( job , new_status , old_status , flush , rc , error_msg , is_killed ) \u2014 Centralized status transition handler </> wrap_job_script ( job ) (str) \u2014 Wrap the job script </> wrapped_job_script ( job ) (SpecPath) \u2014 Get the wrapped job script </> method create_job ( index , cmd , envs=None ) </> Create a job Parameters index (int) \u2014 The index of the job cmd (Union) \u2014 The command of the job Returns (Job) The job method submit_job_and_update_status ( job ) </> Submit and update the status Check if the job is already submitted or running If not, run the hook If the hook is not cancelled, clean the job Submit the job, raising an exception if it fails If the job is submitted successfully, update the status If the job fails to submit, update the status and write stderr to the job file Parameters job (Job) \u2014 The job method transition_job_status ( job , new_status , old_status=None , flush=True , rc=None , error_msg=None , is_killed=False ) </> Centralized status transition handler Handles all aspects of job status transitions: - Status change logging - Hook lifecycle management (ensuring on_job_started is called) - Appropriate hook calls based on new status - RC file updates - Error message appending to stderr - JID file cleanup for terminal states - Pipeline halt on errors if configured Note that this method will not flush status changes to disk (job.status_file).You need to call job.set_status() separately if needed. Parameters job (Job) \u2014 The job to transition new_status (int) \u2014 The new status to transition to old_status (int | none, optional) \u2014 The previous status (if known).If None, will use job._status flush (bool, optional) \u2014 Whether to flush the status to disk rc (str | none, optional) \u2014 Optional return code to write to rc_file error_msg (str | none, optional) \u2014 Optional error message to append to stderr_file is_killed (bool, optional) \u2014 Whether this is a killed job (uses on_job_killed hook) method kill_job_and_update_status ( job ) </> Kill a job and update its status Parameters job (Job) \u2014 The job method retry_job ( job ) </> Retry a job Parameters job (Job) \u2014 The job method count_running_jobs ( jobs ) </> Count currently running/active jobs (lightweight check) This is optimized for the producer to check if new jobs can be submitted. It only counts jobs without refreshing status or calling hooks. Parameters jobs (List) \u2014 The list of jobs Returns (int) Number of jobs currently in active states method check_all_done ( jobs , polling_counter ) </> Check if all jobs are done (full polling with hooks) This does complete status refresh and calls all lifecycle hooks. Used by the main polling loop to track job completion. Parameters jobs (List) \u2014 The list of jobs polling_counter (int) \u2014 The polling counter for hook calls Returns (bool) True if all jobs are done, False otherwise method kill_running_jobs ( jobs ) </> Try to kill all running jobs Parameters jobs (List) \u2014 The list of jobs method job_is_submitted_or_running ( job ) </> Check if a job is already submitted or running Parameters job (Job) \u2014 The job Returns (bool) True if yes otherwise False. method job_fails_before_running ( job ) </> Check if a job fails before running. For some schedulers, the job might fail before running (after submission). For example, the job might fail to allocate resources. In such a case, the wrapped script might not be executed, and the job status will not be updated (stays in SUBMITTED). We need to check such jobs and mark them as FAILED. For the instant scheduler, for example, the local scheduler, the failure will be immediately reported when submitting the job, so we don't need to check such jobs. Parameters job (Job) \u2014 The job to check Returns (bool) True if the job fails before running, otherwise False. method jobcmd_end ( job ) \u2192 str </> The job command end method wrap_job_script ( job ) </> Wrap the job script Parameters job (Job) \u2014 The job Returns (str) The wrapped script method kill_job ( job ) </> Kill a job asynchronously Parameters job (Job) \u2014 The job method job_is_running ( job ) </> Tell if a job is really running, not only the job.jid_file In case where the jid file is not cleaned when job is done. Parameters job (Job) \u2014 The job Returns (bool) True if it is, otherwise False method jobcmd_prep ( job ) \u2192 str </> The job command preparation method wrapped_job_script ( job ) </> Get the wrapped job script Parameters job (Job) \u2014 The job Returns (SpecPath) The path of the wrapped job script method jobcmd_shebang ( job ) \u2192 str </> The shebang of the wrapper script method submit_job ( job ) </> Submit a job locally Parameters job (Job) \u2014 The job Returns (int) The process id method jobcmd_init ( job ) \u2192 str </> The job command init function pipen.scheduler. get_scheduler ( scheduler ) </> Get the scheduler by name of the scheduler class itself Parameters scheduler (Union) \u2014 The scheduler class or name Returns (Type) The scheduler class","title":"pipen.scheduler"},{"location":"api/pipen.scheduler/#pipenscheduler","text":"</> Provide builting schedulers Classes SchedulerPostInit \u2014 Provides post init function for all schedulers </> LocalScheduler \u2014 Local scheduler </> SgeScheduler \u2014 SGE scheduler </> SlurmScheduler \u2014 Slurm scheduler </> SshScheduler \u2014 SSH scheduler </> GbatchScheduler \u2014 Google Cloud Batch scheduler </> ContainerScheduler \u2014 Scheduler to run jobs via containers (Docker/Podman/Apptainer) </> Functions get_scheduler ( scheduler ) (Type) \u2014 Get the scheduler by name of the scheduler class itself </> class","title":"pipen.scheduler"},{"location":"api/pipen.scheduler/#pipenschedulerschedulerpostinit","text":"</> Provides post init function for all schedulers class","title":"pipen.scheduler.SchedulerPostInit"},{"location":"api/pipen.scheduler/#pipenschedulerlocalscheduler","text":"</> Bases pipen.scheduler.SchedulerPostInit xqute.schedulers.local_scheduler.LocalScheduler xqute.scheduler.Scheduler Local scheduler Parameters workdir (str | Path) \u2014 The working directory forks (int, optional) \u2014 Max number of job forks error_strategy (str, optional) \u2014 The strategy when there is error happened num_retries (int, optional) \u2014 Max number of retries when error_strategy is retry prescript (str, optional) \u2014 The prescript to run before the job commandIt is a piece of script that inserted into the wrapper script, running on the scheduler system. postscript (str, optional) \u2014 The postscript to run when job finishedIt is a piece of script that inserted into the wrapper script, running on the scheduler system. jobname_prefix (str | none, optional) \u2014 The prefix for the job name submission_batch (int | none, optional) \u2014 The number of consumers to submit jobs. This allowsmultiple jobs to be submitted in parallel. This is useful when there are many jobs to be submitted and the scheduler has a high latency for each submission. Set this to a smaller number if the scheduler cannot handle too many simultaneous submissions. recheck_interval (int, optional) \u2014 The number of polling iterations between rechecks ofwhether a job is still running on the scheduler. Helps detect jobs that fail before the wrapped script updates status (e.g., resource allocation failures). Each iteration takes xqute.defaults.SLEEP_INTERVAL_POLLING_JOBS cwd (str | Path, optional) \u2014 The working directory for the job command wrapper **kwargs \u2014 Other arguments for the scheduler Attributes job_class \u2014 The job class jobcmd_wrapper_init \u2014 The init script for the job command wrapper jobcmd_wrapper_init (str) \u2014 The init script for the job command wrapper </> name \u2014 The name of the scheduler Methods check_all_done ( jobs , polling_counter ) (bool) \u2014 Check if all jobs are done (full polling with hooks) </> count_running_jobs ( jobs ) (int) \u2014 Count currently running/active jobs (lightweight check) </> create_job ( index , cmd , envs ) (Job) \u2014 Create a job </> job_fails_before_running ( job ) (bool) \u2014 Check if a job fails before running. </> job_is_running ( job ) (bool) \u2014 Tell if a job is really running, not only the job.jid_file </> job_is_submitted_or_running ( job ) (bool) \u2014 Check if a job is already submitted or running </> jobcmd_end ( job ) (str) \u2014 The job command end </> jobcmd_init ( job ) (str) \u2014 The job command init </> jobcmd_prep ( job ) (str) \u2014 The job command preparation </> jobcmd_shebang ( job ) (str) \u2014 The shebang of the wrapper script </> kill_job ( job ) \u2014 Kill a job asynchronously </> kill_job_and_update_status ( job ) \u2014 Kill a job and update its status </> kill_running_jobs ( jobs ) \u2014 Try to kill all running jobs </> retry_job ( job ) \u2014 Retry a job </> submit_job ( job ) (int) \u2014 Submit a job locally </> submit_job_and_update_status ( job ) \u2014 Submit and update the status </> transition_job_status ( job , new_status , old_status , flush , rc , error_msg , is_killed ) \u2014 Centralized status transition handler </> wrap_job_script ( job ) (str) \u2014 Wrap the job script </> wrapped_job_script ( job ) (SpecPath) \u2014 Get the wrapped job script </> method","title":"pipen.scheduler.LocalScheduler"},{"location":"api/pipen.scheduler/#xquteschedulerschedulercreate_job","text":"</> Create a job Parameters index (int) \u2014 The index of the job cmd (Union) \u2014 The command of the job Returns (Job) The job method","title":"xqute.scheduler.Scheduler.create_job"},{"location":"api/pipen.scheduler/#xquteschedulerschedulersubmit_job_and_update_status","text":"</> Submit and update the status Check if the job is already submitted or running If not, run the hook If the hook is not cancelled, clean the job Submit the job, raising an exception if it fails If the job is submitted successfully, update the status If the job fails to submit, update the status and write stderr to the job file Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.submit_job_and_update_status"},{"location":"api/pipen.scheduler/#xquteschedulerschedulertransition_job_status","text":"</> Centralized status transition handler Handles all aspects of job status transitions: - Status change logging - Hook lifecycle management (ensuring on_job_started is called) - Appropriate hook calls based on new status - RC file updates - Error message appending to stderr - JID file cleanup for terminal states - Pipeline halt on errors if configured Note that this method will not flush status changes to disk (job.status_file).You need to call job.set_status() separately if needed. Parameters job (Job) \u2014 The job to transition new_status (int) \u2014 The new status to transition to old_status (int | none, optional) \u2014 The previous status (if known).If None, will use job._status flush (bool, optional) \u2014 Whether to flush the status to disk rc (str | none, optional) \u2014 Optional return code to write to rc_file error_msg (str | none, optional) \u2014 Optional error message to append to stderr_file is_killed (bool, optional) \u2014 Whether this is a killed job (uses on_job_killed hook) method","title":"xqute.scheduler.Scheduler.transition_job_status"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerkill_job_and_update_status","text":"</> Kill a job and update its status Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.kill_job_and_update_status"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerretry_job","text":"</> Retry a job Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.retry_job"},{"location":"api/pipen.scheduler/#xquteschedulerschedulercount_running_jobs","text":"</> Count currently running/active jobs (lightweight check) This is optimized for the producer to check if new jobs can be submitted. It only counts jobs without refreshing status or calling hooks. Parameters jobs (List) \u2014 The list of jobs Returns (int) Number of jobs currently in active states method","title":"xqute.scheduler.Scheduler.count_running_jobs"},{"location":"api/pipen.scheduler/#xquteschedulerschedulercheck_all_done","text":"</> Check if all jobs are done (full polling with hooks) This does complete status refresh and calls all lifecycle hooks. Used by the main polling loop to track job completion. Parameters jobs (List) \u2014 The list of jobs polling_counter (int) \u2014 The polling counter for hook calls Returns (bool) True if all jobs are done, False otherwise method","title":"xqute.scheduler.Scheduler.check_all_done"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerkill_running_jobs","text":"</> Try to kill all running jobs Parameters jobs (List) \u2014 The list of jobs method","title":"xqute.scheduler.Scheduler.kill_running_jobs"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjob_is_submitted_or_running","text":"</> Check if a job is already submitted or running Parameters job (Job) \u2014 The job Returns (bool) True if yes otherwise False. method","title":"xqute.scheduler.Scheduler.job_is_submitted_or_running"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjob_fails_before_running","text":"</> Check if a job fails before running. For some schedulers, the job might fail before running (after submission). For example, the job might fail to allocate resources. In such a case, the wrapped script might not be executed, and the job status will not be updated (stays in SUBMITTED). We need to check such jobs and mark them as FAILED. For the instant scheduler, for example, the local scheduler, the failure will be immediately reported when submitting the job, so we don't need to check such jobs. Parameters job (Job) \u2014 The job to check Returns (bool) True if the job fails before running, otherwise False. method","title":"xqute.scheduler.Scheduler.job_fails_before_running"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjobcmd_shebang","text":"</> The shebang of the wrapper script method","title":"xqute.scheduler.Scheduler.jobcmd_shebang"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjobcmd_init","text":"</> The job command init method","title":"xqute.scheduler.Scheduler.jobcmd_init"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjobcmd_end","text":"</> The job command end method","title":"xqute.scheduler.Scheduler.jobcmd_end"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerwrap_job_script","text":"</> Wrap the job script Parameters job (Job) \u2014 The job Returns (str) The wrapped script method","title":"xqute.scheduler.Scheduler.wrap_job_script"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerwrapped_job_script","text":"</> Get the wrapped job script Parameters job (Job) \u2014 The job Returns (SpecPath) The path of the wrapped job script method","title":"xqute.scheduler.Scheduler.wrapped_job_script"},{"location":"api/pipen.scheduler/#xquteschedulerslocal_schedulerlocalschedulersubmit_job","text":"</> Submit a job locally Parameters job (Job) \u2014 The job Returns (int) The process id method","title":"xqute.schedulers.local_scheduler.LocalScheduler.submit_job"},{"location":"api/pipen.scheduler/#xquteschedulerslocal_schedulerlocalschedulerkill_job","text":"</> Kill a job asynchronously Parameters job (Job) \u2014 The job method","title":"xqute.schedulers.local_scheduler.LocalScheduler.kill_job"},{"location":"api/pipen.scheduler/#xquteschedulerslocal_schedulerlocalschedulerjob_is_running","text":"</> Tell if a job is really running, not only the job.jid_file In case where the jid file is not cleaned when job is done. Parameters job (Job) \u2014 The job Returns (bool) True if it is, otherwise False method","title":"xqute.schedulers.local_scheduler.LocalScheduler.job_is_running"},{"location":"api/pipen.scheduler/#xquteschedulerslocal_schedulerlocalschedulerjobcmd_prep","text":"</> The job command preparation class","title":"xqute.schedulers.local_scheduler.LocalScheduler.jobcmd_prep"},{"location":"api/pipen.scheduler/#pipenschedulersgescheduler","text":"</> Bases pipen.scheduler.SchedulerPostInit xqute.schedulers.sge_scheduler.SgeScheduler xqute.scheduler.Scheduler SGE scheduler Parameters **kwargs \u2014 Other arguments for the scheduler Attributes job_class \u2014 The job class jobcmd_wrapper_init \u2014 The init script for the job command wrapper jobcmd_wrapper_init (str) \u2014 The init script for the job command wrapper </> name \u2014 The name of the scheduler Methods check_all_done ( jobs , polling_counter ) (bool) \u2014 Check if all jobs are done (full polling with hooks) </> count_running_jobs ( jobs ) (int) \u2014 Count currently running/active jobs (lightweight check) </> create_job ( index , cmd , envs ) (Job) \u2014 Create a job </> job_fails_before_running ( job ) (bool) \u2014 Check if a job fails before running. </> job_is_running ( job ) (bool) \u2014 Tell if a job is really running, not only the job.jid_file </> job_is_submitted_or_running ( job ) (bool) \u2014 Check if a job is already submitted or running </> jobcmd_end ( job ) (str) \u2014 The job command end </> jobcmd_init ( job ) (str) \u2014 The job command init </> jobcmd_prep ( job ) (str) \u2014 The job command preparation </> jobcmd_shebang ( job ) (str) \u2014 The shebang of the wrapper script </> kill_job ( job ) \u2014 Kill a job on SGE </> kill_job_and_update_status ( job ) \u2014 Kill a job and update its status </> kill_running_jobs ( jobs ) \u2014 Try to kill all running jobs </> retry_job ( job ) \u2014 Retry a job </> submit_job ( job ) (str) \u2014 Submit a job to SGE </> submit_job_and_update_status ( job ) \u2014 Submit and update the status </> transition_job_status ( job , new_status , old_status , flush , rc , error_msg , is_killed ) \u2014 Centralized status transition handler </> wrap_job_script ( job ) (str) \u2014 Wrap the job script </> wrapped_job_script ( job ) (SpecPath) \u2014 Get the wrapped job script </> method","title":"pipen.scheduler.SgeScheduler"},{"location":"api/pipen.scheduler/#xquteschedulerschedulercreate_job_1","text":"</> Create a job Parameters index (int) \u2014 The index of the job cmd (Union) \u2014 The command of the job Returns (Job) The job method","title":"xqute.scheduler.Scheduler.create_job"},{"location":"api/pipen.scheduler/#xquteschedulerschedulersubmit_job_and_update_status_1","text":"</> Submit and update the status Check if the job is already submitted or running If not, run the hook If the hook is not cancelled, clean the job Submit the job, raising an exception if it fails If the job is submitted successfully, update the status If the job fails to submit, update the status and write stderr to the job file Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.submit_job_and_update_status"},{"location":"api/pipen.scheduler/#xquteschedulerschedulertransition_job_status_1","text":"</> Centralized status transition handler Handles all aspects of job status transitions: - Status change logging - Hook lifecycle management (ensuring on_job_started is called) - Appropriate hook calls based on new status - RC file updates - Error message appending to stderr - JID file cleanup for terminal states - Pipeline halt on errors if configured Note that this method will not flush status changes to disk (job.status_file).You need to call job.set_status() separately if needed. Parameters job (Job) \u2014 The job to transition new_status (int) \u2014 The new status to transition to old_status (int | none, optional) \u2014 The previous status (if known).If None, will use job._status flush (bool, optional) \u2014 Whether to flush the status to disk rc (str | none, optional) \u2014 Optional return code to write to rc_file error_msg (str | none, optional) \u2014 Optional error message to append to stderr_file is_killed (bool, optional) \u2014 Whether this is a killed job (uses on_job_killed hook) method","title":"xqute.scheduler.Scheduler.transition_job_status"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerkill_job_and_update_status_1","text":"</> Kill a job and update its status Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.kill_job_and_update_status"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerretry_job_1","text":"</> Retry a job Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.retry_job"},{"location":"api/pipen.scheduler/#xquteschedulerschedulercount_running_jobs_1","text":"</> Count currently running/active jobs (lightweight check) This is optimized for the producer to check if new jobs can be submitted. It only counts jobs without refreshing status or calling hooks. Parameters jobs (List) \u2014 The list of jobs Returns (int) Number of jobs currently in active states method","title":"xqute.scheduler.Scheduler.count_running_jobs"},{"location":"api/pipen.scheduler/#xquteschedulerschedulercheck_all_done_1","text":"</> Check if all jobs are done (full polling with hooks) This does complete status refresh and calls all lifecycle hooks. Used by the main polling loop to track job completion. Parameters jobs (List) \u2014 The list of jobs polling_counter (int) \u2014 The polling counter for hook calls Returns (bool) True if all jobs are done, False otherwise method","title":"xqute.scheduler.Scheduler.check_all_done"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerkill_running_jobs_1","text":"</> Try to kill all running jobs Parameters jobs (List) \u2014 The list of jobs method","title":"xqute.scheduler.Scheduler.kill_running_jobs"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjob_is_submitted_or_running_1","text":"</> Check if a job is already submitted or running Parameters job (Job) \u2014 The job Returns (bool) True if yes otherwise False. method","title":"xqute.scheduler.Scheduler.job_is_submitted_or_running"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjob_fails_before_running_1","text":"</> Check if a job fails before running. For some schedulers, the job might fail before running (after submission). For example, the job might fail to allocate resources. In such a case, the wrapped script might not be executed, and the job status will not be updated (stays in SUBMITTED). We need to check such jobs and mark them as FAILED. For the instant scheduler, for example, the local scheduler, the failure will be immediately reported when submitting the job, so we don't need to check such jobs. Parameters job (Job) \u2014 The job to check Returns (bool) True if the job fails before running, otherwise False. method","title":"xqute.scheduler.Scheduler.job_fails_before_running"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjobcmd_init_1","text":"</> The job command init method","title":"xqute.scheduler.Scheduler.jobcmd_init"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjobcmd_prep","text":"</> The job command preparation method","title":"xqute.scheduler.Scheduler.jobcmd_prep"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjobcmd_end_1","text":"</> The job command end method","title":"xqute.scheduler.Scheduler.jobcmd_end"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerwrap_job_script_1","text":"</> Wrap the job script Parameters job (Job) \u2014 The job Returns (str) The wrapped script method","title":"xqute.scheduler.Scheduler.wrap_job_script"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerwrapped_job_script_1","text":"</> Get the wrapped job script Parameters job (Job) \u2014 The job Returns (SpecPath) The path of the wrapped job script method","title":"xqute.scheduler.Scheduler.wrapped_job_script"},{"location":"api/pipen.scheduler/#xquteschedulerssge_schedulersgeschedulerjobcmd_shebang","text":"</> The shebang of the wrapper script method","title":"xqute.schedulers.sge_scheduler.SgeScheduler.jobcmd_shebang"},{"location":"api/pipen.scheduler/#xquteschedulerssge_schedulersgeschedulersubmit_job","text":"</> Submit a job to SGE Parameters job (Job) \u2014 The job Returns (str) The job id method","title":"xqute.schedulers.sge_scheduler.SgeScheduler.submit_job"},{"location":"api/pipen.scheduler/#xquteschedulerssge_schedulersgeschedulerkill_job","text":"</> Kill a job on SGE Parameters job (Job) \u2014 The job method","title":"xqute.schedulers.sge_scheduler.SgeScheduler.kill_job"},{"location":"api/pipen.scheduler/#xquteschedulerssge_schedulersgeschedulerjob_is_running","text":"</> Tell if a job is really running, not only the job.jid_file In case where the jid file is not cleaned when job is done. Parameters job (Job) \u2014 The job Returns (bool) True if it is, otherwise False class","title":"xqute.schedulers.sge_scheduler.SgeScheduler.job_is_running"},{"location":"api/pipen.scheduler/#pipenschedulerslurmscheduler","text":"</> Bases pipen.scheduler.SchedulerPostInit xqute.schedulers.slurm_scheduler.SlurmScheduler xqute.scheduler.Scheduler Slurm scheduler Parameters **kwargs \u2014 Other arguments for the scheduler Attributes job_class \u2014 The job class jobcmd_wrapper_init \u2014 The init script for the job command wrapper jobcmd_wrapper_init (str) \u2014 The init script for the job command wrapper </> name \u2014 The name of the scheduler Methods check_all_done ( jobs , polling_counter ) (bool) \u2014 Check if all jobs are done (full polling with hooks) </> count_running_jobs ( jobs ) (int) \u2014 Count currently running/active jobs (lightweight check) </> create_job ( index , cmd , envs ) (Job) \u2014 Create a job </> job_fails_before_running ( job ) (bool) \u2014 Check if a job fails before running. </> job_is_running ( job ) (bool) \u2014 Tell if a job is really running, not only the job.jid_file </> job_is_submitted_or_running ( job ) (bool) \u2014 Check if a job is already submitted or running </> jobcmd_end ( job ) (str) \u2014 The job command end </> jobcmd_init ( job ) (str) \u2014 The job command init </> jobcmd_prep ( job ) (str) \u2014 The job command preparation </> jobcmd_shebang ( job ) (str) \u2014 The shebang of the wrapper script </> kill_job ( job ) \u2014 Kill a job on Slurm </> kill_job_and_update_status ( job ) \u2014 Kill a job and update its status </> kill_running_jobs ( jobs ) \u2014 Try to kill all running jobs </> retry_job ( job ) \u2014 Retry a job </> submit_job ( job ) (str) \u2014 Submit a job to Slurm </> submit_job_and_update_status ( job ) \u2014 Submit and update the status </> transition_job_status ( job , new_status , old_status , flush , rc , error_msg , is_killed ) \u2014 Centralized status transition handler </> wrap_job_script ( job ) (str) \u2014 Wrap the job script </> wrapped_job_script ( job ) (SpecPath) \u2014 Get the wrapped job script </> method","title":"pipen.scheduler.SlurmScheduler"},{"location":"api/pipen.scheduler/#xquteschedulerschedulercreate_job_2","text":"</> Create a job Parameters index (int) \u2014 The index of the job cmd (Union) \u2014 The command of the job Returns (Job) The job method","title":"xqute.scheduler.Scheduler.create_job"},{"location":"api/pipen.scheduler/#xquteschedulerschedulersubmit_job_and_update_status_2","text":"</> Submit and update the status Check if the job is already submitted or running If not, run the hook If the hook is not cancelled, clean the job Submit the job, raising an exception if it fails If the job is submitted successfully, update the status If the job fails to submit, update the status and write stderr to the job file Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.submit_job_and_update_status"},{"location":"api/pipen.scheduler/#xquteschedulerschedulertransition_job_status_2","text":"</> Centralized status transition handler Handles all aspects of job status transitions: - Status change logging - Hook lifecycle management (ensuring on_job_started is called) - Appropriate hook calls based on new status - RC file updates - Error message appending to stderr - JID file cleanup for terminal states - Pipeline halt on errors if configured Note that this method will not flush status changes to disk (job.status_file).You need to call job.set_status() separately if needed. Parameters job (Job) \u2014 The job to transition new_status (int) \u2014 The new status to transition to old_status (int | none, optional) \u2014 The previous status (if known).If None, will use job._status flush (bool, optional) \u2014 Whether to flush the status to disk rc (str | none, optional) \u2014 Optional return code to write to rc_file error_msg (str | none, optional) \u2014 Optional error message to append to stderr_file is_killed (bool, optional) \u2014 Whether this is a killed job (uses on_job_killed hook) method","title":"xqute.scheduler.Scheduler.transition_job_status"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerkill_job_and_update_status_2","text":"</> Kill a job and update its status Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.kill_job_and_update_status"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerretry_job_2","text":"</> Retry a job Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.retry_job"},{"location":"api/pipen.scheduler/#xquteschedulerschedulercount_running_jobs_2","text":"</> Count currently running/active jobs (lightweight check) This is optimized for the producer to check if new jobs can be submitted. It only counts jobs without refreshing status or calling hooks. Parameters jobs (List) \u2014 The list of jobs Returns (int) Number of jobs currently in active states method","title":"xqute.scheduler.Scheduler.count_running_jobs"},{"location":"api/pipen.scheduler/#xquteschedulerschedulercheck_all_done_2","text":"</> Check if all jobs are done (full polling with hooks) This does complete status refresh and calls all lifecycle hooks. Used by the main polling loop to track job completion. Parameters jobs (List) \u2014 The list of jobs polling_counter (int) \u2014 The polling counter for hook calls Returns (bool) True if all jobs are done, False otherwise method","title":"xqute.scheduler.Scheduler.check_all_done"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerkill_running_jobs_2","text":"</> Try to kill all running jobs Parameters jobs (List) \u2014 The list of jobs method","title":"xqute.scheduler.Scheduler.kill_running_jobs"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjob_is_submitted_or_running_2","text":"</> Check if a job is already submitted or running Parameters job (Job) \u2014 The job Returns (bool) True if yes otherwise False. method","title":"xqute.scheduler.Scheduler.job_is_submitted_or_running"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjobcmd_init_2","text":"</> The job command init method","title":"xqute.scheduler.Scheduler.jobcmd_init"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjobcmd_prep_1","text":"</> The job command preparation method","title":"xqute.scheduler.Scheduler.jobcmd_prep"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjobcmd_end_2","text":"</> The job command end method","title":"xqute.scheduler.Scheduler.jobcmd_end"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerwrap_job_script_2","text":"</> Wrap the job script Parameters job (Job) \u2014 The job Returns (str) The wrapped script method","title":"xqute.scheduler.Scheduler.wrap_job_script"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerwrapped_job_script_2","text":"</> Get the wrapped job script Parameters job (Job) \u2014 The job Returns (SpecPath) The path of the wrapped job script method","title":"xqute.scheduler.Scheduler.wrapped_job_script"},{"location":"api/pipen.scheduler/#xquteschedulersslurm_schedulerslurmschedulerjobcmd_shebang","text":"</> The shebang of the wrapper script method","title":"xqute.schedulers.slurm_scheduler.SlurmScheduler.jobcmd_shebang"},{"location":"api/pipen.scheduler/#xquteschedulersslurm_schedulerslurmschedulersubmit_job","text":"</> Submit a job to Slurm Parameters job (Job) \u2014 The job Returns (str) The job id method","title":"xqute.schedulers.slurm_scheduler.SlurmScheduler.submit_job"},{"location":"api/pipen.scheduler/#xquteschedulersslurm_schedulerslurmschedulerkill_job","text":"</> Kill a job on Slurm Parameters job (Job) \u2014 The job method","title":"xqute.schedulers.slurm_scheduler.SlurmScheduler.kill_job"},{"location":"api/pipen.scheduler/#xquteschedulersslurm_schedulerslurmschedulerjob_fails_before_running","text":"</> Check if a job fails before running. For some schedulers, the job might fail before running (after submission). For example, the job might fail to allocate resources. In such a case, the wrapped script might not be executed, and the job status will not be updated (stays in SUBMITTED). We need to check such jobs and mark them as FAILED. For the instant scheduler, for example, the local scheduler, the failure will be immediately reported when submitting the job, so we don't need to check such jobs. Parameters job (Job) \u2014 The job to check Returns (bool) True if the job fails before running, otherwise False. method","title":"xqute.schedulers.slurm_scheduler.SlurmScheduler.job_fails_before_running"},{"location":"api/pipen.scheduler/#xquteschedulersslurm_schedulerslurmschedulerjob_is_running","text":"</> Tell if a job is really running, not only the job.jid_file In case where the jid file is not cleaned when job is done. Parameters job (Job) \u2014 The job Returns (bool) True if it is, otherwise False class","title":"xqute.schedulers.slurm_scheduler.SlurmScheduler.job_is_running"},{"location":"api/pipen.scheduler/#pipenschedulersshscheduler","text":"</> Bases pipen.scheduler.SchedulerPostInit xqute.schedulers.ssh_scheduler.scheduler.SshScheduler xqute.schedulers.local_scheduler.LocalScheduler xqute.scheduler.Scheduler SSH scheduler Parameters **kwargs \u2014 Other arguments for the scheduler Attributes job_class \u2014 The job class jobcmd_wrapper_init \u2014 The init script for the job command wrapper jobcmd_wrapper_init (str) \u2014 The init script for the job command wrapper </> name \u2014 The name of the scheduler Methods check_all_done ( jobs , polling_counter ) (bool) \u2014 Check if all jobs are done (full polling with hooks) </> count_running_jobs ( jobs ) (int) \u2014 Count currently running/active jobs (lightweight check) </> create_job ( index , cmd , envs ) (Job) \u2014 Create a job </> job_fails_before_running ( job ) (bool) \u2014 Check if a job fails before running. </> job_is_running ( job ) (bool) \u2014 Tell if a job is really running, not only the job.jid_file </> job_is_submitted_or_running ( job ) (bool) \u2014 Check if a job is already submitted or running </> jobcmd_end ( job ) (str) \u2014 The job command end </> jobcmd_init ( job ) (str) \u2014 The job command init </> jobcmd_prep ( job ) (str) \u2014 The job command preparation </> jobcmd_shebang ( job ) (str) \u2014 The shebang of the wrapper script </> kill_job ( job ) \u2014 Kill a job on SSH </> kill_job_and_update_status ( job ) \u2014 Kill a job and update its status </> kill_running_jobs ( jobs ) \u2014 Try to kill all running jobs </> retry_job ( job ) \u2014 Retry a job </> submit_job ( job ) (str) \u2014 Submit a job to SSH </> submit_job_and_update_status ( job ) \u2014 Submit and update the status </> transition_job_status ( job , new_status , old_status , flush , rc , error_msg , is_killed ) \u2014 Centralized status transition handler </> wrap_job_script ( job ) (str) \u2014 Wrap the job script </> wrapped_job_script ( job ) (SpecPath) \u2014 Get the wrapped job script </> method","title":"pipen.scheduler.SshScheduler"},{"location":"api/pipen.scheduler/#xquteschedulerschedulercreate_job_3","text":"</> Create a job Parameters index (int) \u2014 The index of the job cmd (Union) \u2014 The command of the job Returns (Job) The job method","title":"xqute.scheduler.Scheduler.create_job"},{"location":"api/pipen.scheduler/#xquteschedulerschedulersubmit_job_and_update_status_3","text":"</> Submit and update the status Check if the job is already submitted or running If not, run the hook If the hook is not cancelled, clean the job Submit the job, raising an exception if it fails If the job is submitted successfully, update the status If the job fails to submit, update the status and write stderr to the job file Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.submit_job_and_update_status"},{"location":"api/pipen.scheduler/#xquteschedulerschedulertransition_job_status_3","text":"</> Centralized status transition handler Handles all aspects of job status transitions: - Status change logging - Hook lifecycle management (ensuring on_job_started is called) - Appropriate hook calls based on new status - RC file updates - Error message appending to stderr - JID file cleanup for terminal states - Pipeline halt on errors if configured Note that this method will not flush status changes to disk (job.status_file).You need to call job.set_status() separately if needed. Parameters job (Job) \u2014 The job to transition new_status (int) \u2014 The new status to transition to old_status (int | none, optional) \u2014 The previous status (if known).If None, will use job._status flush (bool, optional) \u2014 Whether to flush the status to disk rc (str | none, optional) \u2014 Optional return code to write to rc_file error_msg (str | none, optional) \u2014 Optional error message to append to stderr_file is_killed (bool, optional) \u2014 Whether this is a killed job (uses on_job_killed hook) method","title":"xqute.scheduler.Scheduler.transition_job_status"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerkill_job_and_update_status_3","text":"</> Kill a job and update its status Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.kill_job_and_update_status"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerretry_job_3","text":"</> Retry a job Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.retry_job"},{"location":"api/pipen.scheduler/#xquteschedulerschedulercount_running_jobs_3","text":"</> Count currently running/active jobs (lightweight check) This is optimized for the producer to check if new jobs can be submitted. It only counts jobs without refreshing status or calling hooks. Parameters jobs (List) \u2014 The list of jobs Returns (int) Number of jobs currently in active states method","title":"xqute.scheduler.Scheduler.count_running_jobs"},{"location":"api/pipen.scheduler/#xquteschedulerschedulercheck_all_done_3","text":"</> Check if all jobs are done (full polling with hooks) This does complete status refresh and calls all lifecycle hooks. Used by the main polling loop to track job completion. Parameters jobs (List) \u2014 The list of jobs polling_counter (int) \u2014 The polling counter for hook calls Returns (bool) True if all jobs are done, False otherwise method","title":"xqute.scheduler.Scheduler.check_all_done"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerkill_running_jobs_3","text":"</> Try to kill all running jobs Parameters jobs (List) \u2014 The list of jobs method","title":"xqute.scheduler.Scheduler.kill_running_jobs"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjob_is_submitted_or_running_3","text":"</> Check if a job is already submitted or running Parameters job (Job) \u2014 The job Returns (bool) True if yes otherwise False. method","title":"xqute.scheduler.Scheduler.job_is_submitted_or_running"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjob_fails_before_running_2","text":"</> Check if a job fails before running. For some schedulers, the job might fail before running (after submission). For example, the job might fail to allocate resources. In such a case, the wrapped script might not be executed, and the job status will not be updated (stays in SUBMITTED). We need to check such jobs and mark them as FAILED. For the instant scheduler, for example, the local scheduler, the failure will be immediately reported when submitting the job, so we don't need to check such jobs. Parameters job (Job) \u2014 The job to check Returns (bool) True if the job fails before running, otherwise False. method","title":"xqute.scheduler.Scheduler.job_fails_before_running"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjobcmd_shebang_1","text":"</> The shebang of the wrapper script method","title":"xqute.scheduler.Scheduler.jobcmd_shebang"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjobcmd_init_3","text":"</> The job command init method","title":"xqute.scheduler.Scheduler.jobcmd_init"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjobcmd_end_3","text":"</> The job command end method","title":"xqute.scheduler.Scheduler.jobcmd_end"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerwrap_job_script_3","text":"</> Wrap the job script Parameters job (Job) \u2014 The job Returns (str) The wrapped script method","title":"xqute.scheduler.Scheduler.wrap_job_script"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerwrapped_job_script_3","text":"</> Get the wrapped job script Parameters job (Job) \u2014 The job Returns (SpecPath) The path of the wrapped job script method","title":"xqute.scheduler.Scheduler.wrapped_job_script"},{"location":"api/pipen.scheduler/#xquteschedulerslocal_schedulerlocalschedulerjobcmd_prep_1","text":"</> The job command preparation method","title":"xqute.schedulers.local_scheduler.LocalScheduler.jobcmd_prep"},{"location":"api/pipen.scheduler/#xquteschedulersssh_schedulerschedulersshschedulersubmit_job","text":"</> Submit a job to SSH Parameters job (Job) \u2014 The job Returns (str) The job id method","title":"xqute.schedulers.ssh_scheduler.scheduler.SshScheduler.submit_job"},{"location":"api/pipen.scheduler/#xquteschedulersssh_schedulerschedulersshschedulerkill_job","text":"</> Kill a job on SSH Parameters job (Job) \u2014 The job method","title":"xqute.schedulers.ssh_scheduler.scheduler.SshScheduler.kill_job"},{"location":"api/pipen.scheduler/#xquteschedulersssh_schedulerschedulersshschedulerjob_is_running","text":"</> Tell if a job is really running, not only the job.jid_file In case where the jid file is not cleaned when job is done. Parameters job (Job) \u2014 The job Returns (bool) True if it is, otherwise False class","title":"xqute.schedulers.ssh_scheduler.scheduler.SshScheduler.job_is_running"},{"location":"api/pipen.scheduler/#pipenschedulergbatchscheduler","text":"</> Bases pipen.scheduler.SchedulerPostInit xqute.schedulers.gbatch_scheduler.GbatchScheduler xqute.scheduler.Scheduler Google Cloud Batch scheduler Parameters **kwargs \u2014 Keyword arguments for the configuration of a job (e.g. taskGroups).See more details at https://cloud.google.com/batch/docs/get-started . *args \u2014 Positional arguments for the base class project (str) \u2014 Google Cloud project ID location (str) \u2014 Google Cloud region or zone mount (str | Sequence[str] | None, optional) \u2014 GCS path to mount (e.g. gs://my-bucket:/mnt/my-bucket)You can pass a list of mounts. service_account (str | none, optional) \u2014 GCP service account email (e.g. test-account@example.com) network (str | none, optional) \u2014 GCP network (e.g. default-network) subnetwork (str | none, optional) \u2014 GCP subnetwork (e.g. regions/us-central1/subnetworks/default) no_external_ip_address (bool | none, optional) \u2014 Whether to disable external IP address machine_type (str | none, optional) \u2014 GCP machine type (e.g. e2-standard-4) provisioning_model (str | none, optional) \u2014 GCP provisioning model (e.g. SPOT) image_uri (str | none, optional) \u2014 Container image URI (e.g. ubuntu-2004-lts) entrypoint (str, optional) \u2014 Container entrypoint (e.g. /bin/bash) commands (str | Sequence[str] | None, optional) \u2014 The command list to run in the container.There are three ways to specify the commands: 1. If no entrypoint is specified, the final command will be [commands, wrapped_script], where the entrypoint is the wrapper script interpreter that is determined by JOBCMD_WRAPPER_LANG (e.g. /bin/bash), commands is the list you provided, and wrapped_script is the path to the wrapped job script. 2. You can specify something like \"-c\", then the final command will be [\"-c\", \"wrapper_script_interpreter, wrapper_script\"] 3. You can use the placeholders {lang} and {script} in the commands list, where {lang} will be replaced with the interpreter (e.g. /bin/bash) and {script} will be replaced with the path to the wrapped job script. For example, you can specify [\"{lang} {script}\"] and the final command will be [\"wrapper_interpreter, wrapper_script\"] runnables (Sequence[dict] | None, optional) \u2014 Additional runnables to run before or after the main job.Each runnable should be a dictionary that follows the GCP Batch API specification . You can also specify an \"order\" key in the dictionary to control the execution order of the runnables. Runnables with negative order will be executed before the main job, and those with non-negative order will be executed after the main job. The main job runnable will always be executed in the order it is defined in the list. Attributes jobcmd_wrapper_init \u2014 The init script for the job command wrapper jobcmd_wrapper_init (str) \u2014 The init script for the job command wrapper </> name \u2014 The name of the scheduler Methods check_all_done ( jobs , polling_counter ) (bool) \u2014 Check if all jobs are done (full polling with hooks) </> count_running_jobs ( jobs ) (int) \u2014 Count currently running/active jobs (lightweight check) </> create_job ( index , cmd , envs ) (Job) \u2014 Create a job </> job_fails_before_running ( job ) (bool) \u2014 Check if a job fails before running. </> job_is_running ( job ) (bool) \u2014 Check if a job is really running </> job_is_submitted_or_running ( job ) (bool) \u2014 Check if a job is already submitted or running </> jobcmd_end ( job ) (str) \u2014 The job command end </> jobcmd_init ( job ) (str) \u2014 The job command init </> jobcmd_prep ( job ) (str) \u2014 The job command preparation </> jobcmd_shebang ( job ) (str) \u2014 The shebang of the wrapper script </> kill_job ( job ) \u2014 Kill a job </> kill_job_and_update_status ( job ) \u2014 Kill a job and update its status </> kill_running_jobs ( jobs ) \u2014 Try to kill all running jobs </> retry_job ( job ) \u2014 Retry a job </> submit_job ( job ) (str) \u2014 Submit a job </> submit_job_and_update_status ( job ) \u2014 Submit and update the status </> transition_job_status ( job , new_status , old_status , flush , rc , error_msg , is_killed ) \u2014 Centralized status transition handler </> wrap_job_script ( job ) (str) \u2014 Wrap the job script </> wrapped_job_script ( job ) (SpecPath) \u2014 Get the wrapped job script </> method","title":"pipen.scheduler.GbatchScheduler"},{"location":"api/pipen.scheduler/#xquteschedulerschedulercreate_job_4","text":"</> Create a job Parameters index (int) \u2014 The index of the job cmd (Union) \u2014 The command of the job Returns (Job) The job method","title":"xqute.scheduler.Scheduler.create_job"},{"location":"api/pipen.scheduler/#xquteschedulerschedulersubmit_job_and_update_status_4","text":"</> Submit and update the status Check if the job is already submitted or running If not, run the hook If the hook is not cancelled, clean the job Submit the job, raising an exception if it fails If the job is submitted successfully, update the status If the job fails to submit, update the status and write stderr to the job file Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.submit_job_and_update_status"},{"location":"api/pipen.scheduler/#xquteschedulerschedulertransition_job_status_4","text":"</> Centralized status transition handler Handles all aspects of job status transitions: - Status change logging - Hook lifecycle management (ensuring on_job_started is called) - Appropriate hook calls based on new status - RC file updates - Error message appending to stderr - JID file cleanup for terminal states - Pipeline halt on errors if configured Note that this method will not flush status changes to disk (job.status_file).You need to call job.set_status() separately if needed. Parameters job (Job) \u2014 The job to transition new_status (int) \u2014 The new status to transition to old_status (int | none, optional) \u2014 The previous status (if known).If None, will use job._status flush (bool, optional) \u2014 Whether to flush the status to disk rc (str | none, optional) \u2014 Optional return code to write to rc_file error_msg (str | none, optional) \u2014 Optional error message to append to stderr_file is_killed (bool, optional) \u2014 Whether this is a killed job (uses on_job_killed hook) method","title":"xqute.scheduler.Scheduler.transition_job_status"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerkill_job_and_update_status_4","text":"</> Kill a job and update its status Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.kill_job_and_update_status"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerretry_job_4","text":"</> Retry a job Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.retry_job"},{"location":"api/pipen.scheduler/#xquteschedulerschedulercount_running_jobs_4","text":"</> Count currently running/active jobs (lightweight check) This is optimized for the producer to check if new jobs can be submitted. It only counts jobs without refreshing status or calling hooks. Parameters jobs (List) \u2014 The list of jobs Returns (int) Number of jobs currently in active states method","title":"xqute.scheduler.Scheduler.count_running_jobs"},{"location":"api/pipen.scheduler/#xquteschedulerschedulercheck_all_done_4","text":"</> Check if all jobs are done (full polling with hooks) This does complete status refresh and calls all lifecycle hooks. Used by the main polling loop to track job completion. Parameters jobs (List) \u2014 The list of jobs polling_counter (int) \u2014 The polling counter for hook calls Returns (bool) True if all jobs are done, False otherwise method","title":"xqute.scheduler.Scheduler.check_all_done"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerkill_running_jobs_4","text":"</> Try to kill all running jobs Parameters jobs (List) \u2014 The list of jobs method","title":"xqute.scheduler.Scheduler.kill_running_jobs"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjob_is_submitted_or_running_4","text":"</> Check if a job is already submitted or running Parameters job (Job) \u2014 The job Returns (bool) True if yes otherwise False. method","title":"xqute.scheduler.Scheduler.job_is_submitted_or_running"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjobcmd_shebang_2","text":"</> The shebang of the wrapper script method","title":"xqute.scheduler.Scheduler.jobcmd_shebang"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjobcmd_prep_2","text":"</> The job command preparation method","title":"xqute.scheduler.Scheduler.jobcmd_prep"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjobcmd_end_4","text":"</> The job command end method","title":"xqute.scheduler.Scheduler.jobcmd_end"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerwrap_job_script_4","text":"</> Wrap the job script Parameters job (Job) \u2014 The job Returns (str) The wrapped script method","title":"xqute.scheduler.Scheduler.wrap_job_script"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerwrapped_job_script_4","text":"</> Get the wrapped job script Parameters job (Job) \u2014 The job Returns (SpecPath) The path of the wrapped job script method","title":"xqute.scheduler.Scheduler.wrapped_job_script"},{"location":"api/pipen.scheduler/#xquteschedulersgbatch_schedulergbatchschedulersubmit_job","text":"</> Submit a job Parameters job (Job) \u2014 The job Returns (str) The unique id in the scheduler system method","title":"xqute.schedulers.gbatch_scheduler.GbatchScheduler.submit_job"},{"location":"api/pipen.scheduler/#xquteschedulersgbatch_schedulergbatchschedulerkill_job","text":"</> Kill a job Parameters job (Job) \u2014 The job method","title":"xqute.schedulers.gbatch_scheduler.GbatchScheduler.kill_job"},{"location":"api/pipen.scheduler/#xquteschedulersgbatch_schedulergbatchschedulerjob_fails_before_running","text":"</> Check if a job fails before running. For some schedulers, the job might fail before running (after submission). For example, the job might fail to allocate resources. In such a case, the wrapped script might not be executed, and the job status will not be updated (stays in SUBMITTED). We need to check such jobs and mark them as FAILED. For the instant scheduler, for example, the local scheduler, the failure will be immediately reported when submitting the job, so we don't need to check such jobs. Parameters job (Job) \u2014 The job to check Returns (bool) True if the job fails before running, otherwise False. method","title":"xqute.schedulers.gbatch_scheduler.GbatchScheduler.job_fails_before_running"},{"location":"api/pipen.scheduler/#xquteschedulersgbatch_schedulergbatchschedulerjob_is_running","text":"</> Check if a job is really running Parameters job (Job) \u2014 The job Returns (bool) True if yes otherwise False. method","title":"xqute.schedulers.gbatch_scheduler.GbatchScheduler.job_is_running"},{"location":"api/pipen.scheduler/#xquteschedulersgbatch_schedulergbatchschedulerjobcmd_init","text":"</> The job command init class","title":"xqute.schedulers.gbatch_scheduler.GbatchScheduler.jobcmd_init"},{"location":"api/pipen.scheduler/#pipenschedulercontainerscheduler","text":"</> Bases pipen.scheduler.SchedulerPostInit xqute.schedulers.container_scheduler.ContainerScheduler xqute.schedulers.local_scheduler.LocalScheduler xqute.scheduler.Scheduler Scheduler to run jobs via containers (Docker/Podman/Apptainer) Parameters **kwargs \u2014 Additional arguments passed to parent Scheduler image (str) \u2014 Container image to use for running jobs entrypoint (str | List[str], optional) \u2014 Entrypoint command for the container bin (str, optional) \u2014 Path to container runtime binary (e.g. /path/to/docker) volumes (str | Sequence[str] | None, optional) \u2014 host:container volume mapping string or stringsor named volume mapping like MOUNTED=/path/on/host then it will be mounted to /mnt/disks/MOUNTED in the container. You can use environment variable MOUNTED in your job scripts to refer to the mounted path. remove (bool, optional) \u2014 Whether to remove the container after execution.Only applies to Docker/Podman. user (str | none, optional) \u2014 User to run the container as (only for Docker/Podman)By default, it runs as the current user (os.getuid() and os.getgid()) bin_args (List[str] | None, optional) \u2014 Additional arguments to pass to the container runtime Attributes job_class \u2014 The job class jobcmd_wrapper_init \u2014 The init script for the job command wrapper jobcmd_wrapper_init (str) \u2014 The init script for the job command wrapper </> name \u2014 The name of the scheduler Methods check_all_done ( jobs , polling_counter ) (bool) \u2014 Check if all jobs are done (full polling with hooks) </> count_running_jobs ( jobs ) (int) \u2014 Count currently running/active jobs (lightweight check) </> create_job ( index , cmd , envs ) (Job) \u2014 Create a job </> job_fails_before_running ( job ) (bool) \u2014 Check if a job fails before running. </> job_is_running ( job ) (bool) \u2014 Tell if a job is really running, not only the job.jid_file </> job_is_submitted_or_running ( job ) (bool) \u2014 Check if a job is already submitted or running </> jobcmd_end ( job ) (str) \u2014 The job command end </> jobcmd_init ( job ) (str) \u2014 The job command init </> jobcmd_prep ( job ) (str) \u2014 The job command preparation </> jobcmd_shebang ( job ) (str) \u2014 The shebang of the wrapper script </> kill_job ( job ) \u2014 Kill a job asynchronously </> kill_job_and_update_status ( job ) \u2014 Kill a job and update its status </> kill_running_jobs ( jobs ) \u2014 Try to kill all running jobs </> retry_job ( job ) \u2014 Retry a job </> submit_job ( job ) (int) \u2014 Submit a job locally </> submit_job_and_update_status ( job ) \u2014 Submit and update the status </> transition_job_status ( job , new_status , old_status , flush , rc , error_msg , is_killed ) \u2014 Centralized status transition handler </> wrap_job_script ( job ) (str) \u2014 Wrap the job script </> wrapped_job_script ( job ) (SpecPath) \u2014 Get the wrapped job script </> method","title":"pipen.scheduler.ContainerScheduler"},{"location":"api/pipen.scheduler/#xquteschedulerschedulercreate_job_5","text":"</> Create a job Parameters index (int) \u2014 The index of the job cmd (Union) \u2014 The command of the job Returns (Job) The job method","title":"xqute.scheduler.Scheduler.create_job"},{"location":"api/pipen.scheduler/#xquteschedulerschedulersubmit_job_and_update_status_5","text":"</> Submit and update the status Check if the job is already submitted or running If not, run the hook If the hook is not cancelled, clean the job Submit the job, raising an exception if it fails If the job is submitted successfully, update the status If the job fails to submit, update the status and write stderr to the job file Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.submit_job_and_update_status"},{"location":"api/pipen.scheduler/#xquteschedulerschedulertransition_job_status_5","text":"</> Centralized status transition handler Handles all aspects of job status transitions: - Status change logging - Hook lifecycle management (ensuring on_job_started is called) - Appropriate hook calls based on new status - RC file updates - Error message appending to stderr - JID file cleanup for terminal states - Pipeline halt on errors if configured Note that this method will not flush status changes to disk (job.status_file).You need to call job.set_status() separately if needed. Parameters job (Job) \u2014 The job to transition new_status (int) \u2014 The new status to transition to old_status (int | none, optional) \u2014 The previous status (if known).If None, will use job._status flush (bool, optional) \u2014 Whether to flush the status to disk rc (str | none, optional) \u2014 Optional return code to write to rc_file error_msg (str | none, optional) \u2014 Optional error message to append to stderr_file is_killed (bool, optional) \u2014 Whether this is a killed job (uses on_job_killed hook) method","title":"xqute.scheduler.Scheduler.transition_job_status"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerkill_job_and_update_status_5","text":"</> Kill a job and update its status Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.kill_job_and_update_status"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerretry_job_5","text":"</> Retry a job Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.retry_job"},{"location":"api/pipen.scheduler/#xquteschedulerschedulercount_running_jobs_5","text":"</> Count currently running/active jobs (lightweight check) This is optimized for the producer to check if new jobs can be submitted. It only counts jobs without refreshing status or calling hooks. Parameters jobs (List) \u2014 The list of jobs Returns (int) Number of jobs currently in active states method","title":"xqute.scheduler.Scheduler.count_running_jobs"},{"location":"api/pipen.scheduler/#xquteschedulerschedulercheck_all_done_5","text":"</> Check if all jobs are done (full polling with hooks) This does complete status refresh and calls all lifecycle hooks. Used by the main polling loop to track job completion. Parameters jobs (List) \u2014 The list of jobs polling_counter (int) \u2014 The polling counter for hook calls Returns (bool) True if all jobs are done, False otherwise method","title":"xqute.scheduler.Scheduler.check_all_done"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerkill_running_jobs_5","text":"</> Try to kill all running jobs Parameters jobs (List) \u2014 The list of jobs method","title":"xqute.scheduler.Scheduler.kill_running_jobs"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjob_is_submitted_or_running_5","text":"</> Check if a job is already submitted or running Parameters job (Job) \u2014 The job Returns (bool) True if yes otherwise False. method","title":"xqute.scheduler.Scheduler.job_is_submitted_or_running"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjob_fails_before_running_3","text":"</> Check if a job fails before running. For some schedulers, the job might fail before running (after submission). For example, the job might fail to allocate resources. In such a case, the wrapped script might not be executed, and the job status will not be updated (stays in SUBMITTED). We need to check such jobs and mark them as FAILED. For the instant scheduler, for example, the local scheduler, the failure will be immediately reported when submitting the job, so we don't need to check such jobs. Parameters job (Job) \u2014 The job to check Returns (bool) True if the job fails before running, otherwise False. method","title":"xqute.scheduler.Scheduler.job_fails_before_running"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjobcmd_end_5","text":"</> The job command end method","title":"xqute.scheduler.Scheduler.jobcmd_end"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerwrap_job_script_5","text":"</> Wrap the job script Parameters job (Job) \u2014 The job Returns (str) The wrapped script method","title":"xqute.scheduler.Scheduler.wrap_job_script"},{"location":"api/pipen.scheduler/#xquteschedulerslocal_schedulerlocalschedulerkill_job_1","text":"</> Kill a job asynchronously Parameters job (Job) \u2014 The job method","title":"xqute.schedulers.local_scheduler.LocalScheduler.kill_job"},{"location":"api/pipen.scheduler/#xquteschedulerslocal_schedulerlocalschedulerjob_is_running_1","text":"</> Tell if a job is really running, not only the job.jid_file In case where the jid file is not cleaned when job is done. Parameters job (Job) \u2014 The job Returns (bool) True if it is, otherwise False method","title":"xqute.schedulers.local_scheduler.LocalScheduler.job_is_running"},{"location":"api/pipen.scheduler/#xquteschedulerslocal_schedulerlocalschedulerjobcmd_prep_2","text":"</> The job command preparation method","title":"xqute.schedulers.local_scheduler.LocalScheduler.jobcmd_prep"},{"location":"api/pipen.scheduler/#xquteschedulerscontainer_schedulercontainerschedulerwrapped_job_script","text":"</> Get the wrapped job script Parameters job (Job) \u2014 The job Returns (SpecPath) The path of the wrapped job script method","title":"xqute.schedulers.container_scheduler.ContainerScheduler.wrapped_job_script"},{"location":"api/pipen.scheduler/#xquteschedulerscontainer_schedulercontainerschedulerjobcmd_shebang","text":"</> The shebang of the wrapper script method","title":"xqute.schedulers.container_scheduler.ContainerScheduler.jobcmd_shebang"},{"location":"api/pipen.scheduler/#xquteschedulerscontainer_schedulercontainerschedulersubmit_job","text":"</> Submit a job locally Parameters job (Job) \u2014 The job Returns (int) The process id method","title":"xqute.schedulers.container_scheduler.ContainerScheduler.submit_job"},{"location":"api/pipen.scheduler/#xquteschedulerscontainer_schedulercontainerschedulerjobcmd_init","text":"</> The job command init function","title":"xqute.schedulers.container_scheduler.ContainerScheduler.jobcmd_init"},{"location":"api/pipen.scheduler/#pipenschedulerget_scheduler","text":"</> Get the scheduler by name of the scheduler class itself Parameters scheduler (Union) \u2014 The scheduler class or name Returns (Type) The scheduler class","title":"pipen.scheduler.get_scheduler"},{"location":"api/pipen.template/","text":"module pipen. template </> Template adaptor for pipen Classes Template ( source , **kwargs ) \u2014 Base class wrapper to wrap template for pipen </> TemplateLiquid \u2014 Liquidpy template wrapper. </> TemplateJinja2 \u2014 Jinja2 template wrapper </> Functions get_template_engine ( template ) (Type) \u2014 Get the template engine by name or the template engine itself </> abstract class pipen.template. Template ( source , **kwargs ) </> Base class wrapper to wrap template for pipen Methods render ( data ) (str) \u2014 Render the template@parmas: data (dict): The data used to render </> method render ( data=None ) \u2192 str </> Render the template@parmas: data (dict): The data used to render class pipen.template. TemplateLiquid ( source , **kwargs ) </> Bases pipen.template.Template Liquidpy template wrapper. Methods render ( data ) (str) \u2014 Render the template@parmas: data (dict): The data used to render </> method render ( data=None ) \u2192 str </> Render the template@parmas: data (dict): The data used to render class pipen.template. TemplateJinja2 ( source , **kwargs ) </> Bases pipen.template.Template Jinja2 template wrapper Methods render ( data ) (str) \u2014 Render the template@parmas: data (dict): The data used to render </> method render ( data=None ) \u2192 str </> Render the template@parmas: data (dict): The data used to render function pipen.template. get_template_engine ( template ) </> Get the template engine by name or the template engine itself Parameters template (Union) \u2014 The name of the template engine or the template engine itself Returns (Type) The template engine","title":"pipen.template"},{"location":"api/pipen.template/#pipentemplate","text":"</> Template adaptor for pipen Classes Template ( source , **kwargs ) \u2014 Base class wrapper to wrap template for pipen </> TemplateLiquid \u2014 Liquidpy template wrapper. </> TemplateJinja2 \u2014 Jinja2 template wrapper </> Functions get_template_engine ( template ) (Type) \u2014 Get the template engine by name or the template engine itself </> abstract class","title":"pipen.template"},{"location":"api/pipen.template/#pipentemplatetemplate","text":"</> Base class wrapper to wrap template for pipen Methods render ( data ) (str) \u2014 Render the template@parmas: data (dict): The data used to render </> method","title":"pipen.template.Template"},{"location":"api/pipen.template/#pipentemplatetemplaterender","text":"</> Render the template@parmas: data (dict): The data used to render class","title":"pipen.template.Template.render"},{"location":"api/pipen.template/#pipentemplatetemplateliquid","text":"</> Bases pipen.template.Template Liquidpy template wrapper. Methods render ( data ) (str) \u2014 Render the template@parmas: data (dict): The data used to render </> method","title":"pipen.template.TemplateLiquid"},{"location":"api/pipen.template/#pipentemplatetemplaterender_1","text":"</> Render the template@parmas: data (dict): The data used to render class","title":"pipen.template.Template.render"},{"location":"api/pipen.template/#pipentemplatetemplatejinja2","text":"</> Bases pipen.template.Template Jinja2 template wrapper Methods render ( data ) (str) \u2014 Render the template@parmas: data (dict): The data used to render </> method","title":"pipen.template.TemplateJinja2"},{"location":"api/pipen.template/#pipentemplatetemplaterender_2","text":"</> Render the template@parmas: data (dict): The data used to render function","title":"pipen.template.Template.render"},{"location":"api/pipen.template/#pipentemplateget_template_engine","text":"</> Get the template engine by name or the template engine itself Parameters template (Union) \u2014 The name of the template engine or the template engine itself Returns (Type) The template engine","title":"pipen.template.get_template_engine"},{"location":"api/pipen.utils/","text":"module pipen. utils </> Provide some utilities Classes RichHandler \u2014 Subclass of rich.logging.RichHandler, showing log levels as a singlecharacter </> RichConsole \u2014 A high level console interface. </> Functions brief_list ( blist ) (str) \u2014 Briefly show an integer list, combine the continuous numbers. </> copy_dict ( dic , depth ) (Mapping) \u2014 Deep copy a dict </> desc_from_docstring ( obj , base ) (str) \u2014 Get the description from docstring </> get_base ( klass , abc_base , value , value_getter ) (Type) \u2014 Get the base class where the value was first defined </> get_logger ( name , level ) (LoggerAdapter) \u2014 Get the logger by given plugin name </> get_logpanel_width ( ) (int) \u2014 Get the width of the log content </> get_marked ( cls , mark_name , default ) (Any) \u2014 Get the marked value from a proc </> get_mtime ( path , dir_depth ) (float) \u2014 Get the modification time of a path.If path is a directory, try to get the last modification time of the contents in the directory at given dir_depth </> get_shebang ( script ) (str) \u2014 Get the shebang of the script </> ignore_firstline_dedent ( text ) (str) \u2014 Like textwrap.dedent(), but ignore first empty lines </> is_loading_pipeline ( *flags , argv ) (bool) \u2014 Check if we are loading the pipeline. Works only when argv0 is \"@pipen\" while loading the pipeline. </> is_subclass ( obj , cls ) (bool) \u2014 Tell if obj is a subclass of clsDifferences with issubclass is that we don't raise Type error if obj is not a class </> is_valid_name ( name ) (bool) \u2014 Check if a name is valid for a proc or pipen </> load_entrypoints ( group ) (Iterable) \u2014 Load objects from setuptools entrypoints by given group name </> load_pipeline ( obj , argv0 , argv1p , **kwargs ) (Pipen) \u2014 Load a pipeline from a Pipen, Proc or ProcGroup object </> log_rich_renderable ( renderable , color , logfunc , *args , **kwargs ) \u2014 Log a rich renderable to logger </> make_df_colnames_unique_inplace ( thedf ) \u2014 Make the columns of a data frame unique </> mark ( **kwargs ) (Callable) \u2014 Mark a class (e.g. Proc) with given kwargs as metadata </> path_is_symlink ( path ) (bool) \u2014 Check if a path is a symlink. </> path_is_symlink_sync ( path ) (bool) \u2014 Check if a path is a symlink synchronously. </> path_symlink_to ( src , dst , target_is_directory ) \u2014 Create a symbolic link pointing to src named dst. </> pipen_banner ( ) (RenderableType) \u2014 The banner for pipen </> strsplit ( string , sep , maxsplit , trim ) (List) \u2014 Split the string, with the ability to trim each part. </> truncate_text ( text , width , end ) (str) \u2014 Truncate a text not based on words/whitespacesOtherwise, we could use textwrap.shorten. </> update_dict ( parent , new , depth , try_list ) (Mapping) \u2014 Update the new dict to the parent, but make sure parent does not change </> class pipen.utils. RichHandler ( level=0 , console=None , show_time=True , omit_repeated_times=True , show_level=True , show_path=True , enable_link_path=True , highlighter=None , markup=False , rich_tracebacks=False , tracebacks_width=None , tracebacks_code_width=88 , tracebacks_extra_lines=3 , tracebacks_theme=None , tracebacks_word_wrap=True , tracebacks_show_locals=False , tracebacks_suppress=() , tracebacks_max_frames=100 , locals_max_length=10 , locals_max_string=80 , log_time_format='[%x %X]' , keywords=None ) </> Bases rich.logging.RichHandler logging.Handler logging.Filterer Subclass of rich.logging.RichHandler, showing log levels as a singlecharacter Parameters level (Union, optional) \u2014 Log level. Defaults to logging.NOTSET. show_time (bool, optional) \u2014 Show a column for the time. Defaults to True. omit_repeated_times (bool, optional) \u2014 Omit repetition of the same time. Defaults to True. show_level (bool, optional) \u2014 Show a column for the level. Defaults to True. show_path (bool, optional) \u2014 Show the path to the original log call. Defaults to True. enable_link_path (bool, optional) \u2014 Enable terminal link of path column to file. Defaults to True. highlighter (Optional, optional) \u2014 Highlighter to style log messages, or None to use ReprHighlighter. Defaults to None. markup (bool, optional) \u2014 Enable console markup in log messages. Defaults to False. rich_tracebacks (bool, optional) \u2014 Enable rich tracebacks with syntax highlighting and formatting. Defaults to False. tracebacks_width (Optional, optional) \u2014 Number of characters used to render tracebacks, or None for full width. Defaults to None. tracebacks_code_width (Optional, optional) \u2014 Number of code characters used to render tracebacks, or None for full width. Defaults to 88. tracebacks_extra_lines (int, optional) \u2014 Additional lines of code to render tracebacks, or None for full width. Defaults to None. tracebacks_theme (Optional, optional) \u2014 Override pygments theme used in traceback. tracebacks_word_wrap (bool, optional) \u2014 Enable word wrapping of long tracebacks lines. Defaults to True. tracebacks_show_locals (bool, optional) \u2014 Enable display of locals in tracebacks. Defaults to False. tracebacks_suppress (Iterable, optional) \u2014 Optional sequence of modules or paths to exclude from traceback. tracebacks_max_frames (int, optional) \u2014 Optional maximum number of frames returned by traceback. locals_max_length (int, optional) \u2014 Maximum length of containers before abbreviating, or None for no abbreviation.Defaults to 10. locals_max_string (int, optional) \u2014 Maximum length of string before truncating, or None to disable. Defaults to 80. log_time_format (Union, optional) \u2014 If log_time is enabled, either string for strftime or callable that formats the time. Defaults to \"[%x %X] \". keywords (Optional, optional) \u2014 List of words to highlight instead of RichHandler.KEYWORDS . Methods acquire ( ) \u2014 Acquire the I/O thread lock. </> addFilter ( filter ) \u2014 Add the specified filter to this handler. </> close ( ) \u2014 Tidy up any resources used by the handler. </> createLock ( ) \u2014 Acquire a thread lock for serializing access to the underlying I/O. </> emit ( record ) \u2014 Invoked by logging. </> filter ( record ) \u2014 Determine if a record is loggable by consulting all the filters. </> flush ( ) \u2014 Ensure all logging output has been flushed. </> format ( record ) \u2014 Format the specified record. </> get_level_text ( record ) (Text) \u2014 Get the level name from the record. </> handle ( record ) \u2014 Conditionally emit the specified logging record. </> handleError ( record ) \u2014 Handle errors which occur during an emit() call. </> release ( ) \u2014 Release the I/O thread lock. </> removeFilter ( filter ) \u2014 Remove the specified filter from this handler. </> render ( record , traceback , message_renderable ) (ConsoleRenderable) \u2014 Render log for display. </> render_message ( record , message ) (ConsoleRenderable) \u2014 Render message text in to Text. </> setFormatter ( fmt ) \u2014 Set the formatter for this handler. </> setLevel ( level ) \u2014 Set the logging level of this handler. level must be an int or a str. </> method addFilter ( filter ) </> Add the specified filter to this handler. method removeFilter ( filter ) </> Remove the specified filter from this handler. method filter ( record ) </> Determine if a record is loggable by consulting all the filters. The default is to allow the record to be logged; any filter can veto this by returning a false value. If a filter attached to a handler returns a log record instance, then that instance is used in place of the original log record in any further processing of the event by that handler. If a filter returns any other true value, the original log record is used in any further processing of the event by that handler. If none of the filters return false values, this method returns a log record. If any of the filters return a false value, this method returns a false value. .. versionchanged:: 3.2 Allow filters to be just callables. .. versionchanged:: 3.12 Allow filters to return a LogRecord instead of modifying it in place. method createLock ( ) </> Acquire a thread lock for serializing access to the underlying I/O. method acquire ( ) </> Acquire the I/O thread lock. method release ( ) </> Release the I/O thread lock. method setLevel ( level ) </> Set the logging level of this handler. level must be an int or a str. method format ( record ) </> Format the specified record. If a formatter is set, use it. Otherwise, use the default formatter for the module. method handle ( record ) </> Conditionally emit the specified logging record. Emission depends on filters which may have been added to the handler. Wrap the actual emission of the record with acquisition/release of the I/O thread lock. Returns an instance of the log record that was emitted if it passed all filters, otherwise a false value is returned. method setFormatter ( fmt ) </> Set the formatter for this handler. method flush ( ) </> Ensure all logging output has been flushed. This version does nothing and is intended to be implemented by subclasses. method close ( ) </> Tidy up any resources used by the handler. This version removes the handler from an internal map of handlers, _handlers, which is used for handler lookup by name. Subclasses should ensure that this gets called from overridden close() methods. method handleError ( record ) </> Handle errors which occur during an emit() call. This method should be called from handlers when an exception is encountered during an emit() call. If raiseExceptions is false, exceptions get silently ignored. This is what is mostly wanted for a logging system - most users will not care about errors in the logging system, they are more interested in application errors. You could, however, replace this with a custom handler if you wish. The record which was being processed is passed in to this method. method emit ( record ) </> Invoked by logging. method render_message ( record , message ) </> Render message text in to Text. Parameters record (LogRecord) \u2014 logging Record. message (str) \u2014 String containing log message. Returns (ConsoleRenderable) Renderable to display log message. method render ( record , traceback , message_renderable ) </> Render log for display. Parameters record (LogRecord) \u2014 logging Record. traceback (Optional[Traceback]) \u2014 Traceback instance or None for no Traceback. message_renderable (ConsoleRenderable) \u2014 Renderable (typically Text) containing log message contents. Returns (ConsoleRenderable) Renderable to display log. method get_level_text ( record ) </> Get the level name from the record. Parameters record (LogRecord) \u2014 LogRecord instance. Returns (Text) A tuple of the style and level name. class pipen.utils. RichConsole ( *args , **kwargs ) </> Bases rich.console.Console A high level console interface. Attributes color_system \u2014 Get color system string. </> encoding \u2014 Get the encoding of the console file, e.g. \"utf-8\" . </> file (IO) \u2014 Get the file object to write to. </> height \u2014 Get the height of the console. </> is_alt_screen \u2014 Check if the alt screen was enabled. </> is_dumb_terminal \u2014 Detect dumb terminal. </> is_terminal \u2014 Check if the console is writing to a terminal. </> options (ConsoleOptions) \u2014 Get default console options. </> size \u2014 Get the size of the console. </> width \u2014 Get the width of the console. </> Methods __enter__ ( ) (Console) \u2014 Own context manager to enter buffer context. </> __exit__ ( exc_type , exc_value , traceback ) \u2014 Exit buffer context. </> begin_capture ( ) \u2014 Begin capturing console output. Call :meth: end_capture to exit capture mode and return output. </> bell ( ) \u2014 Play a 'bell' sound (if supported by the terminal). </> capture ( ) (Capture) \u2014 A context manager to capture the result of print() or log() in a string,rather than writing it to the console. </> clear ( home ) \u2014 Clear the screen. </> clear_live ( ) \u2014 Clear the Live instance. Used by the Live context manager (no need to call directly). </> control ( *control ) \u2014 Insert non-printing control codes. </> end_capture ( ) (str) \u2014 End capture mode and return captured string. </> export_html ( theme , clear , code_format , inline_styles ) (str) \u2014 Generate HTML from console contents (requires record=True argument in constructor). </> export_svg ( title , theme , clear , code_format , font_aspect_ratio , unique_id ) (str) \u2014 Generate an SVG from the console contents (requires record=True in Console constructor). </> export_text ( clear , styles ) (str) \u2014 Generate text from console contents (requires record=True argument in constructor). </> get_style ( name , default ) (Style) \u2014 Get a Style instance by its theme name or parse a definition. </> input ( prompt , markup , emoji , password , stream ) (str) \u2014 Displays a prompt and waits for input from the user. The prompt may contain color / style. </> line ( count ) \u2014 Write new line(s). </> log ( *objects , sep , end , style , justify , emoji , markup , highlight , log_locals , _stack_offset ) \u2014 Log rich content to the terminal. </> measure ( renderable , options ) (Measurement) \u2014 Measure a renderable. Returns a :class: ~rich.measure.Measurement object which containsinformation regarding the number of characters required to print the renderable. </> on_broken_pipe ( ) \u2014 This function is called when a BrokenPipeError is raised. </> out ( *objects , sep , end , style , highlight ) \u2014 Output to the terminal. This is a low-level way of writing to the terminal which unlike:meth: ~rich.console.Console.print won't pretty print, wrap text, or apply markup, but will optionally apply highlighting and a basic style. </> pager ( pager , styles , links ) (PagerContext) \u2014 A context manager to display anything printed within a \"pager\". The pager applicationis defined by the system and will typically support at least pressing a key to scroll. </> pop_render_hook ( ) \u2014 Pop the last renderhook from the stack. </> pop_theme ( ) \u2014 Remove theme from top of stack, restoring previous theme. </> print ( *objects , sep , end , style , justify , overflow , no_wrap , emoji , markup , highlight , width , height , crop , soft_wrap , new_line_start ) \u2014 Print to the console. </> print_exception ( width , extra_lines , theme , word_wrap , show_locals , suppress , max_frames ) \u2014 Prints a rich render of the last exception and traceback. </> print_json ( json , data , indent , highlight , skip_keys , ensure_ascii , check_circular , allow_nan , default , sort_keys ) \u2014 Pretty prints JSON. Output will be valid JSON. </> push_render_hook ( hook ) \u2014 Add a new render hook to the stack. </> push_theme ( theme , inherit ) \u2014 Push a new theme on to the top of the stack, replacing the styles from the previous theme.Generally speaking, you should call :meth: ~rich.console.Console.use_theme to get a context manager, rather than calling this method directly. </> render ( renderable , options ) (Iterable[Segment]) \u2014 Render an object in to an iterable of Segment instances. </> render_lines ( renderable , options , style , pad , new_lines ) (List) \u2014 Render objects in to a list of lines. </> render_str ( text , style , justify , overflow , emoji , markup , highlight , highlighter ) (ConsoleRenderable) \u2014 Convert a string to a Text instance. This is called automatically ifyou print or log a string. </> rule ( title , characters , style , align ) \u2014 Draw a line with optional centered title. </> save_html ( path , theme , clear , code_format , inline_styles ) \u2014 Generate HTML from console contents and write to a file (requires record=True argument in constructor). </> save_svg ( path , title , theme , clear , code_format , font_aspect_ratio , unique_id ) \u2014 Generate an SVG file from the console contents (requires record=True in Console constructor). </> save_text ( path , clear , styles ) \u2014 Generate text from console and save to a given location (requires record=True argument in constructor). </> screen ( hide_cursor , style ) (~ScreenContext) \u2014 Context manager to enable and disable 'alternative screen' mode. </> set_alt_screen ( enable ) (bool) \u2014 Enables alternative screen mode. </> set_live ( live ) (bool) \u2014 Set Live instance. Used by Live context manager (no need to call directly). </> set_window_title ( title ) (bool) \u2014 Set the title of the console terminal window. </> show_cursor ( show ) (bool) \u2014 Show or hide the cursor. </> status ( status , spinner , spinner_style , speed , refresh_per_second ) (Status) \u2014 Display a status and spinner. </> update_screen ( renderable , region , options ) \u2014 Update the screen at a given offset. </> update_screen_lines ( lines , x , y ) \u2014 Update lines of the screen at a given offset. </> use_theme ( theme , inherit ) (ThemeContext) \u2014 Use a different theme for the duration of the context manager. </> method set_live ( live ) </> Set Live instance. Used by Live context manager (no need to call directly). Parameters live (Live) \u2014 Live instance using this Console. Returns (bool) Boolean that indicates if the live is the topmost of the stack. Raises errors.LiveError \u2014 If this Console has a Live context currently active. method clear_live ( ) </> Clear the Live instance. Used by the Live context manager (no need to call directly). method push_render_hook ( hook ) </> Add a new render hook to the stack. Parameters hook (RenderHook) \u2014 Render hook instance. method pop_render_hook ( ) </> Pop the last renderhook from the stack. method __enter__ ( ) \u2192 Console </> Own context manager to enter buffer context. method __exit__ ( exc_type , exc_value , traceback ) </> Exit buffer context. method begin_capture ( ) </> Begin capturing console output. Call :meth: end_capture to exit capture mode and return output. method end_capture ( ) </> End capture mode and return captured string. Returns (str) Console output. method push_theme ( theme , inherit=True ) </> Push a new theme on to the top of the stack, replacing the styles from the previous theme.Generally speaking, you should call :meth: ~rich.console.Console.use_theme to get a context manager, rather than calling this method directly. Parameters theme (Theme) \u2014 A theme instance. inherit (bool, optional) \u2014 Inherit existing styles. Defaults to True. method pop_theme ( ) </> Remove theme from top of stack, restoring previous theme. method use_theme ( theme , inherit=True ) </> Use a different theme for the duration of the context manager. Parameters theme (Theme) \u2014 Theme instance to user. inherit (bool, optional) \u2014 Inherit existing console styles. Defaults to True. Returns (ThemeContext) [description] method bell ( ) </> Play a 'bell' sound (if supported by the terminal). method capture ( ) </> A context manager to capture the result of print() or log() in a string,rather than writing it to the console. Example >>> from rich.console import Console >>> console = Console () >>> with console . capture () as capture : ... console . print ( \"[bold magenta]Hello World[/]\" ) >>> print ( capture . get ()) Returns (Capture) Context manager with disables writing to the terminal. method pager ( pager=None , styles=False , links=False ) </> A context manager to display anything printed within a \"pager\". The pager applicationis defined by the system and will typically support at least pressing a key to scroll. Parameters pager (Pager, optional) \u2014 A pager object, or None to use :class: ~rich.pager.SystemPager . Defaults to None. styles (bool, optional) \u2014 Show styles in pager. Defaults to False. links (bool, optional) \u2014 Show links in pager. Defaults to False. Example >>> from rich.console import Console >>> from rich.__main__ import make_test_card >>> console = Console () >>> with console . pager (): console . print ( make_test_card ()) Returns (PagerContext) A context manager. method line ( count=1 ) </> Write new line(s). Parameters count (int, optional) \u2014 Number of new lines. Defaults to 1. method clear ( home=True ) </> Clear the screen. Parameters home (bool, optional) \u2014 Also move the cursor to 'home' position. Defaults to True. method status ( status , spinner='dots' , spinner_style='status.spinner' , speed=1.0 , refresh_per_second=12.5 ) </> Display a status and spinner. Parameters status (RenderableType) \u2014 A status renderable (str or Text typically). spinner (str, optional) \u2014 Name of spinner animation (see python -m rich.spinner). Defaults to \"dots\". spinner_style (StyleType, optional) \u2014 Style of spinner. Defaults to \"status.spinner\". speed (float, optional) \u2014 Speed factor for spinner animation. Defaults to 1.0. refresh_per_second (float, optional) \u2014 Number of refreshes per second. Defaults to 12.5. Returns (Status) A Status object that may be used as a context manager. method show_cursor ( show=True ) \u2192 bool </> Show or hide the cursor. Parameters show (bool, optional) \u2014 Set visibility of the cursor. method set_alt_screen ( enable=True ) </> Enables alternative screen mode. Note, if you enable this mode, you should ensure that is disabled before the application exits. See :meth: ~rich.Console.screen for a context manager that handles this for you. Parameters enable (bool, optional) \u2014 Enable (True) or disable (False) alternate screen. Defaults to True. Returns (bool) True if the control codes were written. method set_window_title ( title ) </> Set the title of the console terminal window. Warning: There is no means within Rich of \"resetting\" the window title to its previous value, meaning the title you set will persist even after your application exits. fish shell resets the window title before and after each command by default, negating this issue. Windows Terminal and command prompt will also reset the title for you. Most other shells and terminals, however, do not do this. Some terminals may require configuration changes before you can set the title. Some terminals may not support setting the title at all. Other software (including the terminal itself, the shell, custom prompts, plugins, etc.) may also set the terminal window title. This could result in whatever value you write using this method being overwritten. Parameters title (str) \u2014 The new title of the terminal window. Returns (bool) True if the control code to change the terminal title was written, otherwise False. Note that a return value of True does not guarantee that the window title has actually changed, since the feature may be unsupported/disabled in some terminals. method screen ( hide_cursor=True , style=None ) </> Context manager to enable and disable 'alternative screen' mode. Parameters hide_cursor (bool, optional) \u2014 Also hide the cursor. Defaults to False. style (Style, optional) \u2014 Optional style for screen. Defaults to None. Returns (~ScreenContext) Context which enables alternate screen on enter, and disables it on exit. method measure ( renderable , options=None ) </> Measure a renderable. Returns a :class: ~rich.measure.Measurement object which containsinformation regarding the number of characters required to print the renderable. Parameters renderable (RenderableType) \u2014 Any renderable or string. options (Optional[ConsoleOptions], optional) \u2014 Options to use when measuring, or Noneto use default options. Defaults to None. Returns (Measurement) A measurement of the renderable. generator render ( renderable , options=None ) </> Render an object in to an iterable of Segment instances. This method contains the logic for rendering objects with the console protocol. You are unlikely to need to use it directly, unless you are extending the library. Parameters renderable (RenderableType) \u2014 An object supporting the console protocol, oran object that may be converted to a string. options (ConsoleOptions, optional) \u2014 An options object, or None to use self.options. Defaults to None. Returns (Iterable[Segment]) An iterable of segments that may be rendered. method render_lines ( renderable , options=None , style=None , pad=True , new_lines=False ) \u2192 List </> Render objects in to a list of lines. The output of render_lines is useful when further formatting of rendered console text is required, such as the Panel class which draws a border around any renderable object. Args: renderable (RenderableType): Any object renderable in the console. options (Optional[ConsoleOptions], optional): Console options, or None to use self.options. Default to ``None``. style (Style, optional): Optional style to apply to renderables. Defaults to ``None``. pad (bool, optional): Pad lines shorter than render width. Defaults to ``True``. new_lines (bool, optional): Include \" \" characters at end of lines. Returns: List[List[Segment]]: A list of lines, where a line is a list of Segment objects. method render_str ( text , style='' , justify=None , overflow=None , emoji=None , markup=None , highlight=None , highlighter=None ) </> Convert a string to a Text instance. This is called automatically ifyou print or log a string. Parameters text (str) \u2014 Text to render. style (Union[str, Style], optional) \u2014 Style to apply to rendered text. justify (str, optional) \u2014 Justify method: \"default\", \"left\", \"center\", \"full\", or \"right\". Defaults to None . overflow (str, optional) \u2014 Overflow method: \"crop\", \"fold\", or \"ellipsis\". Defaults to None . emoji (Optional[bool], optional) \u2014 Enable emoji, or None to use Console default. markup (Optional[bool], optional) \u2014 Enable markup, or None to use Console default. highlight (Optional[bool], optional) \u2014 Enable highlighting, or None to use Console default. highlighter (HighlighterType, optional) \u2014 Optional highlighter to apply. Returns (ConsoleRenderable) Renderable object. method get_style ( name , default=None ) </> Get a Style instance by its theme name or parse a definition. Parameters name (str) \u2014 The name of a style or a style definition. Returns (Style) A Style object. Raises MissingStyle \u2014 If no style could be parsed from name. method rule ( title='' , characters='\u2500' , style='rule.line' , align='center' ) </> Draw a line with optional centered title. Parameters title (str, optional) \u2014 Text to render over the rule. Defaults to \"\". characters (str, optional) \u2014 Character(s) to form the line. Defaults to \"\u2500\". style (str, optional) \u2014 Style of line. Defaults to \"rule.line\". align (str, optional) \u2014 How to align the title, one of \"left\", \"center\", or \"right\". Defaults to \"center\". method control ( *control ) </> Insert non-printing control codes. method out ( *objects , sep=' ' , end='\\n' , style=None , highlight=None ) </> Output to the terminal. This is a low-level way of writing to the terminal which unlike:meth: ~rich.console.Console.print won't pretty print, wrap text, or apply markup, but will optionally apply highlighting and a basic style. Parameters sep (str, optional) \u2014 String to write between print data. Defaults to \" \". end (str, optional) \u2014 String to write at end of print data. Defaults to \"\\n\". style (Union[str, Style], optional) \u2014 A style to apply to output. Defaults to None. highlight (Optional[bool], optional) \u2014 Enable automatic highlighting, or None to useconsole default. Defaults to None . method print ( *objects , sep=' ' , end='\\n' , style=None , justify=None , overflow=None , no_wrap=None , emoji=None , markup=None , highlight=None , width=None , height=None , crop=True , soft_wrap=None , new_line_start=False ) </> Print to the console. Parameters sep (str, optional) \u2014 String to write between print data. Defaults to \" \". end (str, optional) \u2014 String to write at end of print data. Defaults to \"\\n\". style (Union[str, Style], optional) \u2014 A style to apply to output. Defaults to None. justify (str, optional) \u2014 Justify method: \"default\", \"left\", \"right\", \"center\", or \"full\". Defaults to None . overflow (str, optional) \u2014 Overflow method: \"ignore\", \"crop\", \"fold\", or \"ellipsis\". Defaults to None. no_wrap (Optional[bool], optional) \u2014 Disable word wrapping. Defaults to None. emoji (Optional[bool], optional) \u2014 Enable emoji code, or None to use console default. Defaults to None . markup (Optional[bool], optional) \u2014 Enable markup, or None to use console default. Defaults to None . highlight (Optional[bool], optional) \u2014 Enable automatic highlighting, or None to use console default. Defaults to None . width (Optional[int], optional) \u2014 Width of output, or None to auto-detect. Defaults to None . crop (Optional[bool], optional) \u2014 Crop output to width of terminal. Defaults to True. soft_wrap (bool, optional) \u2014 Enable soft wrap mode which disables word wrapping and cropping of text or None forConsole default. Defaults to None . new_line_start (bool, False) \u2014 Insert a new line at the start if the output contains more than one line. Defaults to False . method print_json ( json=None , data=None , indent=2 , highlight=True , skip_keys=False , ensure_ascii=False , check_circular=True , allow_nan=True , default=None , sort_keys=False ) </> Pretty prints JSON. Output will be valid JSON. Parameters json (Optional[str]) \u2014 A string containing JSON. data (Any) \u2014 If json is not supplied, then encode this data. indent (Union[None, int, str], optional) \u2014 Number of spaces to indent. Defaults to 2. highlight (bool, optional) \u2014 Enable highlighting of output: Defaults to True. skip_keys (bool, optional) \u2014 Skip keys not of a basic type. Defaults to False. ensure_ascii (bool, optional) \u2014 Escape all non-ascii characters. Defaults to False. check_circular (bool, optional) \u2014 Check for circular references. Defaults to True. allow_nan (bool, optional) \u2014 Allow NaN and Infinity values. Defaults to True. default (Callable, optional) \u2014 A callable that converts values that can not be encodedin to something that can be JSON encoded. Defaults to None. sort_keys (bool, optional) \u2014 Sort dictionary keys. Defaults to False. method update_screen ( renderable , region=None , options=None ) </> Update the screen at a given offset. Parameters renderable (RenderableType) \u2014 A Rich renderable. region (Region, optional) \u2014 Region of screen to update, or None for entire screen. Defaults to None. Raises errors.NoAltScreen \u2014 If the Console isn't in alt screen mode. method update_screen_lines ( lines , x=0 , y=0 ) </> Update lines of the screen at a given offset. Parameters lines (List[List[Segment]]) \u2014 Rendered lines (as produced by :meth: ~rich.Console.render_lines ). x (int, optional) \u2014 x offset (column no). Defaults to 0. y (int, optional) \u2014 y offset (column no). Defaults to 0. Raises errors.NoAltScreen \u2014 If the Console isn't in alt screen mode. method print_exception ( width=100 , extra_lines=3 , theme=None , word_wrap=False , show_locals=False , suppress=() , max_frames=100 ) </> Prints a rich render of the last exception and traceback. Parameters width (Optional[int], optional) \u2014 Number of characters used to render code. Defaults to 100. extra_lines (int, optional) \u2014 Additional lines of code to render. Defaults to 3. theme (str, optional) \u2014 Override pygments theme used in traceback word_wrap (bool, optional) \u2014 Enable word wrapping of long lines. Defaults to False. show_locals (bool, optional) \u2014 Enable display of local variables. Defaults to False. suppress (Iterable[Union[str, ModuleType]]) \u2014 Optional sequence of modules or paths to exclude from traceback. max_frames (int) \u2014 Maximum number of frames to show in a traceback, 0 for no maximum. Defaults to 100. method log ( *objects , sep=' ' , end='\\n' , style=None , justify=None , emoji=None , markup=None , highlight=None , log_locals=False , _stack_offset=1 ) </> Log rich content to the terminal. Parameters sep (str, optional) \u2014 String to write between print data. Defaults to \" \". end (str, optional) \u2014 String to write at end of print data. Defaults to \"\\n\". style (Union[str, Style], optional) \u2014 A style to apply to output. Defaults to None. justify (str, optional) \u2014 One of \"left\", \"right\", \"center\", or \"full\". Defaults to None . emoji (Optional[bool], optional) \u2014 Enable emoji code, or None to use console default. Defaults to None. markup (Optional[bool], optional) \u2014 Enable markup, or None to use console default. Defaults to None. highlight (Optional[bool], optional) \u2014 Enable automatic highlighting, or None to use console default. Defaults to None. log_locals (bool, optional) \u2014 Boolean to enable logging of locals where log() was called. Defaults to False. _stack_offset (int, optional) \u2014 Offset of caller from end of call stack. Defaults to 1. method on_broken_pipe ( ) </> This function is called when a BrokenPipeError is raised. This can occur when piping Textual output in Linux and macOS. The default implementation is to exit the app, but you could implement this method in a subclass to change the behavior. See https://docs.python.org/3/library/signal.html#note-on-sigpipe for details. method input ( prompt='' , markup=True , emoji=True , password=False , stream=None ) </> Displays a prompt and waits for input from the user. The prompt may contain color / style. It works in the same way as Python's builtin :func: input function and provides elaborate line editing and history features if Python's builtin :mod: readline module is previously loaded. Parameters prompt (Union[str, Text]) \u2014 Text to render in the prompt. markup (bool, optional) \u2014 Enable console markup (requires a str prompt). Defaults to True. emoji (bool, optional) \u2014 Enable emoji (requires a str prompt). Defaults to True. password (bool, optional) \u2014 (bool, optional): Hide typed text. Defaults to False. stream (Optional, optional) \u2014 (TextIO, optional): Optional file to read input from (rather than stdin). Defaults to None. Returns (str) Text read from stdin. method export_text ( clear=True , styles=False ) </> Generate text from console contents (requires record=True argument in constructor). Parameters clear (bool, optional) \u2014 Clear record buffer after exporting. Defaults to True . styles (bool, optional) \u2014 If True , ansi escape codes will be included. False for plain text.Defaults to False . Returns (str) String containing console contents. method save_text ( path , clear=True , styles=False ) </> Generate text from console and save to a given location (requires record=True argument in constructor). Parameters path (str) \u2014 Path to write text files. clear (bool, optional) \u2014 Clear record buffer after exporting. Defaults to True . styles (bool, optional) \u2014 If True , ansi style codes will be included. False for plain text.Defaults to False . method export_html ( theme=None , clear=True , code_format=None , inline_styles=False ) </> Generate HTML from console contents (requires record=True argument in constructor). Parameters theme (TerminalTheme, optional) \u2014 TerminalTheme object containing console colors. clear (bool, optional) \u2014 Clear record buffer after exporting. Defaults to True . code_format (str, optional) \u2014 Format string to render HTML. In addition to '{foreground}','{background}', and '{code}', should contain '{stylesheet}' if inline_styles is False . inline_styles (bool, optional) \u2014 If True styles will be inlined in to spans, which makes fileslarger but easier to cut and paste markup. If False , styles will be embedded in a style tag. Defaults to False. Returns (str) String containing console contents as HTML. method save_html ( path , theme=None , clear=True , code_format='<!DOCTYPE html>\\n<html>\\n<head>\\n<meta charset=\"UTF-8\">\\n<style>\\n{stylesheet}\\nbody {{\\n color: {foreground};\\n background-color: {background};\\n}}\\n</style>\\n</head>\\n<body>\\n <pre style=\"font-family:Menlo,\\'DejaVu Sans Mono\\',consolas,\\'Courier New\\',monospace\"><code style=\"font-family:inherit\">{code}</code></pre>\\n</body>\\n</html>\\n' , inline_styles=False ) </> Generate HTML from console contents and write to a file (requires record=True argument in constructor). Parameters path (str) \u2014 Path to write html file. theme (TerminalTheme, optional) \u2014 TerminalTheme object containing console colors. clear (bool, optional) \u2014 Clear record buffer after exporting. Defaults to True . code_format (str, optional) \u2014 Format string to render HTML. In addition to '{foreground}','{background}', and '{code}', should contain '{stylesheet}' if inline_styles is False . inline_styles (bool, optional) \u2014 If True styles will be inlined in to spans, which makes fileslarger but easier to cut and paste markup. If False , styles will be embedded in a style tag. Defaults to False. method export_svg ( title='Rich' , theme=None , clear=True , code_format='<svg class=\"rich-terminal\" viewBox=\"0 0 {width} {height}\" xmlns=\"http://www.w3.org/2000/svg\">\\n <!-- Generated with Rich https://www.textualize.io -->\\n <style>\\n\\n @font-face {{\\n font-family: \"Fira Code\";\\n src: local(\"FiraCode-Regular\"),\\n url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Regular.woff2\") format(\"woff2\"),\\n url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Regular.woff\") format(\"woff\");\\n font-style: normal;\\n font-weight: 400;\\n }}\\n @font-face {{\\n font-family: \"Fira Code\";\\n src: local(\"FiraCode-Bold\"),\\n url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Bold.woff2\") format(\"woff2\"),\\n url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Bold.woff\") format(\"woff\");\\n font-style: bold;\\n font-weight: 700;\\n }}\\n\\n .{unique_id}-matrix {{\\n font-family: Fira Code, monospace;\\n font-size: {char_height}px;\\n line-height: {line_height}px;\\n font-variant-east-asian: full-width;\\n }}\\n\\n .{unique_id}-title {{\\n font-size: 18px;\\n font-weight: bold;\\n font-family: arial;\\n }}\\n\\n {styles}\\n </style>\\n\\n <defs>\\n <clipPath id=\"{unique_id}-clip-terminal\">\\n <rect x=\"0\" y=\"0\" width=\"{terminal_width}\" height=\"{terminal_height}\" />\\n </clipPath>\\n {lines}\\n </defs>\\n\\n {chrome}\\n <g transform=\"translate({terminal_x}, {terminal_y})\" clip-path=\"url(#{unique_id}-clip-terminal)\">\\n {backgrounds}\\n <g class=\"{unique_id}-matrix\">\\n {matrix}\\n </g>\\n </g>\\n</svg>\\n' , font_aspect_ratio=0.61 , unique_id=None ) \u2192 str </> Generate an SVG from the console contents (requires record=True in Console constructor). Parameters title (str, optional) \u2014 The title of the tab in the output image theme (TerminalTheme, optional) \u2014 The TerminalTheme object to use to style the terminal clear (bool, optional) \u2014 Clear record buffer after exporting. Defaults to True code_format (str, optional) \u2014 Format string used to generate the SVG. Rich will inject a number of variablesinto the string in order to form the final SVG output. The default template used and the variables injected by Rich can be found by inspecting the console.CONSOLE_SVG_FORMAT variable. font_aspect_ratio (float, optional) \u2014 The width to height ratio of the font used in the code_format string. Defaults to 0.61, which is the width to height ratio of Fira Code (the default font). If you aren't specifying a different font inside code_format , you probably don't need this. unique_id (str, optional) \u2014 unique id that is used as the prefix for various elements (CSS styles, nodeids). If not set, this defaults to a computed value based on the recorded content. method save_svg ( path , title='Rich' , theme=None , clear=True , code_format='<svg class=\"rich-terminal\" viewBox=\"0 0 {width} {height}\" xmlns=\"http://www.w3.org/2000/svg\">\\n <!-- Generated with Rich https://www.textualize.io -->\\n <style>\\n\\n @font-face {{\\n font-family: \"Fira Code\";\\n src: local(\"FiraCode-Regular\"),\\n url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Regular.woff2\") format(\"woff2\"),\\n url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Regular.woff\") format(\"woff\");\\n font-style: normal;\\n font-weight: 400;\\n }}\\n @font-face {{\\n font-family: \"Fira Code\";\\n src: local(\"FiraCode-Bold\"),\\n url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Bold.woff2\") format(\"woff2\"),\\n url(\"https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Bold.woff\") format(\"woff\");\\n font-style: bold;\\n font-weight: 700;\\n }}\\n\\n .{unique_id}-matrix {{\\n font-family: Fira Code, monospace;\\n font-size: {char_height}px;\\n line-height: {line_height}px;\\n font-variant-east-asian: full-width;\\n }}\\n\\n .{unique_id}-title {{\\n font-size: 18px;\\n font-weight: bold;\\n font-family: arial;\\n }}\\n\\n {styles}\\n </style>\\n\\n <defs>\\n <clipPath id=\"{unique_id}-clip-terminal\">\\n <rect x=\"0\" y=\"0\" width=\"{terminal_width}\" height=\"{terminal_height}\" />\\n </clipPath>\\n {lines}\\n </defs>\\n\\n {chrome}\\n <g transform=\"translate({terminal_x}, {terminal_y})\" clip-path=\"url(#{unique_id}-clip-terminal)\">\\n {backgrounds}\\n <g class=\"{unique_id}-matrix\">\\n {matrix}\\n </g>\\n </g>\\n</svg>\\n' , font_aspect_ratio=0.61 , unique_id=None ) </> Generate an SVG file from the console contents (requires record=True in Console constructor). Parameters path (str) \u2014 The path to write the SVG to. title (str, optional) \u2014 The title of the tab in the output image theme (TerminalTheme, optional) \u2014 The TerminalTheme object to use to style the terminal clear (bool, optional) \u2014 Clear record buffer after exporting. Defaults to True code_format (str, optional) \u2014 Format string used to generate the SVG. Rich will inject a number of variablesinto the string in order to form the final SVG output. The default template used and the variables injected by Rich can be found by inspecting the console.CONSOLE_SVG_FORMAT variable. font_aspect_ratio (float, optional) \u2014 The width to height ratio of the font used in the code_format string. Defaults to 0.61, which is the width to height ratio of Fira Code (the default font). If you aren't specifying a different font inside code_format , you probably don't need this. unique_id (str, optional) \u2014 unique id that is used as the prefix for various elements (CSS styles, nodeids). If not set, this defaults to a computed value based on the recorded content. function pipen.utils. get_logger ( name='core' , level=None ) </> Get the logger by given plugin name Parameters level (str | int, optional) \u2014 The initial level of the logger Returns (LoggerAdapter) The logger function pipen.utils. desc_from_docstring ( obj , base ) </> Get the description from docstring Only extract the summary. Parameters obj (Type[Pipen | Proc]) \u2014 The object with docstring Returns (str) The summary as desc function pipen.utils. update_dict ( parent , new , depth=0 , try_list=False ) </> Update the new dict to the parent, but make sure parent does not change Parameters parent (Mapping) \u2014 The parent dictionary new (Mapping) \u2014 The new dictionary depth (int, optional) \u2014 The depth to be copied. 0 for updating to the deepest level. try_list (bool, optional) \u2014 If True, try to also update the dict in the list Examples >>> parent = { \"a\" : { \"b\" : 1 }} >>> new = { \"a\" : { \"c\" : 2 }} >>> update_dict ( parent , new ) >>> # {\"a\": {\"b\": 1, \"c\": 2}} >>> parent = { \"a\" : [{ \"b\" : 1 }]} >>> new = { \"a\" : [{ \"c\" : 2 }]} >>> update_dict ( parent , new , try_list = True ) >>> # {\"a\": [{\"b\": 1, \"c\": 2}]} Returns (Mapping) The updated dictionary or None if both parent and new are None. function pipen.utils. copy_dict ( dic , depth=1 ) </> Deep copy a dict Parameters dic (Mapping) \u2014 The dict to be copied depth (int, optional) \u2014 The depth to be deep copied Returns (Mapping) The deep-copied dict function pipen.utils. strsplit ( string , sep , maxsplit=-1 , trim='both' ) \u2192 List </> Split the string, with the ability to trim each part. function pipen.utils. get_shebang ( script ) </> Get the shebang of the script Parameters script (str) \u2014 The script string Returns (str) None if the script does not contain a shebang, otherwise the shebangwithout #! prefix function pipen.utils. ignore_firstline_dedent ( text ) </> Like textwrap.dedent(), but ignore first empty lines Parameters text (str) \u2014 The text the be dedented Returns (str) The dedented text function pipen.utils. get_logpanel_width ( ) </> Get the width of the log content Parameters max_width \u2014 The maximum width to returnNote that it's not the console width. With console width, you have to subtract the width of the log meta info (CONSOLE_WIDTH_SHIFT). Returns (int) The width of the log content function pipen.utils. log_rich_renderable ( renderable , color , logfunc , *args , **kwargs ) </> Log a rich renderable to logger Parameters renderable (RenderableType) \u2014 The rich renderable logfunc (Callable) \u2014 The log function, if message is not the first argument,use functools.partial to wrap it *args (Any) \u2014 The arguments to the log function **kwargs (Any) \u2014 The keyword arguments to the log function splitline \u2014 Whether split the lines or log the entire message function pipen.utils. brief_list ( blist ) </> Briefly show an integer list, combine the continuous numbers. Parameters blist (List) \u2014 The list Returns (str) The string to show for the briefed list. function pipen.utils. pipen_banner ( ) </> The banner for pipen Returns (RenderableType) The banner renderable function pipen.utils. get_mtime ( path , dir_depth=1 ) </> Get the modification time of a path.If path is a directory, try to get the last modification time of the contents in the directory at given dir_depth Parameters dir_depth (int, optional) \u2014 The depth of the directory to check thelast modification time 0 means only check the path itself (don't go into directory or follow symlink) Returns (float) The last modification time of path function pipen.utils. is_subclass ( obj , cls ) </> Tell if obj is a subclass of clsDifferences with issubclass is that we don't raise Type error if obj is not a class Parameters obj (Any) \u2014 The object to check cls (type) \u2014 The class to check Returns (bool) True if obj is a subclass of cls otherwise False generator pipen.utils. load_entrypoints ( group ) </> Load objects from setuptools entrypoints by given group name Parameters group (str) \u2014 The group name of the entrypoints Returns (Iterable) An iterable of tuples with name and the loaded object function pipen.utils. truncate_text ( text , width , end='\u2026' ) </> Truncate a text not based on words/whitespacesOtherwise, we could use textwrap.shorten. Parameters text (str) \u2014 The text to be truncated width (int) \u2014 The max width of the the truncated text end (str, optional) \u2014 The end string of the truncated text Returns (str) The truncated text with end appended. function pipen.utils. make_df_colnames_unique_inplace ( thedf ) </> Make the columns of a data frame unique Parameters thedf (pandas.DataFrame) \u2014 The data frame function pipen.utils. get_base ( klass , abc_base , value , value_getter ) </> Get the base class where the value was first defined Parameters klass (Type) \u2014 The class abc_base (Type) \u2014 The very base class to check in bases value (Any) \u2014 The value to check value_getter (Callable) \u2014 How to get the value from the class Returns (Type) The base class function pipen.utils. mark ( **kwargs ) </> Mark a class (e.g. Proc) with given kwargs as metadata These marks will not be inherited by the subclasses if the class is a subclass of Proc or ProcGroup . Parameters **kwargs \u2014 The kwargs to mark the proc Returns (Callable) The decorator function pipen.utils. get_marked ( cls , mark_name , default=None ) </> Get the marked value from a proc Parameters cls (type) \u2014 The proc mark_name (str) \u2014 The mark name default (Any, optional) \u2014 The default value if the mark is not found Returns (Any) The marked value function pipen.utils. is_valid_name ( name ) </> Check if a name is valid for a proc or pipen Parameters name (str) \u2014 The name to check Returns (bool) True if valid, otherwise False function pipen.utils. load_pipeline ( obj , argv0=None , argv1p=None , **kwargs ) </> Load a pipeline from a Pipen, Proc or ProcGroup object It does not only load the Pipen object or convert the Proc/ProcGroup object to Pipen, but also build the process relationships. So that we can access pipeline.procs and requires/nexts of each proc. To avoid running the pipeline and notify the plugins that this is just for loading the pipeline, sys.argv[0] is set to @pipen . Parameters obj (str | Type[Proc] | Type[ProcGroup] | Type[Pipen] | Pipen) \u2014 The Pipen, Proc or ProcGroup object. It can also be a string inthe format of part1:part2 to load the pipeline, where part1 is a path to a python file or package directory, and part2 is the name of the proc, procgroup or pipeline to load. It should be able to be loaded by getattr(module, part2) , where module is loaded from part1 . argv0 (str | none, optional) \u2014 The value to replace sys.argv[0]. \"@pipen\" will be usedby default. argv1p (Optional, optional) \u2014 The values to replace sys.argv[1:]. Do not replace by default. kwargs \u2014 The kwargs to pass to the Pipen constructor Returns (Pipen) The loaded Pipen object Raises TypeError \u2014 If obj or loaded obj is not a Pipen, Proc or ProcGroup function pipen.utils. is_loading_pipeline ( *flags , argv=None ) </> Check if we are loading the pipeline. Works only when argv0 is \"@pipen\" while loading the pipeline. Note if you are using this function at compile time, make sure you load your pipeline using the string form ( part1:part2 ) See more with load_pipline() . Parameters *flags (str) \u2014 Additional flags to check in sys.argv (e.g. \"-h\", \"--help\")to determine if we are loading the pipeline argv (Optional, optional) \u2014 The arguments to check. sys.argv is used by default.Note that the first argument should be included in the check. You could typically pass [sys.argv[0], *your_args] to this if you want to check if sys.argv[0] is \"@pipen\" or your_args contains some flags. Returns (bool) True if we are loading the pipeline (argv[0] == \"@pipen\"),otherwise False function pipen.utils. path_is_symlink_sync ( path ) </> Check if a path is a symlink synchronously. We don't only check the real symlink, but also the fake symlink files created by path_symlink_to() . Parameters path (PanPath) \u2014 The path to check Returns (bool) True if the path is a symlink, otherwise False function pipen.utils. path_is_symlink ( path ) </> Check if a path is a symlink. We don't only check the real symlink, but also the fake symlink files created by path_symlink_to() . Parameters path (PanPath) \u2014 The path to check Returns (bool) True if the path is a symlink, otherwise False function pipen.utils. path_symlink_to ( src , dst , target_is_directory=False ) </> Create a symbolic link pointing to src named dst. Parameters src (PanPath) \u2014 The source path dst (PanPath) \u2014 The destination path target_is_directory (bool, optional) \u2014 If True, the symbolic link will be to a directory.","title":"pipen.utils"},{"location":"api/pipen.utils/#pipenutils","text":"</> Provide some utilities Classes RichHandler \u2014 Subclass of rich.logging.RichHandler, showing log levels as a singlecharacter </> RichConsole \u2014 A high level console interface. </> Functions brief_list ( blist ) (str) \u2014 Briefly show an integer list, combine the continuous numbers. </> copy_dict ( dic , depth ) (Mapping) \u2014 Deep copy a dict </> desc_from_docstring ( obj , base ) (str) \u2014 Get the description from docstring </> get_base ( klass , abc_base , value , value_getter ) (Type) \u2014 Get the base class where the value was first defined </> get_logger ( name , level ) (LoggerAdapter) \u2014 Get the logger by given plugin name </> get_logpanel_width ( ) (int) \u2014 Get the width of the log content </> get_marked ( cls , mark_name , default ) (Any) \u2014 Get the marked value from a proc </> get_mtime ( path , dir_depth ) (float) \u2014 Get the modification time of a path.If path is a directory, try to get the last modification time of the contents in the directory at given dir_depth </> get_shebang ( script ) (str) \u2014 Get the shebang of the script </> ignore_firstline_dedent ( text ) (str) \u2014 Like textwrap.dedent(), but ignore first empty lines </> is_loading_pipeline ( *flags , argv ) (bool) \u2014 Check if we are loading the pipeline. Works only when argv0 is \"@pipen\" while loading the pipeline. </> is_subclass ( obj , cls ) (bool) \u2014 Tell if obj is a subclass of clsDifferences with issubclass is that we don't raise Type error if obj is not a class </> is_valid_name ( name ) (bool) \u2014 Check if a name is valid for a proc or pipen </> load_entrypoints ( group ) (Iterable) \u2014 Load objects from setuptools entrypoints by given group name </> load_pipeline ( obj , argv0 , argv1p , **kwargs ) (Pipen) \u2014 Load a pipeline from a Pipen, Proc or ProcGroup object </> log_rich_renderable ( renderable , color , logfunc , *args , **kwargs ) \u2014 Log a rich renderable to logger </> make_df_colnames_unique_inplace ( thedf ) \u2014 Make the columns of a data frame unique </> mark ( **kwargs ) (Callable) \u2014 Mark a class (e.g. Proc) with given kwargs as metadata </> path_is_symlink ( path ) (bool) \u2014 Check if a path is a symlink. </> path_is_symlink_sync ( path ) (bool) \u2014 Check if a path is a symlink synchronously. </> path_symlink_to ( src , dst , target_is_directory ) \u2014 Create a symbolic link pointing to src named dst. </> pipen_banner ( ) (RenderableType) \u2014 The banner for pipen </> strsplit ( string , sep , maxsplit , trim ) (List) \u2014 Split the string, with the ability to trim each part. </> truncate_text ( text , width , end ) (str) \u2014 Truncate a text not based on words/whitespacesOtherwise, we could use textwrap.shorten. </> update_dict ( parent , new , depth , try_list ) (Mapping) \u2014 Update the new dict to the parent, but make sure parent does not change </> class","title":"pipen.utils"},{"location":"api/pipen.utils/#pipenutilsrichhandler","text":"</> Bases rich.logging.RichHandler logging.Handler logging.Filterer Subclass of rich.logging.RichHandler, showing log levels as a singlecharacter Parameters level (Union, optional) \u2014 Log level. Defaults to logging.NOTSET. show_time (bool, optional) \u2014 Show a column for the time. Defaults to True. omit_repeated_times (bool, optional) \u2014 Omit repetition of the same time. Defaults to True. show_level (bool, optional) \u2014 Show a column for the level. Defaults to True. show_path (bool, optional) \u2014 Show the path to the original log call. Defaults to True. enable_link_path (bool, optional) \u2014 Enable terminal link of path column to file. Defaults to True. highlighter (Optional, optional) \u2014 Highlighter to style log messages, or None to use ReprHighlighter. Defaults to None. markup (bool, optional) \u2014 Enable console markup in log messages. Defaults to False. rich_tracebacks (bool, optional) \u2014 Enable rich tracebacks with syntax highlighting and formatting. Defaults to False. tracebacks_width (Optional, optional) \u2014 Number of characters used to render tracebacks, or None for full width. Defaults to None. tracebacks_code_width (Optional, optional) \u2014 Number of code characters used to render tracebacks, or None for full width. Defaults to 88. tracebacks_extra_lines (int, optional) \u2014 Additional lines of code to render tracebacks, or None for full width. Defaults to None. tracebacks_theme (Optional, optional) \u2014 Override pygments theme used in traceback. tracebacks_word_wrap (bool, optional) \u2014 Enable word wrapping of long tracebacks lines. Defaults to True. tracebacks_show_locals (bool, optional) \u2014 Enable display of locals in tracebacks. Defaults to False. tracebacks_suppress (Iterable, optional) \u2014 Optional sequence of modules or paths to exclude from traceback. tracebacks_max_frames (int, optional) \u2014 Optional maximum number of frames returned by traceback. locals_max_length (int, optional) \u2014 Maximum length of containers before abbreviating, or None for no abbreviation.Defaults to 10. locals_max_string (int, optional) \u2014 Maximum length of string before truncating, or None to disable. Defaults to 80. log_time_format (Union, optional) \u2014 If log_time is enabled, either string for strftime or callable that formats the time. Defaults to \"[%x %X] \". keywords (Optional, optional) \u2014 List of words to highlight instead of RichHandler.KEYWORDS . Methods acquire ( ) \u2014 Acquire the I/O thread lock. </> addFilter ( filter ) \u2014 Add the specified filter to this handler. </> close ( ) \u2014 Tidy up any resources used by the handler. </> createLock ( ) \u2014 Acquire a thread lock for serializing access to the underlying I/O. </> emit ( record ) \u2014 Invoked by logging. </> filter ( record ) \u2014 Determine if a record is loggable by consulting all the filters. </> flush ( ) \u2014 Ensure all logging output has been flushed. </> format ( record ) \u2014 Format the specified record. </> get_level_text ( record ) (Text) \u2014 Get the level name from the record. </> handle ( record ) \u2014 Conditionally emit the specified logging record. </> handleError ( record ) \u2014 Handle errors which occur during an emit() call. </> release ( ) \u2014 Release the I/O thread lock. </> removeFilter ( filter ) \u2014 Remove the specified filter from this handler. </> render ( record , traceback , message_renderable ) (ConsoleRenderable) \u2014 Render log for display. </> render_message ( record , message ) (ConsoleRenderable) \u2014 Render message text in to Text. </> setFormatter ( fmt ) \u2014 Set the formatter for this handler. </> setLevel ( level ) \u2014 Set the logging level of this handler. level must be an int or a str. </> method","title":"pipen.utils.RichHandler"},{"location":"api/pipen.utils/#loggingfiltereraddfilter","text":"</> Add the specified filter to this handler. method","title":"logging.Filterer.addFilter"},{"location":"api/pipen.utils/#loggingfiltererremovefilter","text":"</> Remove the specified filter from this handler. method","title":"logging.Filterer.removeFilter"},{"location":"api/pipen.utils/#loggingfiltererfilter","text":"</> Determine if a record is loggable by consulting all the filters. The default is to allow the record to be logged; any filter can veto this by returning a false value. If a filter attached to a handler returns a log record instance, then that instance is used in place of the original log record in any further processing of the event by that handler. If a filter returns any other true value, the original log record is used in any further processing of the event by that handler. If none of the filters return false values, this method returns a log record. If any of the filters return a false value, this method returns a false value. .. versionchanged:: 3.2 Allow filters to be just callables. .. versionchanged:: 3.12 Allow filters to return a LogRecord instead of modifying it in place. method","title":"logging.Filterer.filter"},{"location":"api/pipen.utils/#logginghandlercreatelock","text":"</> Acquire a thread lock for serializing access to the underlying I/O. method","title":"logging.Handler.createLock"},{"location":"api/pipen.utils/#logginghandleracquire","text":"</> Acquire the I/O thread lock. method","title":"logging.Handler.acquire"},{"location":"api/pipen.utils/#logginghandlerrelease","text":"</> Release the I/O thread lock. method","title":"logging.Handler.release"},{"location":"api/pipen.utils/#logginghandlersetlevel","text":"</> Set the logging level of this handler. level must be an int or a str. method","title":"logging.Handler.setLevel"},{"location":"api/pipen.utils/#logginghandlerformat","text":"</> Format the specified record. If a formatter is set, use it. Otherwise, use the default formatter for the module. method","title":"logging.Handler.format"},{"location":"api/pipen.utils/#logginghandlerhandle","text":"</> Conditionally emit the specified logging record. Emission depends on filters which may have been added to the handler. Wrap the actual emission of the record with acquisition/release of the I/O thread lock. Returns an instance of the log record that was emitted if it passed all filters, otherwise a false value is returned. method","title":"logging.Handler.handle"},{"location":"api/pipen.utils/#logginghandlersetformatter","text":"</> Set the formatter for this handler. method","title":"logging.Handler.setFormatter"},{"location":"api/pipen.utils/#logginghandlerflush","text":"</> Ensure all logging output has been flushed. This version does nothing and is intended to be implemented by subclasses. method","title":"logging.Handler.flush"},{"location":"api/pipen.utils/#logginghandlerclose","text":"</> Tidy up any resources used by the handler. This version removes the handler from an internal map of handlers, _handlers, which is used for handler lookup by name. Subclasses should ensure that this gets called from overridden close() methods. method","title":"logging.Handler.close"},{"location":"api/pipen.utils/#logginghandlerhandleerror","text":"</> Handle errors which occur during an emit() call. This method should be called from handlers when an exception is encountered during an emit() call. If raiseExceptions is false, exceptions get silently ignored. This is what is mostly wanted for a logging system - most users will not care about errors in the logging system, they are more interested in application errors. You could, however, replace this with a custom handler if you wish. The record which was being processed is passed in to this method. method","title":"logging.Handler.handleError"},{"location":"api/pipen.utils/#richloggingrichhandleremit","text":"</> Invoked by logging. method","title":"rich.logging.RichHandler.emit"},{"location":"api/pipen.utils/#richloggingrichhandlerrender_message","text":"</> Render message text in to Text. Parameters record (LogRecord) \u2014 logging Record. message (str) \u2014 String containing log message. Returns (ConsoleRenderable) Renderable to display log message. method","title":"rich.logging.RichHandler.render_message"},{"location":"api/pipen.utils/#richloggingrichhandlerrender","text":"</> Render log for display. Parameters record (LogRecord) \u2014 logging Record. traceback (Optional[Traceback]) \u2014 Traceback instance or None for no Traceback. message_renderable (ConsoleRenderable) \u2014 Renderable (typically Text) containing log message contents. Returns (ConsoleRenderable) Renderable to display log. method","title":"rich.logging.RichHandler.render"},{"location":"api/pipen.utils/#pipenutilsrichhandlerget_level_text","text":"</> Get the level name from the record. Parameters record (LogRecord) \u2014 LogRecord instance. Returns (Text) A tuple of the style and level name. class","title":"pipen.utils.RichHandler.get_level_text"},{"location":"api/pipen.utils/#pipenutilsrichconsole","text":"</> Bases rich.console.Console A high level console interface. Attributes color_system \u2014 Get color system string. </> encoding \u2014 Get the encoding of the console file, e.g. \"utf-8\" . </> file (IO) \u2014 Get the file object to write to. </> height \u2014 Get the height of the console. </> is_alt_screen \u2014 Check if the alt screen was enabled. </> is_dumb_terminal \u2014 Detect dumb terminal. </> is_terminal \u2014 Check if the console is writing to a terminal. </> options (ConsoleOptions) \u2014 Get default console options. </> size \u2014 Get the size of the console. </> width \u2014 Get the width of the console. </> Methods __enter__ ( ) (Console) \u2014 Own context manager to enter buffer context. </> __exit__ ( exc_type , exc_value , traceback ) \u2014 Exit buffer context. </> begin_capture ( ) \u2014 Begin capturing console output. Call :meth: end_capture to exit capture mode and return output. </> bell ( ) \u2014 Play a 'bell' sound (if supported by the terminal). </> capture ( ) (Capture) \u2014 A context manager to capture the result of print() or log() in a string,rather than writing it to the console. </> clear ( home ) \u2014 Clear the screen. </> clear_live ( ) \u2014 Clear the Live instance. Used by the Live context manager (no need to call directly). </> control ( *control ) \u2014 Insert non-printing control codes. </> end_capture ( ) (str) \u2014 End capture mode and return captured string. </> export_html ( theme , clear , code_format , inline_styles ) (str) \u2014 Generate HTML from console contents (requires record=True argument in constructor). </> export_svg ( title , theme , clear , code_format , font_aspect_ratio , unique_id ) (str) \u2014 Generate an SVG from the console contents (requires record=True in Console constructor). </> export_text ( clear , styles ) (str) \u2014 Generate text from console contents (requires record=True argument in constructor). </> get_style ( name , default ) (Style) \u2014 Get a Style instance by its theme name or parse a definition. </> input ( prompt , markup , emoji , password , stream ) (str) \u2014 Displays a prompt and waits for input from the user. The prompt may contain color / style. </> line ( count ) \u2014 Write new line(s). </> log ( *objects , sep , end , style , justify , emoji , markup , highlight , log_locals , _stack_offset ) \u2014 Log rich content to the terminal. </> measure ( renderable , options ) (Measurement) \u2014 Measure a renderable. Returns a :class: ~rich.measure.Measurement object which containsinformation regarding the number of characters required to print the renderable. </> on_broken_pipe ( ) \u2014 This function is called when a BrokenPipeError is raised. </> out ( *objects , sep , end , style , highlight ) \u2014 Output to the terminal. This is a low-level way of writing to the terminal which unlike:meth: ~rich.console.Console.print won't pretty print, wrap text, or apply markup, but will optionally apply highlighting and a basic style. </> pager ( pager , styles , links ) (PagerContext) \u2014 A context manager to display anything printed within a \"pager\". The pager applicationis defined by the system and will typically support at least pressing a key to scroll. </> pop_render_hook ( ) \u2014 Pop the last renderhook from the stack. </> pop_theme ( ) \u2014 Remove theme from top of stack, restoring previous theme. </> print ( *objects , sep , end , style , justify , overflow , no_wrap , emoji , markup , highlight , width , height , crop , soft_wrap , new_line_start ) \u2014 Print to the console. </> print_exception ( width , extra_lines , theme , word_wrap , show_locals , suppress , max_frames ) \u2014 Prints a rich render of the last exception and traceback. </> print_json ( json , data , indent , highlight , skip_keys , ensure_ascii , check_circular , allow_nan , default , sort_keys ) \u2014 Pretty prints JSON. Output will be valid JSON. </> push_render_hook ( hook ) \u2014 Add a new render hook to the stack. </> push_theme ( theme , inherit ) \u2014 Push a new theme on to the top of the stack, replacing the styles from the previous theme.Generally speaking, you should call :meth: ~rich.console.Console.use_theme to get a context manager, rather than calling this method directly. </> render ( renderable , options ) (Iterable[Segment]) \u2014 Render an object in to an iterable of Segment instances. </> render_lines ( renderable , options , style , pad , new_lines ) (List) \u2014 Render objects in to a list of lines. </> render_str ( text , style , justify , overflow , emoji , markup , highlight , highlighter ) (ConsoleRenderable) \u2014 Convert a string to a Text instance. This is called automatically ifyou print or log a string. </> rule ( title , characters , style , align ) \u2014 Draw a line with optional centered title. </> save_html ( path , theme , clear , code_format , inline_styles ) \u2014 Generate HTML from console contents and write to a file (requires record=True argument in constructor). </> save_svg ( path , title , theme , clear , code_format , font_aspect_ratio , unique_id ) \u2014 Generate an SVG file from the console contents (requires record=True in Console constructor). </> save_text ( path , clear , styles ) \u2014 Generate text from console and save to a given location (requires record=True argument in constructor). </> screen ( hide_cursor , style ) (~ScreenContext) \u2014 Context manager to enable and disable 'alternative screen' mode. </> set_alt_screen ( enable ) (bool) \u2014 Enables alternative screen mode. </> set_live ( live ) (bool) \u2014 Set Live instance. Used by Live context manager (no need to call directly). </> set_window_title ( title ) (bool) \u2014 Set the title of the console terminal window. </> show_cursor ( show ) (bool) \u2014 Show or hide the cursor. </> status ( status , spinner , spinner_style , speed , refresh_per_second ) (Status) \u2014 Display a status and spinner. </> update_screen ( renderable , region , options ) \u2014 Update the screen at a given offset. </> update_screen_lines ( lines , x , y ) \u2014 Update lines of the screen at a given offset. </> use_theme ( theme , inherit ) (ThemeContext) \u2014 Use a different theme for the duration of the context manager. </> method","title":"pipen.utils.RichConsole"},{"location":"api/pipen.utils/#richconsoleconsoleset_live","text":"</> Set Live instance. Used by Live context manager (no need to call directly). Parameters live (Live) \u2014 Live instance using this Console. Returns (bool) Boolean that indicates if the live is the topmost of the stack. Raises errors.LiveError \u2014 If this Console has a Live context currently active. method","title":"rich.console.Console.set_live"},{"location":"api/pipen.utils/#richconsoleconsoleclear_live","text":"</> Clear the Live instance. Used by the Live context manager (no need to call directly). method","title":"rich.console.Console.clear_live"},{"location":"api/pipen.utils/#richconsoleconsolepush_render_hook","text":"</> Add a new render hook to the stack. Parameters hook (RenderHook) \u2014 Render hook instance. method","title":"rich.console.Console.push_render_hook"},{"location":"api/pipen.utils/#richconsoleconsolepop_render_hook","text":"</> Pop the last renderhook from the stack. method","title":"rich.console.Console.pop_render_hook"},{"location":"api/pipen.utils/#richconsoleconsoleenter","text":"</> Own context manager to enter buffer context. method","title":"rich.console.Console.enter"},{"location":"api/pipen.utils/#richconsoleconsoleexit","text":"</> Exit buffer context. method","title":"rich.console.Console.exit"},{"location":"api/pipen.utils/#richconsoleconsolebegin_capture","text":"</> Begin capturing console output. Call :meth: end_capture to exit capture mode and return output. method","title":"rich.console.Console.begin_capture"},{"location":"api/pipen.utils/#richconsoleconsoleend_capture","text":"</> End capture mode and return captured string. Returns (str) Console output. method","title":"rich.console.Console.end_capture"},{"location":"api/pipen.utils/#richconsoleconsolepush_theme","text":"</> Push a new theme on to the top of the stack, replacing the styles from the previous theme.Generally speaking, you should call :meth: ~rich.console.Console.use_theme to get a context manager, rather than calling this method directly. Parameters theme (Theme) \u2014 A theme instance. inherit (bool, optional) \u2014 Inherit existing styles. Defaults to True. method","title":"rich.console.Console.push_theme"},{"location":"api/pipen.utils/#richconsoleconsolepop_theme","text":"</> Remove theme from top of stack, restoring previous theme. method","title":"rich.console.Console.pop_theme"},{"location":"api/pipen.utils/#richconsoleconsoleuse_theme","text":"</> Use a different theme for the duration of the context manager. Parameters theme (Theme) \u2014 Theme instance to user. inherit (bool, optional) \u2014 Inherit existing console styles. Defaults to True. Returns (ThemeContext) [description] method","title":"rich.console.Console.use_theme"},{"location":"api/pipen.utils/#richconsoleconsolebell","text":"</> Play a 'bell' sound (if supported by the terminal). method","title":"rich.console.Console.bell"},{"location":"api/pipen.utils/#richconsoleconsolecapture","text":"</> A context manager to capture the result of print() or log() in a string,rather than writing it to the console. Example >>> from rich.console import Console >>> console = Console () >>> with console . capture () as capture : ... console . print ( \"[bold magenta]Hello World[/]\" ) >>> print ( capture . get ()) Returns (Capture) Context manager with disables writing to the terminal. method","title":"rich.console.Console.capture"},{"location":"api/pipen.utils/#richconsoleconsolepager","text":"</> A context manager to display anything printed within a \"pager\". The pager applicationis defined by the system and will typically support at least pressing a key to scroll. Parameters pager (Pager, optional) \u2014 A pager object, or None to use :class: ~rich.pager.SystemPager . Defaults to None. styles (bool, optional) \u2014 Show styles in pager. Defaults to False. links (bool, optional) \u2014 Show links in pager. Defaults to False. Example >>> from rich.console import Console >>> from rich.__main__ import make_test_card >>> console = Console () >>> with console . pager (): console . print ( make_test_card ()) Returns (PagerContext) A context manager. method","title":"rich.console.Console.pager"},{"location":"api/pipen.utils/#richconsoleconsoleline","text":"</> Write new line(s). Parameters count (int, optional) \u2014 Number of new lines. Defaults to 1. method","title":"rich.console.Console.line"},{"location":"api/pipen.utils/#richconsoleconsoleclear","text":"</> Clear the screen. Parameters home (bool, optional) \u2014 Also move the cursor to 'home' position. Defaults to True. method","title":"rich.console.Console.clear"},{"location":"api/pipen.utils/#richconsoleconsolestatus","text":"</> Display a status and spinner. Parameters status (RenderableType) \u2014 A status renderable (str or Text typically). spinner (str, optional) \u2014 Name of spinner animation (see python -m rich.spinner). Defaults to \"dots\". spinner_style (StyleType, optional) \u2014 Style of spinner. Defaults to \"status.spinner\". speed (float, optional) \u2014 Speed factor for spinner animation. Defaults to 1.0. refresh_per_second (float, optional) \u2014 Number of refreshes per second. Defaults to 12.5. Returns (Status) A Status object that may be used as a context manager. method","title":"rich.console.Console.status"},{"location":"api/pipen.utils/#richconsoleconsoleshow_cursor","text":"</> Show or hide the cursor. Parameters show (bool, optional) \u2014 Set visibility of the cursor. method","title":"rich.console.Console.show_cursor"},{"location":"api/pipen.utils/#richconsoleconsoleset_alt_screen","text":"</> Enables alternative screen mode. Note, if you enable this mode, you should ensure that is disabled before the application exits. See :meth: ~rich.Console.screen for a context manager that handles this for you. Parameters enable (bool, optional) \u2014 Enable (True) or disable (False) alternate screen. Defaults to True. Returns (bool) True if the control codes were written. method","title":"rich.console.Console.set_alt_screen"},{"location":"api/pipen.utils/#richconsoleconsoleset_window_title","text":"</> Set the title of the console terminal window. Warning: There is no means within Rich of \"resetting\" the window title to its previous value, meaning the title you set will persist even after your application exits. fish shell resets the window title before and after each command by default, negating this issue. Windows Terminal and command prompt will also reset the title for you. Most other shells and terminals, however, do not do this. Some terminals may require configuration changes before you can set the title. Some terminals may not support setting the title at all. Other software (including the terminal itself, the shell, custom prompts, plugins, etc.) may also set the terminal window title. This could result in whatever value you write using this method being overwritten. Parameters title (str) \u2014 The new title of the terminal window. Returns (bool) True if the control code to change the terminal title was written, otherwise False. Note that a return value of True does not guarantee that the window title has actually changed, since the feature may be unsupported/disabled in some terminals. method","title":"rich.console.Console.set_window_title"},{"location":"api/pipen.utils/#richconsoleconsolescreen","text":"</> Context manager to enable and disable 'alternative screen' mode. Parameters hide_cursor (bool, optional) \u2014 Also hide the cursor. Defaults to False. style (Style, optional) \u2014 Optional style for screen. Defaults to None. Returns (~ScreenContext) Context which enables alternate screen on enter, and disables it on exit. method","title":"rich.console.Console.screen"},{"location":"api/pipen.utils/#richconsoleconsolemeasure","text":"</> Measure a renderable. Returns a :class: ~rich.measure.Measurement object which containsinformation regarding the number of characters required to print the renderable. Parameters renderable (RenderableType) \u2014 Any renderable or string. options (Optional[ConsoleOptions], optional) \u2014 Options to use when measuring, or Noneto use default options. Defaults to None. Returns (Measurement) A measurement of the renderable. generator","title":"rich.console.Console.measure"},{"location":"api/pipen.utils/#richconsoleconsolerender","text":"</> Render an object in to an iterable of Segment instances. This method contains the logic for rendering objects with the console protocol. You are unlikely to need to use it directly, unless you are extending the library. Parameters renderable (RenderableType) \u2014 An object supporting the console protocol, oran object that may be converted to a string. options (ConsoleOptions, optional) \u2014 An options object, or None to use self.options. Defaults to None. Returns (Iterable[Segment]) An iterable of segments that may be rendered. method","title":"rich.console.Console.render"},{"location":"api/pipen.utils/#richconsoleconsolerender_lines","text":"</> Render objects in to a list of lines. The output of render_lines is useful when further formatting of rendered console text is required, such as the Panel class which draws a border around any renderable object. Args: renderable (RenderableType): Any object renderable in the console. options (Optional[ConsoleOptions], optional): Console options, or None to use self.options. Default to ``None``. style (Style, optional): Optional style to apply to renderables. Defaults to ``None``. pad (bool, optional): Pad lines shorter than render width. Defaults to ``True``. new_lines (bool, optional): Include \" \" characters at end of lines. Returns: List[List[Segment]]: A list of lines, where a line is a list of Segment objects. method","title":"rich.console.Console.render_lines"},{"location":"api/pipen.utils/#richconsoleconsolerender_str","text":"</> Convert a string to a Text instance. This is called automatically ifyou print or log a string. Parameters text (str) \u2014 Text to render. style (Union[str, Style], optional) \u2014 Style to apply to rendered text. justify (str, optional) \u2014 Justify method: \"default\", \"left\", \"center\", \"full\", or \"right\". Defaults to None . overflow (str, optional) \u2014 Overflow method: \"crop\", \"fold\", or \"ellipsis\". Defaults to None . emoji (Optional[bool], optional) \u2014 Enable emoji, or None to use Console default. markup (Optional[bool], optional) \u2014 Enable markup, or None to use Console default. highlight (Optional[bool], optional) \u2014 Enable highlighting, or None to use Console default. highlighter (HighlighterType, optional) \u2014 Optional highlighter to apply. Returns (ConsoleRenderable) Renderable object. method","title":"rich.console.Console.render_str"},{"location":"api/pipen.utils/#richconsoleconsoleget_style","text":"</> Get a Style instance by its theme name or parse a definition. Parameters name (str) \u2014 The name of a style or a style definition. Returns (Style) A Style object. Raises MissingStyle \u2014 If no style could be parsed from name. method","title":"rich.console.Console.get_style"},{"location":"api/pipen.utils/#richconsoleconsolerule","text":"</> Draw a line with optional centered title. Parameters title (str, optional) \u2014 Text to render over the rule. Defaults to \"\". characters (str, optional) \u2014 Character(s) to form the line. Defaults to \"\u2500\". style (str, optional) \u2014 Style of line. Defaults to \"rule.line\". align (str, optional) \u2014 How to align the title, one of \"left\", \"center\", or \"right\". Defaults to \"center\". method","title":"rich.console.Console.rule"},{"location":"api/pipen.utils/#richconsoleconsolecontrol","text":"</> Insert non-printing control codes. method","title":"rich.console.Console.control"},{"location":"api/pipen.utils/#richconsoleconsoleout","text":"</> Output to the terminal. This is a low-level way of writing to the terminal which unlike:meth: ~rich.console.Console.print won't pretty print, wrap text, or apply markup, but will optionally apply highlighting and a basic style. Parameters sep (str, optional) \u2014 String to write between print data. Defaults to \" \". end (str, optional) \u2014 String to write at end of print data. Defaults to \"\\n\". style (Union[str, Style], optional) \u2014 A style to apply to output. Defaults to None. highlight (Optional[bool], optional) \u2014 Enable automatic highlighting, or None to useconsole default. Defaults to None . method","title":"rich.console.Console.out"},{"location":"api/pipen.utils/#richconsoleconsoleprint","text":"</> Print to the console. Parameters sep (str, optional) \u2014 String to write between print data. Defaults to \" \". end (str, optional) \u2014 String to write at end of print data. Defaults to \"\\n\". style (Union[str, Style], optional) \u2014 A style to apply to output. Defaults to None. justify (str, optional) \u2014 Justify method: \"default\", \"left\", \"right\", \"center\", or \"full\". Defaults to None . overflow (str, optional) \u2014 Overflow method: \"ignore\", \"crop\", \"fold\", or \"ellipsis\". Defaults to None. no_wrap (Optional[bool], optional) \u2014 Disable word wrapping. Defaults to None. emoji (Optional[bool], optional) \u2014 Enable emoji code, or None to use console default. Defaults to None . markup (Optional[bool], optional) \u2014 Enable markup, or None to use console default. Defaults to None . highlight (Optional[bool], optional) \u2014 Enable automatic highlighting, or None to use console default. Defaults to None . width (Optional[int], optional) \u2014 Width of output, or None to auto-detect. Defaults to None . crop (Optional[bool], optional) \u2014 Crop output to width of terminal. Defaults to True. soft_wrap (bool, optional) \u2014 Enable soft wrap mode which disables word wrapping and cropping of text or None forConsole default. Defaults to None . new_line_start (bool, False) \u2014 Insert a new line at the start if the output contains more than one line. Defaults to False . method","title":"rich.console.Console.print"},{"location":"api/pipen.utils/#richconsoleconsoleprint_json","text":"</> Pretty prints JSON. Output will be valid JSON. Parameters json (Optional[str]) \u2014 A string containing JSON. data (Any) \u2014 If json is not supplied, then encode this data. indent (Union[None, int, str], optional) \u2014 Number of spaces to indent. Defaults to 2. highlight (bool, optional) \u2014 Enable highlighting of output: Defaults to True. skip_keys (bool, optional) \u2014 Skip keys not of a basic type. Defaults to False. ensure_ascii (bool, optional) \u2014 Escape all non-ascii characters. Defaults to False. check_circular (bool, optional) \u2014 Check for circular references. Defaults to True. allow_nan (bool, optional) \u2014 Allow NaN and Infinity values. Defaults to True. default (Callable, optional) \u2014 A callable that converts values that can not be encodedin to something that can be JSON encoded. Defaults to None. sort_keys (bool, optional) \u2014 Sort dictionary keys. Defaults to False. method","title":"rich.console.Console.print_json"},{"location":"api/pipen.utils/#richconsoleconsoleupdate_screen","text":"</> Update the screen at a given offset. Parameters renderable (RenderableType) \u2014 A Rich renderable. region (Region, optional) \u2014 Region of screen to update, or None for entire screen. Defaults to None. Raises errors.NoAltScreen \u2014 If the Console isn't in alt screen mode. method","title":"rich.console.Console.update_screen"},{"location":"api/pipen.utils/#richconsoleconsoleupdate_screen_lines","text":"</> Update lines of the screen at a given offset. Parameters lines (List[List[Segment]]) \u2014 Rendered lines (as produced by :meth: ~rich.Console.render_lines ). x (int, optional) \u2014 x offset (column no). Defaults to 0. y (int, optional) \u2014 y offset (column no). Defaults to 0. Raises errors.NoAltScreen \u2014 If the Console isn't in alt screen mode. method","title":"rich.console.Console.update_screen_lines"},{"location":"api/pipen.utils/#richconsoleconsoleprint_exception","text":"</> Prints a rich render of the last exception and traceback. Parameters width (Optional[int], optional) \u2014 Number of characters used to render code. Defaults to 100. extra_lines (int, optional) \u2014 Additional lines of code to render. Defaults to 3. theme (str, optional) \u2014 Override pygments theme used in traceback word_wrap (bool, optional) \u2014 Enable word wrapping of long lines. Defaults to False. show_locals (bool, optional) \u2014 Enable display of local variables. Defaults to False. suppress (Iterable[Union[str, ModuleType]]) \u2014 Optional sequence of modules or paths to exclude from traceback. max_frames (int) \u2014 Maximum number of frames to show in a traceback, 0 for no maximum. Defaults to 100. method","title":"rich.console.Console.print_exception"},{"location":"api/pipen.utils/#richconsoleconsolelog","text":"</> Log rich content to the terminal. Parameters sep (str, optional) \u2014 String to write between print data. Defaults to \" \". end (str, optional) \u2014 String to write at end of print data. Defaults to \"\\n\". style (Union[str, Style], optional) \u2014 A style to apply to output. Defaults to None. justify (str, optional) \u2014 One of \"left\", \"right\", \"center\", or \"full\". Defaults to None . emoji (Optional[bool], optional) \u2014 Enable emoji code, or None to use console default. Defaults to None. markup (Optional[bool], optional) \u2014 Enable markup, or None to use console default. Defaults to None. highlight (Optional[bool], optional) \u2014 Enable automatic highlighting, or None to use console default. Defaults to None. log_locals (bool, optional) \u2014 Boolean to enable logging of locals where log() was called. Defaults to False. _stack_offset (int, optional) \u2014 Offset of caller from end of call stack. Defaults to 1. method","title":"rich.console.Console.log"},{"location":"api/pipen.utils/#richconsoleconsoleon_broken_pipe","text":"</> This function is called when a BrokenPipeError is raised. This can occur when piping Textual output in Linux and macOS. The default implementation is to exit the app, but you could implement this method in a subclass to change the behavior. See https://docs.python.org/3/library/signal.html#note-on-sigpipe for details. method","title":"rich.console.Console.on_broken_pipe"},{"location":"api/pipen.utils/#richconsoleconsoleinput","text":"</> Displays a prompt and waits for input from the user. The prompt may contain color / style. It works in the same way as Python's builtin :func: input function and provides elaborate line editing and history features if Python's builtin :mod: readline module is previously loaded. Parameters prompt (Union[str, Text]) \u2014 Text to render in the prompt. markup (bool, optional) \u2014 Enable console markup (requires a str prompt). Defaults to True. emoji (bool, optional) \u2014 Enable emoji (requires a str prompt). Defaults to True. password (bool, optional) \u2014 (bool, optional): Hide typed text. Defaults to False. stream (Optional, optional) \u2014 (TextIO, optional): Optional file to read input from (rather than stdin). Defaults to None. Returns (str) Text read from stdin. method","title":"rich.console.Console.input"},{"location":"api/pipen.utils/#richconsoleconsoleexport_text","text":"</> Generate text from console contents (requires record=True argument in constructor). Parameters clear (bool, optional) \u2014 Clear record buffer after exporting. Defaults to True . styles (bool, optional) \u2014 If True , ansi escape codes will be included. False for plain text.Defaults to False . Returns (str) String containing console contents. method","title":"rich.console.Console.export_text"},{"location":"api/pipen.utils/#richconsoleconsolesave_text","text":"</> Generate text from console and save to a given location (requires record=True argument in constructor). Parameters path (str) \u2014 Path to write text files. clear (bool, optional) \u2014 Clear record buffer after exporting. Defaults to True . styles (bool, optional) \u2014 If True , ansi style codes will be included. False for plain text.Defaults to False . method","title":"rich.console.Console.save_text"},{"location":"api/pipen.utils/#richconsoleconsoleexport_html","text":"</> Generate HTML from console contents (requires record=True argument in constructor). Parameters theme (TerminalTheme, optional) \u2014 TerminalTheme object containing console colors. clear (bool, optional) \u2014 Clear record buffer after exporting. Defaults to True . code_format (str, optional) \u2014 Format string to render HTML. In addition to '{foreground}','{background}', and '{code}', should contain '{stylesheet}' if inline_styles is False . inline_styles (bool, optional) \u2014 If True styles will be inlined in to spans, which makes fileslarger but easier to cut and paste markup. If False , styles will be embedded in a style tag. Defaults to False. Returns (str) String containing console contents as HTML. method","title":"rich.console.Console.export_html"},{"location":"api/pipen.utils/#richconsoleconsolesave_html","text":"</> Generate HTML from console contents and write to a file (requires record=True argument in constructor). Parameters path (str) \u2014 Path to write html file. theme (TerminalTheme, optional) \u2014 TerminalTheme object containing console colors. clear (bool, optional) \u2014 Clear record buffer after exporting. Defaults to True . code_format (str, optional) \u2014 Format string to render HTML. In addition to '{foreground}','{background}', and '{code}', should contain '{stylesheet}' if inline_styles is False . inline_styles (bool, optional) \u2014 If True styles will be inlined in to spans, which makes fileslarger but easier to cut and paste markup. If False , styles will be embedded in a style tag. Defaults to False. method","title":"rich.console.Console.save_html"},{"location":"api/pipen.utils/#richconsoleconsoleexport_svg","text":"</> Generate an SVG from the console contents (requires record=True in Console constructor). Parameters title (str, optional) \u2014 The title of the tab in the output image theme (TerminalTheme, optional) \u2014 The TerminalTheme object to use to style the terminal clear (bool, optional) \u2014 Clear record buffer after exporting. Defaults to True code_format (str, optional) \u2014 Format string used to generate the SVG. Rich will inject a number of variablesinto the string in order to form the final SVG output. The default template used and the variables injected by Rich can be found by inspecting the console.CONSOLE_SVG_FORMAT variable. font_aspect_ratio (float, optional) \u2014 The width to height ratio of the font used in the code_format string. Defaults to 0.61, which is the width to height ratio of Fira Code (the default font). If you aren't specifying a different font inside code_format , you probably don't need this. unique_id (str, optional) \u2014 unique id that is used as the prefix for various elements (CSS styles, nodeids). If not set, this defaults to a computed value based on the recorded content. method","title":"rich.console.Console.export_svg"},{"location":"api/pipen.utils/#richconsoleconsolesave_svg","text":"</> Generate an SVG file from the console contents (requires record=True in Console constructor). Parameters path (str) \u2014 The path to write the SVG to. title (str, optional) \u2014 The title of the tab in the output image theme (TerminalTheme, optional) \u2014 The TerminalTheme object to use to style the terminal clear (bool, optional) \u2014 Clear record buffer after exporting. Defaults to True code_format (str, optional) \u2014 Format string used to generate the SVG. Rich will inject a number of variablesinto the string in order to form the final SVG output. The default template used and the variables injected by Rich can be found by inspecting the console.CONSOLE_SVG_FORMAT variable. font_aspect_ratio (float, optional) \u2014 The width to height ratio of the font used in the code_format string. Defaults to 0.61, which is the width to height ratio of Fira Code (the default font). If you aren't specifying a different font inside code_format , you probably don't need this. unique_id (str, optional) \u2014 unique id that is used as the prefix for various elements (CSS styles, nodeids). If not set, this defaults to a computed value based on the recorded content. function","title":"rich.console.Console.save_svg"},{"location":"api/pipen.utils/#pipenutilsget_logger","text":"</> Get the logger by given plugin name Parameters level (str | int, optional) \u2014 The initial level of the logger Returns (LoggerAdapter) The logger function","title":"pipen.utils.get_logger"},{"location":"api/pipen.utils/#pipenutilsdesc_from_docstring","text":"</> Get the description from docstring Only extract the summary. Parameters obj (Type[Pipen | Proc]) \u2014 The object with docstring Returns (str) The summary as desc function","title":"pipen.utils.desc_from_docstring"},{"location":"api/pipen.utils/#pipenutilsupdate_dict","text":"</> Update the new dict to the parent, but make sure parent does not change Parameters parent (Mapping) \u2014 The parent dictionary new (Mapping) \u2014 The new dictionary depth (int, optional) \u2014 The depth to be copied. 0 for updating to the deepest level. try_list (bool, optional) \u2014 If True, try to also update the dict in the list Examples >>> parent = { \"a\" : { \"b\" : 1 }} >>> new = { \"a\" : { \"c\" : 2 }} >>> update_dict ( parent , new ) >>> # {\"a\": {\"b\": 1, \"c\": 2}} >>> parent = { \"a\" : [{ \"b\" : 1 }]} >>> new = { \"a\" : [{ \"c\" : 2 }]} >>> update_dict ( parent , new , try_list = True ) >>> # {\"a\": [{\"b\": 1, \"c\": 2}]} Returns (Mapping) The updated dictionary or None if both parent and new are None. function","title":"pipen.utils.update_dict"},{"location":"api/pipen.utils/#pipenutilscopy_dict","text":"</> Deep copy a dict Parameters dic (Mapping) \u2014 The dict to be copied depth (int, optional) \u2014 The depth to be deep copied Returns (Mapping) The deep-copied dict function","title":"pipen.utils.copy_dict"},{"location":"api/pipen.utils/#pipenutilsstrsplit","text":"</> Split the string, with the ability to trim each part. function","title":"pipen.utils.strsplit"},{"location":"api/pipen.utils/#pipenutilsget_shebang","text":"</> Get the shebang of the script Parameters script (str) \u2014 The script string Returns (str) None if the script does not contain a shebang, otherwise the shebangwithout #! prefix function","title":"pipen.utils.get_shebang"},{"location":"api/pipen.utils/#pipenutilsignore_firstline_dedent","text":"</> Like textwrap.dedent(), but ignore first empty lines Parameters text (str) \u2014 The text the be dedented Returns (str) The dedented text function","title":"pipen.utils.ignore_firstline_dedent"},{"location":"api/pipen.utils/#pipenutilsget_logpanel_width","text":"</> Get the width of the log content Parameters max_width \u2014 The maximum width to returnNote that it's not the console width. With console width, you have to subtract the width of the log meta info (CONSOLE_WIDTH_SHIFT). Returns (int) The width of the log content function","title":"pipen.utils.get_logpanel_width"},{"location":"api/pipen.utils/#pipenutilslog_rich_renderable","text":"</> Log a rich renderable to logger Parameters renderable (RenderableType) \u2014 The rich renderable logfunc (Callable) \u2014 The log function, if message is not the first argument,use functools.partial to wrap it *args (Any) \u2014 The arguments to the log function **kwargs (Any) \u2014 The keyword arguments to the log function splitline \u2014 Whether split the lines or log the entire message function","title":"pipen.utils.log_rich_renderable"},{"location":"api/pipen.utils/#pipenutilsbrief_list","text":"</> Briefly show an integer list, combine the continuous numbers. Parameters blist (List) \u2014 The list Returns (str) The string to show for the briefed list. function","title":"pipen.utils.brief_list"},{"location":"api/pipen.utils/#pipenutilspipen_banner","text":"</> The banner for pipen Returns (RenderableType) The banner renderable function","title":"pipen.utils.pipen_banner"},{"location":"api/pipen.utils/#pipenutilsget_mtime","text":"</> Get the modification time of a path.If path is a directory, try to get the last modification time of the contents in the directory at given dir_depth Parameters dir_depth (int, optional) \u2014 The depth of the directory to check thelast modification time 0 means only check the path itself (don't go into directory or follow symlink) Returns (float) The last modification time of path function","title":"pipen.utils.get_mtime"},{"location":"api/pipen.utils/#pipenutilsis_subclass","text":"</> Tell if obj is a subclass of clsDifferences with issubclass is that we don't raise Type error if obj is not a class Parameters obj (Any) \u2014 The object to check cls (type) \u2014 The class to check Returns (bool) True if obj is a subclass of cls otherwise False generator","title":"pipen.utils.is_subclass"},{"location":"api/pipen.utils/#pipenutilsload_entrypoints","text":"</> Load objects from setuptools entrypoints by given group name Parameters group (str) \u2014 The group name of the entrypoints Returns (Iterable) An iterable of tuples with name and the loaded object function","title":"pipen.utils.load_entrypoints"},{"location":"api/pipen.utils/#pipenutilstruncate_text","text":"</> Truncate a text not based on words/whitespacesOtherwise, we could use textwrap.shorten. Parameters text (str) \u2014 The text to be truncated width (int) \u2014 The max width of the the truncated text end (str, optional) \u2014 The end string of the truncated text Returns (str) The truncated text with end appended. function","title":"pipen.utils.truncate_text"},{"location":"api/pipen.utils/#pipenutilsmake_df_colnames_unique_inplace","text":"</> Make the columns of a data frame unique Parameters thedf (pandas.DataFrame) \u2014 The data frame function","title":"pipen.utils.make_df_colnames_unique_inplace"},{"location":"api/pipen.utils/#pipenutilsget_base","text":"</> Get the base class where the value was first defined Parameters klass (Type) \u2014 The class abc_base (Type) \u2014 The very base class to check in bases value (Any) \u2014 The value to check value_getter (Callable) \u2014 How to get the value from the class Returns (Type) The base class function","title":"pipen.utils.get_base"},{"location":"api/pipen.utils/#pipenutilsmark","text":"</> Mark a class (e.g. Proc) with given kwargs as metadata These marks will not be inherited by the subclasses if the class is a subclass of Proc or ProcGroup . Parameters **kwargs \u2014 The kwargs to mark the proc Returns (Callable) The decorator function","title":"pipen.utils.mark"},{"location":"api/pipen.utils/#pipenutilsget_marked","text":"</> Get the marked value from a proc Parameters cls (type) \u2014 The proc mark_name (str) \u2014 The mark name default (Any, optional) \u2014 The default value if the mark is not found Returns (Any) The marked value function","title":"pipen.utils.get_marked"},{"location":"api/pipen.utils/#pipenutilsis_valid_name","text":"</> Check if a name is valid for a proc or pipen Parameters name (str) \u2014 The name to check Returns (bool) True if valid, otherwise False function","title":"pipen.utils.is_valid_name"},{"location":"api/pipen.utils/#pipenutilsload_pipeline","text":"</> Load a pipeline from a Pipen, Proc or ProcGroup object It does not only load the Pipen object or convert the Proc/ProcGroup object to Pipen, but also build the process relationships. So that we can access pipeline.procs and requires/nexts of each proc. To avoid running the pipeline and notify the plugins that this is just for loading the pipeline, sys.argv[0] is set to @pipen . Parameters obj (str | Type[Proc] | Type[ProcGroup] | Type[Pipen] | Pipen) \u2014 The Pipen, Proc or ProcGroup object. It can also be a string inthe format of part1:part2 to load the pipeline, where part1 is a path to a python file or package directory, and part2 is the name of the proc, procgroup or pipeline to load. It should be able to be loaded by getattr(module, part2) , where module is loaded from part1 . argv0 (str | none, optional) \u2014 The value to replace sys.argv[0]. \"@pipen\" will be usedby default. argv1p (Optional, optional) \u2014 The values to replace sys.argv[1:]. Do not replace by default. kwargs \u2014 The kwargs to pass to the Pipen constructor Returns (Pipen) The loaded Pipen object Raises TypeError \u2014 If obj or loaded obj is not a Pipen, Proc or ProcGroup function","title":"pipen.utils.load_pipeline"},{"location":"api/pipen.utils/#pipenutilsis_loading_pipeline","text":"</> Check if we are loading the pipeline. Works only when argv0 is \"@pipen\" while loading the pipeline. Note if you are using this function at compile time, make sure you load your pipeline using the string form ( part1:part2 ) See more with load_pipline() . Parameters *flags (str) \u2014 Additional flags to check in sys.argv (e.g. \"-h\", \"--help\")to determine if we are loading the pipeline argv (Optional, optional) \u2014 The arguments to check. sys.argv is used by default.Note that the first argument should be included in the check. You could typically pass [sys.argv[0], *your_args] to this if you want to check if sys.argv[0] is \"@pipen\" or your_args contains some flags. Returns (bool) True if we are loading the pipeline (argv[0] == \"@pipen\"),otherwise False function","title":"pipen.utils.is_loading_pipeline"},{"location":"api/pipen.utils/#pipenutilspath_is_symlink_sync","text":"</> Check if a path is a symlink synchronously. We don't only check the real symlink, but also the fake symlink files created by path_symlink_to() . Parameters path (PanPath) \u2014 The path to check Returns (bool) True if the path is a symlink, otherwise False function","title":"pipen.utils.path_is_symlink_sync"},{"location":"api/pipen.utils/#pipenutilspath_is_symlink","text":"</> Check if a path is a symlink. We don't only check the real symlink, but also the fake symlink files created by path_symlink_to() . Parameters path (PanPath) \u2014 The path to check Returns (bool) True if the path is a symlink, otherwise False function","title":"pipen.utils.path_is_symlink"},{"location":"api/pipen.utils/#pipenutilspath_symlink_to","text":"</> Create a symbolic link pointing to src named dst. Parameters src (PanPath) \u2014 The source path dst (PanPath) \u2014 The destination path target_is_directory (bool, optional) \u2014 If True, the symbolic link will be to a directory.","title":"pipen.utils.path_symlink_to"},{"location":"api/pipen.version/","text":"module pipen. version </> Provide version of pipen","title":"pipen.version"},{"location":"api/pipen.version/#pipenversion","text":"</> Provide version of pipen","title":"pipen.version"},{"location":"api/source/pipen.channel/","text":"SOURCE CODE pipen. channel DOCS \"\"\"Provide some function for creating and modifying channels (dataframes)\"\"\" from __future__ import annotations from os import path from typing import Any , Iterable , List import pandas from panpath import PanPath from pandas import DataFrame from pipda import register_verb from .utils import path_is_symlink , path_is_symlink_sync # ---------------------------------------------------------------- # Creators class Channel ( DataFrame ): DOCS \"\"\"A DataFrame wrapper with creators\"\"\" @classmethod DOCS def create ( cls , value : DataFrame | List [ Any ]) -> DataFrame : \"\"\"Create a channel from a list. The second dimension is identified by tuple. if all elements are tuple, then a channel is created directly. Otherwise, elements are converted to tuples first and channels are created then. Examples: >>> Channel.create([1, 2, 3]) # 3 rows, 1 column >>> Channel.create([(1,2,3)]) # 1 row, 3 columns Args: value: The value to create a channel Returns: A channel (dataframe) \"\"\" if isinstance ( value , DataFrame ): return value if all ( isinstance ( elem , tuple ) for elem in value ): return cls ( value ) return cls (( val ,) for val in value ) @classmethod DOCS def from_glob ( cls , pattern : str , ftype : str = \"any\" , sortby : str = \"name\" , reverse : bool = False , ) -> DataFrame : \"\"\"Create a channel with a glob pattern Args: ftype: The file type, one of any, link, dir and file sortby: How the files should be sorted. One of name, mtime and size reverse: Whether sort them in a reversed way. Returns: The channel \"\"\" def sort_key ( file : PanPath ) -> Any : if sortby == \"mtime\" : return file . stat () . st_mtime if sortby == \"size\" : return file . stat () . st_size return str ( file ) # sort by name def file_filter ( file : PanPath ) -> bool : if ftype == \"link\" : return path_is_symlink_sync ( file ) if ftype == \"dir\" : return file . is_dir () if ftype == \"file\" : return file . is_file () return True pattern = str ( pattern ) parts = pattern . split ( \"/\" ) wildcard_index = - 1 for i , part in enumerate ( parts ): if \"*\" in part or \"?\" in part or \"[\" in part : wildcard_index = i break if wildcard_index == - 1 : files : Iterable [ PanPath ] = ( [ PanPath ( pattern )] if file_filter ( PanPath ( pattern )) else [] ) return cls . create ([ str ( file ) for file in files ]) base_path = PanPath ( \"/\" . join ( parts [: wildcard_index ])) sub_pattern = \"/\" . join ( parts [ wildcard_index :]) files = ( PanPath ( file ) for file in base_path . glob ( sub_pattern ) if file_filter ( PanPath ( file )) ) return cls . create ( [ str ( file ) for file in sorted ( files , key = sort_key if sortby in ( \"name\" , \"mtime\" , \"size\" ) else None , reverse = reverse , ) # type: ignore ] ) @classmethod DOCS async def a_from_glob ( cls , pattern : str , ftype : str = \"any\" , sortby : str = \"name\" , reverse : bool = False , ) -> DataFrame : \"\"\"Create a channel with a glob pattern asynchronously Args: pattern: The glob pattern, supported: \"dir1/dir2/*.txt\" ftype: The file type, one of any, link, dir and file sortby: How the files should be sorted. One of name, mtime and size reverse: Whether sort them in a reversed way. Returns: The channel \"\"\" async def get_sort_key ( file : PanPath , sort_by : str ) -> Any : if sortby == \"mtime\" : # pragma: no cover return ( await file . a_stat ()) . st_mtime if sortby == \"size\" : return ( await file . a_stat ()) . st_size return str ( file ) # sort by name async def file_filter ( file : PanPath ) -> bool : if ftype == \"link\" : # pragma: no cover return await path_is_symlink ( file ) if ftype == \"dir\" : # pragma: no cover return await file . a_is_dir () if ftype == \"file\" : return await file . a_is_file () return True pattern = str ( pattern ) parts = pattern . split ( \"/\" ) wildcard_index = - 1 for i , part in enumerate ( parts ): if \"*\" in part or \"?\" in part or \"[\" in part : wildcard_index = i break if wildcard_index == - 1 : files = [ PanPath ( pattern )] if await file_filter ( PanPath ( pattern )) else [] return cls . create ([ str ( file ) for file in files ]) base_path = PanPath ( \"/\" . join ( parts [: wildcard_index ])) sub_pattern = \"/\" . join ( parts [ wildcard_index :]) files = [ PanPath ( file ) async for file in base_path . a_glob ( sub_pattern ) if await file_filter ( PanPath ( file )) ] sort_keys = dict ( [ ( file , await get_sort_key ( file , sortby ), ) for file in files ] ) return cls . create ( [ str ( file ) for file in sorted ( files , key = ( sort_keys . get # type: ignore[arg-type] if sortby in ( \"name\" , \"mtime\" , \"size\" ) else None ), reverse = reverse , ) ] ) @classmethod DOCS def from_pairs ( cls , pattern : str , ftype : str = \"any\" , sortby : str = \"name\" , reverse : bool = False , ) -> DataFrame : \"\"\"Create a width=2 channel with a glob pattern Args: ftype: The file type, one of any, link, dir and file sortby: How the files should be sorted. One of name, mtime and size reverse: Whether sort them in a reversed way. Returns: The channel \"\"\" mates = cls . from_glob ( pattern , ftype , sortby , reverse ) return pandas . concat ( ( mates . iloc [:: 2 ] . reset_index ( drop = True ), mates . iloc [ 1 :: 2 ] . reset_index ( drop = True ), ), axis = 1 , ) @classmethod DOCS async def a_from_pairs ( cls , pattern : str , ftype : str = \"any\" , sortby : str = \"name\" , reverse : bool = False , ) -> DataFrame : \"\"\"Create a width=2 channel with a glob pattern Args: ftype: The file type, one of any, link, dir and file sortby: How the files should be sorted. One of name, mtime and size reverse: Whether sort them in a reversed way. Returns: The channel \"\"\" mates = await cls . a_from_glob ( pattern , ftype , sortby , reverse ) return pandas . concat ( ( mates . iloc [:: 2 ] . reset_index ( drop = True ), mates . iloc [ 1 :: 2 ] . reset_index ( drop = True ), ), axis = 1 , ) @classmethod DOCS def from_csv ( cls , * args , ** kwargs ): \"\"\"Create a channel from a csv file Uses pandas.read_csv() to create a channel Args: *args: and **kwargs: Arguments passing to pandas.read_csv() \"\"\" return pandas . read_csv ( * args , ** kwargs ) @classmethod DOCS def from_excel ( cls , * args , ** kwargs ): \"\"\"Create a channel from an excel file. Uses pandas.read_excel() to create a channel Args: *args: and **kwargs: Arguments passing to pandas.read_excel() \"\"\" return pandas . read_excel ( * args , ** kwargs ) @classmethod DOCS def from_table ( cls , * args , ** kwargs ): \"\"\"Create a channel from a table file. Uses pandas.read_table() to create a channel Args: *args: and **kwargs: Arguments passing to pandas.read_table() \"\"\" return pandas . read_table ( * args , ** kwargs ) # ---------------------------------------------------------------- # Verbs @register_verb ( DataFrame ) DOCS def expand_dir ( data : DataFrame , col : str | int = 0 , pattern : str = \"*\" , ftype : str = \"any\" , sortby : str = \"name\" , reverse : bool = False , ) -> DataFrame : \"\"\"Expand a Channel according to the files in <col>, other cols will keep the same. This is only applicable to a 1-row channel. Examples: >>> ch = channel.create([('./', 1)]) >>> ch >> expand() >>> [['./a', 1], ['./b', 1], ['./c', 1]] Args: col: the index or name of the column used to expand pattern: use a pattern to filter the files/dirs, default: `*` ftype: the type of the files/dirs to include - 'dir', 'file', 'link' or 'any' (default) sortby: how the list is sorted - 'name' (default), 'mtime', 'size' reverse: reverse sort. Returns: The expanded channel \"\"\" assert data . shape [ 0 ] == 1 , \"Can only expand a single row DataFrame.\" col_loc = col if isinstance ( col , int ) else data . columns . get_loc ( col ) full_pattern = f \" { data . iloc [ 0 , col_loc ] } / { pattern } \" expanded = Channel . from_glob ( full_pattern , ftype , sortby , reverse , ) . iloc [:, 0 ] ret = pandas . concat ([ data ] * expanded . size , axis = 0 , ignore_index = True ) ret . iloc [:, col_loc ] = expanded . values return ret . reset_index ( drop = True ) @register_verb ( DataFrame ) DOCS def collapse_files ( data : DataFrame , col : str | int = 0 ) -> DataFrame : \"\"\"Collapse a Channel according to the files in <col>, other cols will use the values in row 0. Note that other values in other rows will be discarded. Examples: >>> ch = channel.create([['./a', 1], ['./b', 1], ['./c', 1]]) >>> ch >> collapse() >>> [['.', 1]] Args: data: The original channel col: the index or name of the column used to collapse on Returns: The collapsed channel \"\"\" assert data . shape [ 0 ] > 0 , \"Cannot collapse on an empty DataFrame.\" col_loc = col if isinstance ( col , int ) else data . columns . get_loc ( col ) paths = list ( data . iloc [:, col_loc ]) compx = path . dirname ( path . commonprefix ( paths )) ret = data . iloc [[ 0 ], :] . copy () ret . iloc [ 0 , col_loc ] = compx return ret","title":"pipen.channel"},{"location":"api/source/pipen.cli.help/","text":"SOURCE CODE pipen.cli. help DOCS \"\"\"Print help for commands\"\"\" from __future__ import annotations from typing import TYPE_CHECKING from ._hooks import CLIPlugin if TYPE_CHECKING : from argx import ArgumentParser from argparse import Namespace __all__ = ( \"CLIHelpPlugin\" ,) class CLIHelpPlugin ( CLIPlugin ): DOCS \"\"\"Print help for commands\"\"\" name = \"help\" def __init__ ( self , parser : ArgumentParser , subparser : ArgumentParser ): super () . __init__ ( parser , subparser ) subparser . add_argument ( \"cmd\" , nargs = \"?\" , choices = [ n for n in parser . _subparsers . _group_actions [ 0 ] . choices if n != \"help\" ], help = \"The command to show help for\" , ) def exec_command ( self , args : Namespace ) -> None : DOCS \"\"\"Run the command\"\"\" if not args . cmd : self . parser . parse_args ([ \"--help\" ]) else : self . parser . parse_args ([ args . cmd , \"--help\" ])","title":"pipen.cli.help"},{"location":"api/source/pipen.cli/","text":"SOURCE CODE pipen. cli DOCS \"\"\"Provide CLI for pipen\"\"\" from ._hooks import CLIPlugin , AsyncCLIPlugin from ._main import main","title":"pipen.cli"},{"location":"api/source/pipen.cli.plugins/","text":"SOURCE CODE pipen.cli. plugins DOCS \"\"\"List plugins\"\"\" from __future__ import annotations from typing import TYPE_CHECKING , Any , Iterable , List , Tuple from rich import print from ._hooks import CLIPlugin from ..defaults import ( CLI_ENTRY_GROUP , SCHEDULER_ENTRY_GROUP , TEMPLATE_ENTRY_GROUP , ) from ..utils import load_entrypoints if TYPE_CHECKING : from argx import ArgumentParser from argparse import Namespace COMMAND = \"plugins\" GROUPS = [ \"pipen\" , SCHEDULER_ENTRY_GROUP , TEMPLATE_ENTRY_GROUP , CLI_ENTRY_GROUP , ] GROUP_NAMES = { \"pipen\" : \"Pipen\" , SCHEDULER_ENTRY_GROUP : \"Scheduler\" , TEMPLATE_ENTRY_GROUP : \"Template\" , CLI_ENTRY_GROUP : \"CLI\" , } __all__ = ( \"CliPluginsPlugin\" ,) def _get_plugins_by_group ( group : str ) -> Iterable [ Tuple [ str , Any ]]: \"\"\"Get plugins from entry points by group name Args: group: The name of the group Returns: A list of tuples with the plugin name and the plugin itself \"\"\" for name , obj in load_entrypoints ( group ): yield name , obj def _list_group_plugins ( group : str , plugins : List [ Tuple [ str , Any ]], ) -> None : \"\"\"List plugins in a single group Args: group: The group of the plugins plugins: A list of tuples with name and plugin \"\"\" print ( \"\" ) print ( f \"[bold][u] { GROUP_NAMES [ group ] } plugins:[/u][/bold]\" ) namelen = max ( len ( name ) for name , _ in plugins ) if plugins else 0 for name , plugin in plugins : try : ver = plugin . version except AttributeError : try : ver = plugin . __version__ except AttributeError : ver = \"unknown\" print ( f \"- { name . ljust ( namelen ) } : (version: { ver } )\" ) def _list_plugins ( plugins : List [ Tuple [ str , str , Any ]]) -> None : \"\"\"List plugins Args: plugins: A list of tuples with group, name and plugin \"\"\" pipen_plugins = [ ( name , plugin ) for group , name , plugin in plugins if group == \"pipen\" ] sched_plugins = [ ( name , plugin ) for group , name , plugin in plugins if group == SCHEDULER_ENTRY_GROUP ] tpl_plugins = [ ( name , plugin ) for group , name , plugin in plugins if group == TEMPLATE_ENTRY_GROUP ] cli_plugins = [ ( name , plugin ) for group , name , plugin in plugins if group == CLI_ENTRY_GROUP ] _list_group_plugins ( \"pipen\" , pipen_plugins ) _list_group_plugins ( SCHEDULER_ENTRY_GROUP , sched_plugins ) _list_group_plugins ( TEMPLATE_ENTRY_GROUP , tpl_plugins ) _list_group_plugins ( CLI_ENTRY_GROUP , cli_plugins ) class CliPluginsPlugin ( CLIPlugin ): DOCS \"\"\"List installed plugins\"\"\" name = \"plugins\" def __init__ ( self , parser : ArgumentParser , subparser : ArgumentParser , ) -> None : super () . __init__ ( parser , subparser ) subparser . add_argument ( \"-g\" , \"--group\" , choices = GROUPS + [ \"all\" ], default = \"all\" , help = \"The name of the entry point group. Show all if not provided\" , ) def exec_command ( self , args : Namespace ) -> None : DOCS \"\"\"Execute the command\"\"\" from ..version import __version__ print ( \"Pipen version:\" , __version__ ) plugins : List [ Tuple [ str , str , Any ]] = [] if args . group and args . group != \"all\" : for name , plugin in _get_plugins_by_group ( args . group ): plugins . append (( args . group , name , plugin )) else : # args.name for group in GROUPS : for name , plugin in _get_plugins_by_group ( group ): plugins . append (( group , name , plugin )) _list_plugins ( plugins )","title":"pipen.cli.plugins"},{"location":"api/source/pipen.cli.profile/","text":"SOURCE CODE pipen.cli. profile DOCS \"\"\"List available profiles.\"\"\" from __future__ import annotations from typing import TYPE_CHECKING from rich import print from rich.panel import Panel from rich.syntax import Syntax from simpleconf import ProfileConfig from ._hooks import AsyncCLIPlugin from ..defaults import CONFIG , CONFIG_FILES if TYPE_CHECKING : from argx import ArgumentParser from argparse import Namespace __all__ = ( \"CLIProfilePlugin\" ,) class CLIProfilePlugin ( AsyncCLIPlugin ): DOCS \"\"\"List available profiles.\"\"\" name = \"profile\" def __init__ ( self , parser : ArgumentParser , subparser : ArgumentParser , ) -> None : super () . __init__ ( parser , subparser ) subparser . add_argument ( \"-n\" , \"--name\" , default = \"\" , help = \"The name of the profile to show. Show all if not provided.\" , ) subparser . add_argument ( \"-l\" , \"--list\" , action = \"store_true\" , default = False , help = \"List the names of all available profiles (-n won't work).\" , ) async def exec_command ( self , args : Namespace ) -> None : DOCS \"\"\"Run the command\"\"\" config = await ProfileConfig . a_load ( { \"default\" : CONFIG }, * CONFIG_FILES , ignore_nonexist = True , ) if args . list : print ( \" \\n \" . join ( ProfileConfig . profiles ( config ))) return print ( \"Configurations loaded from:\" ) print ( \"- pipen.defaults.CONFIG (python dictionary)\" ) for conffile in reversed ( CONFIG_FILES ): print ( f \"- { conffile } \" ) print ( \"\" ) print ( \"Note:\" ) print ( \"- The same profile from different configuration files \" \"are inherited.\" ) print ( \"- These configurations can still be overriden by \" \"Pipen constructor and process definition.\" ) print ( \"\" ) if not args . name : for profile in ProfileConfig . profiles ( config ): with ProfileConfig . with_profile ( config , profile ): conf = ProfileConfig . detach ( config ) print ( Panel ( Syntax ( conf . to_toml (), \"toml\" ), title = f \"Profile: { profile } \" , title_align = \"left\" , ) ) else : if not ProfileConfig . has_profile ( config , args . name ): raise ValueError ( f \"No such profile: { args . name } \" ) ProfileConfig . use_profile ( config , args . name ) conf = ProfileConfig . detach ( config ) print ( Panel ( Syntax ( conf . to_toml (), \"toml\" ), title = f \"Profile: { args . name } \" , title_align = \"left\" , ) )","title":"pipen.cli.profile"},{"location":"api/source/pipen.cli.version/","text":"SOURCE CODE pipen.cli. version DOCS \"\"\"Print help for commands\"\"\" from __future__ import annotations from typing import TYPE_CHECKING from rich import print from ._hooks import CLIPlugin if TYPE_CHECKING : from argparse import Namespace __all__ = ( \"CLIVersionPlugin\" ,) class CLIVersionPlugin ( CLIPlugin ): DOCS \"\"\"Print versions of pipen and its dependencies\"\"\" name = \"version\" def exec_command ( self , args : Namespace ) -> None : DOCS \"\"\"Run the command\"\"\" import sys from importlib.metadata import version from .. import __version__ versions = { \"python\" : sys . version , \"pipen\" : __version__ } for pkg in ( \"liquidpy\" , \"pandas\" , \"enlighten\" , \"argx\" , \"xqute\" , \"python-simpleconf\" , \"pipda\" , \"varname\" , ): versions [ pkg ] = version ( pkg ) keylen = max ( map ( len , versions )) for key in versions : ver = versions [ key ] verlines = ver . splitlines () print ( f \" { key . ljust ( keylen ) } : { verlines . pop ( 0 ) } \" ) for verline in verlines : # pragma: no cover print ( f \" { ' ' * keylen } { verline } \" )","title":"pipen.cli.version"},{"location":"api/source/pipen.defaults/","text":"SOURCE CODE pipen. defaults DOCS \"\"\"Provide some default values/objects\"\"\" from pathlib import Path from typing import ClassVar from diot import Diot from xqute import JobErrorStrategy from xqute.utils import logger as xqute_logger # Remove the rich handler _xqute_handlers = xqute_logger . handlers if _xqute_handlers : # The very first handler is the rich handler xqute_logger . removeHandler ( _xqute_handlers [ 0 ]) LOGGER_NAME = \"core\" CONFIG_FILES = ( Path ( \"~/.pipen.toml\" ) . expanduser (), \"./.pipen.toml\" , \"PIPEN.osenv\" , ) CONFIG = Diot ( # pipeline level: The logging level loglevel = \"info\" , # process level: The cache option, True/False/export cache = True , # process level: Whether expand directory to check signature dirsig = 1 , # process level: # How to deal with the errors # retry, ignore, halt # halt to halt the whole pipeline, no submitting new jobs # terminate to just terminate the job itself error_strategy = JobErrorStrategy . IGNORE , # process level: # How many times to retry to jobs once error occurs num_retries = 3 , # process level: # The directory to export the output files forks = 1 , # process level: Default shell/language lang = \"bash\" , # process level: # How many jobs to be submitted in a batch # Use the default value from the scheduler itself if None submission_batch = None , # pipeline level: # The working directory for the pipeline workdir = \"./.pipen\" , # process level: template engine template = \"liquid\" , # process level: template options template_opts = {}, # process level: scheduler scheduler = \"local\" , # process level: scheduler options scheduler_opts = {}, # pipeline level: plugins plugins = None , # pipeline level: plugin opts plugin_opts = {}, ) # Just the total width of the terminal # when logging with a rich.Panel() CONSOLE_WIDTH_WITH_PANEL = 100 # The width of the terminal when the width cannot be detected, # we are probably logging into a file CONSOLE_DEFAULT_WIDTH = 2048 # [05/16/22 11:46:40] I # v0.3.4: # 05-16 11:11:11 I # The markup code is included # Don't modify this unless the logger formatter is changed CONSOLE_WIDTH_SHIFT = 25 # For pipen scheduler plugins SCHEDULER_ENTRY_GROUP = \"pipen_sched\" # For pipen template plugins TEMPLATE_ENTRY_GROUP = \"pipen_tpl\" # For pipen template cli plugins CLI_ENTRY_GROUP = \"pipen_cli\" class ProcInputType : DOCS \"\"\"Types for process inputs\"\"\" VAR : ClassVar [ str ] = \"var\" FILE : ClassVar [ str ] = \"file\" DIR : ClassVar [ str ] = \"dir\" FILES : ClassVar [ str ] = \"files\" DIRS : ClassVar [ str ] = \"dirs\" class ProcOutputType : DOCS \"\"\"Types for process outputs\"\"\" VAR : ClassVar [ str ] = \"var\" DIR : ClassVar [ str ] = \"dir\" FILE : ClassVar [ str ] = \"file\"","title":"pipen.defaults"},{"location":"api/source/pipen.exceptions/","text":"SOURCE CODE pipen. exceptions DOCS \"\"\"Provide exception classes\"\"\" class PipenException ( Exception ): DOCS \"\"\"Base exception class for pipen\"\"\" class PipenSetDataError ( PipenException , ValueError ): DOCS \"\"\"When trying to set input data to processes with input_data already set using Pipen.set_data().\"\"\" class ProcInputTypeError ( PipenException , TypeError ): DOCS \"\"\"When an unsupported input type is provided\"\"\" class ProcInputKeyError ( PipenException , KeyError ): DOCS \"\"\"When an unsupported input key is provided\"\"\" class ProcInputValueError ( PipenException , ValueError ): DOCS \"\"\"When an unsupported input value is provided\"\"\" class ProcScriptFileNotFound ( PipenException , FileNotFoundError ): DOCS \"\"\"When script file specified as 'file://' cannot be found\"\"\" class ProcOutputNameError ( PipenException , NameError ): DOCS \"\"\"When no name or malformatted output is provided\"\"\" class ProcOutputTypeError ( PipenException , TypeError ): DOCS \"\"\"When an unsupported output type is provided\"\"\" class ProcOutputValueError ( PipenException , ValueError ): DOCS \"\"\"When a malformatted output value is provided\"\"\" class ProcDependencyError ( PipenException ): DOCS \"\"\"When there is something wrong the process dependencies\"\"\" class NoSuchSchedulerError ( PipenException ): DOCS \"\"\"When specified scheduler cannot be found\"\"\" class WrongSchedulerTypeError ( PipenException , TypeError ): DOCS \"\"\"When specified scheduler is not a subclass of Scheduler\"\"\" class NoSuchTemplateEngineError ( PipenException ): DOCS \"\"\"When specified template engine cannot be found\"\"\" class WrongTemplateEngineTypeError ( PipenException , TypeError ): DOCS \"\"\"When specified tempalte engine is not a subclass of Scheduler\"\"\" class TemplateRenderingError ( PipenException ): DOCS \"\"\"Failed to render a template\"\"\" class ConfigurationError ( PipenException ): DOCS \"\"\"When something wrong set as configuration\"\"\" class PipenOrProcNameError ( PipenException ): DOCS \"\"\" \"When more than one processes are sharing the same workdir\"\"\"","title":"pipen.exceptions"},{"location":"api/source/pipen.job/","text":"SOURCE CODE pipen. job DOCS \"\"\"Provide the Job class\"\"\" from __future__ import annotations import logging import shlex from contextlib import suppress from collections.abc import Iterable from functools import cached_property from pathlib import Path from typing import TYPE_CHECKING , Any , Dict , Mapping from panpath import PanPath , CloudPath , LocalPath from diot import OrderedDiot from xqute import Job as XquteJob from xqute.path import SpecPath , MountedPath from ._job_caching import JobCaching from .defaults import ProcInputType , ProcOutputType from .exceptions import ( ProcInputTypeError , ProcOutputNameError , ProcOutputTypeError , ProcOutputValueError , TemplateRenderingError , ) from .template import Template from .utils import logger , strsplit , path_is_symlink , path_symlink_to from .pluginmgr import plugin if TYPE_CHECKING : # pragma: no cover from .proc import Proc def _process_input_file_or_dir ( inkey : str , intype : str , inval : Any , index : int | None = None , proc_name : str | None = None , ) -> CloudPath | MountedPath : \"\"\"Process the input value for file or dir\"\"\" if inval is None or not isinstance ( inval , ( str , Path )): msg = ( f \"[ { proc_name } ] Got < { type ( inval ) . __name__ } > instead of \" f \"path-like object for input: { inkey + ':' + intype !r} \" ) if index is not None : msg = f \" { msg } at index { index } \" raise ProcInputTypeError ( msg ) if isinstance ( inval , MountedPath ): return inval if isinstance ( inval , SpecPath ): return inval . mounted if isinstance ( inval , CloudPath ): # pragma: no cover return MountedPath ( inval ) if not isinstance ( inval , str ): # other path-like types, should be all local return MountedPath ( PanPath ( inval ) . expanduser () . absolute ()) # str # Let's see if it a path in str format, which is path1:path2 # However, there is also a colon in cloud paths colon_count = inval . count ( \":\" ) if colon_count == 0 : # a/b return MountedPath ( PanPath ( inval ) . expanduser () . absolute ()) if colon_count > 3 : # a:b:c:d msg = ( f \"[ { proc_name } ] Invalid input value: { inkey + ':' + intype !r} \" \"(too many ':')\" ) if index is not None : msg = f \" { msg } at index { index } \" raise ProcInputTypeError ( msg ) if colon_count == 1 : # gs://a/b or a/b:c/d if isinstance ( PanPath ( inval ), CloudPath ): # gs://a/b return MountedPath ( inval ) path1 , path2 = inval . split ( \":\" ) elif inval . count ( \":\" ) == 3 : # gs://a/b:gs://c/d p1 , p2 , path2 = inval . split ( \":\" , 2 ) path1 = p1 + \":\" + p2 else : # gs://a/b:c/d or a/b:gs://c/d p1 , p2 , p3 = inval . split ( \":\" , 2 ) path1 , path2 = p1 + \":\" + p2 , p3 if not isinstance ( PanPath ( path1 ), CloudPath ): path1 , path2 = p1 , p2 + \":\" + p3 path1 = PanPath ( path1 ) # type: ignore path2 = PanPath ( path2 ) # type: ignore if isinstance ( path1 , LocalPath ): path1 = path1 . expanduser () . absolute () if isinstance ( path2 , LocalPath ): path2 = path2 . expanduser () . absolute () return MountedPath ( path2 , spec = path1 ) class Job ( XquteJob , JobCaching ): DOCS \"\"\"The job for pipen\"\"\" __slots__ = XquteJob . __slots__ + ( \"proc\" , \"_output_types\" , \"outdir\" , \"output\" ) def __init__ ( self , * args : Any , ** kwargs : Any , ) -> None : super () . __init__ ( * args , ** kwargs ) self . proc : Proc = None self . _output_types : Dict [ str , str ] = {} # Where the real output directory is self . outdir : SpecPath = None self . output : Mapping [ str , Any ] = None async def prepare ( self , proc : Proc ) -> None : DOCS \"\"\"Prepare the job by given process Primarily prepare the script, and provide cmd to the job for xqute to wrap and run Args: proc: the process object \"\"\" # Attach the process self . proc = proc # Where the jobs of \"export\" process should put their outputs export_outdir = proc . pipeline . outdir / proc . name # type: ignore # Where the jobs of \"export\" process should put their outputs # (in the mounted filesystem) sched_mounted_outdir = getattr ( proc . xqute . scheduler , \"MOUNTED_OUTDIR\" , None ) if sched_mounted_outdir is not None : # pragma: no cover if ( isinstance ( proc . pipeline . outdir , SpecPath ) and proc . pipeline . outdir . mounted . is_mounted () ): raise ValueError ( \"The pipeline outdir is a SpecPath, \" \"but the MOUNTED_OUTDIR is provided by the scheduler \" f \"< { proc . xqute . scheduler . __class__ . __name__ } >. \" ) mounted_outdir = PanPath ( sched_mounted_outdir ) / proc . name elif isinstance ( proc . pipeline . outdir , SpecPath ): # pragma: no cover # In the case it is modified by a plugin # A dual path can not be specified as outdir of a pipeline mounted_outdir = proc . pipeline . outdir . mounted / proc . name # type: ignore else : mounted_outdir = None if self . proc . export : # Don't put index if it is a single-job process self . outdir = SpecPath ( export_outdir , mounted = mounted_outdir , ) # Put job output in a subdirectory with index # if it is a multi-job process if self . proc . size > 1 : self . outdir = self . outdir / str ( self . index ) # type: ignore if sched_mounted_outdir is None : # Create the output directory if it is not mounted by the scheduler await self . outdir . mounted . a_mkdir ( parents = True , exist_ok = True ) else : # For non-export process, the output directory is the metadir self . outdir = self . metadir / \"output\" # compute the output await self . prepare_output () await self . prepare_outdir () if not proc . script : self . cmd = ( \"true\" ,) else : try : script = proc . script . render ( self . template_data ) except Exception as exc : raise TemplateRenderingError ( f \"[ { self . proc . name } ] Failed to render script.\" ) from exc if ( await self . script_file . a_is_file () and await self . script_file . a_read_text () != script ): self . log ( \"debug\" , \"Job script updated.\" ) await self . script_file . a_write_text ( script ) elif not await self . script_file . a_is_file (): await self . script_file . a_write_text ( script ) lang = proc . lang or proc . pipeline . config . lang script_file = self . script_file . mounted script_file = await script_file . get_fspath () self . cmd = tuple (( * shlex . split ( lang ), script_file )) # self.cmd = tuple(shlex.split(lang) + [self.script_file.mounted.fspath]) await plugin . hooks . on_job_init ( self ) @property DOCS def script_file ( self ) -> SpecPath : \"\"\"Get the path to script file Returns: The path to the script file \"\"\" return self . metadir / \"job.script\" async def prepare_outdir ( self ) -> SpecPath : DOCS \"\"\"Get the path to the output directory. When proc.export is True, the output directory is based on the pipeline.outdir and the process name. Otherwise, it is based on the metadir. When the job is running in a detached system (a VM, typically), this will return the mounted path to the output directory. To access the real path, use self.outdir Returns: The path to the job output directory \"\"\" # if ret is a dead link # when switching a proc from end/nonend to nonend/end # if path_is_symlink(self.outdir) and not self.outdir.exists(): if await path_is_symlink ( self . outdir ) and ( # type: ignore # A local deak link not await self . outdir . a_exists () # A cloud fake link or isinstance ( getattr ( self . outdir , \"path\" , self . outdir ), CloudPath ) ): await self . outdir . a_unlink () # pragma: no cover await self . outdir . a_mkdir ( parents = True , exist_ok = True ) # If it is somewhere else, make a symbolic link to the metadir metaout = self . metadir / \"output\" if self . outdir != metaout : if await path_is_symlink ( metaout ) or await metaout . a_is_file (): await metaout . a_unlink () elif await metaout . a_is_dir (): # Remove the directory, it is inconsistent with current setting await metaout . a_rmtree () await path_symlink_to ( metaout , self . outdir ) # type: ignore return self . outdir @cached_property def input ( self ) -> Mapping [ str , Any ]: \"\"\"Get the input data for this job Returns: A key-value map, where keys are the input keys \"\"\" import pandas ret = self . proc . input . data . iloc [ self . index , :] . to_dict () # check types for inkey , intype in self . proc . input . type . items (): if intype == ProcInputType . VAR or ret [ inkey ] is None : continue # pragma: no cover, covered actually if intype in ( ProcInputType . FILE , ProcInputType . DIR ): ret [ inkey ] = _process_input_file_or_dir ( inkey , intype , ret [ inkey ], None , self . proc . name ) if intype in ( ProcInputType . FILES , ProcInputType . DIRS ): if isinstance ( ret [ inkey ], pandas . DataFrame ): # pragma: no cover # // todo: nested dataframe ret [ inkey ] = ret [ inkey ] . iloc [ 0 , 0 ] if isinstance ( ret [ inkey ], ( str , Path )): # if a single file, convert to list ret [ inkey ] = [ ret [ inkey ]] if not isinstance ( ret [ inkey ], Iterable ): raise ProcInputTypeError ( f \"[ { self . proc . name } ] Expected an iterable for input: \" f \" { inkey + ':' + intype !r} , got { type ( ret [ inkey ]) } \" ) for i , file in enumerate ( ret [ inkey ]): ret [ inkey ][ i ] = _process_input_file_or_dir ( inkey , intype , file , i , self . proc . name ) return ret async def prepare_output ( self ) -> None : DOCS \"\"\"Get the output data of the job Returns: The key-value map where the keys are the output keys \"\"\" output_template = self . proc . output if not output_template : self . output = {} return data = { \"job\" : dict ( index = self . index , metadir = self . metadir . mounted , outdir = self . outdir . mounted , stdout_file = self . stdout_file . mounted , stderr_file = self . stderr_file . mounted , jid_file = self . jid_file . mounted , ), \"in\" : self . input , \"in_\" : self . input , \"proc\" : self . proc , \"envs\" : self . proc . envs , } try : if isinstance ( output_template , Template ): # // TODO: check ',' in output value? outputs = strsplit ( output_template . render ( data ), \",\" ) else : outputs = [ oput . render ( data ) for oput in output_template ] except Exception as exc : raise TemplateRenderingError ( f \"[ { self . proc . name } ] Failed to render output.\" ) from exc self . output = ret = OrderedDiot () for oput in outputs : if \":\" not in oput : raise ProcOutputNameError ( f \"[ { self . proc . name } ] No name given in output.\" ) if oput . count ( \":\" ) == 1 : output_name , output_value = oput . split ( \":\" ) output_type = ProcOutputType . VAR else : output_name , output_type , output_value = oput . split ( \":\" , 2 ) if output_type not in ProcOutputType . __dict__ . values (): raise ProcOutputTypeError ( f \"[ { self . proc . name } ] \" f \"Unsupported output type: { output_type } \" ) self . _output_types [ output_name ] = output_type if output_type == ProcOutputType . VAR : ret [ output_name ] = output_value else : ov = PanPath ( output_value ) if isinstance ( ov , CloudPath ) or ( isinstance ( ov , LocalPath ) and ov . is_absolute () ): raise ProcOutputValueError ( f \"[ { self . proc . name } ] \" f \"output path must be a segment: { output_value } \" ) # self.outdir is already awaited out = self . outdir / output_value # type: ignore if output_type == ProcOutputType . DIR : with suppress ( Exception ): # pragma: no cover # Likely aiohttp.ClientResponseError # In case we have many jobs and this is running in parallel # Parallelly creating the same directory may cause # a rate limit error in cloud storages await out . a_mkdir ( parents = True , exist_ok = True ) ret [ output_name ] = out . mounted @cached_property def template_data ( self ) -> Mapping [ str , Any ]: \"\"\"Get the data for template rendering Returns: The data for template rendering \"\"\" return { \"job\" : dict ( index = self . index , metadir = self . metadir . mounted , outdir = self . outdir . mounted , script_file = self . script_file . mounted , stdout_file = self . stdout_file . mounted , stderr_file = self . stderr_file . mounted , jid_file = self . jid_file . mounted , ), \"in\" : self . input , \"in_\" : self . input , \"out\" : self . output , \"proc\" : self . proc , \"envs\" : self . proc . envs , } def log ( DOCS self , level : int | str , msg : str , * args , limit : int = 3 , limit_indicator : bool = True , logger : logging . LoggerAdapter = logger , ) -> None : \"\"\"Log message for the jobs Args: level: The log level of the record msg: The message to log *args: The arguments to format the message limit: limitation of the log (don't log for all jobs) limit_indicator: Whether to show an indicator saying the log has been limited (the level of the indicator will be DEBUG) logger: The logger used to log \"\"\" if self . index > limit : return if self . index == limit : if limit_indicator : msg = f \" { msg } (not showing similar logs)\" if self . proc . size == 1 : job_index_indicator = \"\" else : job_index_indicator = \"[ %s / %s ] \" % ( str ( self . index ) . zfill ( len ( str ( self . proc . size - 1 ))), self . proc . size - 1 , ) self . proc . log ( level , job_index_indicator + msg , * args , logger = logger )","title":"pipen.job"},{"location":"api/source/pipen/","text":"SOURCE CODE pipen DOCS \"\"\"A pipeline framework for python\"\"\" from .pipen import Pipen , run , async_run from .proc import Proc from .procgroup import ProcGroup # Use from pipen.channel import Channel instead of # from pipen import Channel # This slows down import # from .channel import Channel from .pluginmgr import plugin from .version import __version__","title":"pipen"},{"location":"api/source/pipen.pipen/","text":"SOURCE CODE pipen. pipen DOCS \"\"\"Main entry module, provide the Pipen class\"\"\" from __future__ import annotations import asyncio from pathlib import Path from typing import Any , ClassVar , Iterable , List , Sequence , Type from diot import Diot from rich import box from rich.panel import Panel from rich.text import Text from simpleconf import ProfileConfig from varname import varname , VarnameException from panpath import PanPath , LocalPath from .defaults import CONFIG , CONFIG_FILES from .exceptions import ( PipenOrProcNameError , ProcDependencyError , PipenSetDataError , ) from .pluginmgr import plugin from .proc import Proc from .progressbar import PipelinePBar from .utils import ( copy_dict , desc_from_docstring , get_logpanel_width , is_valid_name , log_rich_renderable , logger , pipen_banner , ) class Pipen : DOCS \"\"\"The Pipen class provides interface to assemble and run the pipeline Attributes: name: The name of the pipeline desc: The description of the pipeline outdir: The output directory of the results procs: The processes pbar: The progress bar starts: The start processes config: The configurations workdir: The workdir for the pipeline profile: The profile of the configurations to run the pipeline _kwargs: The extra configrations passed to overwrite the default ones PIPELINE_COUNT: How many pipelines are loaded SETUP: Whether the one-time setup hook is called Args: name: The name of the pipeline desc: The description of the pipeline outdir: The output directory of the results **kwargs: Other configurations \"\"\" PIPELINE_COUNT : ClassVar [ int ] = 0 SETUP : ClassVar [ bool ] = False name : str | None = None desc : str | None = None outdir : str | Path = None starts : Type [ Proc ] | List [ Type [ Proc ]] = [] data : Iterable | None = None # other configs def __init__ ( self , name : str | None = None , desc : str | None = None , outdir : str | Path = None , ** kwargs , ) -> None : \"\"\"Constructor\"\"\" self . procs : List [ Type [ Proc ]] = None self . pbar : PipelinePBar = None if name is not None : self . name = name elif self . __class__ . name is not None : self . name = self . __class__ . name else : try : self . name = varname () # type: ignore except VarnameException : if self . __class__ . PIPELINE_COUNT == 0 : self . name = self . __class__ . __name__ else : self . name = ( f \" { self . __class__ . __name__ } -\" f \" { self . __class__ . PIPELINE_COUNT } \" ) if not is_valid_name ( self . name ): raise PipenOrProcNameError ( rf \"Invalid pipeline name: { self . name } , expecting '^[\\w.-]$'\" ) self . desc = ( desc or self . __class__ . desc or desc_from_docstring ( self . __class__ , Pipen ) ) self . outdir = PanPath ( # type: ignore outdir or self . __class__ . outdir or f \"./ { self . name } -output\" ) if isinstance ( self . outdir , LocalPath ): self . outdir = self . outdir . absolute () self . workdir : str | Path | None = None self . profile : str = \"default\" self . starts : List [ Type [ Proc ]] = self . __class__ . starts if self . starts and not isinstance ( self . starts , ( tuple , list )): self . starts = [ self . starts ] self . config = Diot ( copy_dict ( CONFIG , 3 )) # We shouldn't update the config here, since we don't know # the profile yet self . _kwargs = { key : value for key , value in self . __class__ . __dict__ . items () if key in self . config } self . _kwargs . setdefault ( \"plugin_opts\" , {}) . update ( kwargs . pop ( \"plugin_opts\" , {})) self . _kwargs . setdefault ( \"template_opts\" , {}) . update ( kwargs . pop ( \"template_opts\" , {}) ) self . _kwargs . setdefault ( \"scheduler_opts\" , {}) . update ( kwargs . pop ( \"scheduler_opts\" , {}) ) self . _kwargs . update ( kwargs ) # Initialize the workdir, as workdir is created before _init() # But the config is updated in _init() # Here we hack it to have the workdir passed in. if \"workdir\" in kwargs : self . config . workdir = kwargs [ \"workdir\" ] if not self . __class__ . SETUP : # pragma: no cover # Load plugins from entrypotins at runtime to avoid # cyclic imports plugin . load_entrypoints () plugins = self . _kwargs . get ( \"plugins\" , None ) if plugins is None : plugins = self . config . plugins self . plugin_context = plugin . plugins_context ( plugins ) self . plugin_context . __enter__ () # make sure core plugin is enabled plugin . get_plugin ( \"core\" ) . enable () if not Pipen . SETUP : # pragma: no cover plugin . hooks . on_setup ( self ) Pipen . SETUP = True self . __class__ . PIPELINE_COUNT += 1 if self . __class__ . data is not None : self . set_data ( * self . __class__ . data ) def __init_subclass__ ( cls ) -> None : DOCS cls . PIPELINE_COUNT = 0 async def async_run ( self , profile : str = \"default\" ) -> bool : DOCS \"\"\"Run the processes one by one Args: profile: The default profile to use for the run Returns: True if the pipeline ends successfully else False \"\"\" self . profile = profile self . workdir = PanPath ( str ( self . config . workdir )) / self . name # type: ignore succeeded = True await self . _init () logger . setLevel ( self . config . loglevel . upper ()) log_rich_renderable ( pipen_banner (), \"magenta\" , logger . info ) try : self . build_proc_relationships () self . _log_pipeline_info () logger . info ( \"Initializing plugins ...\" ) await plugin . hooks . on_start ( self ) for proc in self . procs : self . pbar . update_proc_running () proc_obj = proc ( self ) await proc_obj . _init () if proc in self . starts and proc . input_data is None : # type: ignore proc_obj . log ( \"warning\" , \"This is a start process, but no 'input_data' specified.\" , ) # await proc_obj.init() await proc_obj . run () if proc_obj . succeeded : self . pbar . update_proc_done () else : self . pbar . update_proc_error () succeeded = False break proc_obj . gc () logger . info ( \"\" ) except Exception : raise else : await plugin . hooks . on_complete ( self , succeeded ) finally : self . plugin_context . __exit__ () if self . pbar : self . pbar . done () return succeeded def run ( DOCS self , profile : str = \"default\" , ) -> bool : \"\"\"Run the pipeline with the given profile This is just a sync wrapper for the async `async_run` function using `asyncio.run()` Args: profile: The default profile to use for the run Returns: True if the pipeline ends successfully else False \"\"\" return asyncio . run ( self . async_run ( profile )) def set_data ( self , * indata : Any ) -> Pipen : DOCS \"\"\"Set the input_data for start processes Args: *indata: The input data for the start processes The data will set for the processes in the order determined by `set_starts()`. If a process has input_data set, an error will be raised. To use that input_data, set None here in the corresponding position for the process Raises: ProcInputDataError: When trying to set input data to processes with input_data already set Returns: `self` to chain the operations \"\"\" for start , data in zip ( self . starts , indata ): # type: ignore if data is None : continue if start . input_data is not None : raise PipenSetDataError ( f \"`input_data` has already set for { start } . \" \"If you want to use it, set `None` at the position of \" \"this process for `Pipen.set_data()`.\" ) start . input_data = data return self def set_starts ( DOCS self , * procs : Type [ Proc ] | Sequence [ Type [ Proc ]], clear : bool = True , ): \"\"\"Set the starts Args: *procs: The processes to set as starts of the pipeline. clear: Wether to clear previous set starts Raises: ProcDependencyError: When processes set as starts repeatedly Returns: `self` to chain the operations \"\"\" if clear : self . starts = [] self . procs = None for proc in procs : if isinstance ( proc , ( list , tuple )): self . set_starts ( * proc , clear = False ) elif not isinstance ( proc , type ) or not issubclass ( proc , Proc ): raise ProcDependencyError ( f \" { proc !r} is not a subclass of 'pipen.Proc'.\" ) elif proc not in self . starts : # type: ignore self . starts . append ( proc ) # type: ignore else : raise ProcDependencyError ( f \" { proc } is already a start process.\" ) return self # In case people forget the \"s\" set_start = set_starts def _log_pipeline_info ( self ) -> None : \"\"\"Print the information of the pipeline\"\"\" logger . info ( \"\" ) # Pipeline line and description log_rich_renderable ( Panel ( self . desc or Text ( self . name . upper (), justify = \"center\" ), width = get_logpanel_width (), # padding=(0, 1), box = box . DOUBLE_EDGE , title = self . name . upper () if self . desc else None , ), \"magenta\" , logger . info , ) fmt = \"[bold][magenta] %-16s :[/magenta][/bold] %s \" enabled_plugins = ( \" {name} [cyan] {version} [/cyan]\" . format ( name = name , version = ( f \"v { plg . version } \" if plg . version else \"\" ), ) for name , plg in plugin . get_enabled_plugins () . items () if name != \"core\" ) for i , plug in enumerate ( enabled_plugins ): logger . info ( fmt , \"plugins\" if i == 0 else \"\" , plug ) logger . info ( fmt , \"# procs\" , len ( self . procs )) logger . info ( fmt , \"profile\" , self . profile ) logger . info ( fmt , \"outdir\" , self . outdir ) logger . info ( fmt , \"cache\" , self . config . cache ) logger . info ( fmt , \"dirsig\" , self . config . dirsig ) logger . info ( fmt , \"error_strategy\" , self . config . error_strategy ) logger . info ( fmt , \"forks\" , self . config . forks ) logger . info ( fmt , \"lang\" , self . config . lang ) logger . info ( fmt , \"loglevel\" , self . config . loglevel ) logger . info ( fmt , \"num_retries\" , self . config . num_retries ) logger . info ( fmt , \"scheduler\" , self . config . scheduler ) logger . info ( fmt , \"submission_batch\" , self . config . submission_batch ) logger . info ( fmt , \"template\" , self . config . template ) logger . info ( fmt , \"workdir\" , self . workdir ) for i , ( key , val ) in enumerate ( self . config . plugin_opts . items ()): logger . info ( fmt , \"plugin_opts\" if i == 0 else \"\" , f \" { key } = { val } \" ) for i , ( key , val ) in enumerate ( self . config . scheduler_opts . items ()): logger . info ( fmt , \"scheduler_opts\" if i == 0 else \"\" , f \" { key } = { val } \" ) for i , ( key , val ) in enumerate ( self . config . template_opts . items ()): logger . info ( fmt , \"template_opts\" if i == 0 else \"\" , f \" { key } = { val } \" ) async def _init ( self ) -> None : \"\"\"Compute the configurations for the pipeline based on the priorities Configurations (priority from low to high) 1. The default config in .defaults 2. The plugin_opts defined in plugins (via on_setup() hook) (see __init__()) 3. Configuration files 4. **kwargs from Pipen(..., **kwargs) 5. Those defined in each Proc class \"\"\" # Then load the configurations from config files config = await ProfileConfig . a_load ( { \"default\" : self . config }, * CONFIG_FILES , ignore_nonexist = True , ) self . config = ProfileConfig . use_profile ( config , self . profile , copy = True ) # configs from files and CONFIG are loaded # allow plugins to change the default configs await plugin . hooks . on_init ( self ) # Then load the extra configurations passed from __init__(**kwargs) # Make sure dict options get inherited self . config . template_opts . update ( self . _kwargs . pop ( \"template_opts\" , {})) self . config . scheduler_opts . update ( self . _kwargs . pop ( \"scheduler_opts\" , {})) self . config . plugin_opts . update ( self . _kwargs . pop ( \"plugin_opts\" , {})) self . config . update ( self . _kwargs ) if \"workdir\" in self . _kwargs : self . workdir = PanPath ( self . _kwargs [ \"workdir\" ]) / self . name # type: ignore await self . workdir . a_mkdir ( # type: ignore[union-attr] parents = True , exist_ok = True , ) def build_proc_relationships ( self ) -> None : DOCS \"\"\"Build the proc relationships for the pipeline\"\"\" if self . procs : return if not self . starts : raise ProcDependencyError ( \"No start processes specified. \" \"Did you forget to call `Pipen.set_starts()`?\" ) # build proc relationships # Allow starts to be set as a tuple self . procs = list ( self . starts ) # type: ignore nexts = set ( sum (( proc . nexts or [] for proc in self . procs ), [])) # type: ignore logger . debug ( \"\" ) logger . debug ( \"Building process relationships:\" ) logger . debug ( \"- Start processes: %s \" , self . procs ) while nexts : logger . debug ( \"- Next processes: %s \" , nexts ) # pick up one that can be added to procs for proc in sorted ( nexts , key = lambda prc : ( prc . order or 0 , prc . name )): if proc in self . procs : raise ProcDependencyError ( f \"Cyclic dependency: { proc . name } \" ) if proc . name in [ p . name for p in self . procs ]: raise PipenOrProcNameError ( f \"' { proc . name } ' is already used by another process.\" ) # Add proc to self.procs if all their requires # are added to self.procs # Then remove proc from nexts # If there are still procs in nexts # meaning some requires of those procs cannot run before # those procs. if not set ( proc . requires ) - set ( self . procs ): # type: ignore self . procs . append ( proc ) # type: ignore nexts . remove ( proc ) nexts |= set ( proc . nexts or ()) break else : if nexts : raise ProcDependencyError ( f \"No available next processes for { nexts } . \" \"Did you forget to start with their \" \"required processes?\" ) self . pbar = PipelinePBar ( len ( self . procs ), self . name . upper ()) async def async_run ( DOCS name : str , starts : Type [ Proc ] | List [ Type [ Proc ]], data : Iterable = None , * , desc : str = None , outdir : str | Path | None = None , profile : str = \"default\" , ** kwargs , ) -> bool : \"\"\"Shortcut to run a pipeline Args: name: The name of the pipeline starts: The start processes data: The input data for the start processes desc: The description of the pipeline outdir: The output directory of the results profile: The profile to use **kwargs: Other options pass to Pipen to create the pipeline Returns: True if the pipeline ends successfully else False \"\"\" pipeline = Pipen ( name = name , desc = desc , outdir = outdir , ** kwargs , ) pipeline . set_starts ( starts ) . set_data ( data ) return await pipeline . async_run ( profile ) def run ( DOCS name : str , starts : Type [ Proc ] | List [ Type [ Proc ]], data : Iterable = None , * , desc : str = None , outdir : str | Path | None = None , profile : str = \"default\" , ** kwargs , ) -> bool : \"\"\"Shortcut to run a pipeline Args: name: The name of the pipeline starts: The start processes data: The input data for the start processes desc: The description of the pipeline outdir: The output directory of the results profile: The profile to use **kwargs: Other options pass to Pipen to create the pipeline Returns: True if the pipeline ends successfully else False \"\"\" return asyncio . run ( async_run ( name , starts , data , desc = desc , outdir = outdir , profile = profile , ** kwargs , ) )","title":"pipen.pipen"},{"location":"api/source/pipen.pluginmgr/","text":"SOURCE CODE pipen. pluginmgr DOCS \"\"\"Define hooks specifications and provide plugin manager\"\"\" from __future__ import annotations from typing import TYPE_CHECKING from simplug import Simplug , SimplugResult from xqute import JobStatus , Scheduler from .defaults import ProcOutputType if TYPE_CHECKING : # pragma: no cover import signal from xqute import Xqute from .job import Job from .proc import Proc from .pipen import Pipen plugin = Simplug ( \"pipen\" ) @plugin . spec DOCS def on_setup ( pipen : Pipen ) -> None : \"\"\"Setup for plugins, primarily used for the plugins to setup some default configurations. This is only called once for all pipelines. Args: pipen: The Pipen object \"\"\" @plugin . spec DOCS async def on_init ( pipen : Pipen ) -> None : \"\"\"When the pipeline is initialized, and default configs are loaded Args: pipen: The Pipen object \"\"\" @plugin . spec DOCS async def on_start ( pipen : Pipen ) -> None : \"\"\"Right before the pipeline starts running. Process relationships are inferred. Args: pipen: The Pipen object \"\"\" @plugin . spec DOCS async def on_complete ( pipen : Pipen , succeeded : bool ): \"\"\"The the pipeline is completed. Args: pipen: The Pipen object succeeded: Whether the pipeline has successfully completed. \"\"\" @plugin . spec DOCS def on_proc_create ( proc : Proc ): \"\"\"Called Proc constructor when a process is created. Enables plugins to modify the default attributes of processes Args: proc: The Proc object \"\"\" @plugin . spec DOCS async def on_proc_input_computed ( proc : Proc ): \"\"\"Called after process input data is computed. Args: proc: The Proc object \"\"\" @plugin . spec DOCS async def on_proc_script_computed ( proc : Proc ): \"\"\"Called after process script is computed. The script is computed as a string that is about to compiled into a template. Args: proc: The Proc object \"\"\" @plugin . spec DOCS async def on_proc_start ( proc : Proc ): \"\"\"When a process is starting Args: proc: The process \"\"\" @plugin . spec ( result = SimplugResult . TRY_ALL_FIRST_AVAIL ) DOCS def on_proc_shutdown ( proc : Proc , sig : signal . Signals ) -> None : \"\"\"When pipeline is shutting down, by Ctrl-c for example. Return False to stop shutting down, but you have to shut it down by yourself, for example, `proc.xqute.task.cancel()` Only the first return value will be used. Args: pipen: The xqute object sig: The signal. `None` means a natural shutdown \"\"\" @plugin . spec DOCS async def on_proc_done ( proc : Proc , succeeded : bool | str ) -> None : \"\"\"When a process is done Args: proc: The process succeeded: Whether the process succeeded or not. 'cached' if all jobs are cached. \"\"\" @plugin . spec DOCS async def on_job_init ( job : Job ): \"\"\"When a job is initialized Args: job: The job \"\"\" @plugin . spec DOCS async def on_job_queued ( job : Job ): \"\"\"When a job is queued in xqute. Note it might not be queued yet in the scheduler system. Args: job: The job \"\"\" @plugin . spec ( result = SimplugResult . TRY_ALL_FIRST_AVAIL ) DOCS async def on_job_submitting ( job : Job ) -> bool : \"\"\"When a job is submitting. The first plugin (based on priority) have this hook return False will cancel the submission Args: job: The job Returns: False to cancel submission \"\"\" @plugin . spec DOCS async def on_job_submitted ( job : Job ): \"\"\"When a job is submitted in the scheduler system. Args: job: The job \"\"\" @plugin . spec DOCS async def on_job_started ( job : Job ): \"\"\"When a job starts to run in then scheduler system. Note that the job might not be running yet in the scheduler system. Args: job: The job \"\"\" @plugin . spec DOCS async def on_job_polling ( job : Job , counter : int ): \"\"\"When status of a job is being polled. Args: job: The job \"\"\" @plugin . spec ( result = SimplugResult . TRY_ALL_FIRST_AVAIL ) DOCS async def on_job_killing ( job : Job ) -> bool : \"\"\"When a job is being killed. The first plugin (based on priority) have this hook return False will cancel the killing Args: job: The job Returns: False to cancel killing \"\"\" @plugin . spec DOCS async def on_job_killed ( job : Job ): \"\"\"When a job is killed Args: job: The job \"\"\" @plugin . spec DOCS async def on_job_succeeded ( job : Job ): \"\"\"When a job completes successfully. Args: job: The job \"\"\" @plugin . spec DOCS async def on_job_cached ( job : Job ): \"\"\"When a job is cached. Args: job: The job \"\"\" @plugin . spec DOCS async def on_job_failed ( job : Job ): \"\"\"When a job is done but failed. Args: job: The job \"\"\" @plugin . spec ( result = SimplugResult . ALL_AVAILS ) DOCS def on_jobcmd_init ( job : Job ) -> str : \"\"\"When the job command wrapper script is initialized before the prescript is run This should return a piece of bash code to be inserted in the wrapped job script (template), which is a python template string, with the following variables available: `status` and `job`. `status` is the class `JobStatus` from `xqute.defaults.py` and `job` is the `Job` instance. For multiple plugins, the code will be inserted in the order of the plugin priority. The code will replace the `#![jobcmd_init]` placeholder in the wrapped job script. See also <https://github.com/pwwang/xqute/blob/master/xqute/defaults.py#L95> Args: job: The job object Returns: The bash code to be inserted \"\"\" @plugin . spec ( result = SimplugResult . ALL_AVAILS ) DOCS def on_jobcmd_prep ( job : Job ) -> str : \"\"\"When the job command right about to be run This should return a piece of bash code to be inserted in the wrapped job script (template), which is a python template string, with the following variables available: `status` and `job`. `status` is the class `JobStatus` from `xqute.defaults.py` and `job` is the `Job` instance. The bash variable `$cmd` is accessible in the context. It is also possible to modify the `cmd` variable. Just remember to assign the modified value to `cmd`. For multiple plugins, the code will be inserted in the order of the plugin priority. Keep in mind that the `$cmd` may be modified by other plugins. The code will replace the `#![jobcmd_prep]` placeholder in the wrapped job script. See also <https://github.com/pwwang/xqute/blob/master/xqute/defaults.py#L95> Args: job: The job object Returns: The bash code to be inserted \"\"\" @plugin . spec ( result = SimplugResult . ALL_AVAILS ) DOCS def on_jobcmd_end ( job : Job ) -> str : \"\"\"When the job command finishes and after the postscript is run This should return a piece of bash code to be inserted in the wrapped job script (template), which is a python template string, with the following variables available: `status` and `job`. `status` is the class `JobStatus` from `xqute.defaults.py` and `job` is the `Job` instance. The bash variable `$rc` is accessible in the context, which is the return code of the job command. For multiple plugins, the code will be inserted in the order of the plugin priority. The code will replace the `#![jobcmd_end]` placeholder in the wrapped job script. See also <https://github.com/pwwang/xqute/blob/master/xqute/defaults.py#L95> Args: job: The job object Returns: The bash code to be inserted \"\"\" class PipenMainPlugin : DOCS \"\"\"The builtin core plugin, used to update the progress bar and cache the job\"\"\" name = \"core\" # The priority is set to -1000 to make sure it is the first plugin # to be called priority = - 1000 @plugin . impl def on_proc_shutdown ( proc : Proc , sig : signal . Signals ): # type: ignore[misc] \"\"\"When a process is shutting down\"\"\" if sig : # pragma: no cover proc . log ( \"warning\" , \"Got signal %r , trying a graceful shutdown ...\" , sig . name , ) @plugin . impl async def on_job_init ( job : Job ): # type: ignore[misc] \"\"\"Update the progress bar when a job is submitted\"\"\" job . proc . pbar . update_job_inited () @plugin . impl async def on_job_queued ( job : Job ): # type: ignore[misc] \"\"\"Update the progress bar when a job is submitted\"\"\" job . proc . pbar . update_job_queued () @plugin . impl async def on_job_started ( job : Job ): # type: ignore[misc] \"\"\"Update the progress bar when a job starts to run\"\"\" job . proc . pbar . update_job_running () @plugin . impl async def on_job_submitted ( job : Job ): # type: ignore[misc] \"\"\"Update the progress bar when a job is submitted\"\"\" job . proc . pbar . update_job_submitted () @plugin . impl async def on_job_cached ( job : Job ): # type: ignore[misc] \"\"\"Update the progress bar when a job is cached\"\"\" job . proc . pbar . update_job_queued () job . proc . pbar . update_job_submitted () job . proc . pbar . update_job_running () job . proc . pbar . update_job_succeeded ( cached = True ) await job . set_status ( JobStatus . FINISHED ) @plugin . impl async def on_job_succeeded ( job : Job ): # type: ignore[misc] \"\"\"Cache the job and update the progress bar when a job is succeeded\"\"\" # now the returncode is 0, however, we need to check if output files # have been created or not, this makes sure job.cache not fail for outkey , outtype in job . _output_types . items (): if outtype == ProcOutputType . VAR : continue path = job . output [ outkey ] . spec output_exists = await path . a_exists () if outtype == ProcOutputType . DIR : is_empty = True async for _ in path . a_iterdir (): is_empty = False break output_exists = output_exists and ( not is_empty ) if not output_exists : await job . set_status ( JobStatus . FAILED ) job . proc . pbar . update_job_failed () try : # in case the stderr file in the cloud not being synced yet stderr = await job . stderr_file . a_read_text () except Exception : # pragma: no cover stderr = \"\" stderr = f \" { stderr } \\n\\n Output { outtype } { outkey !r} is not generated\" if outtype == ProcOutputType . DIR : stderr += \" or is empty.\" else : stderr += \".\" await job . stderr_file . a_write_text ( stderr ) break else : await job . cache () job . proc . pbar . update_job_succeeded () @plugin . impl async def on_job_failed ( job : Job ): # type: ignore[misc] \"\"\"Update the progress bar when a job is failed\"\"\" job . proc . pbar . update_job_failed () if job . _error_retry and job . trial_count < job . _num_retries : # pragma: no cover job . log ( \"debug\" , \"Retrying # %s \" , job . trial_count + 1 ) job . proc . pbar . update_job_retrying () @plugin . impl async def on_job_killed ( job : Job ): # type: ignore[misc] \"\"\"Update the status of a killed job\"\"\" # instead of FINISHED to force the whole pipeline to quit await job . set_status ( JobStatus . FAILED ) # pragma: no cover plugin . register ( PipenMainPlugin ) xqute_plugin = Simplug ( \"xqute\" ) class XqutePipenPlugin : DOCS \"\"\"The plugin for xqute working as proxy for pipen plugin hooks\"\"\" name = \"xqute.pipen\" @xqute_plugin . impl def on_shutdown ( xqute : Xqute , sig : signal . Signals ): # type: ignore[misc] \"\"\"When a process is shutting down\"\"\" return plugin . hooks . on_proc_shutdown ( xqute . proc , sig ) # @xqute_plugin.impl # async def on_job_init(scheduler: Scheduler, job: Job): # \"\"\"When a job is initialized\"\"\" # await plugin.hooks.on_job_init(job) @xqute_plugin . impl async def on_job_queued ( scheduler : Scheduler , job : Job ): # type: ignore[misc] \"\"\"When a job is queued\"\"\" await plugin . hooks . on_job_queued ( job ) @xqute_plugin . impl async def on_job_submitting ( scheduler : Scheduler , job : Job ): # type: ignore[misc] \"\"\"When a job is being submitted\"\"\" return await plugin . hooks . on_job_submitting ( job ) @xqute_plugin . impl async def on_job_submitted ( scheduler : Scheduler , job : Job ): # type: ignore[misc] \"\"\"When a job is submitted\"\"\" await plugin . hooks . on_job_submitted ( job ) @xqute_plugin . impl async def on_job_started ( scheduler : Scheduler , job : Job ): # type: ignore[misc] \"\"\"When a job starts to run\"\"\" await plugin . hooks . on_job_started ( job ) @xqute_plugin . impl async def on_job_polling ( # type: ignore[misc] scheduler : Scheduler , job : Job , counter : int , ): \"\"\"When a job starts to run\"\"\" await plugin . hooks . on_job_polling ( job , counter ) @xqute_plugin . impl async def on_job_killing ( scheduler : Scheduler , job : Job ): # type: ignore[misc] \"\"\"When a job is being killed\"\"\" return await plugin . hooks . on_job_killing ( job ) # pragma: no cover @xqute_plugin . impl async def on_job_killed ( scheduler : Scheduler , job : Job ): # type: ignore[misc] \"\"\"When a job is killed\"\"\" await plugin . hooks . on_job_killed ( job ) # pragma: no cover @xqute_plugin . impl async def on_job_succeeded ( scheduler : Scheduler , job : Job ): # type: ignore[misc] \"\"\"When a job is succeeded\"\"\" await plugin . hooks . on_job_succeeded ( job ) @xqute_plugin . impl async def on_job_failed ( scheduler : Scheduler , job : Job ): # type: ignore[misc] \"\"\"When a job is failed\"\"\" await plugin . hooks . on_job_failed ( job ) @xqute_plugin . impl def on_jobcmd_init ( scheduler : Scheduler , job : Job ): # type: ignore[misc] \"\"\"When the job command wrapper script is initialized\"\"\" codes = plugin . hooks . on_jobcmd_init ( job ) if not codes : return None return \" \\n\\n \" . join ( codes ) @xqute_plugin . impl def on_jobcmd_prep ( scheduler : Scheduler , job : Job ): # type: ignore[misc] \"\"\"When the job command is about to be run\"\"\" codes = plugin . hooks . on_jobcmd_prep ( job ) if not codes : return None return \" \\n\\n \" . join ( codes ) @xqute_plugin . impl def on_jobcmd_end ( scheduler : Scheduler , job : Job ): # type: ignore[misc] \"\"\"When the job command finishes\"\"\" codes = plugin . hooks . on_jobcmd_end ( job ) if not codes : return None return \" \\n\\n \" . join ( codes ) xqute_plugin . register ( XqutePipenPlugin )","title":"pipen.pluginmgr"},{"location":"api/source/pipen.proc/","text":"SOURCE CODE pipen. proc DOCS \"\"\"Provides the process class: Proc\"\"\" from __future__ import annotations import asyncio import inspect import logging from abc import ABC , ABCMeta from functools import cached_property from typing import ( Any , Dict , List , Mapping , Sequence , Type , TYPE_CHECKING , ) from diot import Diot from rich import box from rich.panel import Panel from varname import VarnameException , varname from panpath import PanPath from xqute import JobStatus , Xqute from .defaults import ProcInputType from .exceptions import ( ProcInputKeyError , ProcInputTypeError , ProcScriptFileNotFound , PipenOrProcNameError , ) from .pluginmgr import plugin from .scheduler import get_scheduler from .template import Template , get_template_engine from .utils import ( brief_list , copy_dict , desc_from_docstring , get_logpanel_width , ignore_firstline_dedent , is_subclass , is_valid_name , log_rich_renderable , logger , make_df_colnames_unique_inplace , strsplit , update_dict , get_shebang , get_base , ) if TYPE_CHECKING : # pragma: no cover from pathlib import Path from .pipen import Pipen from .scheduler import Scheduler from .progressbar import ProcPBar class ProcMeta ( ABCMeta ): DOCS \"\"\"Meta class for Proc\"\"\" _INSTANCES : Dict [ Type , Proc ] = {} def __repr__ ( cls ) -> str : DOCS \"\"\"Representation for the Proc subclasses\"\"\" return f \"<Proc: { cls . name } >\" def __setattr__ ( cls , name : str , value : Any ) -> None : if name == \"requires\" : value = cls . _compute_requires ( value ) return super () . __setattr__ ( name , value ) def __call__ ( cls , * args : Any , ** kwds : Any ) -> Proc : DOCS \"\"\"Make sure Proc subclasses are singletons Args: *args: and **kwds: Arguments for the constructor Returns: The Proc instance \"\"\" if cls not in cls . _INSTANCES : cls . _INSTANCES [ cls ] = super () . __call__ ( * args , ** kwds ) return cls . _INSTANCES [ cls ] class Proc ( ABC , metaclass = ProcMeta ): DOCS \"\"\"The abstract class for processes. It's an abstract class. You can't instantise a process using it directly. You have to subclass it. The subclass itself can be used as a process directly. Each subclass is a singleton, so to intantise a new process, each subclass an existing `Proc` subclass, or use `Proc.from_proc()`. Never use the constructor directly. The Proc is designed as a singleton class, and is instansiated internally. Attributes: name: The name of the process. Will use the class name by default. desc: The description of the process. Will use the summary from the docstring by default. envs: The arguments that are job-independent, useful for common options across jobs. envs_depth: How deep to update the envs when subclassed. cache: Should we detect whether the jobs are cached? dirsig: When checking the signature for caching, whether should we walk through the content of the directory? This is sometimes time-consuming if the directory is big. export: When True, the results will be exported to `<pipeline.outdir>` Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy: How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries: How many times to retry to jobs once error occurs template: Define the template engine to use. This could be either a template engine or a dict with key `engine` indicating the template engine and the rest the arguments passed to the constructor of the `pipen.template.Template` object. The template engine could be either the name of the engine, currently jinja2 and liquidpy are supported, or a subclass of `pipen.template.Template`. You can subclass `pipen.template.Template` to use your own template engine. forks: How many jobs to run simultaneously? input: The keys for the input channel input_data: The input data (will be computed for dependent processes) lang: The language for the script to run. Should be the path to the interpreter if `lang` is not in `$PATH`. order: The execution order for this process. The bigger the number is, the later the process will be executed. Default: 0. Note that the dependent processes will always be executed first. This doesn't work for start processes either, whose orders are determined by `Pipen.set_starts()` output: The output keys for the output channel (the data will be computed) plugin_opts: Options for process-level plugins requires: The dependency processes scheduler: The scheduler to run the jobs scheduler_opts: The options for the scheduler script: The script template for the process submission_batch: How many jobs to be submited simultaneously. The program entrance for some schedulers may take too much resources when submitting a job or checking the job status. So we may use a smaller number here to limit the simultaneous submissions. nexts: Computed from `requires` to build the process relationships output_data: The output data (to pass to the next processes) \"\"\" name : str = None desc : str = None envs : Mapping [ str , Any ] = None envs_depth : int = None cache : bool = None dirsig : bool = None export : bool = None error_strategy : str = None num_retries : int = None template : str | Type [ Template ] = None template_opts : Mapping [ str , Any ] = None forks : int = None input : str | Sequence [ str ] = None input_data : Any = None lang : str = None order : int = None output : str | Sequence [ str ] = None plugin_opts : Mapping [ str , Any ] = None requires : Type [ Proc ] | Sequence [ Type [ Proc ]] = None scheduler : str = None scheduler_opts : Mapping [ str , Any ] = None script : str = None submission_batch : int = None nexts : Sequence [ Type [ Proc ]] = None output_data : Any = None workdir : str | Path = None # metadata that marks the process # Can also be used for plugins # It's not inheirted __meta__ : Mapping [ str , Any ] = None @classmethod DOCS def from_proc ( cls , proc : Type [ Proc ], name : str = None , desc : str = None , envs : Mapping [ str , Any ] = None , envs_depth : int = None , cache : bool = None , export : bool = None , error_strategy : str = None , num_retries : int = None , forks : int = None , input_data : Any = None , order : int = None , plugin_opts : Mapping [ str , Any ] = None , requires : Sequence [ Type [ Proc ]] = None , scheduler : str = None , scheduler_opts : Mapping [ str , Any ] = None , submission_batch : int = None , ) -> Type [ Proc ]: \"\"\"Create a subclass of Proc using another Proc subclass or Proc itself Args: proc: The Proc subclass name: The new name of the process desc: The new description of the process envs: The arguments of the process, will overwrite parent one The items that are specified will be inherited envs_depth: How deep to update the envs when subclassed. cache: Whether we should check the cache for the jobs export: When True, the results will be exported to `<pipeline.outdir>` Defaults to None, meaning only end processes will export. You can set it to True/False to enable or disable exporting for processes error_strategy: How to deal with the errors - retry, ignore, halt - halt to halt the whole pipeline, no submitting new jobs - terminate to just terminate the job itself num_retries: How many times to retry to jobs once error occurs forks: New forks for the new process input_data: The input data for the process. Only when this process is a start process order: The order to execute the new process plugin_opts: The new plugin options, unspecified items will be inherited. requires: The required processes for the new process scheduler: The new shedular to run the new process scheduler_opts: The new scheduler options, unspecified items will be inherited. submission_batch: How many jobs to be submited simultaneously. The program entrance for some schedulers may take too much resources when submitting a job or checking the job status. So we may use a smaller number here to limit the simultaneous submissions. Returns: The new process class \"\"\" if not name : try : name = varname () # type: ignore except VarnameException as vexc : # pragma: no cover raise ValueError ( \"Process name cannot be detected from assignment, \" \"pass one explicitly to `Proc.from_proc(..., name=...)`\" ) from vexc kwargs : Dict [ str , Any ] = { \"name\" : name , \"export\" : export , \"input_data\" : input_data , \"requires\" : requires , \"nexts\" : None , \"output_data\" : None , } locs = locals () for key in ( \"desc\" , \"envs\" , \"envs_depth\" , \"cache\" , \"forks\" , \"order\" , \"plugin_opts\" , \"scheduler\" , \"scheduler_opts\" , \"error_strategy\" , \"num_retries\" , \"submission_batch\" , ): if locs [ key ] is not None : kwargs [ key ] = locs [ key ] kwargs [ \"__doc__\" ] = proc . __doc__ out = type ( name , ( proc ,), kwargs ) return out def __init_subclass__ ( cls ) -> None : DOCS \"\"\"Do the requirements inferring since we need them to build up the process relationship \"\"\" base = [ mro for mro in cls . __mro__ if issubclass ( mro , Proc ) and mro is not Proc and mro is not cls ] parent = base [ 0 ] if base else None # cls.requires = cls._compute_requires() # triggers cls.__setattr__() to compute requires cls . nexts = [] cls . requires = cls . requires if cls . name is None or ( parent and cls . name == parent . name ): cls . name = cls . __name__ if not is_valid_name ( cls . name ): raise PipenOrProcNameError ( f \" { cls . name } is not a valid process name, expecting \" r \"'^[\\w.-]+$'\" ) envs = update_dict ( parent . envs if parent else None , cls . envs , depth = 0 if not parent or parent . envs_depth is None else parent . envs_depth , ) # So values can be accessed like Proc.envs.a.b cls . envs = envs if isinstance ( envs , Diot ) else Diot ( envs or {}) cls . plugin_opts = update_dict ( parent . plugin_opts if parent else None , cls . plugin_opts , try_list = True , ) cls . scheduler_opts = update_dict ( parent . scheduler_opts if parent else {}, cls . scheduler_opts , try_list = True , ) if callable ( cls . input_data ): cls . input_data = staticmethod ( cls . input_data ) cls . __meta__ = { \"procgroup\" : None } def __init__ ( self , pipeline : Pipen = None ) -> None : \"\"\"Constructor This is called only at runtime. Args: pipeline: The Pipen object \"\"\" # instance properties self . pipeline = pipeline self . pbar : ProcPBar | None = None self . jobs : List [ Any ] = [] self . xqute : Xqute | None = None self . __class__ . workdir = ( PanPath ( self . pipeline . workdir ) / self . name # type: ignore ) # plugins can modify some default attributes plugin . hooks . on_proc_create ( self ) # Compute the properties # otherwise, the property can be accessed directly from class vars if self . desc is None : self . desc : str = desc_from_docstring ( self . __class__ , Proc ) if self . export is None : self . export = bool ( not self . nexts ) # log the basic information self . _log_info () # template self . template = get_template_engine ( self . template or self . pipeline . config . template ) template_opts = copy_dict ( self . pipeline . config . template_opts ) template_opts . update ( self . template_opts or {}) self . template_opts = template_opts plugin_opts = copy_dict ( self . pipeline . config . plugin_opts ) plugin_opts . update ( self . plugin_opts or {}) self . plugin_opts = plugin_opts # run them asynchronously later in Pipen.async_run() # self.script = self._compute_script() # type: ignore # self.workdir.mkdir(exist_ok=True) async def _init ( self ) -> None : \"\"\"Async init for the process\"\"\" # input self . input = self . _compute_input () # type: ignore # output self . output = self . _compute_output () # type: ignore await plugin . hooks . on_proc_input_computed ( self ) # scheduler self . scheduler : Type [ Scheduler ] = get_scheduler ( # type: ignore self . scheduler or self . pipeline . config . scheduler ) if self . submission_batch is None : self . submission_batch = self . pipeline . config . submission_batch self . script = await self . _compute_script () # type: ignore await self . workdir . a_mkdir ( # type: ignore[union-attr] parents = True , exist_ok = True , ) async def _prepare_jobs_in_batch ( self , indexes : Sequence [ int ]) -> List [ int ]: \"\"\"Prepare jobs in batch Args: indexes: The job indexes to prepare \"\"\" cached_jobs = [] for i in indexes : job = await self . xqute . scheduler . create_job ( i , \"\" ) self . jobs . append ( job ) await job . prepare ( self ) if await job . cached : cached_jobs . append ( job . index ) await plugin . hooks . on_job_cached ( job ) else : envs = { \"PIPEN_JOB_INDEX\" : job . index , \"PIPEN_JOB_METADIR_SPEC\" : str ( job . metadir ), \"PIPEN_JOB_OUTDIR_SPEC\" : str ( job . outdir ), \"PIPEN_JOB_METADIR\" : str ( job . metadir . mounted ), \"PIPEN_JOB_OUTDIR\" : str ( job . outdir . mounted ), } await self . xqute . feed ( job , envs = envs ) return cached_jobs async def run ( self ) -> None : DOCS \"\"\"Init all other properties and jobs\"\"\" scheduler_opts = copy_dict ( self . pipeline . config . scheduler_opts or {}, - 1 ) scheduler_opts = update_dict ( scheduler_opts , self . scheduler_opts or {}, try_list = True , ) self . xqute = Xqute ( self . scheduler , workdir = self . workdir , submission_batch = self . submission_batch , error_strategy = self . error_strategy or self . pipeline . config . error_strategy , num_retries = ( self . pipeline . config . num_retries if self . num_retries is None else self . num_retries ), forks = self . forks or self . pipeline . config . forks , jobname_prefix = self . name , scheduler_opts = scheduler_opts , ) self . submission_batch = self . xqute . scheduler . subm_batch await self . xqute . scheduler . post_init ( self ) # for the plugin hooks to access self . xqute . proc = self # init pbar self . pbar = self . pipeline . pbar . proc_bar ( self . input . data . shape [ 0 ], self . name ) await plugin . hooks . on_proc_start ( self ) await self . xqute . run_until_complete ( keep_feeding = True ) # split job preparation into batches (self.submission_batch) # for example, job indexes: 0-9 and submission_batch=3 # will be split into [0,3,6,9], [1,4,7], [2,5,8] job_indexes_batches : List [ List [ int ]] = [ [] for _ in range ( self . submission_batch ) ] for i in range ( self . input . data . shape [ 0 ]): job_indexes_batches [ i % self . submission_batch ] . append ( i ) cached_job_list = await asyncio . gather ( * ( self . _prepare_jobs_in_batch ( batch ) for batch in job_indexes_batches ) ) cached_jobs = [ i for sublist in cached_job_list for i in sublist ] if cached_jobs : self . log ( \"info\" , \"Cached jobs: %s \" , brief_list ( cached_jobs )) await self . xqute . stop_feeding () self . pbar . done () await plugin . hooks . on_proc_done ( self , ( False if not self . succeeded else \"cached\" if len ( cached_jobs ) == self . size else True ), ) def gc ( self ): DOCS \"\"\"GC process for the process to save memory after it's done\"\"\" import pandas # store the output data for the next processes self . __class__ . output_data = pandas . DataFrame ( ( job . output for job in sorted ( self . jobs , key = lambda j : j . index )) ) del self . xqute . jobs [:] self . xqute . jobs = [] del self . xqute self . xqute = None del self . jobs [:] self . jobs = [] del self . pbar self . pbar = None def log ( DOCS self , level : int | str , msg : str , * args , logger : logging . LoggerAdapter = logger , ) -> None : \"\"\"Log message for the process Args: level: The log level of the record msg: The message to log *args: The arguments to format the message logger: The logging logger \"\"\" msg = msg % args if not isinstance ( level , int ): level = logging . getLevelName ( level . upper ()) logger . log ( level , # type: ignore \"[cyan] %s :[/cyan] %s \" , self . name , msg , ) # properties @cached_property def size ( self ) -> int : \"\"\"The size of the process (# of jobs)\"\"\" return self . input . data . shape [ 0 ] @cached_property def succeeded ( self ) -> bool : \"\"\"Check if the process is succeeded (all jobs succeeded)\"\"\" # _status was updated by xqute return all ( job . _status == JobStatus . FINISHED for job in self . jobs ) # Private methods @classmethod def _compute_requires ( cls , requires : Type [ Proc ] | Sequence [ Type [ Proc ]] = None , ) -> Sequence [ Type [ Proc ]]: \"\"\"Compute the required processes and fill the nexts Args: requires: The required processes. If None, will use `cls.requires` Returns: None or sequence of Proc subclasses \"\"\" if requires is None : requires = cls . requires if requires is None : return requires if is_subclass ( requires , Proc ): requires = [ requires ] # type: ignore # if req is in cls.__bases__, then cls.nexts will be affected by # req.nexts my_nexts = None if cls . nexts is None else cls . nexts [:] for req in requires : # type: ignore if not req . nexts : req . nexts = [ cls ] else : req . nexts . append ( cls ) # type: ignore cls . nexts = my_nexts return requires # type: ignore def _compute_input ( self ) -> Mapping [ str , Mapping [ str , Any ]]: \"\"\"Calculate the input based on input and input data Returns: A dict with type and data \"\"\" import pandas from .channel import Channel # split input keys into keys and types input_keys = self . __class__ . input if input_keys and isinstance ( input_keys , str ): input_keys = strsplit ( input_keys , \",\" ) if not input_keys : raise ProcInputKeyError ( f \"[ { self . name } ] No input provided\" ) out = Diot ( type = {}, data = None ) for input_key_type in input_keys : if \":\" not in input_key_type : out . type [ input_key_type ] = ProcInputType . VAR continue input_key , input_type = strsplit ( input_key_type , \":\" , 1 ) if input_type not in ProcInputType . __dict__ . values (): raise ProcInputTypeError ( f \"[ { self . name } ] Unsupported input type: { input_type } \" ) out . type [ input_key ] = input_type # get the data if not self . requires and self . input_data is None : out . data = pandas . DataFrame ([[ None ] * len ( out . type )]) elif not self . requires : out . data = Channel . create ( self . input_data ) elif callable ( self . input_data ): idata_args = ( req . output_data for req in self . requires ) # type: ignore out_data = self . __class__ . input_data ( * idata_args ) if isinstance ( out_data , ( str , bytes )): out_data = [ out_data ] out . data = Channel . create ( out_data ) else : if self . input_data : self . log ( \"warning\" , \"Ignoring input data, this is not a start process.\" , ) out . data = pandas . concat ( ( req . output_data for req in self . requires ), # type: ignore axis = 1 , ) . ffill () make_df_colnames_unique_inplace ( out . data ) # try match the column names # if none matched, use the first columns # rest_cols = out.data.columns.difference(out.type, False) rest_cols = [ col for col in out . data . columns if col not in out . type ] len_rest_cols = len ( rest_cols ) # matched_cols = out.data.columns.intersection(out.type) matched_cols = [ col for col in out . data . columns if col in out . type ] needed_cols = [ col for col in out . type if col not in matched_cols ] len_needed_cols = len ( needed_cols ) if len_rest_cols > len_needed_cols : self . log ( \"warning\" , \"Wasted %s column(s) of input data.\" , len_rest_cols - len_needed_cols , ) elif len_rest_cols < len_needed_cols : self . log ( \"warning\" , \"No data column for input: %s , using None.\" , needed_cols [ len_rest_cols :], ) # Add None # Use loop to keep order for needed_col in needed_cols [ len_rest_cols :]: out . data . insert ( out . data . shape [ 1 ], needed_col , None ) len_needed_cols = len_rest_cols out . data = out . data . rename ( columns = dict ( zip ( rest_cols [: len_needed_cols ], needed_cols )) ) . loc [:, list ( out . type )] return out def _compute_output ( self ) -> Template | List [ Template ]: \"\"\"Compute the output for jobs to render\"\"\" output = self . __class__ . output if not output or isinstance ( self . output , Template ): return self . output # type: ignore[return-value] if isinstance ( output , ( list , tuple )): return [ self . template ( oput , ** self . template_opts ) # type: ignore[operator] for oput in output ] return self . template ( output , ** self . template_opts ) # type: ignore[operator] async def _compute_script ( self ) -> Template : \"\"\"Compute the script for jobs to render\"\"\" if not self . __class__ . script : self . log ( \"warning\" , \"No script specified.\" ) return None script = self . __class__ . script if script . startswith ( \"file://\" ): script_file = PanPath ( script ) if not script_file . is_absolute (): base = get_base ( self . __class__ , Proc , script , lambda klass : getattr ( klass , \"script\" , None ), ) script_file = PanPath ( inspect . getfile ( base )) . parent / script_file if not await script_file . a_is_file (): raise ProcScriptFileNotFound ( f \"No such script file: { script_file } \" ) script = await script_file . a_read_text () self . script = ignore_firstline_dedent ( script ) if not self . lang : self . lang = get_shebang ( self . script ) await plugin . hooks . on_proc_script_computed ( self ) return self . template ( self . script , ** self . template_opts ) # type: ignore def _log_info ( self ): \"\"\"Log some basic information of the process\"\"\" title = ( f \" { self . __meta__ [ 'procgroup' ] . name } / { self . name } \" if self . __meta__ [ \"procgroup\" ] else self . name ) panel = Panel ( self . desc or \"Undescribed\" , title = title , box = ( box . Box ( \"\u256d\u2550\u252c\u256e \\n \" \"\u2551 \u2551\u2551 \\n \" \"\u251c\u2550\u253c\u2524 \\n \" \"\u2551 \u2551\u2551 \\n \" \"\u251c\u2550\u253c\u2524 \\n \" \"\u251c\u2550\u253c\u2524 \\n \" \"\u2551 \u2551\u2551 \\n \" \"\u2570\u2550\u2534\u256f \\n \" ) if self . export else box . ROUNDED ), width = get_logpanel_width (), ) logger . info ( \"\" ) log_rich_renderable ( panel , \"cyan\" , logger . info ) self . log ( \"info\" , \"workdir: %r \" , str ( self . workdir )) self . log ( \"info\" , \"[yellow]<<<[/yellow] %s \" , [ proc . name for proc in self . requires ] if self . requires else \"[START]\" , ) self . log ( \"info\" , \"[yellow]>>>[/yellow] %s \" , [ proc . name for proc in self . nexts ] if self . nexts else \"[END]\" , )","title":"pipen.proc"},{"location":"api/source/pipen.procgroup/","text":"SOURCE CODE pipen. procgroup DOCS \"\"\"Process group that contains a set of processes. It can be easily used to create a pipeline that runs independently or integrated into a larger pipeline. Runs directly: >>> proc_group = ProcGroup(<options>) >>> proc_group.as_pipen(<pipeline options>).set_data(<data>).run() Integrated into a larger pipeline >>> proc_group = ProcGroup(<options>) >>> # proc could be a process within the larger pipeline >>> proc.requires = prog_group.<proc> To add a process to the proc group, use the `add_proc` method: >>> class MyProcGroup(ProcGroup): >>> ... >>> >>> proc_group = MyProcGroup(...) >>> @proc_group.add_proc >>> class MyProc(Proc): >>> ... Or add a process at runtime: >>> class MyProcGroup(ProcGroup): >>> ... >>> >>> @ProcGroup.add_proc >>> def my_proc(self): >>> class MyProc(Proc): >>> # You may use self.options here >>> ... >>> return MyProc >>> proc_group = MyProcGroup(...) \"\"\" from __future__ import annotations from pathlib import Path from functools import wraps , cached_property from typing import Any , Callable , Mapping , Type , List from abc import ABC , ABCMeta from diot import Diot from .pipen import Pipen from .proc import Proc class ProcGropuMeta ( ABCMeta ): DOCS \"\"\"Meta class for ProcGroup\"\"\" _INST = None def __call__ ( cls , * args , ** kwds ): DOCS \"\"\"Make sure Proc subclasses are singletons Args: *args: and **kwds: Arguments for the constructor Returns: The Proc instance \"\"\" if cls . _INST is None : cls . _INST = super () . __call__ ( * args , ** kwds ) return cls . _INST class ProcGroup ( ABC , metaclass = ProcGropuMeta ): DOCS \"\"\"A group of processes that can be run independently or integrated into a larger pipeline. \"\"\" name : str | None = None __meta__ : Mapping [ str , Any ] = {} DEFAULTS = Diot () PRESERVED = { \"opts\" , \"name\" , \"add_proc\" , \"as_pipen\" , \"procs\" , \"starts\" , \"DEFAULTS\" , \"PRESERVED\" , \"_INST\" , } def __init_subclass__ ( cls ) -> None : DOCS # Clear the meta cls . __meta__ = {} def __init__ ( self , ** opts ) -> None : self . opts = Diot ( self . __class__ . DEFAULTS or {}) | ( opts or {}) self . name = self . __class__ . name or self . __class__ . __name__ self . starts : List [ Type [ Proc ]] = [] self . procs = Diot () self . _load_runtime_procs () def _load_runtime_procs ( self ): \"\"\"Load all processes that are added at runtime\"\"\" # Load all processes if they are decorated by ProcGroup.add_proc for name , attr in self . __class__ . __dict__ . items (): if isinstance ( attr , cached_property ): getattr ( self , name ) elif isinstance ( attr , type ) and issubclass ( attr , Proc ): self . add_proc ( attr ) def add_proc ( DOCS self_or_method : ProcGroup | Callable [[ ProcGroup ], Type [ Proc ]], proc : Type [ Proc ] | None = None , ) -> Type [ Proc ] | cached_property : \"\"\"Add a process to the proc group It works either as a decorator to the process directly or as a decorator to a method that returns the process. Args: self_or_method: The proc group instance or a method that returns the process proc: The process class if `self_or_method` is the proc group Returns: The process class if `self_or_method` is the proc group, or a cached property that returns the process class \"\"\" if isinstance ( self_or_method , ProcGroup ): # Called as self.add_proc or pg.add_proc if proc is None : return self_or_method . add_proc # type: ignore if proc . name in self_or_method . __class__ . PRESERVED : raise ValueError ( f \"Process name ` { proc . name } ` is reserved for ProcGroup\" ) setattr ( self_or_method , proc . name , proc ) proc . __meta__ [ \"procgroup\" ] = self_or_method # type: ignore if not proc . requires and proc not in self_or_method . starts : self_or_method . starts . append ( proc ) self_or_method . procs [ proc . name ] = proc return proc @wraps ( self_or_method ) def wrapper ( self ): proc = self_or_method ( self ) if proc is None : return None if not isinstance ( proc , type ) or not issubclass ( proc , Proc ): raise ValueError ( f \"` { proc } ` is not a Proc subclass\" ) proc . __meta__ [ \"procgroup\" ] = self if not proc . requires and proc not in self . starts : self . starts . append ( proc ) self . procs [ proc . name ] = proc return proc return cached_property ( wrapper ) def as_pipen ( DOCS self , name : str | None = None , desc : str | None = None , outdir : str | Path | None = None , ** kwargs , ) -> Pipen : \"\"\"Convert the pipeline to a Pipen instance Args: name: The name of the pipeline desc: The description of the pipeline outdir: The output directory of the pipeline **kwargs: The keyword arguments to pass to Pipen Returns: The Pipen instance \"\"\" name = name or self . __class__ . __name__ if self . __doc__ : desc = desc or self . __doc__ . lstrip () . splitlines ()[ 0 ] pipe = Pipen ( name = name , desc = desc , outdir = outdir , ** kwargs ) pipe . set_start ( self . starts ) return pipe","title":"pipen.procgroup"},{"location":"api/source/pipen.progressbar/","text":"SOURCE CODE pipen. progressbar DOCS \"\"\"Provide the PipelinePBar and ProcPBar classes\"\"\" from __future__ import annotations from typing import TYPE_CHECKING from .utils import truncate_text if TYPE_CHECKING : # pragma: no cover import enlighten # [12/02/20 12:44:06] I core # pipeline: 100%| # | desc_len | PBAR_DESC_LEN = 23 class ProcPBar : DOCS \"\"\"The progress bar for processes\"\"\" __slots__ = ( \"counter\" , \"proc_size\" , \"bar_format\" , \"queued_counter\" , \"submitted_counter\" , \"running_counter\" , \"success_counter\" , \"failure_counter\" , ) def __init__ ( self , manager : enlighten . Manager , proc_size : int , proc_name : str ) -> None : \"\"\"Initialize the progress bar for a process\"\"\" if proc_size > 1 : self . bar_format = None bar_format = ( \" {desc}{desc_pad}{percentage:3.0f} %| {bar} | \" \"I:{count_0:< {len_total} d} \" \"Q:{count_1:< {len_total} d} \" \"S:{count_2:< {len_total} d} \" \"R:{count_3:< {len_total} d} \" \"D:{count_4:> {len_total} d}|{count_5:< {len_total} d} \" \"[ {rate:5.2f}{unit_pad}{unit} /s]\" ) else : self . bar_format = ( \"{{desc}}{{desc_pad}}{{percentage:3.0f}}%|{{bar}}| \" \" {:^9} [{{rate:5.2f}}{{unit_pad}}{{unit}}/s]\" ) bar_format = self . bar_format . format ( \"-----------\" ) self . counter : enlighten . Counter = manager . counter ( total = proc_size , color = \"grey\" , desc = proc_name , unit = \"jobs \" , leave = False , ) self . proc_size = proc_size self . queued_counter : enlighten . SubCounter = self . counter . add_subcounter ( \"lightblue\" ) self . submitted_counter : enlighten . SubCounter = self . counter . add_subcounter ( \"cyan\" ) self . running_counter : enlighten . SubCounter = self . counter . add_subcounter ( \"yellow\" ) self . success_counter : enlighten . SubCounter = self . counter . add_subcounter ( \"green\" ) self . failure_counter : enlighten . SubCounter = self . counter . add_subcounter ( \"red\" ) # defer setting the bar_format, in case self.counter is rendered too early # ValueError: Reserve field 'count_0' specified in format, # but no subcounters are configured self . counter . bar_format = bar_format def update_job_inited ( self ): DOCS \"\"\"Update the progress bar when a job is init'ed\"\"\" if self . bar_format : self . counter . bar_format = self . bar_format . format ( \"Init'ed\" ) self . counter . update () def update_job_queued ( self ): DOCS \"\"\"Update the progress bar when a job is queued\"\"\" if self . bar_format : self . counter . bar_format = self . bar_format . format ( \"Queued\" ) try : self . queued_counter . update_from ( self . counter ) except ValueError : # pragma: no cover pass def update_job_submitted ( self ): DOCS \"\"\"Update the progress bar when a job is init'ed\"\"\" if self . bar_format : self . counter . bar_format = self . bar_format . format ( \"Submitted\" ) try : self . submitted_counter . update_from ( self . queued_counter ) except ValueError : # pragma: no cover pass def update_job_retrying ( self ): # pragma: no cover DOCS \"\"\"Update the progress bar when a job is retrying\"\"\" if self . bar_format : self . counter . bar_format = self . bar_format . format ( \"Retrying\" ) self . failure_counter . update ( - 1 ) self . running_counter . update ( 0 , force = True ) self . submitted_counter . update ( 0 , force = True ) self . queued_counter . update ( 0 , force = True ) self . counter . update ( 1 , force = True ) def update_job_running ( self ): DOCS \"\"\"Update the progress bar when a job is running\"\"\" if self . bar_format : self . counter . bar_format = self . bar_format . format ( \"Running\" ) try : self . running_counter . update_from ( self . submitted_counter ) except ValueError : # pragma: no cover pass def update_job_succeeded ( self , cached : bool = False ): DOCS \"\"\"Update the progress bar when a job is succeeded\"\"\" if self . bar_format : self . counter . bar_format = self . bar_format . format ( \"Cached\" if cached else \"Succeeded\" ) try : self . success_counter . update_from ( self . running_counter ) except ValueError : # pragma: no cover pass def update_job_failed ( self ): DOCS \"\"\"Update the progress bar when a job is failed\"\"\" if self . bar_format : self . counter . bar_format = self . bar_format . format ( \"Failed\" ) try : self . failure_counter . update_from ( self . running_counter ) except ValueError : # pragma: no cover pass def done ( self ): DOCS \"\"\"The process is done\"\"\" try : self . counter . close () except : # noqa: E722 # pragma: no cover pass class PipelinePBar : DOCS \"\"\"Progress bar for the pipeline\"\"\" __slots__ = ( \"manager\" , \"running_counter\" , \"success_counter\" , \"failure_counter\" , \"desc_len\" , ) def __init__ ( self , n_procs : int , ppln_name : str ) -> None : \"\"\"Initialize progress bar for pipeline\"\"\" import enlighten desc_len = PBAR_DESC_LEN ppln_name = truncate_text ( ppln_name , desc_len ) self . manager = enlighten . get_manager () self . running_counter = self . manager . counter ( total = n_procs , color = \"yellow\" , desc = f \" { ppln_name : > { desc_len }} :\" , unit = \"procs\" , bar_format = ( \" {desc}{desc_pad}{percentage:3.0f} %| {bar} | \" f \" {{ count: {{ len_total }} d }} / { n_procs } \" \"[ {rate:5.2f}{unit_pad}{unit} /s]\" ), ) self . success_counter = self . running_counter . add_subcounter ( \"green\" ) self . failure_counter = self . running_counter . add_subcounter ( \"red\" ) self . desc_len = desc_len def proc_bar ( self , proc_size : int , proc_name : str ) -> ProcPBar : DOCS \"\"\"Get the progress bar for a process Args: proc_size: The size of the process proc_name: The name of the process Returns: The progress bar for the given process \"\"\" proc_name = truncate_text ( proc_name , self . desc_len ) proc_name = f \" { proc_name : > { self . desc_len }} :\" return ProcPBar ( self . manager , proc_size , proc_name ) def update_proc_running ( self ): DOCS \"\"\"Update the progress bar when a process is running\"\"\" self . running_counter . update () def update_proc_done ( self ): DOCS \"\"\"Update the progress bar when a process is done\"\"\" self . success_counter . update_from ( self . running_counter ) def update_proc_error ( self ): DOCS \"\"\"Update the progress bar when a process is errored\"\"\" self . failure_counter . update_from ( self . running_counter ) def done ( self ) -> None : DOCS \"\"\"When the pipeline is done\"\"\" try : self . running_counter . close () self . manager . stop () except : # noqa: E722 # pragma: no cover pass","title":"pipen.progressbar"},{"location":"api/source/pipen.scheduler/","text":"SOURCE CODE pipen. scheduler DOCS \"\"\"Provide builting schedulers\"\"\" from __future__ import annotations from typing import TYPE_CHECKING , Type from diot import Diot from panpath import GSPath from xqute import Scheduler from xqute.schedulers.local_scheduler import LocalScheduler as XquteLocalScheduler from xqute.schedulers.sge_scheduler import SgeScheduler as XquteSgeScheduler from xqute.schedulers.slurm_scheduler import SlurmScheduler as XquteSlurmScheduler from xqute.schedulers.ssh_scheduler import SshScheduler as XquteSshScheduler from xqute.schedulers.gbatch_scheduler import ( GbatchScheduler as XquteGbatchScheduler , DEFAULT_MOUNTED_ROOT , ) from xqute.schedulers.container_scheduler import ( ContainerScheduler as XquteContainerScheduler , ) from xqute.path import SpecPath from .defaults import SCHEDULER_ENTRY_GROUP from .exceptions import NoSuchSchedulerError , WrongSchedulerTypeError from .job import Job from .utils import is_subclass , load_entrypoints if TYPE_CHECKING : from .proc import Proc class SchedulerPostInit : DOCS \"\"\"Provides post init function for all schedulers\"\"\" job_class = Job MOUNTED_METADIR : str MOUNTED_OUTDIR : str async def post_init ( self , proc : Proc ) -> None : ... # noqa: E704 class LocalScheduler ( SchedulerPostInit , XquteLocalScheduler ): # type: ignore[misc] DOCS \"\"\"Local scheduler\"\"\" class SgeScheduler ( SchedulerPostInit , XquteSgeScheduler ): # type: ignore[misc] DOCS \"\"\"SGE scheduler\"\"\" class SlurmScheduler ( SchedulerPostInit , XquteSlurmScheduler ): # type: ignore[misc] DOCS \"\"\"Slurm scheduler\"\"\" class SshScheduler ( SchedulerPostInit , XquteSshScheduler ): # type: ignore[misc] DOCS \"\"\"SSH scheduler\"\"\" class GbatchScheduler ( SchedulerPostInit , XquteGbatchScheduler ): # type: ignore[misc] DOCS \"\"\"Google Cloud Batch scheduler Args: *args: Positional arguments for the base class project: Google Cloud project ID location: Google Cloud region or zone mount: GCS path to mount (e.g. gs://my-bucket:/mnt/my-bucket) You can pass a list of mounts. service_account: GCP service account email (e.g. test-account@example.com) network: GCP network (e.g. default-network) subnetwork: GCP subnetwork (e.g. regions/us-central1/subnetworks/default) no_external_ip_address: Whether to disable external IP address machine_type: GCP machine type (e.g. e2-standard-4) provisioning_model: GCP provisioning model (e.g. SPOT) image_uri: Container image URI (e.g. ubuntu-2004-lts) entrypoint: Container entrypoint (e.g. /bin/bash) commands: The command list to run in the container. There are three ways to specify the commands: 1. If no entrypoint is specified, the final command will be [commands, wrapped_script], where the entrypoint is the wrapper script interpreter that is determined by `JOBCMD_WRAPPER_LANG` (e.g. /bin/bash), commands is the list you provided, and wrapped_script is the path to the wrapped job script. 2. You can specify something like \"-c\", then the final command will be [\"-c\", \"wrapper_script_interpreter, wrapper_script\"] 3. You can use the placeholders `{lang}` and `{script}` in the commands list, where `{lang}` will be replaced with the interpreter (e.g. /bin/bash) and `{script}` will be replaced with the path to the wrapped job script. For example, you can specify [\"{lang} {script}\"] and the final command will be [\"wrapper_interpreter, wrapper_script\"] runnables: Additional runnables to run before or after the main job. Each runnable should be a dictionary that follows the [GCP Batch API specification](https://cloud.google.com/batch/docs/reference/rest/v1/projects.locations.jobs#runnable). You can also specify an \"order\" key in the dictionary to control the execution order of the runnables. Runnables with negative order will be executed before the main job, and those with non-negative order will be executed after the main job. The main job runnable will always be executed in the order it is defined in the list. **kwargs: Keyword arguments for the configuration of a job (e.g. taskGroups). See more details at <https://cloud.google.com/batch/docs/get-started>. \"\"\" # noqa: E501 MOUNTED_METADIR : str = f \" { DEFAULT_MOUNTED_ROOT } /pipen-pipeline/workdir\" MOUNTED_OUTDIR : str = f \" { DEFAULT_MOUNTED_ROOT } /pipen-pipeline/outdir\" async def post_init ( self , proc : Proc ): await super () . post_init ( proc ) # Check if pipeline outdir is a GSPath if not isinstance ( proc . pipeline . outdir , GSPath ): raise ValueError ( \"'gbatch' scheduler requires google cloud storage 'outdir'.\" ) mounted_workdir = f \" { self . MOUNTED_METADIR } / { proc . name } \" self . workdir = SpecPath ( self . workdir , # type: ignore mounted = mounted_workdir , ) # update the mounted metadir # instead of mounting the workdir of this specific proc, # we mount the parent dir (the pipeline workdir), because the procs # of the pipeline may share files (e.g. input files from output of other procs) self . config [ \"taskGroups\" ][ 0 ][ \"taskSpec\" ][ \"volumes\" ][ 0 ][ \"gcs\" ][ \"remotePath\" ] = ( \"/\" . join ( self . workdir . parent . parts [ 1 :]) ) # remove 'gs://' self . config [ \"taskGroups\" ][ 0 ][ \"taskSpec\" ][ \"volumes\" ][ 0 ][ \"mountPath\" ] = self . MOUNTED_METADIR # update the config to map the outdir to vm self . config [ \"taskGroups\" ][ 0 ][ \"taskSpec\" ][ \"volumes\" ] . append ( Diot ( { \"gcs\" : { \"remotePath\" : \"/\" . join ( proc . pipeline . outdir . parts [ 1 :])}, \"mountPath\" : self . MOUNTED_OUTDIR , } ) ) # add labels self . config [ \"labels\" ][ \"pipeline\" ] = proc . pipeline . name . lower () self . config [ \"labels\" ][ \"proc\" ] = proc . name . lower () class ContainerScheduler ( # type: ignore[misc] DOCS SchedulerPostInit , XquteContainerScheduler , ): \"\"\"Scheduler to run jobs via containers (Docker/Podman/Apptainer)\"\"\" MOUNTED_METADIR : str = f \" { DEFAULT_MOUNTED_ROOT } /pipen-pipeline/workdir\" MOUNTED_OUTDIR : str = f \" { DEFAULT_MOUNTED_ROOT } /pipen-pipeline/outdir\" async def post_init ( self , proc : Proc ): await super () . post_init ( proc ) mounted_workdir = f \" { self . MOUNTED_METADIR } / { proc . name } \" self . workdir = SpecPath ( str ( self . workdir ), # ignore the mounted_workdir by xqute mounted = mounted_workdir , ) self . volumes [ - 1 ] = f \" { self . workdir } : { self . workdir . mounted } \" # type: ignore await proc . pipeline . outdir . a_mkdir ( parents = True , exist_ok = True ) # type: ignore self . volumes . append ( f \" { proc . pipeline . outdir } : { self . MOUNTED_OUTDIR } \" ) def get_scheduler ( scheduler : str | Type [ Scheduler ]) -> Type [ Scheduler ]: DOCS \"\"\"Get the scheduler by name of the scheduler class itself Args: scheduler: The scheduler class or name Returns: The scheduler class \"\"\" if is_subclass ( scheduler , Scheduler ): return scheduler # type: ignore if scheduler == \"local\" : return LocalScheduler if scheduler == \"sge\" : return SgeScheduler if scheduler == \"slurm\" : return SlurmScheduler if scheduler == \"ssh\" : return SshScheduler if scheduler == \"gbatch\" : return GbatchScheduler if scheduler == \"container\" : return ContainerScheduler for n , obj in load_entrypoints ( SCHEDULER_ENTRY_GROUP ): # pragma: no cover if n == scheduler : if not is_subclass ( obj , Scheduler ): raise WrongSchedulerTypeError ( \"Scheduler should be a subclass of \" \"pipen.scheduler.Scheduler.\" ) return obj raise NoSuchSchedulerError ( str ( scheduler ))","title":"pipen.scheduler"},{"location":"api/source/pipen.template/","text":"SOURCE CODE pipen. template DOCS \"\"\"Template adaptor for pipen\"\"\" from __future__ import annotations from abc import ABC , abstractmethod from typing import Any , Mapping , Type from liquid import Liquid from .defaults import TEMPLATE_ENTRY_GROUP from .exceptions import NoSuchTemplateEngineError , WrongTemplateEngineTypeError from .utils import is_subclass , load_entrypoints __all__ = [ \"Template\" , \"TemplateLiquid\" , \"TemplateJinja2\" , \"get_template_engine\" , ] class Template ( ABC ): DOCS \"\"\"Base class wrapper to wrap template for pipen\"\"\" __slots__ = ( \"engine\" ,) def __init__ ( self , source : Any , ** kwargs : Any , ): \"\"\"Template construct\"\"\" self . engine : Any = None def render ( self , data : Mapping [ str , Any ] = None ) -> str : DOCS \"\"\" Render the template @parmas: data (dict): The data used to render \"\"\" return self . _render ( data or {}) @abstractmethod def _render ( self , data : Mapping [ str , Any ]) -> str : \"\"\"Implement rendering\"\"\" class TemplateLiquid ( Template ): DOCS \"\"\"Liquidpy template wrapper.\"\"\" name = \"liquid\" def __init__ ( self , source : Any , ** kwargs : Any , ): \"\"\"Initiate the engine with source and envs Args: source: The souce text envs: The env data **kwargs: Other arguments for Liquid \"\"\" super () . __init__ ( source ) self . engine = Liquid ( source , from_file = False , mode = \"wild\" , ** kwargs , ) def _render ( self , data : Mapping [ str , Any ]) -> str : \"\"\"Render the template Args: data: The data used for rendering Returns The rendered string \"\"\" return self . engine . render ( data ) class TemplateJinja2 ( Template ): DOCS \"\"\"Jinja2 template wrapper\"\"\" name = \"jinja2\" def __init__ ( self , source : Any , ** kwargs : Any , ): \"\"\"Initiate the engine with source and envs Args: source: The souce text envs: The env data **kwargs: Other arguments for jinja2.Template \"\"\" import jinja2 super () . __init__ ( source ) filters = kwargs . pop ( \"filters\" , {}) envs = kwargs . pop ( \"globals\" , {}) filters = kwargs . pop ( \"filters\" , {}) self . engine = jinja2 . Template ( source , ** kwargs ) self . engine . globals . update ( envs ) self . engine . environment . filters . update ( filters ) def _render ( self , data : Mapping [ str , Any ]) -> str : \"\"\"Render the template Args: data: The data used for rendering Retuens: The rendered string \"\"\" return self . engine . render ( data ) def get_template_engine ( template : str | Type [ Template ]) -> Type [ Template ]: DOCS \"\"\"Get the template engine by name or the template engine itself Args: template: The name of the template engine or the template engine itself Returns: The template engine \"\"\" if is_subclass ( template , Template ): return template # type: ignore if template == \"liquid\" : return TemplateLiquid if template == \"jinja2\" : return TemplateJinja2 for name , obj in load_entrypoints ( TEMPLATE_ENTRY_GROUP ): # pragma: no cover if name == template : if not is_subclass ( obj , Template ): raise WrongTemplateEngineTypeError ( \"Template engine should be a subclass of \" \"pipen.templates.Template.\" ) return obj raise NoSuchTemplateEngineError ( str ( template ))","title":"pipen.template"},{"location":"api/source/pipen.utils/","text":"SOURCE CODE pipen. utils DOCS \"\"\"Provide some utilities\"\"\" from __future__ import annotations import re import sys import importlib import importlib.util import logging import textwrap import typing from copy import deepcopy from datetime import datetime from itertools import groupby from operator import itemgetter from io import StringIO from os import get_terminal_size , environ from collections import defaultdict from pathlib import Path from typing import ( TYPE_CHECKING , Any , Callable , DefaultDict , Iterable , List , Mapping , Sequence , Tuple , Type , ) import diot import simplug from panpath import PanPath , CloudPath , LocalPath from rich.console import Console from rich.logging import RichHandler as _RichHandler from rich.table import Table from rich.text import Text from .defaults import ( CONSOLE_DEFAULT_WIDTH , CONSOLE_WIDTH_WITH_PANEL , CONSOLE_WIDTH_SHIFT , LOGGER_NAME , ) from .version import __version__ from importlib import metadata as importlib_metadata if TYPE_CHECKING : # pragma: no cover import pandas from rich.segment import Segment from rich.console import RenderableType from .pipen import Pipen from .proc import Proc from .procgroup import ProcGroup LOADING_ARGV0 = \"@pipen\" class RichHandler ( _RichHandler ): DOCS \"\"\"Subclass of rich.logging.RichHandler, showing log levels as a single character\"\"\" def get_level_text ( self , record : logging . LogRecord ) -> Text : DOCS \"\"\"Get the level name from the record. Args: record: LogRecord instance. Returns: Text: A tuple of the style and level name. \"\"\" level_name = record . levelname level_text = Text . styled ( level_name [ 0 ] . upper (), f \"logging.level. { level_name . lower () } \" ) return level_text class RichConsole ( Console ): DOCS def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) try : self . _width = get_terminal_size () . columns except ( AttributeError , ValueError , OSError ): # maybe not a terminal if environ . get ( \"JUPYTER_COLUMNS\" ) is not None : # pragma: no cover self . _width = int ( environ . get ( \"JUPYTER_COLUMNS\" )) elif environ . get ( \"COLUMNS\" ) is not None : # pragma: no cover self . _width = int ( environ . get ( \"COLUMNS\" )) else : self . _width = CONSOLE_DEFAULT_WIDTH def _render_buffer ( self , buffer : Iterable [ Segment ]) -> str : out = super () . _render_buffer ( buffer ) return out . rstrip () + \" \\n \" logging . lastResort = logging . NullHandler () # type: ignore logger_console = RichConsole () _logger_handler = RichHandler ( show_path = False , show_level = True , console = logger_console , rich_tracebacks = True , omit_repeated_times = False , # rich 10+ markup = True , log_time_format = \"%m- %d %H:%M:%S\" , tracebacks_extra_lines = 0 , tracebacks_suppress = [ simplug , diot , typing ], ) _logger_handler . setFormatter ( logging . Formatter ( \"[purple] %(plugin_name)-7s [/purple] %(message)s \" ) ) def _excepthook ( type_ : Type [ BaseException ], value : BaseException , traceback : Any , ) -> None : \"\"\"The excepthook for pipen, to show rich traceback\"\"\" if issubclass ( type_ , KeyboardInterrupt ): # pragma: no cover logger . error ( \"\" ) logger . error ( \"Interrupted by user\" ) return print ( \"\" , file = sys . stderr ) _excepthook . oldhook ( type_ , value , traceback ) _excepthook . oldhook = sys . excepthook sys . excepthook = _excepthook def get_logger ( DOCS name : str = LOGGER_NAME , level : str | int = None , ) -> logging . LoggerAdapter : \"\"\"Get the logger by given plugin name Args: level: The initial level of the logger Returns: The logger \"\"\" log = logging . getLogger ( f \"pipen. { name } \" ) log . addHandler ( _logger_handler ) if level is not None : log . setLevel ( level . upper () if isinstance ( level , str ) else level ) return logging . LoggerAdapter ( log , { \"plugin_name\" : name }) logger = get_logger () def desc_from_docstring ( DOCS obj : Type [ Pipen | Proc ], base : Type [ Pipen | Proc ], ) -> str : \"\"\"Get the description from docstring Only extract the summary. Args: obj: The object with docstring Returns: The summary as desc \"\"\" if not obj . __doc__ : # If the docstring is empty, use the base's docstring # Get the base from mro bases = [ cls for cls in obj . __mro__ if is_subclass ( cls , base ) and cls != base and cls != obj ] if not bases : return None return desc_from_docstring ( bases [ 0 ], base ) started : bool = False out : List [ str ] = [] for line in obj . __doc__ . splitlines (): line = line . strip () if not started and not line : continue if not started : out . append ( line ) started = True elif line : out . append ( line ) else : break return \" \" . join ( out ) def update_dict ( DOCS parent : Mapping [ str , Any ], new : Mapping [ str , Any ], depth : int = 0 , try_list : bool = False , ) -> Mapping [ str , Any ]: \"\"\"Update the new dict to the parent, but make sure parent does not change Args: parent: The parent dictionary new: The new dictionary depth: The depth to be copied. 0 for updating to the deepest level. try_list: If True, try to also update the dict in the list Examples: >>> parent = {\"a\": {\"b\": 1}} >>> new = {\"a\": {\"c\": 2}} >>> update_dict(parent, new) >>> # {\"a\": {\"b\": 1, \"c\": 2}} >>> parent = {\"a\": [{\"b\": 1}]} >>> new = {\"a\": [{\"c\": 2}]} >>> update_dict(parent, new, try_list=True) >>> # {\"a\": [{\"b\": 1, \"c\": 2}]} Returns: The updated dictionary or None if both parent and new are None. \"\"\" if parent is None and new is None : return None out = ( parent or {}) . copy () for key , val in ( new or {}) . items (): if ( try_list and isinstance ( out . get ( key ), list ) and isinstance ( val , list ) and depth != 1 ): # If the value is a list, try to update the dict in the list for i , item in enumerate ( val ): if ( isinstance ( item , dict ) and i < len ( out [ key ]) and isinstance ( out [ key ][ i ], dict ) ): out [ key ][ i ] = update_dict ( out [ key ][ i ], item , depth - 1 , True ) elif i < len ( out [ key ]): out [ key ][ i ] = item else : out [ key ] . append ( item ) continue if ( key not in out or not isinstance ( val , dict ) or not isinstance ( out [ key ], dict ) or depth == 1 ): out [ key ] = val else : out [ key ] = update_dict ( out [ key ], val , depth - 1 ) return out def copy_dict ( dic : Mapping [ str , Any ], depth : int = 1 ) -> Mapping [ str , Any ]: DOCS \"\"\"Deep copy a dict Args: dic: The dict to be copied depth: The depth to be deep copied Returns: The deep-copied dict \"\"\" if depth <= 0 : return deepcopy ( dic ) if depth <= 1 : return dic . copy () return { key : copy_dict ( val , depth - 1 ) if isinstance ( val , dict ) else val for key , val in dic . items () } def strsplit ( DOCS string : str , sep : str , maxsplit : int = - 1 , trim : str = \"both\" , ) -> List [ str ]: \"\"\"Split the string, with the ability to trim each part.\"\"\" parts = string . split ( sep , maxsplit = maxsplit ) if trim is None : return parts if trim == \"left\" : return [ part . lstrip () for part in parts ] if trim == \"right\" : return [ part . rstrip () for part in parts ] return [ part . strip () for part in parts ] def get_shebang ( script : str ) -> str : DOCS \"\"\"Get the shebang of the script Args: script: The script string Returns: None if the script does not contain a shebang, otherwise the shebang without `#!` prefix \"\"\" script = script . lstrip () if not script . startswith ( \"#!\" ): return None if \" \\n \" not in script : return script [ 2 :] . strip () shebang_line , _ = strsplit ( script , \" \\n \" , 1 ) return shebang_line [ 2 :] . strip () def ignore_firstline_dedent ( text : str ) -> str : DOCS \"\"\"Like textwrap.dedent(), but ignore first empty lines Args: text: The text the be dedented Returns: The dedented text \"\"\" out = [] started = False for line in text . splitlines (): if not started and not line . strip (): continue if not started : started = True out . append ( line ) return textwrap . dedent ( \" \\n \" . join ( out )) def get_logpanel_width () -> int : DOCS \"\"\"Get the width of the log content Args: max_width: The maximum width to return Note that it's not the console width. With console width, you have to subtract the width of the log meta info (CONSOLE_WIDTH_SHIFT). Returns: The width of the log content \"\"\" return ( min ( logger_console . width , CONSOLE_WIDTH_WITH_PANEL , ) - CONSOLE_WIDTH_SHIFT ) def log_rich_renderable ( DOCS renderable : RenderableType , color : str | None , logfunc : Callable , * args : Any , ** kwargs : Any , ) -> None : \"\"\"Log a rich renderable to logger Args: renderable: The rich renderable splitline: Whether split the lines or log the entire message logfunc: The log function, if message is not the first argument, use functools.partial to wrap it *args: The arguments to the log function **kwargs: The keyword arguments to the log function \"\"\" console = Console ( file = StringIO (), width = logger_console . width - CONSOLE_WIDTH_SHIFT , ) console . print ( renderable ) for line in console . file . getvalue () . splitlines (): logfunc ( f \"[ { color } ] { line } [/ { color } ]\" if color else line , * args , ** kwargs , ) def brief_list ( blist : List [ int ]) -> str : DOCS \"\"\"Briefly show an integer list, combine the continuous numbers. Args: blist: The list Returns: The string to show for the briefed list. \"\"\" ret = [] blist . sort () for _ , g in groupby ( enumerate ( blist ), lambda x : x [ 0 ] - x [ 1 ]): list_group = list ( map ( itemgetter ( 1 ), g )) if len ( list_group ) > 1 : ret . append ( f \" { list_group [ 0 ] } - { list_group [ - 1 ] } \" ) else : ret . append ( str ( list_group [ 0 ])) return \", \" . join ( ret ) def pipen_banner () -> RenderableType : DOCS \"\"\"The banner for pipen Returns: The banner renderable \"\"\" table = Table ( width = get_logpanel_width (), show_header = False , show_edge = False , show_footer = False , show_lines = False , caption = f \"version: { __version__ } \" , ) table . add_column ( justify = \"center\" ) table . add_row ( r \" _____________________________________ __\" ) table . add_row ( r \" ___ __ \\___ _/__ __ \\__ ____/__ | / /\" ) table . add_row ( r \" __ /_/ /__ / __ /_/ /_ __/ __ |/ / \" ) table . add_row ( r \" _ ____/__/ / _ ____/_ /___ _ /| / \" ) table . add_row ( r \"/_/ /___/ /_/ /_____/ /_/ |_/ \" ) table . add_row ( \"\" ) return table async def get_mtime ( DOCS path : str | Path , dir_depth : int = 1 , ) -> float : \"\"\"Get the modification time of a path. If path is a directory, try to get the last modification time of the contents in the directory at given dir_depth Args: dir_depth: The depth of the directory to check the last modification time 0 means only check the path itself (don't go into directory or follow symlink) Returns: The last modification time of path \"\"\" path = getattr ( path , \"path\" , path ) mtime = 0.0 path : Path = PanPath ( path ) # type: ignore[assignment,abstract] if not await path . a_exists (): return mtime # If it is not any kind of symlink if not await path_is_symlink ( path ): # type: ignore[arg-type] if dir_depth == 0 or not await path . a_is_dir (): out = ( await path . a_stat ()) . st_mtime return out . timestamp () if isinstance ( out , datetime ) else out async for file in path . a_iterdir (): mtime = max ( mtime , await get_mtime ( file , dir_depth - 1 )) return mtime # It is a real symlink if isinstance ( path , LocalPath ) and await path . a_is_symlink (): if dir_depth == 0 or not await path . a_is_dir (): out = ( await path . a_stat ( follow_symlinks = False )) . st_mtime return out . timestamp () if isinstance ( out , datetime ) else out async for file in path . a_iterdir (): mtime = max ( mtime , await get_mtime ( file , dir_depth - 1 )) return mtime # Fake symlink dest = ( ( await path . a_read_text ()) . removeprefix ( \"symlink:\" ) . removeprefix ( \"pipen-symlink:\" ) ) dpath = PanPath ( dest ) if dir_depth == 0 or not await dpath . a_is_dir (): out = ( await path . a_stat ( follow_symlinks = False )) . st_mtime return out . timestamp () if isinstance ( out , datetime ) else out async for file in dpath . a_iterdir (): mtime = max ( mtime , await get_mtime ( file , dir_depth - 1 )) return mtime def is_subclass ( obj : Any , cls : type ) -> bool : DOCS \"\"\"Tell if obj is a subclass of cls Differences with issubclass is that we don't raise Type error if obj is not a class Args: obj: The object to check cls: The class to check Returns: True if obj is a subclass of cls otherwise False \"\"\" try : return issubclass ( obj , cls ) except TypeError : return False def load_entrypoints ( group : str ) -> Iterable [ Tuple [ str , Any ]]: # pragma: no cover DOCS \"\"\"Load objects from setuptools entrypoints by given group name Args: group: The group name of the entrypoints Returns: An iterable of tuples with name and the loaded object \"\"\" try : eps = importlib_metadata . entry_points ( group = group ) except TypeError : eps = importlib_metadata . entry_points () . get ( group , []) # type: ignore yield from (( ep . name , ep . load ()) for ep in eps ) def truncate_text ( text : str , width : int , end : str = \"\u2026\" ) -> str : DOCS \"\"\"Truncate a text not based on words/whitespaces Otherwise, we could use textwrap.shorten. Args: text: The text to be truncated width: The max width of the the truncated text end: The end string of the truncated text Returns: The truncated text with end appended. \"\"\" if len ( text ) <= width : return text return text [: ( width - len ( end ))] + end def make_df_colnames_unique_inplace ( thedf : pandas . DataFrame ) -> None : DOCS \"\"\"Make the columns of a data frame unique Args: thedf: The data frame \"\"\" col_counts : DefaultDict = defaultdict ( lambda : 0 ) new_cols = [] for col in thedf . columns : if col_counts [ col ] == 0 : new_cols . append ( col ) else : new_cols . append ( f \" { col } _ { col_counts [ col ] } \" ) col_counts [ col ] += 1 thedf . columns = new_cols def get_base ( DOCS klass : Type , abc_base : Type , value : Any , value_getter : Callable , ) -> Type : \"\"\"Get the base class where the value was first defined Args: klass: The class abc_base: The very base class to check in __bases__ value: The value to check value_getter: How to get the value from the class Returns: The base class \"\"\" bases = [ base for base in klass . __bases__ if issubclass ( base , abc_base ) and value_getter ( base ) == value ] if not bases : return klass return get_base ( bases [ 0 ], abc_base , value , value_getter ) def mark ( ** kwargs ) -> Callable [[ type ], type ]: DOCS \"\"\"Mark a class (e.g. Proc) with given kwargs as metadata These marks will not be inherited by the subclasses if the class is a subclass of `Proc` or `ProcGroup`. Args: **kwargs: The kwargs to mark the proc Returns: The decorator \"\"\" def decorator ( cls : type ) -> type : if not getattr ( cls , \"__meta__\" , None ): cls . __meta__ = {} cls . __meta__ . update ( kwargs ) return cls return decorator def get_marked ( cls : type , mark_name : str , default : Any = None ) -> Any : DOCS \"\"\"Get the marked value from a proc Args: cls: The proc mark_name: The mark name default: The default value if the mark is not found Returns: The marked value \"\"\" if not getattr ( cls , \"__meta__\" , None ): return default return cls . __meta__ . get ( mark_name , default ) def is_valid_name ( name : str ) -> bool : DOCS \"\"\"Check if a name is valid for a proc or pipen Args: name: The name to check Returns: True if valid, otherwise False \"\"\" return re . match ( r \"^[\\w.-]+$\" , name ) is not None def _get_obj_from_spec ( spec : str ) -> Any : \"\"\"Get the object from a spec like `<module[.submodule]>:name` or `/path/to/script.py:name` Args: spec: The spec Returns: The object Raises: AttributeError: If name cannot be found in the module \"\"\" modpath , sep , name = spec . rpartition ( \":\" ) if sep != \":\" : raise ValueError ( f \"Invalid specification: { spec } . \\n \" \"It must be in the format '<module[.submodule]>:name' or \\n \" \"'/path/to/spec.py:name'\" ) path = Path ( modpath ) if path . is_file (): mspec = importlib . util . spec_from_file_location ( path . stem , modpath ) module = importlib . util . module_from_spec ( mspec ) mspec . loader . exec_module ( module ) else : module = importlib . import_module ( modpath ) return getattr ( module , name ) async def load_pipeline ( DOCS obj : str | Type [ Proc ] | Type [ ProcGroup ] | Type [ Pipen ] | Pipen , argv0 : str | None = None , argv1p : Sequence [ str ] | None = None , ** kwargs : Any , ) -> Pipen : \"\"\"Load a pipeline from a Pipen, Proc or ProcGroup object It does not only load the Pipen object or convert the Proc/ProcGroup object to Pipen, but also build the process relationships. So that we can access `pipeline.procs` and `requires/nexts` of each proc. To avoid running the pipeline and notify the plugins that this is just for loading the pipeline, `sys.argv[0]` is set to `@pipen`. Args: obj: The Pipen, Proc or ProcGroup object. It can also be a string in the format of `part1:part2` to load the pipeline, where part1 is a path to a python file or package directory, and part2 is the name of the proc, procgroup or pipeline to load. It should be able to be loaded by `getattr(module, part2)`, where module is loaded from `part1`. argv0: The value to replace sys.argv[0]. \"@pipen\" will be used by default. argv1p: The values to replace sys.argv[1:]. Do not replace by default. kwargs: The kwargs to pass to the Pipen constructor Returns: The loaded Pipen object Raises: TypeError: If obj or loaded obj is not a Pipen, Proc or ProcGroup object \"\"\" from .pipen import Pipen from .proc import Proc from .procgroup import ProcGroup old_argv = sys . argv if argv0 is None : # Set it at runtime to allow LOADING_ARGV0 to be monkey-patched argv0 = LOADING_ARGV0 if argv1p is None : # Set it at runtime to adopt sys.argv changes argv1p = sys . argv [ 1 :] sys . argv = [ argv0 ] + list ( argv1p ) try : if isinstance ( obj , str ): obj = _get_obj_from_spec ( obj ) if isinstance ( obj , Pipen ) or ( isinstance ( obj , type ) and issubclass ( obj , ( Pipen , Proc , ProcGroup )) ): pass else : raise TypeError ( \"Expected a Pipen, Proc, ProcGroup class, or a Pipen object, \" f \"got { type ( obj ) } \" ) pipeline = obj if isinstance ( obj , type ) and issubclass ( obj , Proc ): kwargs . setdefault ( \"name\" , f \" { obj . name } Pipeline\" ) pipeline = Pipen ( ** kwargs ) . set_starts ( obj ) elif isinstance ( obj , type ) and issubclass ( obj , ProcGroup ): pipeline = obj () . as_pipen ( ** kwargs ) # type: ignore elif isinstance ( obj , type ) and issubclass ( obj , Pipen ): # Avoid \"pipeline\" to be used as pipeline name by varname ( pipeline ,) = ( obj ( ** kwargs ),) # type: ignore elif isinstance ( obj , Pipen ): pipeline . _kwargs . update ( kwargs ) # type: ignore # Initialize the pipeline so that the arguments definied by # other plugins (i.e. pipen-args) to take in place. pipeline . workdir = PanPath ( pipeline . config . workdir ) . joinpath ( # type: ignore kwargs . get ( \"name\" , pipeline . name ) ) await pipeline . _init () # type: ignore await pipeline . workdir . a_mkdir ( parents = True , exist_ok = True ) # type: ignore pipeline . build_proc_relationships () # type: ignore finally : sys . argv = old_argv return pipeline # type: ignore def is_loading_pipeline ( * flags : str , argv : Sequence [ str ] | None = None ) -> bool : DOCS \"\"\"Check if we are loading the pipeline. Works only when `argv0` is \"@pipen\" while loading the pipeline. Note if you are using this function at compile time, make sure you load your pipeline using the string form (`part1:part2`) See more with `load_pipline()`. Args: *flags: Additional flags to check in sys.argv (e.g. \"-h\", \"--help\") to determine if we are loading the pipeline argv: The arguments to check. sys.argv is used by default. Note that the first argument should be included in the check. You could typically pass `[sys.argv[0], *your_args]` to this if you want to check if `sys.argv[0]` is \"@pipen\" or `your_args` contains some flags. Returns: True if we are loading the pipeline (argv[0] == \"@pipen\"), otherwise False \"\"\" if argv is None : argv = sys . argv if len ( argv ) > 0 and argv [ 0 ] == LOADING_ARGV0 : return True if flags : return any ( flag in argv for flag in flags ) return False # pragma: no cover def path_is_symlink_sync ( path : PanPath ) -> bool : DOCS \"\"\"Check if a path is a symlink synchronously. We don't only check the real symlink, but also the fake symlink files created by `path_symlink_to()`. Args: path: The path to check Returns: True if the path is a symlink, otherwise False \"\"\" path = getattr ( path , \"path\" , path ) if isinstance ( path , Path ): if path . is_symlink (): return True if not path . exists (): return False # Check if the path is a fake symlink file # Get the size first, to avoid the large files being downloaded try : path_stat = path . stat () except Exception : # pragma: no cover return False if path_stat . st_size > 4096 or path_stat . st_size < 8 : return False try : with path . open ( \"rb\" ) as f : prefix = f . read ( 14 ) return prefix == b \"pipen-symlink:\" or prefix . startswith ( b \"symlink:\" ) except Exception : # pragma: no cover # If we cannot read the file, it is not a symlink return False async def path_is_symlink ( path : PanPath ) -> bool : DOCS \"\"\"Check if a path is a symlink. We don't only check the real symlink, but also the fake symlink files created by `path_symlink_to()`. Args: path: The path to check Returns: True if the path is a symlink, otherwise False \"\"\" path = getattr ( path , \"path\" , path ) if isinstance ( path , Path ): if await path . a_is_symlink (): return True if not await path . a_exists (): # type: ignore[union-attr] return False # Check if the path is a fake symlink file # Get the size first, to avoid the large files being downloaded try : path_stat = await path . a_stat () # type: ignore[union-attr] except Exception : # pragma: no cover return False if path_stat . st_size > 4096 or path_stat . st_size < 8 : return False try : async with path . a_open ( \"rb\" ) as f : # type: ignore[union-attr] prefix = await f . read ( 14 ) return prefix == b \"pipen-symlink:\" or prefix . startswith ( b \"symlink:\" ) except Exception : # If we cannot read the file, it is not a symlink return False async def path_symlink_to ( DOCS src : PanPath , dst : PanPath , target_is_directory : bool = False , ) -> None : \"\"\"Create a symbolic link pointing to src named dst. Args: src: The source path dst: The destination path target_is_directory: If True, the symbolic link will be to a directory. \"\"\" src = getattr ( src , \"path\" , src ) dst = getattr ( dst , \"path\" , dst ) if isinstance ( dst , CloudPath ) or isinstance ( src , CloudPath ): # Create a fake symlink file for cloud paths await src . a_write_text ( f \"pipen-symlink: { dst } \" ) else : await src . a_symlink_to ( dst , target_is_directory = target_is_directory )","title":"pipen.utils"},{"location":"api/source/pipen.version/","text":"SOURCE CODE pipen. version DOCS \"\"\"Provide version of pipen\"\"\" __version__ = \"1.1.8\"","title":"pipen.version"},{"location":"tutorials/beginner/","text":"Your First Pipeline \u00b6 Welcome to pipen! This tutorial will walk you through creating your first pipeline step by step step. What You'll Learn \u00b6 In this tutorial, you will: - Understand the basic components of a pipen pipeline - Create a simple two-process pipeline - Run the pipeline and inspect the results - Learn about channels, jobs, and caching Prerequisites \u00b6 Before starting, ensure you have: - Python 3.9 or higher installed - pipen installed: pip install -U pipen Understanding the Basics \u00b6 What is a Pipeline? \u00b6 A pipeline is a workflow that processes data through multiple steps. Each step (called a process ) takes input data, performs some operation, and produces output data. What is a Process? \u00b6 A process is a single step in your pipeline that: - Defines what input it needs - Specifies what output it produces - Contains the script to execute - Can depend on other processes What is a Job? \u00b6 When a pipeline runs, each row of input data becomes a job . If your input has 100 items, pipen creates 100 jobs that run independently. What is a Channel? \u00b6 A channel is how data flows between processes. It's like a spreadsheet where each row represents a job and each column represents a piece of data that job needs. Step 1: Create Your First Process \u00b6 Let's create a simple process that sorts lines in a file. Create a file called my_pipeline.py : from pipen import Proc class SortFile ( Proc ): \"\"\"Sort the contents of a file.\"\"\" # Define input: expects a file path input = \"infile:file\" # Provide sample data for testing input_data = \"/tmp/data.txt\" # Define output: a file called \"sorted.txt\" output = \"outfile:file:sorted.txt\" # The script to execute # {{ in.infile }} will be replaced with the input file path # {{ out.outfile }} will be replaced with the output file path script = \"\"\" cat {{ in.infile }} | sort > {{ out.outfile }} \"\"\" Explanation: class SortFile(Proc) : Define a process by inheriting from Proc input = \"infile\" : This process expects one piece of input data named \"infile\" input_data = \"/tmp/data.txt\" : Sample input data (a file path) output = \"outfile:file:sorted.txt\" : Output is a file named \"sorted.txt\" script = \"\"\"...\"\"\" : The bash script to execute with template variables Step 2: Create a Second Process \u00b6 Now let's add a second process that adds line numbers to our sorted file. Add to my_pipeline.py : class AddLineNumbers ( Proc ): \"\"\"Add line numbers to each line.\"\"\" # This process depends on SortFile requires = SortFile # Input is a file (output from SortFile) input = \"infile:file\" # Output is a file called \"numbered.txt\" output = \"outfile:file:numbered.txt\" # Script adds line numbers (1-3 in this case) script = \"\"\" paste <(seq 1 3) {{ in.infile }} > {{ out.outfile }} \"\"\" Explanation: requires = SortFile : This process runs after SortFile completes input = \"infile:file\" : The :file type tells pipen this is a file path The output from SortFile automatically becomes the input to AddLineNumbers Step 3: Define and Run the Pipeline \u00b6 Now let's put it all together and run our pipeline. Update my_pipeline.py : from pipen import Proc , run class SortFile ( Proc ): \"\"\"Sort the contents of a file.\"\"\" input = \"infile\" input_data = \"/tmp/data.txt\" output = \"outfile:file:sorted.txt\" script = \"cat {{ in.infile }} | sort > {{ out.outfile }}\" class AddLineNumbers ( Proc ): \"\"\"Add line numbers to each line.\"\"\" requires = SortFile input = \"infile:file\" output = \"outfile:file:numbered.txt\" script = \"paste <(seq 1 3) {{ in.infile }} > {{ out.outfile }}\" # Run the pipeline if __name__ == \"__main__\" : # Create test data import os os . makedirs ( \"/tmp\" , exist_ok = True ) with open ( \"/tmp/data.txt\" , \"w\" ) as f : f . write ( \"3 \\\\ n2 \\\\ n1\" ) run ( \"MyFirstPipeline\" , starts = SortFile ) Step 4: Run Your Pipeline \u00b6 Execute your pipeline: python my_pipeline.py You should see output similar to: 04-17 16:19:35 I core _____________________________________ __ 04-17 16:19:35 I core ___ __ \\___ ____ \\___ | | | | 04-17 16:19:35 I core |\\/ | |\\/ | | | | | | 04-17 16:19:35 I core 04-17 16:19:35 I core version: 1.1.7 04-17 16:19:35 I core 04-17 16:19:35 I core \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 MYFIRSTPIPELINE \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 04-17 16:19:35 I core \u2551 My first pipeline \u2551 04-17 16:19:35 I core \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d 04-17 16:19:35 I core # procs : 2 04-17 16:19:35 I core profile : default 04-17 16:19:35 I core outdir : /path/to/cwd/MyFirstPipeline-output 04-17 16:19:35 I core cache : True 04-17 16:19:35 I core dirsig : 1 04-17 16:19:35 I core error_strategy : ignore 04-17 16:19:35 I core forks : 1 04-17 16:19:35 I core lang : bash 04-17 16:19:35 I core loglevel : info 04-17 16:19:35 I core num_retries : 3 04-17 16:19:35 I core scheduler : local 04-17 16:19:35 I core submission_batch: 8 04-17 16:19:35 I core template : liquid 04-17 16:19:35 I core workdir : /path/to/cwd/.pipen/MyFirstPipeline 04-17 16:19:35 I core Initializing plugins ... 04-17 16:19:36 I core 04-17 16:19:36 I core \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 SortFile \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e 04-17 16:19:36 I core \u2502 Sort the contents of a file. \u2502 04-17 16:19:36 I core \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f 04-17 16:19:36 I core SortFile: Workdir: '/path/to/cwd/.pipen/MyFirstPipeline/SortFile' 04-17 16:19:36 I core SortFile: <<< [START] 04-17 16:19:36 I core SortFile: >>> ['AddLineNumbers'] 04-17 16:19:36 I core SortFile: >>> [END] 04-17 16:19:36 I core 04-17 16:19:36 I core \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 AddLineNumbers \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e 04-17 16:19:36 I core \u2551 Add line numbers to each line. \u2551 04-17 16:19:36 I core \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d 04-17 16:19:36 I core AddLineNumbers: Workdir: '/path/to/cwd/.pipen/MyFirstPipeline/AddLineNumbers' 04-17 16:19:36 I core AddLineNumbers: <<< ['SortFile'] 04-17 16:19:36 I core AddLineNumbers: >>> [END] MYFIRSTPIPELINE: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 0.35 procs/s] What happened: Pipen initialized the pipeline with 2 processes SortFile ran first, created sorted.txt AddLineNumbers ran second, used sorted.txt as input, created numbered.txt Progress bar showed completion of both processes Final output saved to MyFirstPipeline-output/ directory Step 5: Inspect the Results \u00b6 Check your output: cat MyFirstPipeline-output/AddLineNumbers/numbered.txt Expected output: 1 1 2 2 3 3 You can also check the intermediate output: cat .pipen/MyFirstPipeline/SortFile/0/output/sorted.txt Expected output: 1 2 3 Understanding Channels and Jobs \u00b6 Let's modify our pipeline to process multiple files at once. Update my_pipeline.py : from pipen import Proc , run class SortFile ( Proc ): \"\"\"Sort the contents of a file.\"\"\" input = \"infile\" # Multiple input files input_data = [ \"/tmp/data1.txt\" , \"/tmp/data2.txt\" , \"/tmp/data3.txt\" ] output = \"outfile:file:sorted.txt\" script = \"cat {{ in.infile }} | sort > {{ out.outfile }}\" class AddLineNumbers ( Proc ): \"\"\"Add line numbers to each line.\"\"\" requires = SortFile input = \"infile:file\" output = \"outfile:file:numbered.txt\" script = \"paste <(seq 1 3) {{ in.infile }} > {{ out.outfile }}\" if __name__ == \"__main__\" : # Create test data import os os . makedirs ( \"/tmp\" , exist_ok = True ) for i in range ( 1 , 4 ): with open ( f \"/tmp/data { i } .txt\" , \"w\" ) as f : f . write ( f \" { 4 - i } \\\\ n { i - 1 } \\\\ n { i - 2 } \" ) run ( \"MyFirstPipeline\" , starts = SortFile ) Now run it again: python my_pipeline.py You'll see 3 jobs run for SortFile (one for each input file), and then 3 jobs run for AddLineNumbers. Key concept: Each row in the input channel becomes a separate job that runs independently. Step 6: Understanding Caching \u00b6 pipen automatically caches results to avoid recomputing. Run the pipeline again without changing anything: python my_pipeline.py You'll see: SortFile: >>> ['AddLineNumbers'] SortFile: >>> [END] SortFile: CACHED <--- Notice \"CACHED\" instead of running The jobs don't run again because the output already exists and inputs haven't changed. To force re-running: Delete the workdir: rm -rf .pipen/MyFirstPipeline python my_pipeline.py Now all jobs will run again. Step 7: More Complex Input Data \u00b6 Let's use structured input data with multiple columns. Create structured_pipeline.py : from pipen import Proc , run , Channel class ProcessSamples ( Proc ): \"\"\"Process sample files with metadata.\"\"\" # Channel input with multiple columns input_data = Channel . create ([ ( \"sample1.txt\" , \"control\" , \"rep1\" ), ( \"sample2.txt\" , \"control\" , \"rep2\" ), ( \"sample3.txt\" , \"treatment\" , \"rep1\" ), ( \"sample4.txt\" , \"treatment\" , \"rep2\" ), ]) input = [ \"filename:file\" , \"condition:var\" , \"replicate:var\" , ] output = \"result:file:result.txt\" script = \"\"\" echo \"{{ in.condition }} - {{ in.filename }} ({{ in.replicate }})\" > {{ out.result }} \"\"\" if __name__ == \"__main__\" : # Create test files import os os . makedirs ( \"/tmp\" , exist_ok = True ) for filename , _ , _ in [ ( \"sample1.txt\" , \"control\" , \"rep1\" ), ( \"sample2.txt\" , \"control\" , \"rep2\" ), ( \"sample3.txt\" , \"treatment\" , \"rep1\" ), ( \"sample4.txt\" , \"treatment\" , \"rep2\" ), ]: with open ( f \"/tmp/ { filename } \" , \"w\" ) as f : f . write ( \"sample data\" ) run ( \"StructuredPipeline\" , starts = ProcessSamples ) Explanation: Channel.create([...]) : Creates a channel where each tuple is a row Input has 3 columns: filename , condition , replicate The script can access all three columns via template variables Creates 4 jobs, one for each input tuple Common Patterns \u00b6 Pattern 1: Multiple Files from Directory \u00b6 from pipen import Proc , run , Channel class ProcessFiles ( Proc ): \"\"\"Process all files in a directory.\"\"\" # Use glob to find all .txt files input_data = Channel . from_glob ( \"/tmp/*.txt\" , sortby = \"name\" ) input = \"infile:file\" output = \"outfile:file:processed.txt\" script = \"cat {{ in.infile }} > {{ out.outfile }}\" run ( \"GlobPipeline\" , starts = ProcessFiles ) Pattern 2: Conditional Execution \u00b6 from pipen import Proc , run class ConditionalProcess ( Proc ): \"\"\"Process files based on condition.\"\"\" input = [ \"infile:file\" , \"process:var\" , # 'skip' or 'process' ] output = \"outfile:file:output.txt\" script = \"\"\" if [ \"{{ in.process }}\" == \"process\" ]; then cat {{ in.infile }} > {{ out.outfile }} else echo \"Skipped\" fi \"\"\" Pattern 3: Error Handling \u00b6 from pipen import Proc , run class SafeProcess ( Proc ): \"\"\"Process with error handling.\"\"\" input = \"data:var\" output = \"result:file:result.txt\" script = \"\"\" # Exit with error code 1 if data is empty if [ -z \"{{ in.data }}\" ]; then echo \"Error: Empty data\" exit 1 fi echo \"Processing {{ in.data }}\" > {{ out.result }} \"\"\" run ( \"SafePipeline\" , starts = SafeProcess , error_strategy = \"retry\" ) Next Steps \u00b6 Congratulations! You've created your first pipeline. Now you can: Read Basics for more on pipeline structure Learn about Channels for data flow Explore Configurations to customize your pipeline Check the Examples for more complex patterns Read the API Reference for detailed documentation Troubleshooting \u00b6 Problem: Pipeline doesn't run Solution: Check that: - Python 3.9+ is installed: python --version - pipen is installed: pip show pipen - Input files exist: ls /tmp/data.txt Problem: Jobs fail with \"command not found\" Solution: Make sure: - Script uses correct bash syntax - Commands available in your PATH - Template variables are properly formatted: {{ in.varname }} Problem: Output files not created Solution: - Check script writes to output path: {{ out.output_var }} - Verify output directory is writable - Check job logs in .pipen/<pipeline>/<process>/*/job.log For more help, see the Troubleshooting Guide .","title":"Beginner Tutorial"},{"location":"tutorials/beginner/#your-first-pipeline","text":"Welcome to pipen! This tutorial will walk you through creating your first pipeline step by step step.","title":"Your First Pipeline"},{"location":"tutorials/beginner/#what-youll-learn","text":"In this tutorial, you will: - Understand the basic components of a pipen pipeline - Create a simple two-process pipeline - Run the pipeline and inspect the results - Learn about channels, jobs, and caching","title":"What You'll Learn"},{"location":"tutorials/beginner/#prerequisites","text":"Before starting, ensure you have: - Python 3.9 or higher installed - pipen installed: pip install -U pipen","title":"Prerequisites"},{"location":"tutorials/beginner/#understanding-the-basics","text":"","title":"Understanding the Basics"},{"location":"tutorials/beginner/#what-is-a-pipeline","text":"A pipeline is a workflow that processes data through multiple steps. Each step (called a process ) takes input data, performs some operation, and produces output data.","title":"What is a Pipeline?"},{"location":"tutorials/beginner/#what-is-a-process","text":"A process is a single step in your pipeline that: - Defines what input it needs - Specifies what output it produces - Contains the script to execute - Can depend on other processes","title":"What is a Process?"},{"location":"tutorials/beginner/#what-is-a-job","text":"When a pipeline runs, each row of input data becomes a job . If your input has 100 items, pipen creates 100 jobs that run independently.","title":"What is a Job?"},{"location":"tutorials/beginner/#what-is-a-channel","text":"A channel is how data flows between processes. It's like a spreadsheet where each row represents a job and each column represents a piece of data that job needs.","title":"What is a Channel?"},{"location":"tutorials/beginner/#step-1-create-your-first-process","text":"Let's create a simple process that sorts lines in a file. Create a file called my_pipeline.py : from pipen import Proc class SortFile ( Proc ): \"\"\"Sort the contents of a file.\"\"\" # Define input: expects a file path input = \"infile:file\" # Provide sample data for testing input_data = \"/tmp/data.txt\" # Define output: a file called \"sorted.txt\" output = \"outfile:file:sorted.txt\" # The script to execute # {{ in.infile }} will be replaced with the input file path # {{ out.outfile }} will be replaced with the output file path script = \"\"\" cat {{ in.infile }} | sort > {{ out.outfile }} \"\"\" Explanation: class SortFile(Proc) : Define a process by inheriting from Proc input = \"infile\" : This process expects one piece of input data named \"infile\" input_data = \"/tmp/data.txt\" : Sample input data (a file path) output = \"outfile:file:sorted.txt\" : Output is a file named \"sorted.txt\" script = \"\"\"...\"\"\" : The bash script to execute with template variables","title":"Step 1: Create Your First Process"},{"location":"tutorials/beginner/#step-2-create-a-second-process","text":"Now let's add a second process that adds line numbers to our sorted file. Add to my_pipeline.py : class AddLineNumbers ( Proc ): \"\"\"Add line numbers to each line.\"\"\" # This process depends on SortFile requires = SortFile # Input is a file (output from SortFile) input = \"infile:file\" # Output is a file called \"numbered.txt\" output = \"outfile:file:numbered.txt\" # Script adds line numbers (1-3 in this case) script = \"\"\" paste <(seq 1 3) {{ in.infile }} > {{ out.outfile }} \"\"\" Explanation: requires = SortFile : This process runs after SortFile completes input = \"infile:file\" : The :file type tells pipen this is a file path The output from SortFile automatically becomes the input to AddLineNumbers","title":"Step 2: Create a Second Process"},{"location":"tutorials/beginner/#step-3-define-and-run-the-pipeline","text":"Now let's put it all together and run our pipeline. Update my_pipeline.py : from pipen import Proc , run class SortFile ( Proc ): \"\"\"Sort the contents of a file.\"\"\" input = \"infile\" input_data = \"/tmp/data.txt\" output = \"outfile:file:sorted.txt\" script = \"cat {{ in.infile }} | sort > {{ out.outfile }}\" class AddLineNumbers ( Proc ): \"\"\"Add line numbers to each line.\"\"\" requires = SortFile input = \"infile:file\" output = \"outfile:file:numbered.txt\" script = \"paste <(seq 1 3) {{ in.infile }} > {{ out.outfile }}\" # Run the pipeline if __name__ == \"__main__\" : # Create test data import os os . makedirs ( \"/tmp\" , exist_ok = True ) with open ( \"/tmp/data.txt\" , \"w\" ) as f : f . write ( \"3 \\\\ n2 \\\\ n1\" ) run ( \"MyFirstPipeline\" , starts = SortFile )","title":"Step 3: Define and Run the Pipeline"},{"location":"tutorials/beginner/#step-4-run-your-pipeline","text":"Execute your pipeline: python my_pipeline.py You should see output similar to: 04-17 16:19:35 I core _____________________________________ __ 04-17 16:19:35 I core ___ __ \\___ ____ \\___ | | | | 04-17 16:19:35 I core |\\/ | |\\/ | | | | | | 04-17 16:19:35 I core 04-17 16:19:35 I core version: 1.1.7 04-17 16:19:35 I core 04-17 16:19:35 I core \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 MYFIRSTPIPELINE \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 04-17 16:19:35 I core \u2551 My first pipeline \u2551 04-17 16:19:35 I core \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d 04-17 16:19:35 I core # procs : 2 04-17 16:19:35 I core profile : default 04-17 16:19:35 I core outdir : /path/to/cwd/MyFirstPipeline-output 04-17 16:19:35 I core cache : True 04-17 16:19:35 I core dirsig : 1 04-17 16:19:35 I core error_strategy : ignore 04-17 16:19:35 I core forks : 1 04-17 16:19:35 I core lang : bash 04-17 16:19:35 I core loglevel : info 04-17 16:19:35 I core num_retries : 3 04-17 16:19:35 I core scheduler : local 04-17 16:19:35 I core submission_batch: 8 04-17 16:19:35 I core template : liquid 04-17 16:19:35 I core workdir : /path/to/cwd/.pipen/MyFirstPipeline 04-17 16:19:35 I core Initializing plugins ... 04-17 16:19:36 I core 04-17 16:19:36 I core \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 SortFile \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e 04-17 16:19:36 I core \u2502 Sort the contents of a file. \u2502 04-17 16:19:36 I core \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f 04-17 16:19:36 I core SortFile: Workdir: '/path/to/cwd/.pipen/MyFirstPipeline/SortFile' 04-17 16:19:36 I core SortFile: <<< [START] 04-17 16:19:36 I core SortFile: >>> ['AddLineNumbers'] 04-17 16:19:36 I core SortFile: >>> [END] 04-17 16:19:36 I core 04-17 16:19:36 I core \u256d\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550 AddLineNumbers \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256e 04-17 16:19:36 I core \u2551 Add line numbers to each line. \u2551 04-17 16:19:36 I core \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d 04-17 16:19:36 I core AddLineNumbers: Workdir: '/path/to/cwd/.pipen/MyFirstPipeline/AddLineNumbers' 04-17 16:19:36 I core AddLineNumbers: <<< ['SortFile'] 04-17 16:19:36 I core AddLineNumbers: >>> [END] MYFIRSTPIPELINE: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 0.35 procs/s] What happened: Pipen initialized the pipeline with 2 processes SortFile ran first, created sorted.txt AddLineNumbers ran second, used sorted.txt as input, created numbered.txt Progress bar showed completion of both processes Final output saved to MyFirstPipeline-output/ directory","title":"Step 4: Run Your Pipeline"},{"location":"tutorials/beginner/#step-5-inspect-the-results","text":"Check your output: cat MyFirstPipeline-output/AddLineNumbers/numbered.txt Expected output: 1 1 2 2 3 3 You can also check the intermediate output: cat .pipen/MyFirstPipeline/SortFile/0/output/sorted.txt Expected output: 1 2 3","title":"Step 5: Inspect the Results"},{"location":"tutorials/beginner/#understanding-channels-and-jobs","text":"Let's modify our pipeline to process multiple files at once. Update my_pipeline.py : from pipen import Proc , run class SortFile ( Proc ): \"\"\"Sort the contents of a file.\"\"\" input = \"infile\" # Multiple input files input_data = [ \"/tmp/data1.txt\" , \"/tmp/data2.txt\" , \"/tmp/data3.txt\" ] output = \"outfile:file:sorted.txt\" script = \"cat {{ in.infile }} | sort > {{ out.outfile }}\" class AddLineNumbers ( Proc ): \"\"\"Add line numbers to each line.\"\"\" requires = SortFile input = \"infile:file\" output = \"outfile:file:numbered.txt\" script = \"paste <(seq 1 3) {{ in.infile }} > {{ out.outfile }}\" if __name__ == \"__main__\" : # Create test data import os os . makedirs ( \"/tmp\" , exist_ok = True ) for i in range ( 1 , 4 ): with open ( f \"/tmp/data { i } .txt\" , \"w\" ) as f : f . write ( f \" { 4 - i } \\\\ n { i - 1 } \\\\ n { i - 2 } \" ) run ( \"MyFirstPipeline\" , starts = SortFile ) Now run it again: python my_pipeline.py You'll see 3 jobs run for SortFile (one for each input file), and then 3 jobs run for AddLineNumbers. Key concept: Each row in the input channel becomes a separate job that runs independently.","title":"Understanding Channels and Jobs"},{"location":"tutorials/beginner/#step-6-understanding-caching","text":"pipen automatically caches results to avoid recomputing. Run the pipeline again without changing anything: python my_pipeline.py You'll see: SortFile: >>> ['AddLineNumbers'] SortFile: >>> [END] SortFile: CACHED <--- Notice \"CACHED\" instead of running The jobs don't run again because the output already exists and inputs haven't changed. To force re-running: Delete the workdir: rm -rf .pipen/MyFirstPipeline python my_pipeline.py Now all jobs will run again.","title":"Step 6: Understanding Caching"},{"location":"tutorials/beginner/#step-7-more-complex-input-data","text":"Let's use structured input data with multiple columns. Create structured_pipeline.py : from pipen import Proc , run , Channel class ProcessSamples ( Proc ): \"\"\"Process sample files with metadata.\"\"\" # Channel input with multiple columns input_data = Channel . create ([ ( \"sample1.txt\" , \"control\" , \"rep1\" ), ( \"sample2.txt\" , \"control\" , \"rep2\" ), ( \"sample3.txt\" , \"treatment\" , \"rep1\" ), ( \"sample4.txt\" , \"treatment\" , \"rep2\" ), ]) input = [ \"filename:file\" , \"condition:var\" , \"replicate:var\" , ] output = \"result:file:result.txt\" script = \"\"\" echo \"{{ in.condition }} - {{ in.filename }} ({{ in.replicate }})\" > {{ out.result }} \"\"\" if __name__ == \"__main__\" : # Create test files import os os . makedirs ( \"/tmp\" , exist_ok = True ) for filename , _ , _ in [ ( \"sample1.txt\" , \"control\" , \"rep1\" ), ( \"sample2.txt\" , \"control\" , \"rep2\" ), ( \"sample3.txt\" , \"treatment\" , \"rep1\" ), ( \"sample4.txt\" , \"treatment\" , \"rep2\" ), ]: with open ( f \"/tmp/ { filename } \" , \"w\" ) as f : f . write ( \"sample data\" ) run ( \"StructuredPipeline\" , starts = ProcessSamples ) Explanation: Channel.create([...]) : Creates a channel where each tuple is a row Input has 3 columns: filename , condition , replicate The script can access all three columns via template variables Creates 4 jobs, one for each input tuple","title":"Step 7: More Complex Input Data"},{"location":"tutorials/beginner/#common-patterns","text":"","title":"Common Patterns"},{"location":"tutorials/beginner/#pattern-1-multiple-files-from-directory","text":"from pipen import Proc , run , Channel class ProcessFiles ( Proc ): \"\"\"Process all files in a directory.\"\"\" # Use glob to find all .txt files input_data = Channel . from_glob ( \"/tmp/*.txt\" , sortby = \"name\" ) input = \"infile:file\" output = \"outfile:file:processed.txt\" script = \"cat {{ in.infile }} > {{ out.outfile }}\" run ( \"GlobPipeline\" , starts = ProcessFiles )","title":"Pattern 1: Multiple Files from Directory"},{"location":"tutorials/beginner/#pattern-2-conditional-execution","text":"from pipen import Proc , run class ConditionalProcess ( Proc ): \"\"\"Process files based on condition.\"\"\" input = [ \"infile:file\" , \"process:var\" , # 'skip' or 'process' ] output = \"outfile:file:output.txt\" script = \"\"\" if [ \"{{ in.process }}\" == \"process\" ]; then cat {{ in.infile }} > {{ out.outfile }} else echo \"Skipped\" fi \"\"\"","title":"Pattern 2: Conditional Execution"},{"location":"tutorials/beginner/#pattern-3-error-handling","text":"from pipen import Proc , run class SafeProcess ( Proc ): \"\"\"Process with error handling.\"\"\" input = \"data:var\" output = \"result:file:result.txt\" script = \"\"\" # Exit with error code 1 if data is empty if [ -z \"{{ in.data }}\" ]; then echo \"Error: Empty data\" exit 1 fi echo \"Processing {{ in.data }}\" > {{ out.result }} \"\"\" run ( \"SafePipeline\" , starts = SafeProcess , error_strategy = \"retry\" )","title":"Pattern 3: Error Handling"},{"location":"tutorials/beginner/#next-steps","text":"Congratulations! You've created your first pipeline. Now you can: Read Basics for more on pipeline structure Learn about Channels for data flow Explore Configurations to customize your pipeline Check the Examples for more complex patterns Read the API Reference for detailed documentation","title":"Next Steps"},{"location":"tutorials/beginner/#troubleshooting","text":"Problem: Pipeline doesn't run Solution: Check that: - Python 3.9+ is installed: python --version - pipen is installed: pip show pipen - Input files exist: ls /tmp/data.txt Problem: Jobs fail with \"command not found\" Solution: Make sure: - Script uses correct bash syntax - Commands available in your PATH - Template variables are properly formatted: {{ in.varname }} Problem: Output files not created Solution: - Check script writes to output path: {{ out.output_var }} - Verify output directory is writable - Check job logs in .pipen/<pipeline>/<process>/*/job.log For more help, see the Troubleshooting Guide .","title":"Troubleshooting"}]}